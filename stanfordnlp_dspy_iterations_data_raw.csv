,Type,Related url,planned/unplanned,Cause,Body,Start Time,End Time,Duration,Context
1,Knowledge Integration,https://github.com/stanfordnlp/dspy/pull/756,unplanned,Unknown,Updated docs to MIPRO and COPRO SignatureOptimizer names are deprecated,2024-04-02T18:35:15Z,2024-04-02T19:54:02Z,0 days 01:18:47,update docs mipro copro signatureoptimizer name deprecate
3,Knowledge Integration,https://github.com/stanfordnlp/dspy/pull/753,unplanned,Unknown,"Increase chances that VSCode dev container will use Poetry venv ## üìù Changes Description

This MR/PR contains the following changes:
* Builds the dev container Poetry venv within the workspace
* Includes VSCode setting to choose the poetry Python interpreter by default
* Includes additional warning about interpreter at the end of the dev container build to provide instructions on how to choose the correct interpreter if it's not already selected
* Updates contributing markdown file with these additional instructions

We got reports that people were not able to run the unit tests after building the dev container. After further investigation, I found that this is not due to the dev container itself but instead in the Python interpreter selected by the user. If the user has the OS default Python interpreter selected, that interpreter will not have access to the packages installed by Poetry. Because poetry creates it's own venv, we need to use the Poetry interpreter rather than any of the other interpreters installed in the dev container. By default, the dev container should find and use this Poetry interpreter but if a user has previously selected a different interpreter for the workspace, it will not attempt to find the Poetry interpreter. 

To resolve this, we're adding additional instructions in the logs of the dev container build as well as in the `contributing.md` file. Additionally, the Poetry venv will now be built within the VSCode workspace rather than outside of it. Because it's inside the workspace, we're now able to add the Poetry interpreter as the default interpreter in the VSCode settings. If a user has selected a different interpreter, this setting will not override that selection but it will make it easier to select the correct interpreter as this makes the Poetry interpreter not only the recommended interpreter but also the default. The instructions to ensure developers are using the correct interpreter should now be hard to miss and the issue should be less likely to occur in the first place. Unfortunately, there's no way to force VSCode to use the Poetry interpreter. The best we can do is set it explicitly as the default.

## ‚úÖ Contributor Checklist

- [] Pre-Commit checks are passing (locally and remotely)
- [] Title of your PR / MR corresponds to the required format
- [] Commit message follows required format {label}(dspy): {message}

## ‚ö†Ô∏è Warnings

Anything we should be aware of ?
",2024-04-02T02:32:01Z,2024-04-02T18:14:58Z,0 days 15:42:57,increase chance vscode dev container use poetry venv change description mrpr contains follow change build dev container poetry venv within workspace include vscode set choose poetry python interpreter default include additional warning interpreter end dev container build provide instruction choose correct interpreter already select update contribute markdown file additional instruction get report people able run unit test build dev container investigation find due dev container instead python interpreter select user user o default python interpreter select interpreter access package instal poetry poetry create venv need use poetry interpreter rather interpreter instal dev container default dev container find use poetry interpreter user previously select different interpreter workspace attempt find poetry interpreter resolve add additional instruction log dev container build well contributingmd file additionally poetry venv build within vscode workspace rather outside inside workspace able add poetry interpreter default interpreter vscode setting user select different interpreter set override selection make easy select correct interpreter make poetry interpreter recommend interpreter also default instruction ensure developer use correct interpreter hard miss issue less likely occur first place unfortunately theres way force vscode use poetry interpreter best set explicitly default contributor checklist precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message warning anything aware
4,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/751,unplanned,Unknown,"Added support for models on AWS Bedrock and Sagemaker This check-in replaces the old Bedrock module which was incomplete and obsolete. The new code refactors the module into aws_providers and aws_models and allows extending to new models.
Supported aws providers - Bedrock and Sagemaker
Supported models - Mistral, Anthropic, Llama (extensible design to add custom models)

```
""""""Test harness: Testing the AWS modules""""""
import dspy
from dspy import AWSAnthropic, AWSLlama2, AWSMistral, AWSModel, Bedrock, Sagemaker


class QASignature(dspy.Signature):
    """"""answer the question""""""

    question = dspy.InputField()
    answer = dspy.OutputField()


def test_aws_models(lm: AWSModel):
    """"""Test the models on the given AWS provider""""""

    predict_func = dspy.ChainOfThought(QASignature)
    with dspy.context(lm=lm):
        answer = predict_func(question=""What is the capital of France"")
        assert ""paris"" in str(answer).lower()
        print(answer)


if __name__ == ""__main__"":
    # NOTE: Configure your AWS credentials with the AWS CLI before running this test!

    bedrock = Bedrock(region_name=""us-west-2"")
    test_aws_models(AWSMistral(bedrock, model=""mistral.mixtral-8x7b-instruct-v0:1""))
    test_aws_models(AWSMistral(bedrock, ""mistral.mistral-7b-instruct-v0:2""))
    test_aws_models(AWSAnthropic(bedrock, ""anthropic.claude-3-haiku-20240307-v1:0""))
    test_aws_models(AWSAnthropic(bedrock, ""anthropic.claude-3-sonnet-20240229-v1:0""))
    # this is slower than molasses and generates irrelevant content after the answer!!
    # You may have to wait for 10-15min before it returns
    test_aws_models(AWSLlama2(bedrock, ""meta.llama2-70b-chat-v1""))

    # NOTE: Configure your Sagemaker endpoints before running this test!
    sagemaker = Sagemaker(region_name=""us-west-2"")
    # NOTE: Replace model value below with your own endpoint name
    test_aws_models(AWSMistral(sagemaker, model=""g5-48xlarge-mixtral-8x7b""))
```",2024-04-01T17:03:49Z,2024-04-01T21:01:56Z,0 days 03:58:07,added support model aws bedrock sagemaker checkin replaces old bedrock module incomplete obsolete new code refactors module awsproviders awsmodels allow extend new model support aws provider bedrock sagemaker support model mistral anthropic llama extensible design add custom model test harness test aws module import dspy dspy import awsanthropic awsmistral awsmodel bedrock sagemaker class qasignaturedspysignature answer question question dspyinputfield answer dspyoutputfield def testawsmodelslm awsmodel test model give aws provider predictfunc dspychainofthoughtqasignature dspycontextlmlm answer predictfuncquestionwhat capital france assert paris stranswerlower printanswer name main note configure aws credential aws cli run test bedrock testawsmodelsawsmistralbedrock testawsmodelsawsmistralbedrock testawsmodelsawsanthropicbedrock testawsmodelsawsanthropicbedrock slow molasses generate irrelevant content answer may wait return note configure sagemaker endpoint run test sagemaker note replace model value endpoint name testawsmodelsawsmistralsagemaker
5,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/743,unplanned,Unknown,Fixed missing copy in TypedPredictor None,2024-03-30T20:58:22Z,2024-03-30T20:58:29Z,0 days 00:00:07,fix miss copy typedpredictor none
6,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/742,unplanned,Unknown,"docs: examples for saving and loading modules after optimizing ## üìù Added information about saving and loading modules in the documentation

This MR/PR contains the following changes:
* A new section in the Optimizers Building Blocks page about saving and loading an optimized program
* A new section in the Cheat Sheet about saving and loading modules

",2024-03-30T20:18:05Z,2024-03-30T21:52:01Z,0 days 01:33:56,doc example save load module optimize added information save loading module documentation mrpr contain follow change new section optimizers build block page save load optimize program new section cheat sheet save loading module
7,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/741,unplanned,Unknown,docs: add an explain blog post to other resources None,2024-03-30T06:17:06Z,2024-03-30T06:20:48Z,0 days 00:03:42,doc add explain blog post resource none
9,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/731,unplanned,Unknown,"Fix OpenAIVectorizer Fixes #728 

-[ ] Squash commits, obey commit message guidelines

Currently the use of OpenAIVectorizer fails with recent OpenAI API version(s): 

Versions: 
`openai==1.13.3`
`dspy-ai==2.4.0` (same relevant code on main)

Error: 
```
cur_batch_embeddings = [cur_obj['embedding'] for cur_obj in response['data']]
                                                            ~~~~~~~~^^^^^^^^
TypeError: 'CreateEmbeddingResponse' object is not subscriptable
```

A minimal failing example is given in the tests. The tests commit shows the failure before the fix. **The tests require an API key and are therefore skipped if one isn't set.**

Fix currently tested with Python: (3.11, 3.10) Openai (1.14.3, 1.13.3) on Mac:
```
platform darwin -- Python 3.10.13, pytest-6.2.5, py-1.11.0, pluggy-1.4.0
openai==1.14.3
```

Still need to determine the low end openai version this breaks. 
",2024-03-28T18:24:16Z,2024-03-28T18:34:24Z,0 days 00:10:08,fix openaivectorizer fix squash commits obey commit message guideline currently use openaivectorizer fails recent openai api version version relevant code main error curbatchembeddings curobj response typeerror createembeddingresponse object subscriptable minimal fail example give test test commit show failure fix test require api key therefore skip one isnt set fix currently test python openai mac platform darwin python still need determine low end openai version break
10,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/727,unplanned,Unknown,Fix typo in minimal-example.mdx None,2024-03-28T06:59:31Z,2024-03-28T07:03:00Z,0 days 00:03:29,fix typo minimalexamplemdx none
11,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/725,unplanned,Unknown,docs: fix cheatsheet no RM loaded error in ReAct None,2024-03-27T18:52:35Z,2024-03-30T06:15:31Z,2 days 11:22:56,docs fix cheatsheet rm load error react none
12,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/721,unplanned,Unknown,"Added kwargs parameter for retrievers and text to SQL pipeline using DSPy All the retrievers support metadata filtering arguments, which can be passed during `dspy.Retrieve(k=num_passages)` as `**kwargs`. This PR add just the `**kwargs` parameter to retriever which can be passed to the retriever client and we can sample passages based on metadata. For example in ChromaDB, we have

```
retrieve = dspy.Retrieve(k=3)
retrieve(""What is valuation"",where={'book_source': 'Investment_Valuations_techniques'})
```
We will only retrieve documents for the given book source and send to reranker or model for next stages in the pipeline",2024-03-26T07:21:24Z,2024-04-03T01:24:34Z,7 days 18:03:10,add kwargs parameter retriever text sql pipeline use dspy retriever support metadata filter argument pass dspyretrieveknumpassages kwargs pr add kwargs parameter retriever pass retriever client sample passage base metadata example chromadb retrieve retrievewhat valuationwherebooksource investmentvaluationstechniques retrieve document give book source send reranker model next stage pipeline
13,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/719,unplanned,Unknown,"fix(dsp): in `HFClientVLLM`, actually use `kwargs` in the payload instead of discarding them in `dsp.modules.HFClientVLLM` class the `kwargs` are supposed to be passed to the vllm openai endpoint (cf. the docs) but are discarded instead. This is related to the issue here : https://github.com/stanfordnlp/dspy/issues/718

In this PR, there are some modifications to the `dsp.modules.HFClientVLLM` class:

1.  In the `__init__` the `kwargs` were actually unused. In this PR those are attached to `self.` instead of being discarded.
2. The model of the payload was provided by the `kwargs` of `_generate` method, disregarding the value initialized in the `__init__`. This is fixed in this PR.
3. Two optional parameters were made mandatory (`temperature` and `max_tokens`) in the `_generate` method. Those were made mandatory because it was a direct `kwargs[""<parameter>""]`, which would raise `KeyError`. In this PR, those two dictionary access have been deleted.
4. Finally, still in the `_generate` method, the kwargs were never actually passed to the vllm endpoint. In this PR we fix this by unpacking the `kwargs` into the payload so that the extra arguments.",2024-03-25T23:28:16Z,2024-04-01T21:57:25Z,6 days 22:29:09,fixdsp hfclientvllm actually use kwargs payload instead discard dspmoduleshfclientvllm class kwargs suppose pass vllm openai endpoint cf doc discard instead related issue pr modification dspmoduleshfclientvllm class init kwargs actually unused pr attach self instead discard model payload provide kwargs generate method disregard value initialize init fix pr two optional parameter make mandatory temperature maxtokens generate method make mandatory direct kwargs would raise keyerror pr two dictionary access delete finally still generate method kwargs never actually pass vllm endpoint pr fix unpack kwargs payload extra argument
15,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/716,unplanned,Unknown,"Update Cohere LM to use  the `/chat` API and add support for Command-R Cohere has deprecated the /generate API in favor of /chat. More info can be found here: https://docs.cohere.com/reference/chat.

Implementation follows the example of `dsp/modules/gpt3.py`.

[ToDo in the future, not in this PR]
- Would be interesting to look into the `connectors` and `documents` APIs.
- Also would be good to add the same logging in OpenAI to Cohere, `log_usage`",2024-03-25T21:48:34Z,2024-03-27T14:23:18Z,1 days 16:34:44,update cohere lm use chat api add support commandr cohere deprecate generate api favor chat info find implementation follow example would interest look connector document apis also would good add log openai cohere logusage
17,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/714,unplanned,Unknown,docs: missing links in the doc None,2024-03-25T21:00:50Z,2024-03-26T16:22:27Z,0 days 19:21:37,doc miss link doc none
18,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/713,unplanned,Unknown,docs: fix the order of the output section None,2024-03-25T17:00:45Z,2024-03-25T17:09:55Z,0 days 00:09:10,doc fix order output section none
19,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/712,unplanned,Unknown,"fix(dsp/modules/gpt3): fix system_prompt overwrite, as discussed in PR #618 comments The system_prompt is currently overwritten when setting kwargs[‚Äúmessages‚Äù]. This is fixed by setting kwargs[‚Äúmessages‚Äù] = messages, where messages contains the system prompt if self.system_prompt is given.
Full credit to @Markovian99 for spotting this bug in PR #618 comments.",2024-03-25T14:34:33Z,2024-04-01T18:03:55Z,7 days 03:29:22,fix systemprompt overwrite discuss pr comment systemprompt currently overwritten set kwargs fix set kwargs message message contains system prompt selfsystemprompt give full credit spot bug pr comment
20,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/711,unplanned,Unknown,docs(api/retrieval_model_clients/ChromadbRM): fix typo in import statement fixing a minor typo by replacing chroma_rm with chromadb_rm in demo import statement,2024-03-25T13:42:16Z,2024-03-25T13:45:12Z,0 days 00:02:56,docsapiretrievalmodelclientschromadbrm fix typo import statement fix minor typo replace chromarm chromadbrm demo import statement
21,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/710,unplanned,Unknown,"feat(ProgramofThought): added support for custom whitelists and dynamic input and output field naming (continuation of #665) While, the PythonInterpreter and CodePrompt modules allowed for importing a white_list, this functionality hadn't been made available directly inside ProgramofThought. This was causing repeated hinderance when language models would try to use any math packages that were not allowed like sympy etc, which makes for a bad dev UX.
This also updates the ProgramOfThought class to support dynamic input and output field names, improving user experience and flexibility. The changes include:
Modifying the __init__ method to store input and output fields from the provided signature
Updating the generation of input and output field names in the instruction strings
Refactoring the _generate_signature and _generate_instruction methods to handle dynamic field names
Modifying the forward method to use dynamic input field names instead of hardcoded names
These changes allow users to define their own input and output field names in the signature, making the ProgramOfThought class more adaptable to different use cases.",2024-03-25T07:08:55Z,2024-03-25T19:53:00Z,0 days 12:44:05,featprogramofthought add support custom whitelists dynamic input output field name continuation pythoninterpreter codeprompt module allow import whitelist functionality hadnt make available directly inside programofthought cause repeat hinderance language model would try use math package allow like sympy etc make bad dev ux also update programofthought class support dynamic input output field names improve user experience flexibility change include modify init method store input output field provide signature update generation input output field name instruction string refactoring generatesignature generateinstruction method handle dynamic field name modify forward method use dynamic input field name instead hardcoded name change allow user define input output field name signature make programofthought class adaptable different use case
22,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/709,unplanned,Unknown,"[feat] add from_parquet to dataloader A parquet file loader would be a convenient addition to the dataloader class. I particularly like that parquet files preserve types better than a csv file.

Here's the change:

```python
def from_parquet(self, file_path: str, fields: list[str] = None, input_keys: tuple[str] = ()) -> list[dspy.Example]:
        dataset = load_dataset(""parquet"", data_files=file_path)[""train""]

        if not fields:
            fields = list(dataset.features)

        return [dspy.Example({field: row[field] for field in fields}).with_inputs(input_keys) for row in dataset]
```

Note, this PR was originally made in #483, but something with git hooks prevented test from being able to run.",2024-03-25T00:42:22Z,2024-03-25T20:49:05Z,0 days 20:06:43,add fromparquet dataloader parquet file loader would convenient addition dataloader class particularly like parquet file preserve type well csv file here change python def fromparquetself filepath str field list none inputkeys tuple list dataset loaddatasetparquet datafilesfilepath field field listdatasetfeatures return field fieldswithinputsinputkeys row dataset note pr originally make something git hook prevent test able run
23,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/707,unplanned,Unknown,"fix for weaviate outdated client version. Issue 699 I upgraded the client version in the poetry project configuration. I changed the contract of the init method to pass a few additional parameters important for configuring the hybrid query. 

I did not commit the poetry.lock file, as this is giving regular merge problems. I hope that is done before creating a release.",2024-03-24T10:07:55Z,2024-03-25T20:49:34Z,1 days 10:41:39,fix weaviate outdated client version issue upgrade client version poetry project configuration change contract init method pass additional parameter important configure hybrid query commit poetrylock file give regular merge problem hope do create release
24,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/705,unplanned,Unknown,"Added Weaviate installation guide Minor change, adding Weaviate pip install guide (it was missing).",2024-03-23T22:52:49Z,2024-03-23T23:21:22Z,0 days 00:28:33,added weaviate installation guide minor change add weaviate pip install guide miss
25,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/704,unplanned,Unknown,"Add dev container support for project Currently, there's a contributor.md file that looks to be out of date. I think it's confusing for new contributors to join and meaningfully contribute to the project because getting set up is somewhat complicated. This introduces a VSCode dev container solution for all developers to share a dev environment including proper linting, requirements, and extensions. I think this will make it easier for additional contributors to join without friction and improve the project. Let me know if there are any other customizations you'd want developers to be set up with for this project. Because this repo is on github, this will also allows anyone to develop from the web browser using Github codespaces and still have a proper dev environment set up.",2024-03-23T22:47:29Z,2024-03-28T16:34:32Z,4 days 17:47:03,add dev container support project currently theres contributormd file look date think confuse new contributor join meaningfully contribute project get set somewhat complicated introduces vscode dev container solution developer share dev environment include proper linting requirement extension think make easy additional contributor join without friction improve project let know customizations youd want developer set project repo github also allow anyone develop web browser use github codespaces still proper dev environment set
26,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/701,unplanned,Unknown,"feat: Add support for additional Anthropic models This commit updates the `Claude` class to support additional models provided by the Anthropic API:

- claude-2.0
- claude-2.1
- claude-3-opus-20240229
- claude-3-haiku-20240307
- claude-3-sonnet-20240229

The `__init__` method of the `Claude` class has been modified to include a list of supported models. When creating an instance of the `Claude` class, the `model` parameter can be set to any of the supported model names. If an unsupported model is provided, a `ValueError` is raised with a message indicating the supported models.

This change allows users to utilize the newly added models for generating completions using the Anthropic API.",2024-03-23T09:50:29Z,2024-03-26T20:54:18Z,3 days 11:03:49,feat add support additional anthropic model commit updates claude class support additional model provide anthropic api init method claude class modify include list supported model create instance claude class model parameter set support model name unsupported model provide valueerror raise message indicate supported model change allow user utilize newly add model generate completion use anthropic api
27,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/700,unplanned,Unknown,(style) adding docstrings to the synthetic data generator class None,2024-03-23T02:40:07Z,2024-04-01T18:01:14Z,9 days 15:21:07,style add docstrings synthetic data generator class none
28,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/698,unplanned,Unknown,"docs(dspy): fix typo to run tests Signed-off-by: hsm207 <hsm207@users.noreply.github.com>

## üìù Changes Description

This MR/PR contains the following changes:

Fix typo to run tests.

This:

```bash
poetry install --with test
```

gives the following error:

```
Group(s) not found: test (via --with)
```

## ‚úÖ Contributor Checklist

- [x] Pre-Commit checks are passing (locally and remotely)
- [x] Title of your PR / MR corresponds to the required format
- [x] Commit message follows required format {label}(dspy): {message}

## ‚ö†Ô∏è Warnings

Anything we should be aware of ?
No",2024-03-22T18:23:33Z,2024-03-23T23:26:06Z,1 days 05:02:33,docsdspy fix typo run test signedoffby change description mrpr contains follow change fix typo run test bash poetry install test give follow error group find test via contributor checklist precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message warning anything aware
30,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/696,unplanned,Unknown,docs: fixes typos None,2024-03-22T11:19:49Z,2024-03-24T19:53:08Z,2 days 08:33:19,doc fix typos none
32,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/693,unplanned,Unknown,Fix Retry for TypedPredictors This simple PR aims to fix the errors and signature inconsistencies when using Assertions and Suggestions with modules with TypedPredictors.,2024-03-21T19:58:28Z,2024-03-26T23:08:16Z,5 days 03:09:48,fix retry typedpredictors simple pr aim fix error signature inconsistency use assertion suggestion module typedpredictors
33,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/692,unplanned,Unknown,Test for model_validator For Neoxelox,2024-03-21T19:44:39Z,2024-03-21T19:44:48Z,0 days 00:00:09,test modelvalidator neoxelox
35,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/689,unplanned,Unknown,Fixing claude None,2024-03-20T23:21:19Z,2024-03-20T23:21:26Z,0 days 00:00:07,fix claude none
36,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/684,unplanned,Unknown,"Add initial Logger I decided to go ahead and add a small default logger for DSPy.
The implementation is fairly simple to use at the library level, offers rich output, has minimal dependencies, and allows alot of flexibility for default formatting should we choose to set some specific structure up.

At a high level, its implemented using [structlog](https://www.structlog.org/en/stable/index.html).
Structlog is a lightweight library (no dependencies) which wraps logging for improved ergonomics.

It is set up to be used with two methods exposed at the `dspy` level.

## How to Log Anything

```python
import dspy

# To log a basic info message
dspy.logger.info(""hello world!"")

# Similar methods exist for all other log levels (INFO, DEBUG, WARNING, CRITICAL, ERROR):
dspy.logger.debug(""hello world from debug!"")
````

## On Log Levels

The visual log level defaults to INFO, but can be overrided in two simple ways:

**Via Environment Variable:**

```bash
LOG_LEVEL=info python dspy_program.py
````

**Via Code:**

```python
import dspy

# Since the default level is set to INFO, this wont print to the console
dspy.logger.debug(""this won't print to console"")

# Update the log level to show debug items
dspy.set_log_level(""debug"")

dspy.logger.debug(""this WILL print to console"")
````

**Log Output**

A few utilities have been added to either print the log to console, or file, and whether to output it as a string, or json.

````python
import dspy

# This will print to console as a str
dspy.logger.info(""this will show in console str formatted"")

# Change output from str to json
dspy.set_log_output(output_type=""json"")

# This will print to console as json
dspy.logger.info(""this will show in console as a json object"")

# Change output method from console to file
dspy.set_log_output(method=""file"", file_name=""log.txt"")

# This will append to log.txt as a json
dspy.logger.info(""this will show in console as a json object"")

````
",2024-03-20T15:22:22Z,2024-03-25T22:27:53Z,5 days 07:05:31,add initial logger decide go ahead add small default logger dspy implementation fairly simple use library level offer rich output minimal dependency allow alot flexibility default format choose set specific structure high level implement use structlog lightweight library dependency wrap log improve ergonomics set use two method expose dspy level log anything python import dspy log basic info message dspyloggerinfohello world similar method exist log level info debug warn critical error dspyloggerdebughello world debug log level visual log level default info override two simple way via environment variable bash loglevelinfo python dspyprogrampy via code python import dspy since default level set info wont print console dspyloggerdebugthis wont print console update log level show debug item dspysetlogleveldebug dspyloggerdebugthis print console log output utility add either print log console file whether output string json python import dspy print console str dspyloggerinfothis show console str format change output str json dspysetlogoutputoutputtypejson print console json dspyloggerinfothis show console json object change output method console file dspysetlogoutputmethodfile filenamelogtxt append logtxt json dspyloggerinfothis show console json object
38,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/679,unplanned,Unknown,"Added support for the latest Anthropic models. feat: Add support for additional Anthropic models

This commit updates the `Claude` class to support additional models provided by the Anthropic API:

- claude-2.0
- claude-2.1
- claude-3-opus-20240229
- claude-3-haiku-20240307
- claude-3-sonnet-20240229

The `__init__` method of the `Claude` class has been modified to include a list of supported models. When creating an instance of the `Claude` class, the `model` parameter can be set to any of the supported model names. If an unsupported model is provided, a `ValueError` is raised with a message indicating the supported models.

This change allows users to utilize the newly added models for generating completions using the Anthropic API.",2024-03-19T16:52:38Z,2024-03-23T09:27:52Z,3 days 16:35:14,added support late anthropic model feat add support additional anthropic model commit updates claude class support additional model provide anthropic api init method claude class modify include list supported model create instance claude class model parameter set support model name unsupported model provide valueerror raise message indicate supported model change allow user utilize newly add model generate completion use anthropic api
39,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/677,unplanned,Unknown,"Fix save and load state of program with typed predictors This simple PR aims to fix the `Pydantic model is not JSON serializable` error when saving the state of a compiled program that uses `TypedPredictors`.

However, this fix assumes that the demos of the predictors don't need to keep the same types after saving and loading their states. In older versions of DSPy the type was preserved, but after this commit it changed: https://github.com/stanfordnlp/dspy/commit/b9516c45dbcdedbed6b71dab7cd4a6c1024e0dcf

If that is not expected... then I don't how we would load the state to the pipeline. The JSON cannot know about the Pydantic models. A solution would be to pickle everything. Unfortunately I wasn't able to pickle the pipeline either, because it crashes on the dynamically created Pydantic models of the augmented TypedCoTs signatures.",2024-03-19T12:30:05Z,2024-03-25T10:34:33Z,5 days 22:04:28,fix save load state program type predictor simple pr aim fix pydantic model json serializable error save state compile program us typedpredictors however fix assumes demos predictor dont need keep type save load state older version dspy type preserve commit change expect dont would load state pipeline json cannot know pydantic model solution would pickle everything unfortunately wasnt able pickle pipeline either crash dynamically create pydantic model augment typedcots signature
40,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/675,unplanned,Unknown,Add GPT-4 Vision support to LM class None,2024-03-19T03:44:44Z,2024-03-19T18:22:59Z,0 days 14:38:15,add vision support lm class none
41,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/674,unplanned,Unknown,"Complete LM/Backend Interoperability This PR completes the Backend/Language Model v1 interoperability feature for DSPy.

All tests are passing, and we should be fairly close to merge to main.
There were a few small changes to COPRO/MIPRO optimizers, that I believe are due to inconsistencies in the functionality, so we may want to double check those.

@CyrusOfEden ",2024-03-19T00:41:15Z,2024-03-22T15:51:13Z,3 days 15:09:58,complete lmbackend interoperability pr complete backendlanguage model interoperability feature dspy test pass fairly close merge main small change copromipro optimizers believe due inconsistency functionality may want double check cyrusofeden
42,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/673,unplanned,Unknown,"TypedPredictor improvements Main improvement: Self explaining errors, ReACT prompting style. This is an optional feature that sometimes help the LLM learn from past errors.",2024-03-18T21:42:39Z,2024-03-18T21:46:57Z,0 days 00:04:18,typedpredictor improvement main improvement self explain error react prompt style optional feature sometimes help llm learn past error
43,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/671,unplanned,Unknown,integrate guidance library None,2024-03-18T18:34:03Z,2024-03-18T18:35:05Z,0 days 00:01:02,integrate guidance library none
44,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/669,unplanned,Unknown,"bug(dspy): Ran ruff check . --fix-only Ran `ruff check . --fix-only`

Lint error comes from #581 being merged without passing 4/4 CI. Everything working as intended.",2024-03-18T11:33:38Z,2024-03-18T11:36:09Z,0 days 00:02:31,bugdspy run ruff check fixonly run ruff check fixonly lint error come merge without pass ci everything work intend
45,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/667,unplanned,Unknown,"Fix for system-prompt This is PR/Fix for differentiating between system prompts and user inputs within the Chat Completions type of scenario. 

How?
By implementing a signature modification in the class's docstring, we enable the passing of optional system instruction prompts, marked by a specific keyword and delimiter. 

Why?

The distinction is critical for improving the model's interaction accuracy and response quality.

Tests?
Initial impact assessment indicates that critical features like chain of thought and program of thought, along with the predict method, can utilize this approach without needing significant modifications. These components will inherently benefit from the ability to include system instructions seamlessly within their operational logic.  


Note:  This update is proposed as a proof of concept (POC) at this stage. I",2024-03-18T02:43:07Z,2024-03-18T02:44:07Z,0 days 00:01:00,fix systemprompt prfix differentiate system prompt user input within chat completion type scenario implementing signature modification class docstring enable pass optional system instruction prompt mark specific keyword delimiter distinction critical improving model interaction accuracy response quality test initial impact assessment indicate critical feature like chain think program think along predict method utilize approach without need significant modification component inherently benefit ability include system instruction seamlessly within operational logic note update propose proof concept poc stage
47,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/665,unplanned,Unknown,"feat(program_of_thought): added support for custom import whitelist and added dynamic input and output field names for PoT  While, the PythonInterpreter and CodePrompt modules allowed for importing a white_list, this functionality hadn't been made available directly inside ProgramofThought. This was causing repeated hinderance when language models would try to use any math packages that were not allowed like sympy etc, which makes for a bad dev UX.",2024-03-17T18:28:41Z,2024-03-25T07:09:35Z,7 days 12:40:54,featprogramofthought add support custom import whitelist add dynamic input output field names pot pythoninterpreter codeprompt module allow import whitelist functionality hadnt make available directly inside programofthought cause repeat hinderance language model would try use math package allow like sympy etc make bad dev ux
49,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/660,unplanned,Unknown,Recreate intro.ipynb from March 13th None,2024-03-15T21:54:44Z,2024-03-15T22:07:56Z,0 days 00:13:12,recreate introipynb march none
50,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/659,unplanned,Unknown,Intro fix Recreate intro.ipynb from March 13th,2024-03-15T21:53:10Z,2024-03-15T21:53:28Z,0 days 00:00:18,intro fix recreate introipynb march
52,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/657,unplanned,Unknown,"fix(ruff) | Ruff accidentally has been deleted from pre-commit What is the change?
- ruff came back to the repo pre-commit

Why the change is important?
- it will bring lots of merge conflicts later on if the IDE doesn't have the same config.

How to test?
- test it with your IDE

Note:
This has been removed accidentally in https://github.com/stanfordnlp/dspy/pull/611 which I just reverted the file.",2024-03-15T20:24:51Z,2024-03-18T00:48:58Z,2 days 04:24:07,fixruff ruff accidentally delete precommit change ruff come back repo precommit change important bring lot merge conflict later ide doesnt config test test ide note remove accidentally revert file
54,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/654,unplanned,Unknown,Formatting for new optimizers I don't know why this didn't happen automatically,2024-03-15T05:26:45Z,2024-03-15T05:26:51Z,0 days 00:00:06,format new optimizers dont know didnt happen automatically
56,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/652,unplanned,Unknown,Fixed issue with boolean outputs. Also added some more tests for demos.,2024-03-14T22:37:55Z,2024-03-14T22:38:01Z,0 days 00:00:06,fix issue boolean output also add test demo
58,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/649,unplanned,Unknown,"fix(dspy): remove debug print statement (minor) This is a quick fix for unexpected print statement, and also includes linting.",2024-03-14T12:17:17Z,2024-03-14T13:32:03Z,0 days 01:14:46,fixdspy remove debug print statement minor quick fix unexpected print statement also include linting
60,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/647,unplanned,Unknown,"Fix Google API generate_content usage Per [the docs](https://ai.google.dev/api/rest/v1/GenerateContentResponse) GenerateContentResponse doesn't seem to have a `parts` param any more.

Completions are found under `candidates[i].content.parts[i].text`.",2024-03-14T04:59:09Z,2024-03-14T06:29:08Z,0 days 01:29:59,fix google api generatecontent usage per generatecontentresponse doesnt seem part param completion find candidatescontentpartstext
61,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/646,unplanned,Unknown,"Fix do_generate looking for max_tokens when using Google LM The google LM module has ""max_output_tokens"" in its kwargs dict, rather than ""max_tokens"". This causes `do_generate` to sometimes fail when trying to adjust the max tokens in the retry logic.

I considered updating the Google kwargs dict to also contain ""max_tokens"", but then the generation config object complains about extra keys, so it gets kind of annoying to keep track of as you switch back and forth.

It probably won't matter soon anyway, since I think the plan is to migrate to LiteLLM.",2024-03-14T04:15:09Z,2024-03-18T19:42:01Z,4 days 15:26:52,fix dogenerate look maxtokens use google lm google lm module maxoutputtokens kwargs dict rather maxtokens cause dogenerate sometimes fail try adjust max token retry logic consider updating google kwargs dict also contain maxtokens generation config object complain extra key get kind annoy keep track switch back forth probably wont matter soon anyway since think plan migrate litellm
63,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/644,unplanned,Unknown,"feature(dspy): added support for Vertex AI Gemini API Added support for Vertex AI Gemini API, see also https://github.com/stanfordnlp/dspy/discussions/640",2024-03-13T20:40:00Z,2024-03-14T05:01:55Z,0 days 08:21:55,featuredspy add support vertex ai gemini api add support vertex ai gemini api see also
64,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/643,unplanned,Unknown,"fix(dspy): use DataFrame.applymap in older pandas As mentioned in https://github.com/stanfordnlp/dspy/pull/561#issuecomment-1994541167, older Pandas such as 1.5.3 (the default version on Colab) doesn't have `DataFrame.map` yet",2024-03-13T16:48:33Z,2024-03-13T17:56:19Z,0 days 01:07:46,fixdspy use dataframeapplymap old panda mention old panda default version colab doesnt dataframemap yet
65,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/638,unplanned,Unknown,"Added support for multiple API endpoints to `HFClientVLLM` This PR adds support for multiple API endpoints being passed to `HFClientVLLM` via the `url` argument, in a backwards-compatible manner, as discussed in [this](https://github.com/stanfordnlp/dspy/issues/554) issue.",2024-03-13T11:24:31Z,2024-03-20T17:54:22Z,7 days 06:29:51,added support multiple api endpoint hfclientvllm pr add support multiple api endpoint pass hfclientvllm via url argument backwardscompatible manner discuss issue
67,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/635,unplanned,Unknown,"Fix marqo omg, dotdict move to dsp.ustils",2024-03-12T10:14:26Z,2024-03-14T16:41:10Z,2 days 06:26:44,fix marqo omg dotdict move dspustils
68,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/634,unplanned,Unknown,"Add an option to choose embedding model in pgvector * What is this PR for?
  * This PR resolves #613, adding an option to choose whether use openai to get embeddings, or other embedding models like cloud embedding services(google, cohere, nomic, ...) or on-premise embedding models(huggingface).

Features:
* Now installing `openai` is not required. Move it into optional import.
* Now either `openai_client` or `embedding_func` should be specified, otherwise raises AssertionError.

This PR will not change the current behavior as it still accepts `openai_client` for openai embeddings.",2024-03-12T08:14:47Z,2024-03-18T20:28:39Z,6 days 12:13:52,add option choose embed model pgvector pr pr resolve add option choose whether use openai get embeddings embed model like cloud embed servicesgoogle cohere nomic onpremise embed modelshuggingface feature instal openai require move optional import either openaiclient embeddingfunc specify otherwise raise assertionerror pr change current behavior still accept openaiclient openai embeddings
70,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/629,unplanned,Unknown,fix neo4j docs None,2024-03-11T17:15:35Z,2024-03-11T17:39:15Z,0 days 00:23:40,fix doc none
72,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/625,unplanned,Unknown,Fix return_all_scores in evaluate Remade version of https://github.com/stanfordnlp/dspy/pull/555 as requested by @isaacbmiller. ,2024-03-10T20:15:08Z,2024-03-10T20:52:55Z,0 days 00:37:47,fix returnallscores evaluate remade version request isaacbmiller
73,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/623,unplanned,Unknown,"feat(dspy): add support for azure ai search **Feature Addition: Introducing AzureAISearchRM Class**

This pull request introduces the AzureAISearchRM class, designed as a retrieval module utilizing Azure AI Search to retrieve top passages for a given query. Below are the details of this new class:

Key Features:

Integration with Azure AI Search: The class enables seamless integration with Azure AI Search service, facilitating the retrieval of top passages based on user queries.

Flexible Configuration: Users can configure various parameters such as the Azure AI Search service name, API key, search index name, and other optional settings like semantic ranking, filter queries, and speller mode.

Ease of Use: The class provides an intuitive interface for interacting with Azure AI Search, making it straightforward for developers to incorporate search functionalities into their applications.

Checks:

- [x] Pre-Commit checks are passing (locally and remotely)
- [x] Title of your PR / MR corresponds to the required format
- [x] Commit message follows required format {label}(dspy): {message}
",2024-03-10T11:30:41Z,2024-03-24T20:16:02Z,14 days 08:45:21,featdspy add support azure ai search feature addition introduce azureaisearchrm class pull request introduces azureaisearchrm class design retrieval module utilizing azure ai search retrieve top passage give query detail new class key feature integration azure ai search class enable seamless integration azure ai search service facilitate retrieval top passage base user query flexible configuration user configure various parameter azure ai search service name api key search index name optional setting like semantic ranking filter query speller mode ease use class provide intuitive interface interact azure ai search make straightforward developer incorporate search functionality application check precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message
74,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/622,unplanned,Unknown,"fix: return value in the inspect_history beside printing. What is the change?
- in the inspect_history function on top of printing adding the return 

Why the change is important?
- to when we are trying to get the last prompt be able to save it to variable beside printing.

How to test?
- GIVEN you run the compile
- WHEN you use the inspect_history
- THEN be able to save it in variable and it print it too.",2024-03-10T02:11:11Z,2024-03-24T19:39:38Z,14 days 17:28:27,fix return value inspecthistory beside print change inspecthistory function top printing add return change important try get last prompt able save variable beside print test give run compile use inspecthistory able save variable print
75,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/620,unplanned,Unknown,"trace patch updated trace to now be configured as `[]` instead of `None` to avoid manual setting to enable assertions. 
Patch per comments from #434 #467. 

@okhat - feel free to confirm this does not break any existing caches/behavior (@Shangyint and I have checked to ensure this already!)",2024-03-09T19:36:06Z,2024-03-18T00:50:15Z,8 days 05:14:09,trace patch update trace configure instead none avoid manual setting enable assertion patch per comment okhat feel free confirm break exist cachesbehavior shangyint check ensure already
77,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/618,unplanned,Unknown,"Add system prompt to GPT ## Usage

```
system_prompt = ""You are a language model based on Wikipedia, answering users' questions using a formal and serious tone.""
lm = dspy.OpenAI(model='gpt-4-1106-preview', api_key=openai_key, system_prompt=system_prompt, max_tokens=4000)
dspy.settings.configure(lm=lm)
```",2024-03-09T16:31:00Z,2024-03-18T19:49:09Z,9 days 03:18:09,add system prompt gpt usage systemprompt language model base wikipedia answer user question use formal serious tone lm apikeyopenaikey systempromptsystemprompt dspysettingsconfigurelmlm
78,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/614,unplanned,Unknown,"feature(dspy): Support more embedding models and return all fields The PGVector RM currently only supports a single OpenAI embedding model and although it allows users to input a list of fields to select from PGVector, it doesn't use those additional fields in the `dspy.Example`s output. This modification allows users to specify:
1. Any OpenAI embedding model to be used. Still will default to the current model so there's no regression.
2. Return data within the specified fields in the `dspy.Example`s. It will now use the column names in the examples rather than the hardcode fields the current RM specifies.

This PR still needs to be tested. I'll create a local PGVector DB and test once a maintainer has notified me that the goal of this change is desired.",2024-03-09T03:19:42Z,2024-03-26T17:32:01Z,17 days 14:12:19,featuredspy support embed model return field pgvector rm currently support single openai embed model although allow user input list field select pgvector doesnt use additional field dspyexamples output modification allow user specify openai embed model use still default current model there regression return data within specify field dspyexamples use column name examples rather hardcode field current rm specifies pr still need test ill create local pgvector db test maintainer notify goal change desire
80,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/612,unplanned,Unknown,"Adding in ""Open in colab"" badge to MIPRO notebook None",2024-03-08T18:15:00Z,2024-03-11T17:44:16Z,2 days 23:29:16,add open colab badge mipro notebook none
81,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/611,unplanned,Unknown,hf models can use auth tokens now Access to gated HF models previously was not supported by dspy.HFModels. This is now supported - just pass in token=<your hf token> or load it into the env var HF_TOKEN (e.g. via dotenv.load_dotenv()).,2024-03-08T17:44:50Z,2024-03-09T16:53:18Z,0 days 23:08:28,hf model use auth token access gate hf model previously support dspyhfmodels support pas token load env var hftoken eg via dotenvloaddotenv
82,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/610,unplanned,Unknown,Fix to signature optimizer - setting num_candidates None,2024-03-08T17:27:48Z,2024-03-08T17:31:30Z,0 days 00:03:42,fix signature optimizer set numcandidates none
83,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/609,unplanned,Unknown,"feature(dspy): return metadata along long_text for ChromaDB retrieval Returning metadata along long_text for ChromaDB.
Fixes #364 for ChromaDB",2024-03-08T12:38:36Z,2024-03-09T18:52:02Z,1 days 06:13:26,featuredspy return metadata along longtext chromadb retrieval return metadata along longtext chromadb fix chromadb
84,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/608,unplanned,Unknown,"Feature mistralai api I added support for Mistral AI's API. Thanks to this new feature, users will be able to play with the following models easily: mistral-small-latest, mistral-medium-latest, and mistral-large-latest (a GPT-4 level model).

Here is an example of usage:

```python

import dspy

lm = dspy.Mistral(api_key=""your-mistralai-api-key"")

dspy.settings.configure(lm=lm)

mod = dspy.Predict(""question -> answer"")

print(mod(question=""Who is Emmanuel Macron?""))

```",2024-03-08T05:43:38Z,2024-03-24T19:28:53Z,16 days 13:45:15,feature mistralai api add support mistral ai api thanks new feature user able play follow model easily mistralsmalllatest mistralmediumlatest mistrallargelatest level model example usage python import dspy lm dspymistralapikeyyourmistralaiapikey dspysettingsconfigurelmlm mod dspypredictquestion answer printmodquestionwho emmanuel macron
86,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/605,unplanned,Unknown,Update provider name to Claude or Bedrock None,2024-03-08T00:37:48Z,2024-03-09T18:38:17Z,1 days 18:00:29,update provider name claude bedrock none
87,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/604,unplanned,Unknown,[DOCS] Typed Predictor Docs and API References None,2024-03-07T23:20:38Z,2024-03-09T18:12:17Z,1 days 18:51:39,typed predictor doc api reference none
88,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/603,unplanned,Unknown,"Optimizer updates + MIPRO notebook This PR contains the following changes:

1. Adding in an example notebook for using MIPRO
2. Deprecating¬†`signature_optimizer` and¬†`bayesian_signature_optimizer`, in favor of the names¬†`copro_optimizer`¬†and¬†`mipro_optimizer`
3. Deprecating a few variable names in favor of more descriptive ones
4. Adding support for normal `dspy.Predict` modules for both¬†`copro_optimizer`¬†and¬†`mipro_optimizer`
5. Fixing some minor print statements for both¬†`copro_optimizer`¬†and¬†`mipro_optimizer`

Work done in collaboration with @XenonMolecule! ",2024-03-07T22:53:51Z,2024-03-08T16:06:01Z,0 days 17:12:10,optimizer update mipro notebook pr contain follow change add example notebook use mipro deprecate signatureoptimizer bayesiansignatureoptimizer favor name coprooptimizer miprooptimizer deprecate variable name favor descriptive one add support normal dspypredict module coprooptimizer miprooptimizer fix minor print statement coprooptimizer miprooptimizer work do collaboration xenonmolecule
89,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/602,unplanned,Unknown,"Fix code issues in signature-optimizer.mdx 1. use the HotPotQA dataset instead of gsm8k
2. result.rationale instead of result.reason
3. process devset to include input_keys",2024-03-07T22:29:36Z,2024-03-08T19:12:46Z,0 days 20:43:10,fix code issue signatureoptimizermdx use hotpotqa dataset instead resultrationale instead resultreason process devset include inputkeys
90,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/601,unplanned,Unknown,"Better handling of multi-module programs with typed signature optimizer This also changes how `named_paramters()` works.

Might require some notebook testing to make sure nothing has changed.",2024-03-07T22:26:49Z,2024-03-14T18:29:30Z,6 days 20:02:41,well handle multimodule program type signature optimizer also change namedparamters work might require notebook test make sure nothing change
91,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/600,unplanned,Unknown,"ci(dspy): Add main push test run back Removes the comment bot workflow that wasn't working, and adds back tests to run on push to main",2024-03-07T22:23:00Z,2024-03-07T22:24:56Z,0 days 00:01:56,cidspy add main push test run back remove comment bot workflow wasnt work add back test run push main
92,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/599,unplanned,Unknown,"Bad commit ## üìù Changes Description

This MR/PR contains the following changes:
...

## ‚úÖ Contributor Checklist

- [] Pre-Commit checks are passing (locally and remotely)
- [] Title of your PR / MR corresponds to the required format
- [] Commit message follows required format {label}(dspy): {message}

## ‚ö†Ô∏è Warnings

Anything we should be aware of ?
",2024-03-07T22:06:53Z,2024-03-07T22:26:15Z,0 days 00:19:22,bad commit change descriptionthis mrpr contains follow change contributor checklist precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message warningsanything aware
93,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/597,unplanned,Unknown,"Adding MIPRO notebook + optimizers refactor This PR contains the following changes:

1. Adding in an example notebook for using MIPRO
2. Deprecating `signature_optimizer` and `bayesian_signature_optimizer`, in favor of the names `copro_optimizer` and `mipro_optimizer`
3. Deprecating a few variable names in favor of more descriptive ones
4. Adding support for normal dspy.Predict modules for both  `copro_optimizer` and `mipro_optimizer`
5. Fixing some minor print statements for both `copro_optimizer` and `mipro_optimizer`

Work with @XenonMolecule!",2024-03-07T19:22:16Z,2024-03-07T19:49:35Z,0 days 00:27:19,add mipro notebook optimizers refactor pr contains follow change add example notebook use mipro deprecate signatureoptimizer bayesiansignatureoptimizer favor name coprooptimizer miprooptimizer deprecate variable name favor descriptive one add support normal dspypredict module coprooptimizer miprooptimizer fix minor print statement coprooptimizer miprooptimizer work xenonmolecule
95,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/593,unplanned,Unknown,"fix: BootstrapFewShotWithRandomSearch.metric_threshold not set * will result in AttributeError: 'BootstrapFewShotWithRandomSearch' object has no attribute 'metric_threshold' when execute `compile`
* can be reproduced by executing the following colab's `Compilation With Assertions` part.
** https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/longformqa/longformqa_assertions.ipynb#scrollTo=544FmRbtuzgS

## üìù Changes Description

This MR/PR contains the following changes:
...

## ‚úÖ Contributor Checklist

- [‚úÖ] Pre-Commit checks are passing (locally and remotely)
- [‚úÖ] Title of your PR / MR corresponds to the required format
- [‚úÖ] Commit message follows required format {label}(dspy): {message}

## ‚ö†Ô∏è Warnings

Anything we should be aware of ?
",2024-03-07T10:36:21Z,2024-03-07T20:50:03Z,0 days 10:13:42,fix bootstrapfewshotwithrandomsearchmetricthreshold set result attributeerror bootstrapfewshotwithrandomsearch object attribute metricthreshold execute compile reproduce execute follow colabs compilation assertion part change description mrpr contain follow change contributor checklist precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message warning anything aware
98,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/588,unplanned,Unknown,"Adding metric_threshold to BootstrapFewShotWithRandomSearch *Change:*

- add `metric_threshold` parameter to `BootstrapFewShotWithRandomSearch` with default being `None`

*Reason:*

- when using a custom metric that are not 0 or 1, we need `metric_threshold` in `BootstrapFewShot`; however, when using `BootstrapFewShotWithRandomSearch` we don't have `metric_threshold` that can be passed to  `BootstrapFewShot`

@okhat ",2024-03-07T03:35:20Z,2024-03-07T06:34:31Z,0 days 02:59:11,add metricthreshold bootstrapfewshotwithrandomsearch change add metricthreshold parameter bootstrapfewshotwithrandomsearch default none reason use custom metric need metricthreshold bootstrapfewshot however use bootstrapfewshotwithrandomsearch dont metricthreshold pass bootstrapfewshot okhat
100,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/586,unplanned,Unknown,"fix(player.py): Fix syntax error in player file which downloads into ‚Ä¶ ‚Ä¶Python312\Lib\site-packages\dspy upon installation

This ""player.py"" file, though mentioned by name in the 0.0.3 dspy documentation, is not initially present in the github repo itself but downloads into Python312\Lib\site-packages\dspy when DSPy is installed. This fixes a recurring error I have encountered locally, in conda, and on Hugging Face Spaces. I hope this is the correct way to submit this fix, despite the location of the player.py file. Error states on Hugging Face as:
'''
Traceback (most recent call last):
  File ""/home/user/app/app.py"", line 24, in <module>
    from dspy.dspy.agents import Agent
  File ""/home/user/.local/lib/python3.10/site-packages/dspy/__init__.py"", line 12, in <module>
    from dspy.player import Player
  File ""/home/user/.local/lib/python3.10/site-packages/dspy/player.py"", line 38
    print 'Too many generators to append another'
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)? '''",2024-03-06T23:37:37Z,2024-03-06T23:39:07Z,0 days 00:01:30,fixplayerpy fix syntax error player file downloads ‚Ä¶ ‚Ä¶ upon installation playerpy file though mention name dspy documentation initially present github repo download dspy instal fix recur error encounter locally conda hug face space hope correct way submit fix despite location playerpy file error state hug face traceback recent call last file homeuserappapppy line dspydspyagents import agent file line dspyplayer import player file line print many generator append another syntaxerror miss parenthesis call print mean print
101,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/585,unplanned,Unknown,A few optimizer improvements None,2024-03-06T21:28:52Z,2024-03-06T21:28:58Z,0 days 00:00:06,optimizer improvement none
102,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/584,unplanned,Unknown,"Remove unused kwargs passed to requests sent through `AzureOpenAI` This commit is to remove unused kwargs that we include in the requests sent through `AzureOpenAI` class.

This seems to fix this issue: https://github.com/stanfordnlp/dspy/issues/543",2024-03-06T20:57:06Z,2024-03-07T06:32:25Z,0 days 09:35:19,remove unused kwargs pass request send azureopenai commit remove unused kwargs include request send azureopenai class seem fix issue
103,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/581,unplanned,Unknown,env var for cache_utils.py add environmental variable ''DPS_CACHEBOOL' to activate or deactivate cache. By default True.,2024-03-06T17:19:59Z,2024-03-17T22:18:45Z,11 days 04:58:46,env var cacheutilspy add environmental variable dpscachebool activate deactivate cache default true
105,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/579,unplanned,Unknown,"Update ProgramOfThought.md: Example Uses the extended signature syntax by utilizing a class wrapping dspy.Signature, because .attach() seems deprecated",2024-03-06T17:07:58Z,2024-03-11T17:50:11Z,5 days 00:42:13,update programofthoughtmd example use extended signature syntax utilize class wrapping dspysignature attach seem deprecate
110,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/574,unplanned,Unknown,"New Typed Signature Optimizer Also fixed a bug with list inputs, and another bug on predictors with no input fields.",2024-03-06T09:43:39Z,2024-03-06T16:34:28Z,0 days 06:50:49,new typed signature optimizer also fix bug list input another bug predictor input field
112,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/571,unplanned,Unknown,Upgrade pydantic to ^2.6 Fix for https://github.com/stanfordnlp/dspy/issues/570,2024-03-06T05:20:26Z,2024-03-19T13:51:03Z,13 days 08:30:37,upgrade pydantic fix
113,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/569,unplanned,Unknown,"2 commits: 1 bad, 1 good None",2024-03-06T01:46:19Z,2024-03-06T06:14:08Z,0 days 04:27:49,commits bad good none
114,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/567,unplanned,Unknown,"DSPy Only Predict This PR is intended to get a ""DSPy Only Predict"" running, ie. No `dsp` code running. 
This is ultimately the point at which the Backend refactor becomes breaking, 

## TODOs, This includes a few main points:

### 1. Move over `dsp`'s predict primitives:
- [x] Greedy & Partial decoding for Templates, allowing the `dspy.Template` to extract each field regardless of whether all fields were generated successfully.
- [x] Recursive ""healing"" for Template based generation calls. Ie. Some generate calls, often do not include all fields. 
- [x] Tests for BackendTemplates with partially completed calls.
- [x] Ensure parameters for `max_tokens`, `temperature` and recursive workflow `max`_retries` are respected and exposed appropriately.

### 2. Introduce new `DummyLanguageModel` for use with the `BaseLM`.
- [x] Build backend agnostic Dummy Language Model.
- [x] Add tests for `TemplateBackend` generate calls without recovery.
- [x] Add tests for `TemplateBackend` generate calls with recovery.

### 3. Update `Predict`
- [x] Remove all stale attributes (lm)
- [x] Transition away from `dsp.Example` and `dsp.Template` to `dspy.Example` and `dspy.Template`.
- [x] Generate new predictions for Backend infrastructure.
- [x] Expand `Predict` tests to ensure functionality remains consistent.

## Questions

#### 1. Do we expect the 'non-Template' backends to use recursive healing functionality.
 I imagine, non-Templates would not need this functionality. I have currently included it within the `BaseBackend` but it could be pulled out and included within the `TemplateBackend` only.

#### 2. Load/Dump State for the current `dspy.Predict` class will be broken with this change.
Ultimately, starting this integration across the different module classes is going to be breaking. Are there specific concerns we have regarding this, how do we want to manage this? Migration details etc. Also is it worth merging the backend implementations earlier than transitioning the Module classes?

Let me know what you think? @CyrusOfEden @okhat.",2024-03-06T00:42:08Z,2024-03-13T20:37:34Z,7 days 19:55:26,dspy predict pr intend get dspy predict run ie dsp code run ultimately point backend refactor become break todos include main point move dsps predict primitive greedy partial decoding template allow dspytemplate extract field regardless whether field generate successfully recursive heal template base generation call ie generate call often include field test backendtemplates partially complete call ensure parameter maxtokens temperature recursive workflow maxretries respect expose appropriately introduce new dummylanguagemodel use baselm build backend agnostic dummy language model add test templatebackend generate call without recovery add test templatebackend generate call recovery update predict remove stale attribute lm transition away dspexample dsptemplate dspyexample dspytemplate generate new prediction backend infrastructure expand predict test ensure functionality remain consistent question expect nontemplate backends use recursive healing functionality imagine nontemplates would need functionality currently include within basebackend could pull include within templatebackend loaddump state current dspypredict class broken change ultimately start integration across different module class go break specific concern regard want manage migration detail etc also worth merge backend implementation earlier transition module class let know think cyrusofeden okhat
115,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/565,unplanned,Unknown,"ci(dspy): Fix failing tests coming from forks This PR does two main things:
1. It fixes an issue where the code would try to checkout code from a repo it did not have access to. The way I wrote it to checkout ```{{ github.head_ref}}``` was incorrect. This should resolve the issues that CI tests coming from forks face.
2. Starts to add a workflow to upload an artifact to add a comment to run the style checks",2024-03-05T23:28:33Z,2024-03-07T21:50:44Z,1 days 22:22:11,cidspy fix fail test come fork pr two main thing fix issue code would try checkout code repo access way write checkout githubheadref incorrect resolve issue ci test come forks face start add workflow upload artifact add comment run style check
116,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/564,unplanned,Unknown,bug(dspy): Fix CI checking out wrong repo None,2024-03-05T22:00:28Z,2024-03-05T23:37:55Z,0 days 01:37:27,bugdspy fix ci check wrong repo none
117,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/563,unplanned,Unknown,Add optional parameter 'k' to forward method in FaissRM class Testing theory that CI breaking in #559 is due to being on a DSPy fork,2024-03-05T20:21:57Z,2024-03-05T20:25:35Z,0 days 00:03:38,add optional parameter k forward method faissrm class test theory ci break due dspy fork
118,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/562,unplanned,Unknown,Adds native support for Claude models Adds native support for claude wrapper,2024-03-05T19:50:11Z,2024-03-09T20:26:57Z,4 days 00:36:46,add native support claude model add native support claude wrapper
119,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/561,unplanned,Unknown,Fix applymap deprecation warning Fixes #492 ,2024-03-05T19:30:46Z,2024-03-09T18:33:13Z,3 days 23:02:27,fix applymap deprecation warning fix
120,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/559,unplanned,Unknown,add 'k' as argument to FaissRM.forward() add 'k' as argument to FaissRM.forward(),2024-03-05T12:29:58Z,2024-03-10T19:06:33Z,5 days 06:36:35,add k argument faissrmforward add k argument faissrmforward
122,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/555,unplanned,Unknown,Fix return_all_scores in evaluate Addresses https://github.com/stanfordnlp/dspy/issues/477,2024-03-05T04:40:17Z,2024-03-07T21:50:46Z,2 days 17:10:29,fix returnallscores evaluate address
124,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/553,unplanned,Unknown,"docs(dspy): add toy invocation of model to minimal-example The new docs are awesome, and this is a 1 line change that I propose to improve `minimal-example`",2024-03-05T02:22:12Z,2024-03-05T17:49:10Z,0 days 15:26:58,docsdspy add toy invocation model minimalexample new doc awesome line change propose improve minimalexample
125,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/552,unplanned,Unknown,"Introduce dspy.Template The original implementation of `TemplateBackend` relied upon the `signature_to_template` function. This would take in a `dspy.Signature` and return a `dsp.Template`. However, we would pass `dspy.Example` into the `dsp.Template` and the extract function would return `dsp.Example` a different type. Leaving us with two different types of `Example` within a single prediction. To combat this, a new `Template` primitive was introduced into `dspy` to mirror and replace the original `dsp.Template`.

I do not expect this current PR to achieve feature parity with the original class, but initial tests have been introduced to mock and mirror the current prompt structure used in `dsp`.

Let me know what you think @CyrusOfEden, the last piece I believe to start using the `TemplateBackend` is to fully explore the `do_generate` function dealing with incomplete or inaccurate completions.",2024-03-04T21:42:19Z,2024-03-05T00:54:11Z,0 days 03:11:52,introduce dspytemplate original implementation templatebackend rely upon signaturetotemplate function would take dspysignature return dsptemplate however would pass dspyexample dsptemplate extract function would return dspexample different type leave u two different type example within single prediction combat new template primitive introduce dspy mirror replace original dsptemplate expect current pr achieve feature parity original class initial test introduce mock mirror current prompt structure use dsp let know think cyrusofeden last piece believe start use templatebackend fully explore dogenerate function deal incomplete inaccurate completion
126,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/551,unplanned,Unknown,"Fix _print_signature() for verbose=True Fix the following error:

```
[/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/signature_opt.py](https://localhost:8080/#) in _print_signature(self, predictor)
     96                 signature = predictor.extended_signature1
     97             # print(f""i: {signature.instructions}"")
---> 98             # print(f""p: {list(signature.fields().values())[-1].json_schema_extra['prefix']}"")
     99             print()
    100 

TypeError: 'dict' object is not callable
```

for [Signature Optimizer example](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/signature-optimizer). Also, `compiled_prompt_opt = teleprompter.compile(cot, devset=devset, eval_kwargs=kwargs)` should be changed to `compiled_prompt_opt = teleprompter.compile(cot_baseline, devset=devset, eval_kwargs=kwargs)`",2024-03-04T21:36:27Z,2024-03-05T17:40:58Z,0 days 20:04:31,fix printsignature verbosetrue fix follow error printsignatureself predictor signature printfi signatureinstructions printfp listsignaturefieldsvaluesjsonschemaextra print typeerror dict object callable also compiledpromptopt telepromptercompilecot devsetdevset evalkwargskwargs change compiledpromptopt telepromptercompilecotbaseline devsetdevset evalkwargskwargs
128,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/548,unplanned,Unknown,"Add support for neo4j vector index Neo4j has a vector index that can be used for vector retrieval in RAG in other applications. It can be expanded with retrieval query, so that it can traverse through the graph after the initial vector search if needed.",2024-03-04T19:16:56Z,2024-03-09T18:30:58Z,4 days 23:14:02,add support vector index vector index use vector retrieval rag application expand retrieval query traverse graph initial vector search need
129,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/547,unplanned,Unknown,"fix: printing signature fields in verbose mode for signature_opt #482 introduced _print_signature() for verbose mode in signature optimizer.
```python    
def _print_signature(self, predictor):
        if self.verbose:
            if (hasattr(predictor, 'extended_signature')):
                signature = predictor.extended_signature
            else:
                signature = predictor.extended_signature1
            print(f""i: {signature.instructions}"")
            print(f""p: {list(signature.fields().values())[-1].json_schema_extra['prefix']}"")
            print()
```

fields() is a property method in SignatureMeta class. 
```python
@property
def fields(cls):
    # Make sure to give input fields before output fields
    return {**cls.input_fields, **cls.output_fields}`
```
""fields"" method is being used incorrectly in _print_signature()
The current _print_signature() implementation is raising 'TypeError: 'dict' object is not callable' when trying to use signature optimizer as per the tutorial provided in [https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/signature-optimizer](url)

The tutorial for signature optimizer provided in the docs feels a bit incomplete. The tutorial example uses HotPotQA and GSM8K datasets inconsistently. While trying the example with HotPotQA dataset, I encountered an error of input keys not being set. Then I noticed that while creating Example objects for data, the with_inputs() method is not called in HotPotQA whereas it is called correctly in GSM8K. I am not sure if it is a known issue. 

Example objects creation for hotpotqa:
```python
    def _shuffle_and_sample(self, split, data, size, seed=0):
        '''
            The setting (seed=s, size=N) is always a subset
            of the setting (seed=s, size=M) for N < M.
        '''

        data = list(data)

        # Shuffle the data irrespective of the requested size.
        base_rng = random.Random(seed)

        if self.do_shuffle:
            base_rng.shuffle(data)

        data = data[:size]
        output = []

        for example in data:
            output.append(Example(**example, dspy_uuid=str(uuid.uuid4()), dspy_split=split))
        
        # TODO: NOTE: Ideally we use these uuids for dedup internally, for demos and internal train/val splits.
        # Now, some tasks (like convQA and Colors) have overlapping examples. Here, we should allow the user to give us
        # a uuid field that would respect this in some way. This means that we need a more refined concept that
        # uuid (each example is unique) and more like a group_uuid.

        # rng = random.Random(seed)
        # rng.shuffle(data)

        return output
```

Example objects creation for gsm8k:

```python
        trainset = [dspy.Example(**x).with_inputs('question') for x in trainset]
        devset = [dspy.Example(**x).with_inputs('question') for x in devset]
        testset = [dspy.Example(**x).with_inputs('question') for x in testset]
```

I tested the current change with a custom dataset I created from ""math_qa"" huggingface dataset. Verbose mode now works as expected. I am open to a better reasoning or raising my concerns as an issue.
",2024-03-04T14:14:57Z,2024-03-05T17:21:20Z,1 days 03:06:23,fix printing signature field verbose mode signatureopt introduce printsignature verbose mode signature optimizer python def printsignatureself predictor selfverbose hasattrpredictor extendedsignature signature predictorextendedsignature else signature printfi signatureinstructions printfp listsignaturefieldsvaluesjsonschemaextra print field property method signaturemeta class python property def fieldscls make sure give input field output field return clsinputfields clsoutputfields field method use incorrectly printsignature current printsignature implementation raise typeerror dict object callable try use signature optimizer per tutorial provide url tutorial signature optimizer provide docs feel bite incomplete tutorial example use hotpotqa datasets inconsistently try example hotpotqa dataset encounter error input key set noticed create example object data withinputs method call hotpotqa whereas call correctly sure know issue example object creation hotpotqa python def shuffleandsampleself split data size set seed sizen always subset set seed sizem n data listdata shuffle data irrespective request size baserng randomrandomseed selfdoshuffle baserngshuffledata data data output example data outputappendexampleexample dspysplitsplit todo note ideally use uuids dedup internally demos internal trainval split task like convqa color overlap example allow user give u uuid field would respect way mean need refine concept uuid example unique like groupuuid rng randomrandomseed rngshuffledata return output example object creation python trainset devset testset test current change custom dataset create mathqa huggingface dataset verbose mode work expect open well reason raise concern issue
130,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/545,unplanned,Unknown,"Synthetic Data Generation v2 **Usage:**
[1] Features:
```python
import dsp
from dspy.datasets import DataLoader
from dspy.experimental import Synthesizer, SynthesizerArguments

class GenerateAnswer(dspy.Signature):
    """"""Answer questions with short factoid answers.""""""

    context = dspy.InputField(desc=""may contain relevant facts"")
    question = dspy.InputField()
    answer = dspy.OutputField(desc=""often between 1 and 5 words"")

config = SynthesizerArguments()
synthesizer = Synthesizer(config=config)

syn_data = synthesizer.generate(GenerateAnswer, num_data=10)

synthesizer.export(data=syn_data, path=""syn_datacsv"")
```

[2] Batched Generation(Faster):
```python
from dspy.datasets import DataLoader
from dspy.experimental import Synthesizer, SynthesizerArguments

dl = DataLoader()

data = dl.from_huggingface(
    ""gsm8k"", ""main"",
    fields=(""question"", ""answer""),
    input_keys=(""question"",),
    split=""train[:8]""
)

config = SynthesizerArguments()
synthesizer = Synthesizer(config=config)

synthesizer.generate(
    ground_source=data,
    num_data=10,
    batch_size=2,
)
```

[3] Feedback Driven Generation
```python
config = SynthesizerArguments(
    feedback_mode=""llm"", # or ""human"" for human in a loop generation
    num_example_for_feedback=3,
)
synthesizer = Synthesizer(config=config)

synthesizer.generate(
    ground_source=data,
    num_data=10,
    batch_size=2,
)
```

[4] Tweakable LM for input and output gen and support for module based output generation

[5] Example based input generation optimization",2024-03-04T14:02:26Z,2024-03-14T12:37:20Z,9 days 22:34:54,synthetic data generation usage feature python import dsp dspydatasets import dataloader dspyexperimental import synthesizer synthesizerarguments class generateanswerdspysignature answer question short factoid answer context dspyinputfielddescmay contain relevant fact question dspyinputfield answer dspyoutputfielddescoften word config synthesizerarguments synthesizer synthesizerconfigconfig syndata synthesizergenerategenerateanswer synthesizerexportdatasyndata pathsyndatacsv batch generationfaster python dspydatasets import dataloader dspyexperimental import synthesizer synthesizerarguments dl dataloader data dlfromhuggingface main fieldsquestion answer inputkeysquestion splittrain config synthesizerarguments synthesizer synthesizerconfigconfig synthesizergenerate groundsourcedata feedback driven generation python config synthesizerarguments feedbackmodellm human human loop generation synthesizer synthesizerconfigconfig synthesizergenerate groundsourcedata tweakable lm input output gen support module base output generation example base input generation optimization
131,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/542,unplanned,Unknown,bug(dspy): Move ensemble import to teleprompt Adds ensemble to the teleprompt imports. Solves #533 ,2024-03-04T05:56:01Z,2024-03-05T03:03:06Z,0 days 21:07:05,bugdspy move ensemble import teleprompt add ensemble teleprompt import solves
132,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/541,unplanned,Unknown,"ci(dspy): Fix tests failing - remove --no-root flag ## üìù Changes Description

This MR/PR contains the following changes:
...

## ‚úÖ Contributor Checklist

- [] Pre-Commit checks are passing (locally and remotely)
- [] Title of your PR / MR corresponds to the required format
- [] Commit message follows required format {label}(dspy): {message}

## ‚ö†Ô∏è Warnings

Anything we should be aware of ?
",2024-03-04T05:02:43Z,2024-03-04T05:09:41Z,0 days 00:06:58,cidspy fix test fail remove noroot flag change descriptionthis mrpr contains follow change contributor checklist precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message warningsanything aware
133,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/540,unplanned,Unknown,"Improvements to Signature 1. Support for parsed string types, like ""input: int -> output: int"" using ast parsing.
2. Support for `n=...` like the normal predictor's completions.
3. Validation of fields in signature (before pydantic only validated fields in sub types)
4. A few lint fixes",2024-03-03T22:15:32Z,2024-03-03T22:15:39Z,0 days 00:00:07,improvement signature support parse string type like input int output int use ast parse support n like normal predictor completion validation field signature pydantic validate field sub type lint fix
134,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/538,unplanned,Unknown,"ci(dspy): Add Ruff linting workflow Makes CI better:
1. Caches poetry installation
2. Caches python install dependencies
3. Auto commit style fixes (That commit wont be tested, but the tests get run post-lint, so if it breaks something the tests will fail)
",2024-03-03T16:04:34Z,2024-03-04T04:23:58Z,0 days 12:19:24,cidspy add ruff linting workflow make ci well cache poetry installation cache python install dependency auto commit style fix commit wont test test get run postlint break something test fail
135,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/536,unplanned,Unknown,Update skycamp2023.ipynb Referring to completed tutorial more explicitly. Regarding https://github.com/stanfordnlp/dspy/issues/523,2024-03-03T11:24:13Z,2024-03-03T11:53:53Z,0 days 00:29:40,update refer complete tutorial explicitly regard
137,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/532,unplanned,Unknown,"Update README.md What do you think about having a types section in the examples like this?
I can expand it more, or make it shorter.",2024-03-02T22:57:25Z,2024-03-02T23:11:21Z,0 days 00:13:56,update readmemd think type section example like expand make shorter
138,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/530,unplanned,Unknown,"Synthetic Data Generation `Synthesizer` is a synthetic data generation utility that generates synthetic data based on sample input data you provide to it. All you need to pass is a few examples and `num_data` i.e. no. of synthetic datapoints you want to generate and it'll take care of it.

**Usage:** 
```python
from dspy.datasets import DataLoader, Synthesizer

dl = DataLoader()

data = dl.from_huggingface(
    ""gsm8k"", ""main"",
    fields=(""question"", ""answer""),
    input_keys=(""question"",),
    split=""train[:3]""
)

synthesizer = Synthesizer()

syn_data = synthesizer.generate(examples=data, num_data=10)

synthesizer.export(data=syn_data, path=""syn_data_gpt_4.csv"")
```",2024-03-02T18:16:21Z,2024-03-02T22:04:03Z,0 days 03:47:42,synthetic data generation synthesizer synthetic data generation utility generate synthetic data base sample input data provide need pas example numdata ie synthetic datapoints want generate itll take care usage python dspydatasets import dataloader synthesizer dl dataloader data dlfromhuggingface main fieldsquestion answer inputkeysquestion splittrain synthesizer synthesizer syndata synthesizergenerateexamplesdata synthesizerexportdatasyndata
139,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/529,unplanned,Unknown,"Feature data generation This commit introduces the `synthetic_data_generation` function in the utils. The function dynamically generates synthetic data based on a given Pydantic schema class and sample size. The function constructs a custom dspy.Signature class, leverages dspy.Predict for data generation, and organizes the generated data into examples using dspy.Example. This enhancement streamlines the process of creating synthetic datasets for testing and development purposes.

Usage:
```python

# Generating synthetic data via a pydantic model
generator = SyntheticDataGenerator(schema_class=SyntheticFacts)
examples = generator.generate(sample_size=6)

# # Generating synthetic data via existing examples
generator = SyntheticDataGenerator(examples=existing_examples)
examples = generator.generate(sample_size=5)

```
",2024-03-02T15:11:51Z,2024-03-02T22:03:05Z,0 days 06:51:14,feature data generation commit introduces syntheticdatageneration function utils function dynamically generate synthetic data base give pydantic schema class sample size function construct custom dspysignature class leverage dspypredict data generation organizes generate data example use dspyexample enhancement streamlines process create synthetic datasets test development purpose usage python generate synthetic data via pydantic model generator syntheticdatageneratorschemaclasssyntheticfacts examples generate synthetic data via exist example generator syntheticdatageneratorexamplesexistingexamples example
143,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/522,unplanned,Unknown,"BootstrapFewShot failing due to lm.copy for AzureOpenAI, #521 BootstrapFewShot failing due to lm.copy for AzureOpenAI, likely due to positional argument issue from recent changes.

- Issue: https://github.com/stanfordnlp/dspy/issues/521

### Environment
- code: 98304a2eb9d1dddaaa846e30258cd5d8fc6b5d8d
- model azure openai

### Error message
- during `BootstrapFewShot` run, code errored out due to missing arguments, `api_version`

```
>[dspy/dsp/modules/lm.py](https://file+.vscode-resource.vscode-cdn.net/Users/insop/Projects/Github/NLP/dspy/dsp/modules/lm.py)(106)copy()
    102         model = kwargs.pop('model')
    105
--> 106         return self.__class__(model, **kwargs)
```

### Changes

AzureOpenAI expects positional arguments, so it is failing. I am putting my change for review if that will impact other models.

```
class AzureOpenAI(LM):
...

    def __init__(
        self,
        api_base: str,
        api_version: str,
        model: str = ""gpt-3.5-turbo-instruct"",
        api_key: Optional[str] = None,
        model_type: Literal[""chat"", ""text""] = ""chat"",
        **kwargs,
    ):",2024-03-02T01:29:38Z,2024-03-02T23:08:42Z,0 days 21:39:04,bootstrapfewshot fail due lmcopy azureopenai bootstrapfewshot fail due lmcopy azureopenai likely due positional argument issue recent change issue environment code model azure openai error message bootstrapfewshot run code errored due miss argument apiversion model kwargspopmodel return selfclassmodel kwargs change azureopenai expect positional argument fail put change review impact model class azureopenailm def init self apibase str apiversion str model str apikey optional none modeltype literal chat kwargs
148,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/516,unplanned,Unknown,"fix: import of ChromaRM ## üìù Changes Description

This MR/PR contains the following changes:
...

## ‚úÖ Contributor Checklist

- [x] Pre-Commit checks are passing (locally and remotely)
- [x] Title of your PR / MR corresponds to the required format
- [x] Commit message follows required format {label}(dspy): {message}

## ‚ö†Ô∏è Warnings

Anything we should be aware of ?
",2024-03-01T21:45:24Z,2024-03-05T17:45:45Z,3 days 20:00:21,fix import chromarm change descriptionthis mrpr contains follow change contributor checklist precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message warningsanything aware
149,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/515,unplanned,Unknown,"Reformatted all the code with ruff As per our discussions, I ran `ruff . --fix` on the entire code base.
I had to change a few things to get the tests to still run:
```
[tool.ruff.lint.per-file-ignores]
""**/{tests,docs}/*"" = [""ALL""]
""**__init__.py"" = [""F401""]
```
The first line just disables linting/error fixing on the tests.
This was necessary, because some tests do stuff like importing `List` rather than using `list`, because it wants to be backwards compatible.
The line `""**__init__.py"" = [""F401""]` prevents `ruff` from removing unused imports in `__init__.py` files, which are used to make those modules publicly available.",2024-03-01T21:23:55Z,2024-03-02T23:08:00Z,1 days 01:44:05,reformatted code ruff per discussion run ruff fix entire code base change thing get test still run testsdocs initpy first line disables lintingerror fix test necessary test stuff like import list rather use list want backwards compatible line initpy prevents ruff remove unused import initpy file use make module publicly available
150,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/513,unplanned,Unknown,fixed bug in previous commit Apologies. Better tested this time.,2024-03-01T19:59:39Z,2024-03-01T20:00:07Z,0 days 00:00:28,fix bug previous commit apology well test time
151,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/512,unplanned,Unknown,Fixed spelling error Fixed spelling error and added option to not shuffle labeled data.,2024-03-01T19:56:19Z,2024-03-01T19:56:24Z,0 days 00:00:05,fix spell error fix spell error add option shuffle label data
152,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/511,unplanned,Unknown,Support for max_errors in randomsearch None,2024-03-01T19:36:45Z,2024-03-01T19:36:52Z,0 days 00:00:07,support maxerrors randomsearch none
153,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/509,unplanned,Unknown,"Type improvements Cleans up some type stuff (no more pydantic version in error messages),
Removes make_example (now it happens automatically, only if there's actually an error),
Fixes some some issues with generics like List[str]",2024-03-01T19:05:34Z,2024-03-01T19:05:44Z,0 days 00:00:10,type improvement clean type stuff pydantic version error message remove makeexample happen automatically there actually error fixes issue generic like list
154,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/507,unplanned,Unknown,Update skycamp2023.ipynb There are 7 train examples.,2024-03-01T17:13:17Z,2024-03-01T18:22:36Z,0 days 01:09:19,update train example
157,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/502,unplanned,Unknown,"Fixed some issues with generics like list[str] Also created a test, and made sure tests run with python 3.9.
Slightly contaminated by reformatting.",2024-02-29T19:19:37Z,2024-02-29T19:19:43Z,0 days 00:00:06,fix issue generic like list also create test make sure test run python slightly contaminate reformatting
158,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/501,unplanned,Unknown,Update template_v2.py to fix issue #428 Fixed all code that could cause issue #428 - key error with the key 'augmented',2024-02-29T17:34:33Z,2024-02-29T17:39:54Z,0 days 00:05:21,update fix issue fix code could cause issue key error key augment
159,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/500,unplanned,Unknown,Update template_v2.py to fix issue #428 Fix crash that occurs when 'augmented' key does not exist. The error is AttributeError: 'dict' object has no attribute 'augmented',2024-02-29T17:23:19Z,2024-02-29T17:36:57Z,0 days 00:13:38,update fix issue fix crash occurs augment key exist error attributeerror dict object attribute augment
161,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/498,unplanned,Unknown,"Add Template Backend Added an initial TemplateBackend, tested with LiteLLM.

At a high level, it would let us do something like this (a minimal example):

```python
class Emotion(Signature):
    """"""Classify emotion among sadness, joy, love, anger, fear, surprise.""""""
    sentence = InputField()
    sentiment = OutputField()
    
class Predict(Parameter):
    def __init__(self, signature, **kwargs):
        self.signature = ensure_signature(signature)
    
    def __call__(self, **kwargs):
        return self.forward(**kwargs)
        
    def forward(self, **kwargs):
        
        # Get backend and predict
        backend = dspy.settings.get(""backend"")
        return backend(signature, **kwargs)
```

We could even abstract that away to some high level primitive function, taking into account any other concerns as well.

### I have a few additional thoughts at a high level:

**1. I changed the signature of the `BaseBackend` to be:**

````python
class TemplateBackend(BaseBackend):
    def __call__(
        self,
        signature: Signature,
        demos: t.List[str] = [],
        **kwargs) -> ...
````

Given that the underlying language model is completely abstracted away from the backend, I wasnt sure why we would specifically call out certain language model specific arguments in the class signature. This sets the signature to only be those additive arguments necessary for templates specifically, with all kwargs being passed directly to the language model as needed.

**2. Small changes to caching**

I like the function changes in the BaseLM. However, due to the way Pydantic manages attributes, we have to set the `generate_with_cache` callable to be a private function (ie. `_generate_with_cache`) to import correctly.

**3. Argument overriding in older methods.**

I noticed some code in the old `Predict` class, I am unsure of moving forward.

```python
class Predict(Parameter):
    ...
    def forward(self, **kwargs):
        ...
        if (temperature is None or temperature <= 0.15) and num_generations > 1:
            config[""temperature""] = 0.7
        ...
```

Code like this may, make it harder to track arguments that are actually hitting the Language Models. To make it a bit more transparent, maybe we could put a warning at the Language Model level, which would raise if n > 1 and temperature <= 0.15, which would allow the user to define the temperature themselves.

@CyrusOfEden Let me know what you think, going to do a deeper dive into a few of these internals to identify we have everything managed for on the template side.",2024-02-29T15:12:24Z,2024-02-29T22:32:29Z,0 days 07:20:05,add template backend add initial templatebackend test litellm high level would let u something like minimal example python class emotionsignature classify emotion among sadness joy love anger fear surprise sentence inputfield sentiment outputfield class predictparameter def initself signature kwargs selfsignature ensuresignaturesignature def callself kwargs return selfforwardkwargs def forwardself kwargs get backend predict backend dspysettingsgetbackend return backendsignature kwargs could even abstract away high level primitive function take account concern well additional thought high level change signature basebackend python class templatebackendbasebackend def call self signature signature demos tlist kwargs give underlying language model completely abstract away backend wasnt sure would specifically call certain language model specific argument class signature set signature additive argument necessary template specifically kwargs pass directly language model need small change cache like function change baselm however due way pydantic manages attribute set generatewithcache callable private function ie generatewithcache import correctly argument override old method notice code old predict class unsure move forward python class predictparameter def forwardself kwargs temperature none temperature config code like may make hard track argument actually hit language model make bit transparent maybe could put warn language model level would raise n temperature would allow user define temperature cyrusofeden let know think go deeply dive internals identify everything manage template side
163,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/496,unplanned,Unknown,Update language_models.ipynb Colab link Update the notebook. The Colab button is being redirected to signatures.ipynb (on Colab) instead of language_models.ipynb.,2024-02-29T08:59:33Z,2024-03-01T03:21:11Z,0 days 18:21:38,update languagemodelsipynb colab link update notebook colab button redirect signaturesipynb colab instead languagemodelsipynb
165,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/493,unplanned,Unknown,"Docs migration -replaces docs/ folder with hosted DSPy-docs website
-includes documentation on making contributions to docs",2024-02-29T07:09:08Z,2024-03-01T19:12:17Z,1 days 12:03:09,docs migration replaces docs folder host dspydocs website include documentation make contribution docs
168,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/490,unplanned,Unknown,Cleaned up functional.py With new TypedChainOfThought function,2024-02-28T20:14:50Z,2024-02-28T20:43:04Z,0 days 00:28:14,clean functionalpy new typedchainofthought function
171,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/486,unplanned,Unknown,"fix: AttributeError for Non-Prediction Objects in Retriever Suggestion/Fix for this issue - https://github.com/stanfordnlp/dspy/issues/166

The retrieve function, seems to get the passages (as a Prediction object) during the use of certain retrievers - this breaks the downstream flow.

I have added a check to determine if its a Prediction object (currently I'm doing it by checking the attribute, but i feel there must be a better way - will check!)

Please let me know if there's something I should consider or have missed out.",2024-02-28T11:23:55Z,2024-03-24T20:07:35Z,25 days 08:43:40,fix attributeerror nonprediction object retriever suggestionfix issue retrieve function seem get passage prediction object use certain retriever break downstream flow add check determine prediction object currently im check attribute feel must better way check please let know there something consider miss
173,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/483,unplanned,Unknown,"feat: add from_parquet to dataloader A parquet file loader would be a convenient addition to the dataloader class. I particularly like that parquet files preserve types better than a csv file.

The primary change is the addition of the from_parquet() function:

```python
def from_parquet(self, file_path: str, fields: list[str] = None, input_keys: tuple[str] = ()) -> list[dspy.Example]:
        dataset = load_dataset(""parquet"", data_files=file_path)[""train""]

        if not fields:
            fields = list(dataset.features)

        return [dspy.Example({field: row[field] for field in fields}).with_inputs(input_keys) for row in dataset]
```

The rest of the changes were caused by the ruff precommit hook.",2024-02-27T22:13:22Z,2024-03-25T00:34:18Z,26 days 02:20:56,feat add fromparquet dataloader parquet file loader would convenient addition dataloader class particularly like parquet file preserve type well csv file primary change addition fromparquet function python def fromparquetself filepath str field list none inputkeys tuple list dataset loaddatasetparquet datafilesfilepath field field listdatasetfeatures return field fieldswithinputsinputkeys row dataset rest change cause ruff precommit hook
174,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/482,unplanned,Unknown,Fixed verbose for signature_opt The changes made to SignatureOptimizer seem like they need to be applied also when verbose=True.,2024-02-27T19:42:45Z,2024-02-27T19:45:03Z,0 days 00:02:18,fix verbose signatureopt change make signatureoptimizer seem like need apply also verbosetrue
175,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/481,unplanned,Unknown,"[`chore`] Fix typo; update link Hello!

## Pull Request overview
* Fix typo: bsaed -> based
* Update link: https://huggingface.co/sentence-transformers -> https://huggingface.co/models?library=sentence-transformers

## Details
Regarding the link update: Sentence Transformers is compatible with many more models than just the few ones under the `sentence-transformers` organization. It also supports very competitive open models like https://huggingface.co/BAAI/bge-large-en-v1.5.

Also: Fascinating project! I quite like the `torch`-inspired approach that you're taking here. I'd love to learn a bit more about how I can assist.

- Tom Aarsen",2024-02-27T18:34:05Z,2024-02-27T20:16:37Z,0 days 01:42:32,fix typo update link hello pull request overview fix typo bsaed base update link detail regard link update sentence transformer compatible many model one sentencetransformers organization also support competitive open model like also fascinate project quite like torchinspired approach youre take id love learn bit assist tom aarsen
176,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/478,unplanned,Unknown,"fix: default collection embedding function attribute in ChromadbRM constructor The recent PR #449 opened by @smwitkowski  refactored ChromadbRM constructor to use collection default embedding function with the syntax ""self._chromadb_collection.embedding_function"". But ChromaDB collection object has no such attribute and raises attribute error if no embedding function is passed to the constructor. 

This PR modifies the constructor to use Chromadb collection's protected attribute ""self._chromadb_collection._embedding_function"" and the module now works as expected.

I could be wrong in my assumptions as I am relatively new to dspy and am open to possible better resolution. ",2024-02-27T10:58:16Z,2024-03-09T18:21:34Z,11 days 07:23:18,fix default collection embed function attribute chromadbrm constructor recent pr open smwitkowski refactored chromadbrm constructor use collection default embed function syntax selfchromadbcollectionembeddingfunction chromadb collection object attribute raise attribute error embed function pass constructor pr modifies constructor use chromadb collection protect attribute selfchromadbcollectionembeddingfunction module work expect could wrong assumption relatively new dspy open possible good resolution
180,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/474,unplanned,Unknown,Catchup backend-refactor with main Merging main with backend-refactor to accomodate for new tests integrations and signatures.,2024-02-27T02:39:27Z,2024-02-29T00:53:43Z,1 days 22:14:16,catchup backendrefactor main merging main backendrefactor accomodate new test integration signature
181,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/473,unplanned,Unknown,from dspy.teleprompt import BootstrapFewShot Module needed to be imported.,2024-02-27T02:11:59Z,2024-02-27T05:08:39Z,0 days 02:56:40,dspyteleprompt import bootstrapfewshot module need import
182,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/472,unplanned,Unknown,"Fix and rename google.py to googlegenerativeai.py - Solves issue #462 

- Re issue #463 I'm not sure if the vertex API will be more reliable than google.generativeai, so I think it makes sense to rename this one to `googlegenerativeai` rather than just `google`.

- The google.generativeai API has changed significantly since last week. There is a new conv.send_message syntax. I have tested that this works.",2024-02-27T02:11:55Z,2024-02-27T02:28:39Z,0 days 00:16:44,fix rename googlepy googlegenerativeaipy solve issue issue im sure vertex api reliable googlegenerativeai think make sense rename one googlegenerativeai rather google googlegenerativeai api change significantly since last week new convsendmessage syntax test work
183,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/471,unplanned,Unknown,"Merge main into backend-refactor This was a simple attempt to merge catch the backend-refactor branch up with main. I am a bit unsure what happened on this, as it looks like formatting differences between the two branches resulted in a massive line count change. I imagine this can be done cleaner to avoid the git blame mess.",2024-02-26T22:10:45Z,2024-02-26T22:38:52Z,0 days 00:28:07,merge main backendrefactor simple attempt merge catch backendrefactor branch main bit unsure happen look like format difference two branch result massive line count change imagine do clean avoid git blame mess
184,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/470,unplanned,Unknown,"migrated latest OpenAI version import -moved OpenAI dependencies within local functions to avoid OpenAI version incompatibilities in build: `from openai import OpenAI` is compatible with >1.11.1

",2024-02-26T21:43:14Z,2024-02-26T22:47:35Z,0 days 01:04:21,migrate late openai version import move openai dependency within local function avoid openai version incompatibility build openai import openai compatible
186,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/467,unplanned,Unknown,"Teleprompt Base Class -> ABC, Some Static Typing, and Ruff Fixes/Updates Only clear dsp.setting.trace in assertions.py if a trace object exists. 

Adapt YouRM API call signature to match the documentation on https://api.you.com/api-key; update return data to respect self.k (as opposed to returning all snippets from the first K hits).
",2024-02-26T17:59:03Z,2024-03-18T19:24:36Z,21 days 01:25:33,teleprompt base class abc static type ruff fixesupdates clear dspsettingtrace assertionspy trace object exist adapt yourm api call signature match documentation update return data respect selfk oppose return snippet first k hit
188,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/465,unplanned,Unknown,"Feature: add pgvector retriever Implements a retriever that (as the name suggests) uses pgvector to retrieve passages,
    using a raw SQL query and a postgresql connection managed by psycopg2.

    It needs to register the pgvector extension with the psycopg2 connection

    Returns a list of dspy.Example objects

    Args:
        db_url (str): A PostgreSQL database URL in psycopg2's DSN format
        pg_table_name (Optional[str]): name of the table containing passages
        openai_client (openai.OpenAI): OpenAI client to use for computing query embeddings
        k (Optional[int]): Default number of top passages to retrieve. Defaults to 20
        embedding_field (str = ""embedding""): Field containing passage embeddings. Defaults to ""embedding""
        fields (List[str] = ['text']): Fields to retrieve from the table. Defaults to ""text""

    Examples:
        Below is a code snippet that shows how to use PgVector as the default retriever

        ```python
        import dspy
        import openai
        import psycopg2

        openai.api_key = os.environ.get(""OPENAI_API_KEY"", None)
        openai_client = openai.OpenAI()
        
        llm = dspy.OpenAI(model=""gpt-3.5-turbo"")
        
        DATABASE_URL should be in the format postgresql://user:password@host/database 
        db_url=os.getenv(""DATABASE_URL"")

        retriever_model = PgVectorRM(conn, openai_client=openai_client, ""paragraphs"", fields=[""text"", ""document_id""], k=20)
        dspy.settings.configure(lm=llm, rm=retriever_model)
        ```

        Below is a code snippet that shows how to use PgVector in the forward() function of a module
        ```python
        self.retrieve = PgVectorRM(db_url, openai_client=openai_client, ""paragraphs"", fields=[""text"", ""document_id""], k=20)",2024-02-26T12:16:01Z,2024-02-27T05:12:02Z,0 days 16:56:01,feature add pgvector retriever implement retriever name suggest us pgvector retrieve passage use raw sql query postgresql connection manage need register pgvector extension connection return list dspyexample object args dburl str postgresql database url dsn format pgtablename optional name table containing passage openaiclient openaiopenai openai client use compute query embeddings k optional default number top passage retrieve default embeddingfield str embed field contain passage embeddings default embed field list field retrieve table default text example code snippet show use pgvector default retriever python import dspy import openai import openaiapikey osenvirongetopenaiapikey none openaiclient openaiopenai llm databaseurl format postgresqluserpasswordhostdatabase dburlosgetenvdatabaseurl retrievermodel pgvectorrmconn openaiclientopenaiclient paragraph field dspysettingsconfigurelmllm rmretrievermodel code snippet show use pgvector forward function module python selfretrieve pgvectorrmdburl openaiclientopenaiclient paragraph field
191,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/461,unplanned,Unknown,"Improve google LLM support The previous script is basically borrowed from `cohere.py`,  and I found that the previous script doesn't work properly and should be fixed.
* The Google Python API requires different arguments, so I updated it by referring official document.
* Empirically, some requests may be blocked by safety reasons even if the given prompt is not harmful. So I propose to add default safety options with `BLOCK_ONLY_HIGH`.
* For now Google API seems to disallow multiple generations, so I implemented a loop to generate candidates iteratively.
*  Add Google module to DSP and DSPy package so we can access directly through `dspy.Google`.

Please let me know if you have any concerns or feedback.

Best,",2024-02-26T06:56:50Z,2024-02-27T23:34:18Z,1 days 16:37:28,improve google llm support previous script basically borrow coherepy find previous script doesnt work properly fix google python api require different argument update refer official document empirically request may block safety reason even give prompt harmful propose add default safety option blockonlyhigh google api seem disallow multiple generation implement loop generate candidate iteratively add google module dsp dspy package access directly dspygoogle please let know concern feedback best
192,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/460,unplanned,Unknown,Dspy docs page [DSPy Docs Page](https://dspy-docs.vercel.app/),2024-02-26T06:35:33Z,2024-02-26T06:50:58Z,0 days 00:15:25,dspy doc page
194,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/458,unplanned,Unknown,"ci(dspy): Add tests to CI 1. Create a new action to run the tests that @thomasahle implemented. Fix #456 
2. Fix the pre-commit checks workflow
3. Adjusted the pyproject.toml to have pytest as a dependency.

Note: databricks integration does not pass tests. Commented out as that is the only integration to do so. To be added back at a later date
",2024-02-26T02:55:53Z,2024-02-27T06:01:00Z,1 days 03:05:07,cidspy add test ci create new action run test thomasahle implement fix fix precommit check workflow adjust pyprojecttoml pytest dependency note databricks integration pas test comment integration add back later date
196,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/455,unplanned,Unknown,"Further Update YouRM to return an Iterable of Predictions This brings it in line with the behavior of other retriever implementations in `/retrieve`.

Related - we probably want to make dspy.Retrieve a Protocol or AbstractMethod so Pycharm and co can highlight/catch these errors ahead of time. ",2024-02-25T23:43:25Z,2024-02-25T23:46:31Z,0 days 00:03:06,update yourm return iterable prediction bring line behavior retriever implementation retrieve relate probably want make dspyretrieve protocol abstractmethod pycharm co highlightcatch error ahead time
197,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/453,unplanned,Unknown,"Bring YouRM .forward signature in-line with Retrieve child class .forward methods (add K kwarg) The following code (snippet) follow the setup idiom currently used by dspy:

```python

class WebRag(dspy.Module):
    NL_SEP = ""\n----------\n""

    def __init__(self):
        super().__init__()
        # uses you_rm as declared below:
        self.you_rm = dspy.Retrieve(k=5)
        self.chain_of_thought = dspy.ChainOfThought(...irrelevant_signature...)

    def forward(self, question: str):
        web_context = self.NL_SEP.join(self.you_rm(question).passages)
        out = self.chain_of_thought(web_context=web_context, question=question)
        return out.answer
...

rm = YouRM(ydc_api_key=settings.YDC_API_KEY.get_secret_value())
lm = dspy.OpenAI(model=settings.LLM_MODEL, api_key=settings.OPENAI_API_KEY.get_secret_value())
dspy.settings.configure(lm=lm, rm=rm)
```

However, invoking this with a question string raises the following error:

```python

web_rag = WebRag()

out = web_rag(""What is the meaning of life (according to Douglas Adams)?"")

# ERROR:
#  File ""/Users/mgb/miniforge3/envs/nlu/lib/python3.9/site-packages/dspy/retrieve/retrieve.py"", line 29, in __call__
#    return self.forward(*args, **kwargs)
# TypeError: forward() got an unexpected keyword argument 'k'
```

This is due to `retrieveEnsemble` in `search.py` invoking YouRM's .forward with an additional `k` kwarg. 

Inspecting other modules in dspy/retrieve (e.g. chromadb.py) shows that k is desired as a kwarg to retrieve forward invocations; I therefore added this to YouRM's .forward and also to the parent .forward implementation in retrieve.py.
",2024-02-25T23:03:32Z,2024-02-25T23:24:09Z,0 days 00:20:37,bring yourm forward signature inline retrieve child class forward method add k kwarg follow code snippet follow setup idiom currently use dspy python class webragdspymodule nlsep nn def initself superinit use yourm declare selfyourm selfchainofthought dspychainofthoughtirrelevantsignature def forwardself question str webcontext selfnlsepjoinselfyourmquestionpassages selfchainofthoughtwebcontextwebcontext questionquestion return outanswer rm yourmydcapikeysettingsydcapikeygetsecretvalue lm dspyopenaimodelsettingsllmmodel apikeysettingsopenaiapikeygetsecretvalue dspysettingsconfigurelmlm rmrm however invoke question string raise follow error python webrag webrag webragwhat mean life accord douglas adams error file line call return selfforwardargs kwargs typeerror forward get unexpected keyword argument k due retrieveensemble searchpy invoke yourms forward additional k kwarg inspect module dspyretrieve eg chromadbpy show k desire kwarg retrieve forward invocation therefore add yourms forward also parent forward implementation retrievepy
198,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/452,unplanned,Unknown,update Marqo This PR for update marqo,2024-02-25T19:18:18Z,2024-02-25T23:47:05Z,0 days 04:28:47,update marqo pr update marqo
199,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/451,unplanned,Unknown,"Updated Pydantic Branch This PR contains the new Signature type based on Pydantic.
It also contains 112 unit tests.
Finally it has the module dspy/functional which include the TypedPredictor class, that lets you generate constrained on the types in the signature.",2024-02-25T06:40:44Z,2024-02-25T18:50:01Z,0 days 12:09:17,update pydantic branch pr contain new signature type base pydantic also contain unit test finally module dspyfunctional include typedpredictor class let generate constrain type signature
200,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/449,unplanned,Unknown,"Refactor ChromadbRM to use collection's default embedding function The previous PR #400 opened by @animtel set `chromadb.utils.embedding_functions.DefaultEmbeddingFunction` as the default query embedding function. This calls `ONNXMiniLM_L6_V2` , which uses `all-MiniLM-L6-v2` as the base embedding model.

This introduced unintended behavior where queries could be embedded using a different model than documents unless the embedding function was explicitly provided. If a model other than `all-MiniLM-L6-v2` embeds queries, and the embedding function is not specified, the query and document embeddings will use different models. While this may be desired in some cases, this should not be the default behavior.

This PR changes the default query embedding method to use the collection's embedding function, accessed via the class. Users can still provide a custom embedding model.",2024-02-25T03:35:52Z,2024-02-25T23:47:39Z,0 days 20:11:47,refactor chromadbrm use collection default embed function previous pr open animtel set chromadbutilsembeddingfunctionsdefaultembeddingfunction default query embed function call use base embed model introduce unintended behavior query could embed use different model document unless embed function explicitly provide model embeds query embed function specify query document embeddings use different model may desire case default behavior pr change default query embed method use collection embed function access via class user still provide custom embed model
201,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/447,unplanned,Unknown,Looking into merging thomas's `method` branch into `pydantic` (and then to `main` right after) None,2024-02-24T20:49:55Z,2024-02-24T21:09:47Z,0 days 00:19:52,look merge thomas method branch pydantic main right none
203,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/442,unplanned,Unknown,"feat: Exploring global caching for LM calls I took a look into how we can integrate global joblib Memory caching into the `BaseLM` class.

It takes a little bit of misdirection, but the below `BaseLM` class will provide for global caching for all sub-classes that implement `BaseLM`.

```python
_cache_memory = Memory(cachedir, verbose=0)


class BaseLM(BaseModel, ABC):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._cached = _cache_memory.cache(self._call)

    def __call__(self, prompt: str, **kwargs) -> list[str]:
        """"""Generates `n` predictions for the signature output.""""""
        return self._cached(prompt, **kwargs)

    @abstractmethod
    def _call(
        self,
        prompt: str,
        **kwargs,
    ) -> list[str]:
        """"""Generates `n` predictions for the signature output.""""""
        ...

    @abstractmethod
    def count_tokens(self, prompt: str) -> int:
        """"""Counts the number of tokens for a specific prompt.""""""
        ...

```

The only change to downstream LM implementations, is now the abstract method is called `_call` as opposed to `__call__`. Additionally, with this scenario the cache is based on both LM attributes and function arguments. This is essential in the LiteLLM case, as the model name is essential for appropriate hashing, but each subsequent modification in the class itself, in which attributes are added/removed, will use a new cache. To combat this, the `_cache_memory.cache` does have an `ignore` parameter, which we can use to isolate exactly what is necessary for cache hashing.

This also is grabbing the same cache_utils, we are using in the current version of dspy, and I have a few questions on that generally:

1. Would we be okay with just global `joblib.Memory`  for all caching needs?
    - I noticed the current version is using nested caching for what appears to be Notebook first, then global caching. I am unsure exactly what benefit this is providing for.

2. Small note, but can we hide this cachedir behind a hidden folder?
    - Currently, the cachedir is stored at 'Path.home() / cachedir_joblib' can we change this to 'Path.home() / .cachedir_job' with this refactor, so all new LiteLLM caching calls are not cluttering up the visible files in the home directory.
    
Let me know what you think @CyrusOfEden, @okhat, I know cache misses are an important point for some folks.",2024-02-23T20:19:43Z,2024-02-27T19:42:54Z,3 days 23:23:11,feat explore global caching lm call take look integrate global joblib memory cache baselm class take little bit misdirection baselm class provide global caching subclass implement baselm python cachememory memorycachedir class baselmbasemodel abc def initself args kwargs superinitargs kwargs selfcached cachememorycacheselfcall def callself prompt str kwargs list generates n prediction signature output return selfcachedprompt kwargs abstractmethod def call self prompt str kwargs list generates n prediction signature output abstractmethod def counttokensself prompt str int count number token specific prompt change downstream lm implementation abstract method call call oppose call additionally scenario cache base lm attribute function argument essential litellm case model name essential appropriate hashing subsequent modification class attribute addedremoved use new cache combat cachememorycache ignore parameter use isolate exactly necessary cache hash also grab cacheutils use current version dspy question generally would okay global joblibmemory cache need notice current version use nested caching appear notebook first global caching unsure exactly benefit provide small note hide cachedir behind hidden folder currently cachedir store pathhome cachedirjoblib change pathhome cachedirjob refactor new litellm cache call clutter visible file home directory let know think cyrusofeden okhat know cache miss important point folk
204,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/440,unplanned,Unknown,"feat(retrieve): add support for faiss retriever, update setup.md, docs Please note that this Pull Request is just an update to the top of the tree and was already discussed in PR #423",2024-02-23T18:42:01Z,2024-02-25T23:49:26Z,2 days 05:07:25,featretrieve add support faiss retriever update setupmd docs please note pull request update top tree already discuss pr
205,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/439,unplanned,Unknown,feat(Evaluate): callable objects can be used as metric for Evaluate Minor changes to enable the evaluator to accept any callable objects as metrics.,2024-02-23T16:58:55Z,2024-02-25T23:56:30Z,2 days 06:57:35,featevaluate callable object use metric evaluate minor change enable evaluator accept callable object metric
206,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/437,unplanned,Unknown,"[Suggestion] Add token counting to BaseLM With a few projects discussed including Optimization Dry Runs #397 , and Token Budgeting. I thought we may want to add token counting functionality directly in at the BaseLM level within the refactor.

This change includes two small changes, changes to the abstract methods at the BaseLM, and token counting implementation for LiteLLM.

```python
class BaseLM(BaseModel, ABC):
    ...
    
    @abstractmethod
    def count_tokens(self, prompt: str) -> int:
        """"""counts the number of tokens for a specific prompt.""""""
        ...
````

The good news is LiteLLM makes this super simple, and no additional attributes are needed to make this work.

```python
from dspy.backends.lm.litellm import LiteLLM

lm = LiteLLM(model=""gpt-3.5-turbo"")

# returns 13, the number of tokens in a string.
lm.count_tokens(""hello this is a small test"")

````

If there is another place, token counting would be preferred, I am happy to move this functionality over, just thought I would give us an easy starting point.

@CyrusOfEden ",2024-02-23T15:39:26Z,2024-02-23T17:36:18Z,0 days 01:56:52,add token count baselm project discuss include optimization dry run token budget thought may want add token count functionality directly baselm level within refactor change include two small change change abstract method baselm token count implementation litellm python class baselmbasemodel abc abstractmethod def counttokensself prompt str int count number token specific prompt good news litellm make super simple additional attribute need make work python dspybackendslmlitellm import litellm lm return number token string lmcounttokenshello small test another place token counting would prefer happy move functionality think would give u easy start point cyrusofeden
207,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/435,unplanned,Unknown,"[Readme Update] Alphabetically sort Vector DBs and add Weaviate to `pip install dspy-ai[weaviate]` Adds Weaviate to the optional dependencies, and alphabetically sorts Vector DB integrations in the readme and pyproject.toml.",2024-02-23T00:27:45Z,2024-02-23T13:16:16Z,0 days 12:48:31,alphabetically sort vector db add weaviate pip install dspyai add weaviate optional dependency alphabetically sort vector db integration readme pyprojecttoml
212,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/430,unplanned,Unknown,"added Databricks integrations for DSPy LMs/RM configurations integrated Databricks connectors for configuring LMs and RM in DSPy.

LMs - adapted from OpenAI Client SDK in [Databricks Model Serving endpoints](https://docs.databricks.com/en/machine-learning/model-serving/score-model-serving-endpoints.html#language-OpenAI%C2%A0client)
Supports chat, completions, and embeddings model formats.

RM - adapted from [querying Databricks Vector Search Endpoint](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-index) ",2024-02-21T17:56:06Z,2024-02-25T23:51:46Z,4 days 05:55:40,added databricks integration dspy lmsrm configuration integrate databricks connector configure lm rm dspy lm adapt openai client sdk support chat completion embeddings model format rm adapt
216,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/423,unplanned,Unknown,feat(retrieve): add support for faiss retriever A minimal faiss retriever for dspy,2024-02-20T21:38:23Z,2024-02-23T18:29:02Z,2 days 20:50:39,featretrieve add support faiss retriever minimal faiss retriever dspy
218,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/421,unplanned,Unknown,WIP - refactor LM backend Start to refactor backend as mentioned in #390,2024-02-20T21:13:07Z,2024-02-21T03:27:45Z,0 days 06:14:38,wip refactor lm backend start refactor backend mention
219,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/419,unplanned,Unknown,"fix: update imports to allow global dspy.Google Currently, the new Google LM class, does not have consistent imports with the remainder of the LMs.

ie. dspy.Cohere works, but dspy.Google does not.

This PR makes small import changes to ensure consistency.",2024-02-20T16:28:28Z,2024-02-23T13:31:25Z,2 days 21:02:57,fix update import allow global dspygoogle currently new google lm class consistent import remainder lm ie dspycohere work dspygoogle pr make small import change ensure consistency
220,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/418,unplanned,Unknown,"Propagate device_map to hf model ## Description

Currently, when using the HF model backend, the `hf_device_map` parameter may not propagate to the `AutoModel*`. This PR solves that, by passing the `hf_device_map` to the underlying `AutoModel*`.

Ultimately, this allows for splitting models across multiple GPUs/accelerators.",2024-02-20T11:38:20Z,2024-02-22T16:25:43Z,2 days 04:47:23,propagate devicemap hf model description currently use hf model backend hfdevicemap parameter may propagate automodel pr solve pass hfdevicemap underlying automodel ultimately allow splitting model across multiple gpusaccelerators
222,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/415,unplanned,Unknown,Fix typo in intro.ipynb None,2024-02-20T03:45:50Z,2024-02-20T06:57:40Z,0 days 03:11:50,fix typo introipynb none
224,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/413,unplanned,Unknown,"feature: add dl.from_json to support native json dataset formats ```
from dspy.datasets import DataLoader
dl = DataLoader()
json_dataset = dl.from_json(""/home/path-to/sample_responses.json"", fields=[""question"", ""answer"", ""context""])
# joy ensues...
```",2024-02-19T23:22:47Z,2024-02-26T06:15:55Z,6 days 06:53:08,feature add dlfromjson support native json dataset format dspydatasets import dataloader dl dataloader jsondataset dlfromjsonhomepathtosampleresponsesjson field joy ensues
225,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/412,unplanned,Unknown,"Add `metric_threshold` argument to BootstrapFewShot ## How it currently works

In line 151 of `BootstrapFewShot` we currently check examples with:

`success = (self.metric is None) or self.metric(example, prediction, trace)`

If success == True, we then wrap the predictor, inputs, and outputs in an Example object and append it to `name2traces`.

This works great if you have a boolean metric, but maybe we want to extend this with a `metric_threshold` argument for float or int metric return types.

## Metric Threshold

- `metric_threshold` is added as default `None`

Then line 151 is replaced with:

```
if self.metric:
  metric_val = self.metric(example, prediction, trace)
  if self.metric_threshold:
    success = metric_val > self.metric_threshold
  else:
    success = metric_val
else:
  success = True
```

We can now retain the float metrics like LLM rating metrics or what have you, but also add a little more strength to find high quality examples in `BootstrapFewShot`.",2024-02-19T18:41:35Z,2024-02-26T03:34:53Z,6 days 08:53:18,add metricthreshold argument bootstrapfewshot currently work line bootstrapfewshot currently check example success selfmetric none selfmetricexample prediction trace success true wrap predictor input outputs example object append work great boolean metric maybe want extend metricthreshold argument float int metric return type metric threshold metricthreshold add default none line replace selfmetric metricval selfmetricexample prediction trace selfmetricthreshold success metricval selfmetricthreshold else success metricval else success true retain float metric like llm rating metric also add little strength find high quality example bootstrapfewshot
226,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/411,unplanned,Unknown,Enable passing a non-default profile to dspy.Bedrock For workflows using aws profiles other than the default one.,2024-02-19T15:05:40Z,2024-03-18T22:16:25Z,28 days 07:10:45,enable pass nondefault profile dspybedrock workflows use aws profile default one
227,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/410,unplanned,Unknown,"Add CohereVectorizer for Embed 3 Adds a new vectorizer for [Cohere's Embed models](https://docs.cohere.com/reference/embed) analogous to OpenAIVectorizer. Details on the performance of the Embed 3 model can be found [here](https://txt.cohere.com/introducing-embed-v3/)

Of note, the Vectorizer interface doesn't specify whether a query versus a document is being embedded, but Cohere's embedding models produce different embeddings depending on the content you want to embed. The different options are in the docs, but taking this into account can product a fairly dramatic boost to performance.",2024-02-19T14:59:11Z,2024-02-21T15:39:30Z,2 days 00:40:19,add coherevectorizer embed add new vectorizer analogous openaivectorizer detail performance embed model find note vectorizer interface doesnt specify whether query versus document embed coheres embed model produce different embeddings depend content want embed different option docs take account product fairly dramatic boost performance
228,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/407,unplanned,Unknown,"[BUG FIX] Switching trial.suggest_int --> trial.suggest_categorical BUG FIX: Switching suggest_int --> suggest_categorical for suggesting the indices to use for our instruction / demo set in each optuna trial. This is because Optuna's suggest_int assumes that the performance of each selected number is related, which is not the case in our application. suggest_categorical does not have this built in assumption, which is what our application requires.",2024-02-18T22:27:11Z,2024-02-18T22:28:20Z,0 days 00:01:09,switch trialsuggestint trialsuggestcategorical bug fix switch suggestint suggestcategorical suggesting index use instruction demo set optuna trial optunas suggestint assume performance select number related case application suggestcategorical build assumption application require
229,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/406,unplanned,Unknown,"Added saving/loading instructions to Predict and Chain_Of_Thought After the optimizers change the instruction we need to be able to save/load the optimized instructions.  Currently the original (unoptimized) instructions are loaded and only the demos are saved.

This is a quick fix that adds this change for Predict and Chain of Thought.  If more modules are added for instruction optimization then this fix will need to be added to those modules as well.

Thanks to @klopsahlong for help with testing/validating the changes!",2024-02-18T22:09:10Z,2024-02-18T22:31:55Z,0 days 00:22:45,add savingloading instruction predict chainofthought optimizers change instruction need able saveload optimize instruction currently original unoptimized instruction load demo save quick fix add change predict chain think module add instruction optimization fix need add module well thanks klopsahlong help testingvalidating change
230,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/404,unplanned,Unknown,[MINOR] Typo Correction autoamtic -> automatic,2024-02-18T07:48:20Z,2024-02-18T12:02:33Z,0 days 04:14:13,typo correction autoamtic automatic
231,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/403,unplanned,Unknown,"OpenAI v1 Migration Migrated OpenAI GPT3 LM client to v1's OpenAI/AzureOpenAI clients.
The previous version, had kwarg inconsistencies with Azure calls.

Fixes #377 ",2024-02-16T23:17:40Z,2024-02-27T04:40:04Z,10 days 05:22:24,openai migration migrate openai lm client openaiazureopenai client previous version kwarg inconsistency azure call fix
232,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/402,unplanned,Unknown,chore: Bump Qdrant dependencies Updates the dependencies of the `qdrant` extra to latest.,2024-02-16T10:41:00Z,2024-02-17T05:22:16Z,0 days 18:41:16,chore bump qdrant dependency update dependency qdrant extra late
233,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/400,unplanned,Unknown,"Refactored ChromadbRM to use generic EmbeddingFunction I've updated the ChromadbRM retriever to be more flexible and fix a bug:

- **Flexibility with Embeddings**: Removed the direct dependency on OpenAI, allowing for the use of various embedding functions from [Chroma](https://docs.trychroma.com/embeddings). This change makes the retriever more adaptable to different needs.

- **Bug Fix**: Fixed issue #389 where `_get_embeddings` threw a TypeError due to an unexpected keyword argument 'engine'. This ensures smoother functionality.

This revision enhances ChromadbRM by introducing flexibility in embedding selection and resolving a critical error.",2024-02-15T23:49:46Z,2024-02-17T04:03:58Z,1 days 04:14:12,refactored chromadbrm use generic embeddingfunction ive update chromadbrm retriever flexible fix bug flexibility embeddings remove direct dependency openai allow use various embed function change make retriever adaptable different need bug fix fix issue getembeddings throw typeerror due unexpected keyword argument engine ensure smooth functionality revision enhance chromadbrm introduce flexibility embed selection resolve critical error
234,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/398,unplanned,Unknown,"Create Google LM Followed the Cohere LM template. It doesn't look like you can sample multiple generations in one call to the API (linked the Google API docs I'm following in the code). Outside of that, I think it is a pretty standard implementation in sync with the other LMs.

Comment:
- Cool to get a better understanding of how `history` is stored as internal state to the LMs as a `list[dict[str, Any]]`. I would like to explore adding functionality around inspect_history next.",2024-02-15T22:12:28Z,2024-02-17T05:15:42Z,1 days 07:03:14,create google lm follow cohere lm template doesnt look like sample multiple generation one call api link google api doc im follow code outside think pretty standard implementation sync lms comment cool get well understanding history store internal state lm list would like explore add functionality around inspecthistory next
235,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/396,unplanned,Unknown,"fixes _get_embeddings for ChromaDB As described in #389 _get_embeddings is throwing an error when called via retriever(query): _TypeError: Embeddings.create() got an unexpected keyword argument 'engine'._

This is a result of the unnecessary usage of model_args and api_provider. I removed both and now the function works as expected.",2024-02-15T16:15:28Z,2024-02-17T04:02:56Z,1 days 11:47:28,fix getembeddings chromadb describe getembeddings throw error call via retrieverquery typeerror embeddingscreate get unexpected keyword argument engine result unnecessary usage modelargs apiprovider remove function work expect
238,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/393,unplanned,Unknown,fix dspy.assert backtracking None,2024-02-14T22:16:05Z,2024-02-14T22:19:30Z,0 days 00:03:25,fix dspyassert backtrack none
239,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/391,unplanned,Unknown,"[BugFix] QdrantRM Patch, add  long_text key to passages This PR adds the `long_text` key to the `passages` returned by the QdrantRM.

In dsp/primitives/search line 10: `passages = [psg.long_text for psg in passages]` throws an error without this.

I'm not aware of the full context, but this PR will patch the QdrantRM for now.",2024-02-14T21:21:17Z,2024-02-15T01:40:18Z,0 days 04:19:01,qdrantrm patch add longtext key passage pr add longtext key passage return qdrantrm dspprimitivessearch line passage throw error without im aware full context pr patch qdrantrm
241,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/388,unplanned,Unknown,"Bedrock args hotfix Fixing bugs mentioned here where `n` and `max_tokens` throw errors when passed as `kwargs`:
https://github.com/stanfordnlp/dspy/issues/365",2024-02-14T17:03:48Z,2024-02-17T05:17:41Z,2 days 12:13:53,bedrock args hotfix fix bug mention n maxtokens throw error pass kwargs
242,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/387,unplanned,Unknown,"Allow to set LM and RM per module on constructors and forward func This PR allows to set LM and RM per module on their constructors or dynamically when calling them.

Note that I removed the `query_only=True` parameter on the existent `with dsp.settings.context` in the predictor `forward` because I checked and that parameter is only used on `dsp.TemplateV2` but dspy is using `dsp.TemplateV3`.",2024-02-14T15:40:59Z,2024-02-18T10:54:01Z,3 days 19:13:02,allow set lm rm per module constructor forward func pr allows set lm rm per module constructor dynamically call note remove queryonlytrue parameter existent dspsettingscontext predictor forward check parameter use dspy use
245,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/380,unplanned,Unknown,"Fix CohereAPIError Discussed on Discord. Removed `return_likelihoods` argument, per error @erika-cardenas found.",2024-02-12T20:55:45Z,2024-02-12T22:42:22Z,0 days 01:46:37,fix cohereapierror discuss discord remove returnlikelihoods argument per error erikacardenas find
246,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/379,unplanned,Unknown,"Add support to load from any HF dataset and CSV Code Snippet:
```python
dl = DataLoader(
    train_size = 0.5,
    dev_size = 0.2,
    test_size = 0.3   
)

dl.from_huggingface(""databricks/databricks-dolly-15k"")

print(len(dl.train))
print(len(dl.dev))
print(len(dl.test))
```",2024-02-12T13:31:56Z,2024-02-13T18:31:52Z,1 days 04:59:56,add support load hf dataset csv code snippet python dl dataloader trainsize devsize testsize printlendltrain printlendldev printlendltest
250,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/374,unplanned,Unknown,"Return evaluation results # Output Retrieval in Evaluate Class

## Overview
Related to : https://github.com/stanfordnlp/dspy/issues/373 
This PR introduces a new feature to the `Evaluate` class that allows users to optionally retrieve the raw output predictions alongside the evaluation metrics. The goal of this enhancement is to provide more transparency and facilitate a deeper analysis of model performance during the evaluation phase.

## Changes
- Added a `return_outputs` parameter to the `__call__`  and constructor method of the `Evaluate` class.
- When `return_outputs` is set to `True`, the method returns a tuple containing the evaluation score and a list of tuples, each containing an example, its prediction, and the corresponding score.
- Ensured backward compatibility by setting the default value of `return_outputs` to `False`.
- Utilized the new feature in the baleen comparison in intro.ipynb (this is on its own commit that can be rolled back if needed to support the cache logic in intro.ipynb)

## Benefits
- **Transparency:** Users can now see the exact predictions made by their models for each input example.
- **Analysis:** Facilitates detailed error analysis and helps identify patterns in model predictions that may require further investigation.
- **Iterative Development:** By examining model outputs, developers can make targeted adjustments, leading to more effective model tuning.

## Usage
```python
evaluator = Evaluate(devset=my_devset, metric=my_metric, return_outputs=True)
evaluation_score, outputs = evaluator(my_program)
``` ",2024-02-11T10:53:07Z,2024-02-11T17:19:38Z,0 days 06:26:31,return evaluation result output retrieval evaluate class overview relate pr introduces new feature evaluate class allow user optionally retrieve raw output prediction alongside evaluation metric goal enhancement provide transparency facilitate deeper analysis model performance evaluation phase change add returnoutputs parameter call constructor method evaluate class returnoutputs set true method return tuple contain evaluation score list tuples contain example prediction correspond score ensure backward compatibility set default value returnoutputs false utilized new feature baleen comparison introipynb commit roll back need support cache logic introipynb benefit transparency user see exact prediction make model input example analysis facilitate detailed error analysis help identify pattern model prediction may require investigation iterative development examine model output developer make targeted adjustment lead effective model tune usage python evaluator evaluatedevsetmydevset metricmymetric returnoutputstrue evaluationscore output evaluatormyprogram
252,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/372,unplanned,Unknown,"documentation and QuizGen+TweetGen notebooks -documentation for DSPy Assertions
-example notebooks for the QuizGen and TweetGen tasks",2024-02-11T06:46:04Z,2024-02-15T18:10:14Z,4 days 11:24:10,documentation quizgentweetgen notebooks documentation dspy assertion example notebook quizgen tweetgen task
253,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/371,unplanned,Unknown,"fix: manually iterate kwargs['n'] in anyscale Hi,

I've noticed that `dsp.modules.hf_client.Anyscale` fails to produce multiple completions when a `dspy.predict.predict.Predict` object using it has `n>1`. The LLM provider itself does not support this option for now, so I was thinking this may be something DSPy could handle instead.

The following code snippet reproduces the issue:

```python
import os
import dspy

os.environ[""ANYSCALE_API_BASE""] = 'https://api.endpoints.anyscale.com/v1'
os.environ[""ANYSCALE_API_KEY""] = 'esecret_123'  # set to match your anyscale endpoints key

llm = dspy.Anyscale(model='mistralai/Mistral-7B-Instruct-v0.1', max_tokens=1000)
dspy.configure(lm=llm)

N = 3
generate_answer = dspy.Predict('question -> answer', n=N)
r = generate_answer(question='Why did the chicken cross the road?')

assert len(r.completions.answer) == N, f""mismatch! expected {N} but found {len(r.completions.answer)}""
```

With the code on this PR, the above error no longer occurs and completions work as expected. 

Please have a look and let me know if I've missed anything!",2024-02-11T05:37:49Z,2024-02-17T05:18:16Z,5 days 23:40:27,fix manually iterate kwargs anyscale hi ive notice dspmoduleshfclientanyscale fail produce multiple completion dspypredictpredictpredict object use llm provider support option thinking may something dspy could handle instead follow code snippet reproduces issue python import o import dspy osenviron osenviron set match anyscale endpoint key llm dspyconfigurelmllm n generateanswer dspypredictquestion answer nn r generateanswerquestionwhy chicken cross road assert lenrcompletionsanswer n fmismatch expect n find lenrcompletionsanswer code pr error longer occurs completion work expect please look let know ive miss anything
254,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/369,unplanned,Unknown,"Added Ollama and Together model usage information This PR adds descriptions for Ollama and Together model usages in the notebook and md file.

Hi @okhat 
PTAL and let me know if you have any feedback.
Thank you.",2024-02-10T23:45:28Z,2024-02-11T07:13:43Z,0 days 07:28:15,added ollama together model usage information pr add description ollama together model usage notebook md file hi okhat ptal let know feedback thank
255,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/368,unplanned,Unknown,"New Signature class based on pydantic + unit tests This PR creates a new Signature class based on Pydantic, which allows for type signatures.
It doesn't actually do anything with the types for now, but it's the first step.

Maybe more importantly, the PR creates 85 unit tests, covering most of `dspy/`, which were important to make sure the new type signature code didn't break anything, and will hopefully be useful for future refactors.",2024-02-10T21:10:46Z,2024-02-15T17:19:23Z,4 days 20:08:37,new signature class base pydantic unit test pr create new signature class base pydantic allows type signature doesnt actually anything type first step maybe importantly pr creates unit test cover dspy important make sure new type signature code didnt break anything hopefully useful future refactors
257,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/366,unplanned,Unknown,"Fixing bayesian optimizer bug with prompt / task model Previously the prompt and task models were defaulting to ""None"". Now they are defaulting to `dspy.settings.lm`.",2024-02-09T21:42:26Z,2024-02-09T21:43:40Z,0 days 00:01:14,fix bayesian optimizer bug prompt task model previously prompt task model default none default dspysettingslm
259,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/363,unplanned,Unknown,"fix(gmsk8_metric): Look for answer in entire LLM output. Not just the First Line I was going through one of the initial documentation examples.
I noticed that the eval function, `gsm8k_metric`, was returning False even when the LLM (GPT) was giving the correct answer in its response output. It's because the eval metric was calling `parse_integer_answer` which has an argument `only_first_line=True`. Shouldn't the output from the LLM be considered a correct answer even if the answer is at the end of the LLM output and not on the first line? Is there a reason the answer needs to be on the first line? 

The other changes are all linting/formatting changes forced by the ruff linter/formatter when running the pre-commit check. Is that expected? Thanks!",2024-02-09T11:36:55Z,2024-02-09T13:05:22Z,0 days 01:28:27,look answer entire llm output first line go one initial documentation example notice eval function return false even llm gpt give correct answer response output eval metric call parseintegeranswer argument onlyfirstlinetrue shouldnt output llm consider correct answer even answer end llm output first line reason answer need first line change lintingformatting change force ruff linterformatter run precommit check expected thanks
261,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/361,unplanned,Unknown,"Fix a few small typos Thank you for the package! 

When reading through the code, I noticed a few typos, so I took the liberty to fix them.

Only one of them is relevant for performance (it's in the prompt for Bayesian teleprompter).",2024-02-09T10:18:38Z,2024-02-17T05:22:01Z,7 days 19:03:23,fix small typo thank package read code notice typos take liberty fix one relevant performance prompt bayesian teleprompter
262,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/360,unplanned,Unknown,"Allow custom text key to WeaviateRM - resolve issue #359 Lets you specify which text key to return from a call to `dspy.Retrieve`

This does not incur a breaking change thanks to the `Optional` typing and default argument value.

This will resolve https://github.com/stanfordnlp/dspy/issues/359.",2024-02-09T03:38:03Z,2024-02-17T05:31:54Z,8 days 01:53:51,allow custom text key weaviaterm resolve issue let specify text key return call dspyretrieve incur break change thanks optional type default argument value resolve
264,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/358,unplanned,Unknown,"Lower default `max_labeled_demos` in `BootstrapFewShot` Hey team, I think it would be good to lower the default `max_labeled_demos` from 16 to 8. I have been playing with answering FAQs with 16 and it quickly results in compiling a massive prompt such that I exceed the max token limit. There is probably another way around this -- like ofc setting the argument to 8 or 4 or whatever, but I think this might slightly improve the UX for beginners starting with QA RAG and `GPT-3.5-Turbo` / ~4K input length LLMs.",2024-02-08T21:50:34Z,2024-02-19T18:28:24Z,10 days 20:37:50,low default maxlabeleddemos bootstrapfewshot hey team think would good low default maxlabeleddemos play answer faqs quickly result compile massive prompt exceed max token limit probably another way around like ofc set argument whatever think might slightly improve ux beginner start qa rag input length llm
267,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/354,planned,Unknown,"Easier api for transforming modules with assertions Now, it is possible to write `assert_transform_module(module)` to transform a module to handle suggestions and assertions. The default behavior is implemented by the `backtrack_handler`, which automatically backtracks for failing assertions and suggestions.",2024-02-08T00:25:50Z,2024-02-11T03:14:46Z,3 days 02:48:56,easy api transform module assertion possible write asserttransformmodulemodule transform module handle suggestion assertion default behavior implement backtrackhandler automatically backtrack fail assertion suggestion
268,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/353,unplanned,Unknown,fix react.py #276  fix #276 ,2024-02-07T19:33:29Z,2024-02-19T00:40:47Z,11 days 05:07:18,fix reactpy fix
269,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/352,unplanned,Unknown,"fix(datasets): relax datasets version constraint If you try and add dspy with poetry to an existing project with existing datasets version already installed you may get poetry errors. For example, I tried installing into a project that depended on datasets > 2.15 and so I couldn't install dspy. This was recently fixed for openai version constraint in a similar  [PR](https://github.com/stanfordnlp/dspy/pull/347). Can we do the same for datasets? Im not sure if there was a reason it was pinned to 2.14. A better solution might even be > 2.14 but let me know.


PS: Is there a way to build the environment and run a series of unit tests? I did not see anything about that in the CONTRIBUTING.md. The pre-commit config just does linting/formatting as far a I can tell. 

",2024-02-07T11:47:59Z,2024-02-17T05:31:11Z,9 days 17:43:12,fixdatasets relax datasets version constraint try add dspy poetry exist project exist datasets version already instal may get poetry error example try instal project depend datasets couldnt install dspy recently fix openai version constraint similar datasets im sure reason pin good solution might even let know ps way build environment run series unit test see anything contributingmd precommit config lintingformatting far tell
270,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/351,unplanned,Unknown,Fixed Typo - Update skycamp2023.ipynb None,2024-02-07T11:42:07Z,2024-02-16T04:50:56Z,8 days 17:08:49,fix typo update none
271,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/350,unplanned,Unknown,Fixed Typo - Update skycamp2023.ipynb None,2024-02-07T11:37:59Z,2024-02-11T05:06:13Z,3 days 17:28:14,fix typo update none
272,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/349,unplanned,Unknown,Fixed typo - Update skycamp2023.ipynb None,2024-02-07T11:11:47Z,2024-02-07T11:15:07Z,0 days 00:03:20,fix typo update none
273,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/348,unplanned,Unknown,"Allow HFModel to use CPU Quick fix https://github.com/stanfordnlp/dspy/issues/188

I think this minor fix could be very helpful for Mac beginners as well as CPU beginners who just get started using the library.",2024-02-07T10:45:25Z,2024-02-19T00:36:39Z,11 days 13:51:14,allow hfmodel use cpu quick fix think minor fix could helpful mac beginner well cpu beginner get start use library
274,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/347,unplanned,Unknown,"relax openai version constraint Since dspy does in fact support both 0.x and 1.x with [1], the constraints are updated here to reflect that, allowing users of the library to make use of whatever version they would like.

Intended to address https://github.com/stanfordnlp/dspy/issues/258

I opted not to update the poetry.lock file here in order to have this be a minimal change aimed at allowing for more flexible  usage by others, rather than changing the defaults for the project, though an upgrade of the default may make sense as v1 of the OpenAI lib has been out for three months now and is fairly stable.

[1] https://github.com/stanfordnlp/dspy/commit/40c519adc0b848b998d028d1b5d73c6dc631696e (and follow-on work)",2024-02-06T19:01:58Z,2024-02-06T19:03:07Z,0 days 00:01:09,relax openai version constraint since dspy fact support constraint update reflect allow user library make use whatever version would like intended address opt update poetrylock file order minimal change aim allow flexible usage others rather change default project though upgrade default may make sense openai lib three month fairly stable followon work
276,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/345,unplanned,Unknown,"DSPy-Clarifai integration - modified example notebook Objective :
1. Modified notebook example with 2 more models.",2024-02-06T06:16:04Z,2024-02-27T06:04:11Z,20 days 23:48:07,dspyclarifai integration modify example notebook objective modify notebook example model
277,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/343,unplanned,Unknown,"CoT fix + move torch dependeny when using vllm Very cool project üíØ 

This PR fixes a typo introduced in [#333](https://github.com/stanfordnlp/dspy/pull/333). The signature is undefined in this context.

Additionally, I moved the torch dependency to only be required when running the model locally through hf and not when a vllm endpoint is called.",2024-02-05T11:21:46Z,2024-02-06T15:58:09Z,1 days 04:36:23,cot fix move torch dependeny use vllm cool project pr fix typo introduce signature undefined context additionally move torch dependency require run model locally hf vllm endpoint call
279,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/339,unplanned,Unknown,"Small Fix to vLLM Launch Documentation The current documentation for running vLLM with DSPy suggests that the user start an API server via ```python -m vllm.entrypoints.api_server```. However, this leads to errors since the DSPy code assumes an OpenAI-Compatible API server. So the user should instead launch the server via ```python -m vllm.entrypoints.openai.api_server```.",2024-02-04T21:23:28Z,2024-02-04T21:35:13Z,0 days 00:11:45,small fix vllm launch documentation current documentation run vllm dspy suggest user start api server via python vllmentrypointsapiserver however lead error since dspy code assume openaicompatible api server user instead launch server via python vllmentrypointsopenaiapiserver
282,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/335,unplanned,Unknown,"Update together model prompt and stop sequence 
    Update together model prompt and stop sequence
    
    - update prompt based on the model name
    - update stop sequence

*TEST*:
- the following models are tested using `intro.ipynb`
- completion model
```
model=""mistralai/Mistral-7B-v0.1""
turbo = dspy.Together(model=model)
```
- instruction mode
```
model=""mistralai/Mistral-7B-Instruct-v0.2""
turbo = dspy.Together(model=model)
```

",2024-02-03T22:17:22Z,2024-02-05T16:52:55Z,1 days 18:35:33,update together model prompt stop sequence update together model prompt stop sequence update prompt base model name update stop sequence test follow model test use introipynb completion model turbo dspytogethermodelmodel instruction mode turbo dspytogethermodelmodel
284,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/333,unplanned,Unknown,Add retry capabilities to Predict Fixed #310 ,2024-02-02T21:07:57Z,2024-02-02T21:21:06Z,0 days 00:13:09,add retry capability predict fix
285,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/332,unplanned,Unknown,Assert aware metrics None,2024-02-02T20:26:02Z,2024-02-02T20:28:38Z,0 days 00:02:36,assert aware metric none
286,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/331,unplanned,Unknown,"Minor update on chroma rm init - testing tested with chroma==0.4.17

*Note*: this change is not related to the new cohort.

Thank you and let me know your feedback, @okhat 


",2024-02-02T20:01:40Z,2024-02-03T18:29:23Z,0 days 22:27:43,minor update chroma rm init test tested note change relate new cohort thank let know feedback okhat
287,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/330,unplanned,Unknown,"feat(bedrock-lm): Added a LM for Bedrock and a base AWS LM class Created two new LM classes:
- Bedrock
- AWSLM

Bedrock can be used with any Bedrock model once you have configured your AWS credentials with the AWS CLI. AWSLM exists so that we can add support for AWS Sagemaker in the future, which has a very similar API with minor parameter differences.",2024-02-02T17:14:05Z,2024-02-04T21:34:49Z,2 days 04:20:44,featbedrocklm add lm bedrock base aws lm class create two new lm class bedrock awslm bedrock use bedrock model configure aws credential aws cli awslm exist add support aws sagemaker future similar api minor parameter difference
288,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/329,unplanned,Unknown,"Adding code to access together APIs - sameple code
```
lm = dspy.Together(model=""mistralai/Mistral-7B-Instruct-v0.2"")
```
- registration $25 free credit on registration
Register: https://api.together.xyz/

- models: https://api.together.xyz/playground

We recommend to start with mistral, llama2.

- api usage

- document

https://docs.together.ai/docs/quickstart",2024-02-02T10:24:18Z,2024-02-02T12:24:37Z,0 days 02:00:19,add code access together apis sameple code lm registration free credit registration register model recommend start mistral api usage document
290,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/326,unplanned,Unknown,[feat] deeplake retriever implementation None,2024-02-01T20:44:42Z,2024-02-19T09:24:55Z,17 days 12:40:13,deeplake retriever implementation none
291,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/325,unplanned,Unknown,"Ollama fix for TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'  Closes #293 

Solves the issue using: https://github.com/stanfordnlp/dspy/issues/293#issuecomment-1921965485",2024-02-01T18:49:17Z,2024-02-03T18:21:05Z,1 days 23:31:48,ollama fix typeerror unsupported operand type nonetype int close solves issue use
292,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/323,unplanned,Unknown,"Update the code to use ollama, mute verbose print - the following code is tested as part of the `intro.ipynb`
```
mistral = dspy.OllamaLocal(model='mistral')
dspy.settings.configure(lm=mistral, rm=colbertv2_wiki17_abstracts)
```

@okhat , @forwardprogress please take look",2024-02-01T08:42:42Z,2024-02-01T08:58:56Z,0 days 00:16:14,update code use ollama mute verbose print follow code test part introipynb mistral dspyollamalocalmodelmistral dspysettingsconfigurelmmistral okhat forwardprogress please take look
294,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/320,unplanned,Unknown,"[Bug Fix] WeaviateRM This should fix it, accidentally wrapped the return in a `dspy.Prediction`.

Sorry for the back and forth with this, pretty sure it is in good shape now.",2024-02-01T02:53:57Z,2024-02-02T00:57:32Z,0 days 22:03:35,weaviaterm fix accidentally wrap return dspyprediction sorry back forth pretty sure good shape
295,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/318,unplanned,Unknown,"Update README.md -> the `class RAG(dspy.Module)` section of the Docs It seems more correct to describe the function flow this way, first we retrieve the context then we use it and the question to generate an answer, but I'm not sure, I'm reading the docs for the first time and may have misunderstood how DSPy works.. :-)",2024-01-31T22:19:29Z,2024-01-31T22:20:28Z,0 days 00:00:59,update readmemd class ragdspymodule section doc seem correct describe function flow way first retrieve context use question generate answer im sure im reading doc first time may misunderstand dspy work
296,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/317,unplanned,Unknown,"Refactor Integration Examples and WeaviateRM Bug Fixes # [Docs] Refactor Integration Examples
Discussed on discord, this could be a cool way to organize integration notebooks in a cookbook style. This initial commit moves the clarifai example into the example/integrations/clarifai folder.

# [Bug Fix] WeaviateRM

- Apologies, this is the correct configuration of the `long_text` key, silly mistake in the last PR.

# [WIP] Local Weaviate DSPy Integration Test

Started sketching out an integration test with Weaviate embedded that will create a toy `Document` schema with a `content` key, load toy data, configure with `dspy.settings`, and check the type of `dspy.Retrieve(""query test"")`.",2024-01-31T21:39:49Z,2024-01-31T21:42:19Z,0 days 00:02:30,refactor integration examples weaviaterm bug fix refactor integration example discuss discord could cool way organize integration notebook cookbook style initial commit move clarifai example exampleintegrationsclarifai folder weaviaterm apology correct configuration longtext key silly mistake last pr local weaviate dspy integration test start sketch integration test weaviate embed create toy document schema content key load toy data configure dspysettings check type dspyretrievequery test
297,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/316,unplanned,Unknown,remove redundant line correct is re-defined in the next line,2024-01-31T21:25:56Z,2024-02-01T00:16:56Z,0 days 02:51:00,remove redundant line correct redefine next line
298,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/315,unplanned,Unknown,"[Bug Fix] WeaviateRM Patch, add `long_text` key to `passages` This PR adds the `long_text` key to the `passages` returned by the WeaviateRM.

In dsp/primitives/search line 10: `passages = [psg.long_text for psg in passages]` throws an error without this -- maybe worth making a note of removing this. 

Not aware of the full context, but this PR will patch the WeaviateRM for the time being.",2024-01-31T20:11:06Z,2024-01-31T20:24:06Z,0 days 00:13:00,weaviaterm patch add longtext key passage pr add longtext key passage return weaviaterm dspprimitivessearch line passages throw error without maybe worth make note remove aware full context pr patch weaviaterm time
300,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/312,unplanned,Unknown,"Make the IPython dependency optional This modification replaces the IPython import with a try-except block. If IPython is not available, it sets ipython_display to the built-in print function.

Related: https://github.com/stanfordnlp/dspy/pull/269",2024-01-31T13:58:03Z,2024-01-31T16:00:25Z,0 days 02:02:22,make ipython dependency optional modification replaces ipython import tryexcept block ipython available set ipythondisplay builtin print function relate
303,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/309,unplanned,Unknown,"Refactor `dsp.OllamaLocal`'s constructor Remove some hardcoded values from `__init__()`

Previous work: https://github.com/stanfordnlp/dspy/pull/262#discussion_r1440694439",2024-01-31T07:59:22Z,2024-01-31T16:01:54Z,0 days 08:02:32,refactor dspollamalocals constructor remove hardcoded value init previous work
304,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/308,unplanned,Unknown,README: fix link for finetuning example The finetuning example doesn't seem to finetune anything! I think https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/examples/qa/hotpot/multihop_finetune.ipynb is the relevant example here,2024-01-31T04:24:21Z,2024-01-31T04:37:06Z,0 days 00:12:45,readme fix link finetuning example finetuning example doesnt seem finetune anything think relevant example
306,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/305,unplanned,Unknown,Update understanding-signatures.md fix minor formatting bug,2024-01-29T21:52:12Z,2024-01-29T21:53:26Z,0 days 00:01:14,update understandingsignaturesmd fix minor formatting bug
307,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/304,unplanned,Unknown,"DSPy-Clarifai : Integration for LM and retrievers **Description:**
1. Added Clarifai as LLM provider and retrievers.
2. Added notebook examples, which shows how to call clarifai LM class and retrievers.",2024-01-29T11:00:54Z,2024-01-29T15:31:39Z,0 days 04:30:45,dspyclarifai integration lm retriever description add clarifai llm provider retriever add notebook example show call clarifai lm class retriever
308,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/303,unplanned,Unknown,Vectara retriever Implementation of a Vectara retirever,2024-01-29T04:37:43Z,2024-01-29T15:29:19Z,0 days 10:51:36,vectara retriever implementation vectara retirever
309,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/302,unplanned,Unknown,"Adding example awareness to bayesian teleprompter This PR makes the following changes:

1. Adds support for example aware metaprompting. Now users have the choice of using both data aware and example aware metaprompting, either together or separately. These choices can be specified with the `view_data` and `view_examples` flags.
2. Adds in a summarization step on top of data observations in order to condense observations into a 2-3 sentence summary. We've found that this helps to make suggested instructions better, potentially because the metaprompt is less bogged down by extra text. (thanks @XenonMolecule for this solution!)",2024-01-29T02:24:34Z,2024-01-29T02:25:36Z,0 days 00:01:02,add example awareness bayesian teleprompter pr make follow change add support example aware metaprompting user choice use data aware example aware metaprompting either together separately choice specify viewdata viewexamples flag add summarization step top data observation order condense observation sentence summary weve find help make suggested instruction well potentially metaprompt less bogged extra text thanks xenonmolecule solution
310,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/301,unplanned,Unknown,"docs(dspy): Update CONTRIBUTING.md file with bounty board ## üìù Changes Description

This MR/PR contains the following changes:
Add a bounty board to CONTRIBUTING.md to track desired features
Fixes small typos in CONTRIBUTING.md

## ‚úÖ Contributor Checklist

- [] Pre-Commit checks are passing (locally and remotely)
- [x] Title of your PR / MR corresponds to the required format
- [] Commit message follows required format {label}(dspy): {message}

## ‚ö†Ô∏è Warnings

N/A
",2024-01-27T05:58:07Z,2024-01-27T06:13:15Z,0 days 00:15:08,docsdspy update contributingmd file bounty board change description mrpr contains follow change add bounty board contributingmd track desire feature fix small typo contributingmd contributor checklist precommit check pass locally remotely title pr mr corresponds require format commit message follow require format labeldspy message warning na
314,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/296,unplanned,Unknown,"Updating Cohere model names and support This PR points to Cohere's existing generation models (previously defined models are out of date). 

It also tweak the behavior in `predict.py` that required the llm provider to have a parameter called `n` (that OpenAI has) which is called `num_generations` on the Cohere side. The previous behavior prevented the llm from running (it would throw an error due to the additional unknown parameter `n`).",2024-01-25T09:02:06Z,2024-01-29T15:29:57Z,4 days 06:27:51,update cohere model name support pr point cohere exist generation model previously define model date also tweak behavior predictpy require llm provider parameter call n openai call numgenerations cohere side previous behavior prevent llm run would throw error due additional unknown parameter n
316,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/294,unplanned,Unknown,Dspy docs None,2024-01-23T16:52:48Z,2024-01-23T16:53:09Z,0 days 00:00:21,dspy doc none
318,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/292,unplanned,Unknown,"Support convenience import for `OllamaLocal` ### Summary

**tldr;** These changes add a convenience import for the `OllamaLocal` import introduced in https://github.com/stanfordnlp/dspy/pull/262 consistent with what's done for GPT-3.5, et al. I also added a minimal test to show the before/after comparison.

I began working through [`intro.ipynb`](https://github.com/stanfordnlp/dspy/blob/main/intro.ipynb) today and wanted to use a model I was hosting locally with Ollama instead of GPT-3.5. Though the `OpenAI` symbol is actually a reference to `dsp.modules.GPT3`, there's some hoisting to make it a little more convenient to access:
```python
turbo = dspy.OpenAI(model='gpt-3.5-turbo')
```

I expected to do the same for Ollama:

```python
nous_hermes2_mixtral = dsp.OllamaLocal(model=""nous-hermes2-mixtral"", model_type=""chat"")
```

but found it wasn't supported. I've made the appropriate change to `dsp/modules/__init__.py` to make the Ollama module consistent with its siblings.

I wanted to add a test demonstrating the change, but couldn't find any unit tests. I've added a new `tests/` package at the root of the repo to host all test suites, which I understand to be common convention. I've avoided adding any dependencies by relying on the built-in unit testing library in Python.

‚ö†Ô∏è These changes also propose running the new unit test as part of GitHub CI checks. I'm unfamiliar with GitHub's CI system, so if the CI change is acceptable, it deserves some scrutiny to make sure I didn't make a silly mistake.

### Testing

I added a unit test that verifies the convenience import is now supported. Unit tests can be executed from the command line with this command:

```sh-session
% python -m unittest discover tests/
```

**Before**:

```sh-session
E
======================================================================
ERROR: test_convenience_import_of_dsp_ollama_client (test_dsp.DspModuleImportsTest.test_convenience_import_of_dsp_ollama_client)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/forwardprogress/Developer/stanfordnlp/dspy/tests/test_dsp.py"", line 6, in test_convenience_import_of_dsp_ollama_client
    from dsp import OllamaLocal
ImportError: cannot import name 'OllamaLocal' from 'dsp' (/Users/forwardprogress/Developer/stanfordnlp/dspy/dsp/__init__.py)

----------------------------------------------------------------------
Ran 1 test in 0.479s

FAILED (errors=1)
```

**After**:

```sh-session
.
----------------------------------------------------------------------
Ran 1 test in 0.484s

OK
```",2024-01-22T07:58:12Z,2024-01-28T15:15:48Z,6 days 07:17:36,support convenience import ollamalocal summary tldr change add convenience import ollamalocal import introduce consistent whats do et al also add minimal test show beforeafter comparison begin work today want use model host locally ollama instead though openai symbol actually reference there hoist make little convenient access python turbo expect ollama python modeltypechat find wasnt support ive make appropriate change dspmodulesinitpy make ollama module consistent sibling want add test demonstrate change couldnt find unit test ive added new test package root repo host test suite understand common convention ive avoid add dependency rely builtin unit test library python change also propose run new unit test part github ci check im unfamiliar githubs ci system ci change acceptable deserve scrutiny make sure didnt make silly mistake test added unit test verifies convenience import support unit test execute command line command shsession python unittest discover test shsession e error testconvenienceimportofdspollamaclient testdspdspmoduleimportstesttestconvenienceimportofdspollamaclient traceback recent call last file usersforwardprogressdeveloperstanfordnlpdspyteststestdsppy line testconvenienceimportofdspollamaclient dsp import ollamalocal importerror cannot import name ollamalocal dsp usersforwardprogressdeveloperstanfordnlpdspydspinitpy run test fail shsession run test ok
319,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/291,unplanned,Unknown,"Add Optimizer Testing Framework and Data-Aware Bayesian Signature Optimization Optimizer Tester is intended to allow simple and repeatable testing of DSPy Optimizers.  This is a development tool for the creation of more optimizers and the verification that they work across tasks.  This is still under development and subject to change.

## Usage 

```python
task_model = dspy.HFClientTGI(...)
prompt_model = dspy.OpenAI(...)

tester = OptimizerTester(prompt_model=prompt_model, task_model=task_model)

tester.test_baseline(datasets=[""HotPotQA"", ""GSM8K"", ""Scone""])

# Create a function to call your custom optimizer
def your_optimizer_caller(default_program, trainset, devset, test_name, dataset_name, kwargs):
    # initialize your teleprompter (optimizer) here!
    teleprompter = my_custom_optimizer(metric=kwargs[""metric""], ...)

    # call your optimizer on the default_program here!
    compiled_program = teleprompter.compile(default_program.deepcopy(), trainset=trainset, ...)

    # if you wish to tweak any of the outputs to the csv file you can do that here
    output = {
        ""test_name"": ""my_optimizer-"" + dataset_name + ""-"" + test_name
    }

    # return the compiled program and modified output (or empty dict if no changes made)
    return compiled_program, output

tester.test_optimizer_default(your_optimizer_caller, datasets=[""BioDex"", ""Tweet"", ""Tweet Metric""])
```

See more about usage in the associated README file",2024-01-22T07:36:37Z,2024-01-22T19:19:27Z,0 days 11:42:50,add optimizer test framework dataaware bayesian signature optimization optimizer tester intend allow simple repeatable test dspy optimizers development tool creation optimizers verification work across task still development subject change usage python taskmodel dspyhfclienttgi promptmodel dspyopenai tester optimizertesterpromptmodelpromptmodel taskmodeltaskmodel testertestbaselinedatasets create function call custom optimizer def youroptimizercallerdefaultprogram trainset devset testname datasetname kwargs initialize teleprompter optimizer teleprompter mycustomoptimizermetrickwargs call optimizer defaultprogram compiledprogram telepromptercompiledefaultprogramdeepcopy trainsettrainset wish tweak output csv file output testname myoptimizer datasetname testname return compile program modify output empty dict change make return compiledprogram output testertestoptimizerdefaultyouroptimizercaller datasets see usage associate readme file
320,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/290,unplanned,Unknown,"bug fix to teleprompter, making runs deterministic - fixes bug that called an experimental version of inspect_history
- makes optuna runs deterministic",2024-01-21T21:30:13Z,2024-01-21T21:32:38Z,0 days 00:02:25,bug fix teleprompter make run deterministic fix bug call experimental version inspecthistory make optuna run deterministic
321,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/289,unplanned,Unknown,"Add WeaviateRM ## Adds WeaviateRM as a child class to `dspy.Retrieve`

Embedding Inference
- Note this assumes you are using a text2vec embedding service orchestrated in Weaviate

Hard-Coded Data Model
- The RM currently assumes 1 Collection with 1 Text Key and no metadata in the Vector DB. We believe `content` is the most common named text key for this. This is quite an interesting topic re data models for Vector DBs, but I think this model is a great point to start and keep building from here.

Batch Query API
- This integration is based on Weaviate's v3 python client, which does not yet support a batch query API. We are going to be releasing 1.24 very soon, and I will be updating this.

Aggregating Duplicate Passages
- This implementation assumes you will call `dspy.utils.deduplicate` if the module above will be sending multiple queries in one call to `dspy.Retrieve`. Maybe this is too naive, I need to learn a little more about the access patterns for this, I think in the multi-hop case, deduplicate should work well. It might make sense for us (Weaviate) to cook multiple query result aggregation in our batch query API. In addition to the data model broadly, I think this is another key opportunity for future work.

Small Note
- Interfaces with `dspy.Prediction` via a list of strings `passages`",2024-01-21T15:02:09Z,2024-01-21T15:35:55Z,0 days 00:33:46,add weaviaterm add weaviaterm child class dspyretrieve embed inference note assume use embed service orchestrate weaviate hardcoded data model rm currently assume collection text key metadata vector db believe content common name text key quite interesting topic data model vector dbs think model great point start keep building batch query api integration base weaviates python client yet support batch query api go release soon update aggregate duplicate passage implementation assumes call dspyutilsdeduplicate module send multiple query one call dspyretrieve maybe naive need learn little access pattern think multihop case deduplicate work well might make sense u weaviate cook multiple query result aggregation batch query api addition data model broadly think another key opportunity future work small note interface dspyprediction via list string passage
322,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/288,unplanned,Unknown,"Adding BayesianSignatureOptimizer This new optimizer utilizes optuna to jointly optimize fewshot examples and the instruction for a prompt, over a specified number of trials.

Work done in collaboration with @XenonMolecule (Michael Ryan).",2024-01-20T22:42:46Z,2024-01-20T22:43:56Z,0 days 00:01:10,add bayesiansignatureoptimizer new optimizer utilizes optuna jointly optimize fewshot example instruction prompt specify number trial work do collaboration xenonmolecule michael ryan
323,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/284,unplanned,Unknown,ops(dspy): Adding pull request template Adding the pull request template to standardize the way we describe changes (to be refined based on needs),2024-01-19T09:24:08Z,2024-01-20T12:18:44Z,1 days 02:54:36,opsdspy add pull request template add pull request template standardize way describe change refine base need
324,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/282,unplanned,Unknown,"ops(dspy): activating pre-commit global check (and aligning to py3.9) - aligning pre-commit version to py3.9 (do we need this version or can we start with >=3.10, which is x10 faster) and has lot's of nice features for typing etc
- activating pre-commit checks in the repo (you shall not pass principle)",2024-01-19T08:45:37Z,2024-01-22T13:30:40Z,3 days 04:45:03,opsdspy activate precommit global check align align precommit version need version start faster lots nice feature type etc activate precommit check repo shall pass principle
325,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/280,unplanned,Unknown,feat(dspy): adding missing precommit files Some files were deleted after last merge -> so adding them back,2024-01-18T21:00:56Z,2024-01-18T21:07:12Z,0 days 00:06:16,featdspy add miss precommit file file delete last merge add back
326,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/279,unplanned,Unknown,"docs(dspy): setting up `mkdocs` based (versioned) documentation - adding required dependencies (needed for CI/CD auto-build that we can setup later)
- setting up initial example structure and some basic guidelines",2024-01-18T20:42:53Z,2024-01-18T20:48:45Z,0 days 00:05:52,docsdspy set mkdocs base versioned documentation add required dependency need cicd autobuild setup later set initial example structure basic guideline
327,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/278,unplanned,Unknown,"ops(dspy): adding pre-commit + contrib.md initialization - Adding base configurationfor the new advanced pre-commit hook (.toml + pre-commit.yaml)
- Adding base config for pytests coverage reporting (.toml)
- Adding `CONTRIBUTE.md` to start explaining the basic process for setting up the pre-commit hook
",2024-01-18T16:56:12Z,2024-01-18T20:48:47Z,0 days 03:52:35,opsdspy add precommit contribmd initialization add base configurationfor new advanced precommit hook toml precommityaml add base config pytests coverage reporting toml add contributemd start explain basic process set precommit hook
331,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/274,planned,Unknown,"Assertion-aware metrics and optimization **Tasks**
- [x] option to enable assert-aware-metric for each assert/suggest (i.e., optimizable): 
`dspy.Suggest(constraint, msg, is_metric)`
- [x] track suggest/assert failures during evaluation
- [x] propagate suggest/assert failures as an attribute of a bootstrapped program
- [x] penalize optimization score according to the number of failures
- [x] incorporate feedback traces into compilation",2024-01-15T02:24:47Z,2024-02-02T18:25:22Z,18 days 16:00:35,assertionaware metric optimization task option enable assertawaremetric assertsuggest ie optimizable dspysuggestconstraint msg ismetric track suggestassert failures evaluation propagate suggestassert failure attribute bootstrapped program penalize optimization score accord number failure incorporate feedback trace compilation
332,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/272,unplanned,Unknown,WIP Docs None,2024-01-14T00:54:32Z,2024-01-14T02:43:07Z,0 days 01:48:35,wip doc none
333,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/271,unplanned,Unknown,WIP Docs (Don't merge yet!) It's getting there! Will be updated with the last 4 days of changes soon. ,2024-01-12T21:42:41Z,2024-01-12T21:47:05Z,0 days 00:04:24,wip doc dont merge yet get update last day change soon
335,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/269,unplanned,Unknown,"Update evaluate.py This modification replaces the IPython import with a try-except block. If IPython is not available, it sets ipython_display to the built-in print function and defines a dummy HTML lambda function.",2024-01-12T12:37:59Z,2024-01-12T12:41:37Z,0 days 00:03:38,update evaluatepy modification replaces ipython import tryexcept block ipython available set ipythondisplay builtin print function defines dummy html lambda function
339,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/265,unplanned,Unknown,"Fix a nit for openai azure support. Issues:

After the change to support `openai==1.x` (#258), https://github.com/stanfordnlp/dspy/blob/main/dsp/modules/gpt3.py#L70 will always trigger assert when `api_provider == 'azure'`.

Changes:
- Fix this nit.
- Move `openai.api_type = api_provider` to better support switching api providers.

Remaining issues:
- `openai==1.x` suggests using client to manage configurations. It seems that the global configurations do not work for OpenAI Azure service any more. #258 is not fully solved.",2024-01-08T21:43:34Z,2024-01-08T21:49:38Z,0 days 00:06:04,fix nit openai azure support issue change support always trigger assert apiprovider azure change fix nit move openaiapitype apiprovider well support switch api provider remain issue suggest use client manage configuration seem global configuration work openai azure service fully solve
341,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/263,unplanned,Unknown,Add sglang backend None,2024-01-04T03:52:41Z,2024-01-04T15:54:42Z,0 days 12:02:01,add sglang backend none
342,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/262,unplanned,Unknown,"Adding local model integration with Ollama This adds integration for fully local models. Integration is currently through [Ollama](https://github.com/jmorganca/ollama/) which serves the models on localhost. Follows gpt3.py.

Notes: I had issues using httpx with non-stream output so using requests instead. Tested with intro notebook and with some of the advanced notebooks.",2024-01-02T13:24:05Z,2024-01-03T17:12:47Z,1 days 03:48:42,add local model integration ollama add integration fully local model integration currently serve model localhost follow note issue use httpx nonstream output use request instead test intro notebook advance notebook
343,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/261,unplanned,Unknown,"openai 0.28 backwards compatibility for pinecone and chromadb Hi @okhat, sorry for the late response, I think this changes should fix the compatibility with 0.28",2023-12-28T22:15:11Z,2024-01-12T12:43:00Z,14 days 14:27:49,openai backwards compatibility pinecone chromadb hi okhat sorry late response think change fix compatibility
344,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/260,unplanned,Unknown,"Bootstrapping ScoNe CoT demos with a separate LLM from the main one Hi @okhat!

This is a simple example showing how to use a different (and presumably more powerful) model for bootstrapping examples than the one being used for the central predictions.

The example task is the ScoNe ""one scoping negation"" category, which is one of the hardest categories in ScoNe. Zero-shot, turbo is at chance, and using turbo to bootstrap full CoT examples seemed not to work well, but using GPT-4 for bootstrapping (a possibility you made me aware of!) took performance all the way north of 85% accuracy (and one of my runs was at 93%). This extremely high, but everything seems to be set up correctly, so this seems like a nice illustration of the power of this strategy.

Do let me know if I should make an adjustments to the way the notebook is set up. The hope is that this is code people can copy-paste for their own work (almost none of the DSPy code is even specific to ScoNe).

---Chris",2023-12-27T03:54:09Z,2023-12-27T04:00:42Z,0 days 00:06:33,bootstrapping scone cot demo separate llm main one hi okhat simple example show use different presumably powerful model bootstrapping example one used central prediction example task scone one scoping negation category one hardest categories scone zeroshot turbo chance use turbo bootstrap full cot example seem work well use bootstrapping possibility make aware take performance way north accuracy one run extremely high everything seem set correctly seem like nice illustration power strategy let know make adjustment way notebook set hope code people copypaste work almost none dspy code even specific scone chris
345,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/259,unplanned,Unknown,Openai v1 None,2023-12-26T17:39:53Z,2023-12-26T17:41:58Z,0 days 00:02:05,openai none
347,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/257,unplanned,Unknown,(250) - Add MongoDB Atlas Retrieval Model Added MongoDB Atlas Retrieval Model. Currently only supports OpenAI embedding provider.,2023-12-26T06:16:15Z,2024-01-13T02:57:57Z,17 days 20:41:42,add mongodb atlas retrieval model add mongodb atlas retrieval model currently support openai embed provider
348,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/256,unplanned,Unknown,"Updates to SignatureOptimizer [bug fixes + track stats] In this PR, we make a few small updates to the SignatureOptimizer teleprompter:

1. We fix a bug in the previous implementation, where the `prompt_model` defaulted to a string, which threw an error. We now default `prompt_model` to None, and use the default lm as our prompt model when this is the case.
2. We fix a bug where duplicate prompt candidates were being included in the prompt history.
3. We add in the ability to track statistics of the quality of prompts found over each iteration of the SignatureOptimizer algorithm with the field `track_stats`",2023-12-24T19:42:24Z,2023-12-24T19:43:09Z,0 days 00:00:45,update signatureoptimizer pr make small update signatureoptimizer teleprompter fix bug previous implementation promptmodel default string threw error default promptmodel none use default lm prompt model case fix bug duplicate prompt candidate include prompt history add ability track statistic quality prompt find iteration signatureoptimizer algorithm field trackstats
349,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/255,unplanned,Unknown,"Update for compatibility with openai ^1.0 #223 and I think that it can help with #180 , allowing the use of [LiteLLM](https://github.com/BerriAI/litellm) with [Ollama](https://github.com/jmorganca/ollama), I tested the changes with this",2023-12-24T06:15:58Z,2023-12-26T15:43:11Z,2 days 09:27:13,update compatibility openai think help allow use test change
350,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/254,unplanned,Unknown,"Add `pyproject.toml`, github action for compiling.  Addresses issue: https://github.com/stanfordnlp/dspy/issues/154

Goals: Allow poetry integration. Setup documentation dependencies, allow dev dependencies to be added in future. PEP-compliant. 

Also adds github action to ensure project can compile/build, dependencies are installed properly. ",2023-12-23T00:13:21Z,2024-01-13T00:49:00Z,21 days 00:35:39,add pyprojecttoml github action compile address issue goal allow poetry integration setup documentation dependency allow dev dependency add future pepcompliant also add github action ensure project compilebuild dependency instal properly
352,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/248,unplanned,Unknown,"Update hotpotqa_with_assertions notebook intrinsic metric Added intrinsic metric for hotpotqa_with_assertions:
Count how many unique query suggestions work at the end (i.e., all the generated queries are distinct from previous ones).",2023-12-14T21:10:42Z,2023-12-18T14:17:23Z,3 days 17:06:41,update hotpotqawithassertions notebook intrinsic metric add intrinsic metric hotpotqawithassertions count many unique query suggestion work end ie generate query distinct previous one
353,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/247,unplanned,Unknown,Pinecone fixes None,2023-12-10T08:52:33Z,2023-12-19T02:40:46Z,8 days 17:48:13,pinecone fix none
354,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/246,unplanned,Unknown,"added notebook for longformqa/assertions example example for LongFormQA, including usage of new computational constraints code",2023-12-09T19:48:50Z,2024-01-04T19:51:57Z,26 days 00:03:07,add notebook longformqaassertions example example longformqa include usage new computational constraint code
356,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/244,unplanned,Unknown,"Fixed Qdrant integration by adding ""k"" as an optional argument in Qdrant forward method TL;DR: Fixed Qdrant integration by simply adding a k argument to the forward function QdrantRM class

Currently, the Qdrant integration fails to work, throwing a TypeError when using Qdrant as retriever model.

In dspy/retrieve/retrieve.py, passages are retrieved using a retriever model (in this case Qdrant) with a list of queries, and with the argument `k=self.k`. 
```
    def forward(self, query_or_queries):
        queries = [query_or_queries] if isinstance(query_or_queries, str) else query_or_queries
        queries = [query.strip().split('\n')[0].strip() for query in queries]


        # print(queries)
        # TODO: Consider removing any quote-like markers that surround the query too.

        passages = dsp.retrieveEnsemble(queries, k=self.k)
        return Prediction(passages=passages)
  ```
  
 In dspy/retrieve/qdrant_rm.py, the following forward function was implemented previously.
```
    def forward(self, query_or_queries: Union[str, List[str]]) -> dspy.Prediction:
        """"""Search with Qdrant for self.k top passages for query

        Args:
            query_or_queries (Union[str, List[str]]): The query or queries to search for.

        Returns:
            dspy.Prediction: An object containing the retrieved passages.
        """"""
        queries = (
            [query_or_queries]
            if isinstance(query_or_queries, str)
            else query_or_queries
        )
        queries = [q for q in queries if q]  # Filter empty queries
        
        batch_results = self._qdrant_client.query_batch(
            self._qdrant_collection_name, query_texts=queries, limit=self.k)

        passages = defaultdict(float)
        for batch in batch_results:
            for result in batch:
                # If a passage is returned multiple times, the score is accumulated.
                passages[result.document] += result.score

        sorted_passages = sorted(
            passages.items(), key=lambda x: x[1], reverse=True)[:self.k]
        return dspy.Prediction(passages=[passage for passage, _ in sorted_passages])
```

As you can see, the arguments in this forward function do not contain k, thus resulting in a TypeError. 

# My contribution

I simply added k to the arguments in the forward method in qdrant_rm, providing optional customisation of top k passages at runtime.

```
   def forward(self, query_or_queries: Union[str, List[str]], k: Optional[int]) -> dspy.Prediction:
        """"""Search with Qdrant for self.k top passages for query

        Args:
            query_or_queries (Union[str, List[str]]): The query or queries to search for.
            k (Optional[int]): The number of top passages to retrieve. Defaults to self.k.
        Returns:
            dspy.Prediction: An object containing the retrieved passages.
        """"""
        queries = (
            [query_or_queries]
            if isinstance(query_or_queries, str)
            else query_or_queries
        )
        queries = [q for q in queries if q]  # Filter empty queries
        
        k = k if k is not None else self.k
        batch_results = self._qdrant_client.query_batch(
            self._qdrant_collection_name, query_texts=queries, limit=k)

        passages = defaultdict(float)
        for batch in batch_results:
            for result in batch:
                # If a passage is returned multiple times, the score is accumulated.
                passages[result.document] += result.score

        sorted_passages = sorted(
            passages.items(), key=lambda x: x[1], reverse=True)[:k]
        return dspy.Prediction(passages=[passage for passage, _ in sorted_passages])
```
",2023-12-09T16:53:15Z,2023-12-18T19:22:19Z,9 days 02:29:04,fix qdrant integration add k optional argument qdrant forward method tldr fix qdrant integration simply add k argument forward function qdrantrm class currently qdrant integration fail work throw typeerror use qdrant retriever model dspyretrieveretrievepy passage retrieve use retriever model case qdrant list query argument kselfk def forwardself queryorqueries query isinstancequeryorqueries str else queryorqueries query strip query query printqueries todo consider remove quotelike marker surround query passage dspretrieveensemblequeries kselfk return predictionpassagespassages dspyretrieveqdrantrmpy follow forward function implement previously def forwardself queryorqueries union dspyprediction search qdrant selfk top passage query args queryorqueries union query query search return dspyprediction object contain retrieve passage query isinstancequeryorqueries str else queryorqueries query filter empty query batchresults selfqdrantclientquerybatch selfqdrantcollectionname querytextsqueries limitselfk passage defaultdictfloat batch batchresults result batch passage return multiple time score accumulate passage resultscore sortedpassages sort passagesitems keylambda x x reversetrue return dspypredictionpassages see argument forward function contain k thus result typeerror contribution simply add k argument forward method qdrantrm provide optional customisation top k passage runtime def forwardself queryorqueries union k optional dspyprediction search qdrant selfk top passage query args queryorqueries union query query search k optional number top passage retrieve default selfk return dspyprediction object contain retrieve passage query isinstancequeryorqueries str else queryorqueries query filter empty query k k k none else selfk batchresults selfqdrantclientquerybatch selfqdrantcollectionname querytextsqueries limitk passage defaultdictfloat batch batchresults result batch passage return multiple time score accumulate passage resultscore sortedpassages sort passagesitems keylambda x x reversetrue return dspypredictionpassages
357,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/242,unplanned,Unknown,"Add SignatureOptimizer Teleprompter Adds SignatureOptimizer, a teleprompter which optimizes a program's prompt signatures using iterative feedback on replacement signatures, generated by a teacher language model.

```python
# Initialize SignatureOptimizer Teleprompter
teleprompter = SignatureOptimizer(metric=metric, breadth=BREADTH, depth=DEPTH, init_temperature=INIT_TEMPERATURE, prompt_model=prompt_model, output_dir=optimizedV1_output_dir)

# Setup evaluation arguments
kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)

# Compile program
compiled_prompt_opt = teleprompter.compile(program.deepcopy(), devset=devset[:DEV_NUM], eval_kwargs=kwargs)

# Evaluate program
eval_score = evaluate(compiled_prompt_opt, devset=evalset[:EVAL_NUM], **kwargs)
```

The below diagram demonstrates the algorithm for optimization, in this case with GPT 3.5 as the prompt model, and Llama2 as the task model:
![multi_prompt_opt](https://github.com/stanfordnlp/dspy/assets/14205166/8b75993c-277c-4132-b5ba-9e87da3358f3)

Collaboration with @klopsahlong",2023-12-05T01:03:20Z,2023-12-05T01:10:57Z,0 days 00:07:37,add signatureoptimizer teleprompter add signatureoptimizer teleprompter optimize program prompt signature use iterative feedback replacement signature generate teacher language model python initialize signatureoptimizer teleprompter teleprompter signatureoptimizermetricmetric breadthbreadth depthdepth inittemperatureinittemperature promptmodelpromptmodel setup evaluation argument kwargs dictnumthreadsnumthreads displayprogresstrue compile program compiledpromptopt telepromptercompileprogramdeepcopy devsetdevset evalkwargskwargs evaluate program evalscore evaluatecompiledpromptopt devsetevalset kwargs diagram demonstrate algorithm optimization case gpt prompt model task model collaboration klopsahlong
358,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/241,unplanned,Unknown,"README tweak, update order of links out to references This is a trivial change‚Äì I just reversed the chronological order of the list of links to references. Previously, it was: oldest -> newest, and I just swapped it: newest -> oldest.

Reason: people often click the first link in a list to read more something. So it probably makes sense to have the thing we'd actually want folks to read listed first (the most recent preprint).",2023-12-04T14:54:33Z,2023-12-04T23:41:14Z,0 days 08:46:41,readme tweak update order link reference trivial change‚Äì reverse chronological order list link reference previously old new swap new old reason people often click first link list read something probably make sense thing wed actually want folk read list first recent preprint
359,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/239,unplanned,Unknown,"Removed conversion from Example to dict in BootstrapFewShotWithOptuna that breaks template line BootstrapFewShotWithOptuna currently converts demostration examples to dicts, which breaks line 210 in template_v2.py as `augmented` cannot be accessed as an attribute. I tried converting to dotdict at the line instead, but dotdict demos seem to converted to just dicts later on, which therefore would require a more complicated fix.",2023-12-02T01:15:33Z,2024-01-21T10:44:37Z,50 days 09:29:04,remove conversion example dict bootstrapfewshotwithoptuna break template line bootstrapfewshotwithoptuna currently convert demostration example dicts break line augment cannot access attribute try convert dotdict line instead dotdict demo seem convert dicts later therefore would require complicated fix
360,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/238,unplanned,Unknown,remove local os None,2023-12-01T20:53:26Z,2023-12-01T20:53:42Z,0 days 00:00:16,remove local o none
361,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/237,unplanned,Unknown,"Add KNNFewShot Example This change illustrates how to use the KNNFewShot teleprompter in an example .ipynb

Note to reviewer: This PR reflects changes made in https://github.com/stanfordnlp/dspy/pull/235",2023-11-27T22:07:40Z,2023-11-27T22:09:29Z,0 days 00:01:49,add knnfewshot example change illustrate use knnfewshot teleprompter example ipynb note reviewer pr reflect change make
362,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/236,unplanned,Unknown,"Add KNNFewShot Example This change illustrates how to use the KNNFewShot teleprompter in an example `.ipynb`

Note to reviewer: This PR reflects changes made in #235 ",2023-11-27T21:55:58Z,2023-11-27T21:56:34Z,0 days 00:00:36,add knnfewshot example change illustrate use knnfewshot teleprompter example ipynb note reviewer pr reflect change make
363,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/235,unplanned,Unknown,"Remove --- between examples in KNN few-shot prompts This change removes the `---` that are added between every KNN example in a prompt. These dashes were not present in prompts generated by DSP programs implementing KNN few shot, but are present in prompts generated by DSPy programs implementing KNN few shot.

Previously with DSP (and with this change), KNN fewshot demos would (and will) look like:
```
Answer questions with short factoid answers.

---

Follow the following format.

Question: ${question}
Answer: often between 1 and 5 words

---

Question: On the coast of what ocean is the birthplace of Diogal Sakho?
Answer: Atlantic Ocean

Question: Which is taller, the Empire State Building or the Bank of America Tower?
Answer: Empire State Building

Question: Samantha Cristoforetti and Mark Shuttleworth are both best known for being first in their field to go where?
Answer: Space

Question: Which Pakistani cricket umpire who won 3 consecutive ICC umpire of the year awards in 2009, 2010, and 2011 will be in the ICC World Twenty20?
Answer: Aleem Dar

Question: What is the code name for the German offensive that started this Second World War engagement on the Eastern Front (a few hundred kilometers from Moscow) between Soviet and German forces, which included 102nd Infantry Division?
Answer: Operation Citadel

Question: Which of these publications was most recently published, Who Put the Bomp or Self?
Answer: Self

Question: Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?
Answer: Tae Kwon Do Times

---

Question: Are both Cangzhou and Qionghai in the Hebei province of China?
Answer: No
```

With DSPy, KNN examples are formatted like:
```
Answer questions with short factoid answers.

---

Follow the following format.

Question: ${question}
Answer: often between 1 and 5 words

---

Question: On the coast of what ocean is the birthplace of Diogal Sakho?
Answer: Atlantic Ocean

---

Question: Which is taller, the Empire State Building or the Bank of America Tower?
Answer: Empire State Building

---

Question: Samantha Cristoforetti and Mark Shuttleworth are both best known for being first in their field to go where?
Answer: Space

---

Question: Which Pakistani cricket umpire who won 3 consecutive ICC umpire of the year awards in 2009, 2010, and 2011 will be in the ICC World Twenty20?
Answer: Aleem Dar

---

Question: What is the code name for the German offensive that started this Second World War engagement on the Eastern Front (a few hundred kilometers from Moscow) between Soviet and German forces, which included 102nd Infantry Division?
Answer: Operation Citadel

---

Question: Which of these publications was most recently published, Who Put the Bomp or Self?
Answer: Self

---

Question: Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?
Answer: Tae Kwon Do Times

---

Question: Are both Cangzhou and Qionghai in the Hebei province of China?
Answer: No

```",2023-11-27T21:43:07Z,2023-11-28T17:19:33Z,0 days 19:36:26,remove example knn fewshot prompt change remove added every knn example prompt dash present prompt generate dsp program implement knn shot present prompt generate dspy program implement knn shot previously dsp change knn fewshot demo would look like answer question short factoid answer follow follow format question question answer often word question coast ocean birthplace diogal sakho answer atlantic ocean question taller empire state building bank america tower answer empire state build question samantha cristoforetti mark shuttleworth best know first field go answer space question pakistani cricket umpire consecutive icc umpire year award icc world answer aleem dar question code name german offensive start second world war engagement eastern front hundred kilometer moscow soviet german force include infantry division answer operation citadel question publication recently publish put bomp self answer self question magazine publish article scott shaw tae kwon time southwest art answer tae kwon time question cangzhou qionghai hebei province china answer dspy knn example format like answer question short factoid answer follow follow format question question answer often word question coast ocean birthplace diogal sakho answer atlantic ocean question taller empire state building bank america tower answer empire state build question samantha cristoforetti mark shuttleworth best know first field go answer space question pakistani cricket umpire consecutive icc umpire year award icc world answer aleem dar question code name german offensive start second world war engagement eastern front hundred kilometer moscow soviet german force include infantry division answer operation citadel question publication recently publish put bomp self answer self question magazine publish article scott shaw tae kwon time southwest art answer tae kwon time question cangzhou qionghai hebei province china answer
367,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/227,unplanned,Unknown,"add minimum working support for vLLM We had already a PR for vLLM support (https://github.com/stanfordnlp/dspy/pull/93), from which this small patch was adapted to work for the current vLLM version. Feel free to merge this or use it just as a reference to improve the other. Thanks!",2023-11-21T12:09:20Z,2023-11-24T19:05:38Z,3 days 06:56:18,add minimum working support vllm already pr vllm support small patch adapt work current vllm version feel free merge use reference improve thanks
368,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/226,unplanned,Unknown,Add KNNFewShot Example This change illustrates how to use the KNNFewShot teleprompter in an example `.ipynb`,2023-11-20T20:57:10Z,2023-11-24T19:07:33Z,3 days 22:10:23,add knnfewshot example change illustrate use knnfewshot teleprompter example ipynb
372,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/222,unplanned,Unknown,"updated knn_fewshot with fixes -fixed sentence vectorizer for string formatting
-updated KNNFewShot teleprompter with correct args handling ",2023-11-14T09:37:48Z,2023-11-21T22:46:13Z,7 days 13:08:25,update knnfewshot fix fix sentence vectorizer string format updated knnfewshot teleprompter correct args handle
373,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/221,unplanned,Unknown,cleanup: removes unnecessary deps & stick package versions None,2023-11-14T05:17:21Z,2023-11-14T05:25:32Z,0 days 00:08:11,cleanup remove unnecessary deps stick package version none
377,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/217,unplanned,Unknown,cherry pick None,2023-11-07T18:30:32Z,2023-11-07T19:04:08Z,0 days 00:33:36,cherry pick none
378,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/216,unplanned,Unknown,feat: Add Marqo for retrieval **Description**: This PR adds [marqo](https://github.com/marqo-ai/marqo) as a supported retrieval mechanism.,2023-11-07T18:26:16Z,2023-11-07T19:05:02Z,0 days 00:38:46,feat add marqo retrieval description pr add support retrieval mechanism
380,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/214,unplanned,Unknown,Mrbean/smol merge None,2023-11-07T17:34:02Z,2023-11-07T17:34:56Z,0 days 00:00:54,mrbeansmol merge none
382,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/211,unplanned,Unknown,"included usage logging for openai calls -logging of total_tokens usage from OpenAI requests
-thread-safe with Python logging library",2023-11-06T21:21:42Z,2024-01-30T21:36:46Z,85 days 00:15:04,include usage log openai call log totaltokens usage openai request threadsafe python logging library
385,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/208,unplanned,Unknown,merge merge main,2023-11-05T22:12:37Z,2023-11-09T15:45:29Z,3 days 17:32:52,merge merge main
387,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/205,unplanned,Unknown,"feat: Add chromadb support for retrieval  This PR adds [chromadb](https://docs.trychroma.com/getting-started) to the RM.

The PR uses [openai embedding](https://platform.openai.com/docs/guides/embeddings).

This PR is based on existing RM code:
- dspy/retrieve/pinecone_rm.py
- dspy/retrieve/qdrant_rm.py
",2023-11-02T17:58:50Z,2023-11-04T19:11:41Z,2 days 01:12:51,feat add chromadb support retrieval pr add rm pr use pr base exist rm code dspyretrievepineconermpy dspyretrieveqdrantrmpy
388,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/204,unplanned,Unknown,"feat: Add Qdrant support for retrieval This PR intends to add Qdrant as a supported retrieval mechanism.

[Qdrant](https://qdrant.tech/) is a high-performance vector database & vector similarity search engine.

The [Qdrant client library](https://github.com/qdrant/qdrant-client) has built in support for generating sentence embeddings using [FastEmbed](https://github.com/qdrant/fastembed). 

This implementation uses the `query_batch` mechanism to perform similarity searches in a batch and return the relevant docs.",2023-11-02T12:40:00Z,2023-11-02T14:43:31Z,0 days 02:03:31,feat add qdrant support retrieval pr intend add qdrant support retrieval mechanism highperformance vector database vector similarity search engine build support generate sentence embeddings use implementation use querybatch mechanism perform similarity search batch return relevant doc
389,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/203,unplanned,Unknown,"fix 502 openai error Have had this happen for multiple 3.5 trials where openai just craps out for a couple requests. Thought this may help. 

```
80     self._interpret_response_line(
81   File ""/home/dakisho/.pyenv/versions/3.11.6/lib/python3.11/site-packages/openai/api_requestor.py"", line 775, in _interpret_response_line
82     raise self.handle_error_response(
83 openai.error.APIError: Bad gateway. {""error"":{""code"":502,""message"":""Bad gateway."",""param"":null,""type"":""cf_bad_gateway""}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Wed, 01 Nov 2023 09:23:59 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '81f306648c6ab0ed-ATL', 'alt-svc': 'h3="":443""; ma=86400'}
```",2023-11-01T22:20:58Z,2023-11-05T23:39:47Z,4 days 01:18:49,fix openai error happen multiple trial openai craps couple request think may help selfinterpretresponseline file line interpretresponseline raise selfhandleerrorresponse openaierrorapierror bad gateway gatewayparamnulltypecfbadgateway error code message bad gateway param none type cfbadgateway date wed nov gmt contenttype applicationjson contentlength connection keepalive xframeoptions sameorigin referrerpolicy sameorigin cachecontrol private nostore nocache mustrevalidate expire thu jan gmt server cloudflare cfray altsvc
390,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/199,unplanned,Unknown,"Fix KNNFewShot import issues This change 
1. Allows for the `KNNFewShot` teleprompter to be imported 
2. Adds a missing import to the `knn_fewshot.py` file 

Before this change, importing `KNNFewShot` teleprompter gives the following errror:
```
----> 1 from dspy.teleprompt import KNNFewShot
...

ImportError: cannot import name 'KNNFewShot' from 'dspy.teleprompt'
``` 

Additionally, `knn_fewshot.py` was missing an import of `Teleprompter` causing the following error:
```
NameError 
...
----> 8 class KNNFewShot(Teleprompter):
      9     def __init__(self, KNN, k: int, trainset: List[dsp.Example]):
     10         self.KNN = KNN(k, trainset)

NameError: name 'Teleprompter' is not defined
```",2023-10-30T20:40:48Z,2023-10-31T15:27:34Z,0 days 18:46:46,fix knnfewshot import issue change allow knnfewshot teleprompter import add miss import knnfewshotpy file change import knnfewshot teleprompter give follow errror dspyteleprompt import knnfewshot importerror cannot import name knnfewshot dspyteleprompt additionally knnfewshotpy miss import teleprompter cause follow error nameerror class knnfewshotteleprompter def initself knn k int trainset list selfknn knnk trainset nameerror name teleprompter define
392,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/197,planned,Unknown,"dspy assertions Adds assertions (assert and suggest) as first-class primitives in dspy.

Syntax: 
`dspy.Assert(fn(args), msg=""feedback message"")`
`dspy.Suggest(fn(args), msg=""feedback message"")`

Semantics: On using dspy assertions in a program, the compiler promises that, during inference, either _the compiler figures out how to simulate the DSPy program up to that step in a way that passes the assertion_ or it raises a `DSPySuggestionError` exception. For `dspy.Suggest` in the case of a raised exception, the compiler will use the suggestion as a hint, trying on a best-effort basis to make it pass. If it fails, a warning is logged, but execution will continue.

TODOs:
- [x] simplify syntax:`dspy.Assert(fn, *args, msg="""")` ==> `dspy.Assert(fn(args), msg="""")` and make msg positional
- [x] dspy.Suggest: backtrack on failure; dspy.Assert: crash on failure
- [x] add logging for assertions
- [x] bypass assertion errors on last backtrack attempt + allow setting bypass_assert in forward()
- [x] parametric handlers for assertions
- [x] assertion should not crash the program when compiling
- [x] improve error handling: avoid including previous output directly in msg by accessing previous failures
- [x] improve error handling: track previous I/O and assertion failures (use as traces for backtrack)
- [x] include a dspy.Retry module that modifies a module's signature for retry w/ new fields `{[past outputs]*, feedback}`

Good to have:
- [ ] add some simple/safe regression tests (possibly using mock).
- [ ] multi-assertion/suggestion handling
- [ ] thread safety for backtracking
",2023-10-29T20:35:02Z,2023-12-07T21:31:10Z,39 days 00:56:08,dspy assertion add assertion assert suggest firstclass primitive dspy syntax dspyassertfnargs msgfeedback message dspysuggestfnargs msgfeedback message semantics use dspy assertion program compiler promise inference either compiler figure simulate dspy program step way pass assertion raise dspysuggestionerror exception dspysuggest case raise exception compiler use suggestion hint try besteffort basis make pas fails warn logged execution continue todos simplify syntaxdspyassertfn args msg dspyassertfnargs msg make msg positional dspysuggest backtrack failure dspyassert crash failure add log assertion bypass assertion error last backtrack attempt allow set bypassassert forward parametric handler assertion assertion crash program compiling improve error handle avoid include previous output directly msg access previous failure improve error handle track previous io assertion failure use trace backtrack include dspyretry module modifies modules signature retry w new field feedback good add simplesafe regression test possibly use mock multiassertionsuggestion handle thread safety backtracking
393,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/196,unplanned,Unknown,[wip] adding assertions to dspy None,2023-10-29T19:11:09Z,2023-10-29T20:25:19Z,0 days 01:14:10,add assertion dspy none
394,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/195,unplanned,Unknown,"Apply ruff check, ruff formatter and use a pre-commit-config for ci I think this could be beneficial for the project, let me know what you think. I noticed that #125  is trying to include black and isort, but it seems inactive. I didn't want to get involved in changing the imports with wildcards in the __init__.py files due to the potential implications. ",2023-10-29T03:10:58Z,2024-03-05T05:15:26Z,128 days 02:04:28,apply ruff check ruff formatter use precommitconfig ci think could beneficial project let know think noticed try include black isort seem inactive didnt want get involve change import wildcards initpy file due potential implication
396,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/189,unplanned,Unknown,"added AnswerCorrectness and AnswerFaithfulness auto-eval modules AnswerCorrectness - verifies if predicted answer is correct for question based on gold answer and outputs True/False
AnswerFaithfulness - verifies if predicted answer is based off context/reasoning/rationale for question and outputs True/False

(corrects faulty [PR](https://github.com/stanfordnlp/dspy/pull/173)) ",2023-10-25T16:29:17Z,2023-10-25T17:45:34Z,0 days 01:16:17,add answercorrectness answerfaithfulness autoeval module answercorrectness verifies predict answer correct question base gold answer output truefalse answerfaithfulness verifies predict answer base contextreasoningrationale question output truefalse corrects faulty
398,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/187,unplanned,Unknown,"added proper error handling within evaluate and bootstrap -permits up to 5 errors during 1) evaluation and 2) bootstrapping and then raises actual error, halting program for user to fix the error ",2023-10-25T07:37:23Z,2023-10-25T15:19:09Z,0 days 07:41:46,add proper error handle within evaluate bootstrap permit error evaluation bootstrapping raise actual error halt program user fix error
399,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/186,unplanned,Unknown,"added support for OpenAI completion API string prompting follows #3 within [Anyscale OpenAI endpoint handling](https://github.com/anyscale/endpoint-cookbook/blob/main/combine_anyscale_openai.ipynb)

allows for string prompting to models",2023-10-25T07:30:32Z,2023-10-25T15:20:02Z,0 days 07:49:30,added support openai completion api string prompt follow within allows string prompting model
400,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/183,unplanned,Unknown,"Allow no-op decorator to take arguments Fixes the error mentioned here https://github.com/stanfordnlp/dspy/issues/181#issuecomment-1776250172

i.e. 
```
  File ""<snip>/dspy/dsp/modules/hf_client.py"", line 93, in <module>
    @NotebookCacheMemory.cache(ignore=['arg'])
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: noop_decorator() got an unexpected keyword argument 'ignore'

```

Now, if `NotebookCacheMemory.cache` is a `noop_decorator`, it can be called either with or without args/kwargs (i.e. `@NotebookCacheMemory.cache(ignore=[""arg""])` or `@NotebookCacheMemory.cache`).
",2023-10-24T10:45:26Z,2023-10-24T12:07:04Z,0 days 01:21:38,allow noop decorator take argument fix error mention ie file dspydspmoduleshfclientpy line notebookcachememorycacheignore typeerror noopdecorator get unexpected keyword argument ignore notebookcachememorycache noopdecorator call either without argskwargs ie notebookcachememorycacheignore notebookcachememorycache
401,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/182,unplanned,Unknown,Update requirements.txt Adds optuna as a requirement,2023-10-24T10:41:49Z,2023-10-24T12:07:27Z,0 days 01:25:38,update requirementstxt add optuna requirement
403,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/179,unplanned,Unknown,"Allow passing of kwargs for HF TGI request This PR lets the user specify what arguments are provided to the HTTP call to the TGI server. My use case is HTTP basic auth, so I need to pass the relevant kwarg through. ",2023-10-23T14:11:43Z,2023-10-23T17:55:51Z,0 days 03:44:08,allow pass kwargs hf tgi request pr let user specify argument provide http call tgi server use case http basic auth need pas relevant kwarg
405,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/177,unplanned,Unknown,added client integrations for Anyscale -integrated Anyscale endpoint within DSPy,2023-10-20T01:23:08Z,2023-10-20T11:05:49Z,0 days 09:42:41,added client integration anyscale integrate anyscale endpoint within dspy
407,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/173,unplanned,Unknown,"added AnswerCorrectness and AnswerFaithfulness auto-eval programs AnswerCorrectness - verifies if predicted answer is correct for question and outputs True/False
AnswerFaithfulness - verifies if predicted answer is based off context/reasoning/rationale for question and outputs True/False",2023-10-18T20:09:05Z,2023-10-25T16:43:35Z,6 days 20:34:30,add answercorrectness answerfaithfulness autoeval program answercorrectness verifies predict answer correct question output truefalse answerfaithfulness verifies predict answer base contextreasoningrationale question output truefalse
408,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/170,unplanned,Unknown,add you retriever module So I wasn't entirely sure on the difference between the modules in `dsp` vs the retriever module example of pinecone in `dspy`. I went ahead and followed what seemed to be the newer pattern. One thing to note about this implementation is if multiple queries are indeed passed into the new retriever we do not have the ability to rerank the results across queries so we return the passages from query 1 then the passages from query 2. I could also just raise a runtime error if that is a more desirable pattern. Let me know!,2023-10-16T18:09:59Z,2023-10-16T20:02:35Z,0 days 01:52:36,add retriever module wasnt entirely sure difference module dsp vs retriever module example pinecone dspy go ahead follow seem new pattern one thing note implementation multiple query indeed pass new retriever ability rerank result across query return passage query passage query could also raise runtime error desirable pattern let know
411,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/165,unplanned,Unknown,refactor: remove redundant async lm code None,2023-10-13T21:13:13Z,2023-10-13T21:13:28Z,0 days 00:00:15,refactor remove redundant async lm code none
415,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/160,unplanned,Unknown,feat: support async bootstrap few shot Add support for running async BootstrapFewShot,2023-10-11T21:46:17Z,2023-10-11T21:47:52Z,0 days 00:01:35,feat support async bootstrap shot add support run async bootstrapfewshot
418,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/156,unplanned,Unknown,"supporting initializing TGI server within DSPy -supports loading TGI server directly within DSPy
-prerequisites: user has already cloned the [TGI  ](https://github.com/huggingface/text-generation-inference/) repository
-especially useful for loading fine-tuned models created from BootstrapFinetune without manual intervention for server initialization",2023-10-10T17:43:05Z,2023-10-25T15:17:59Z,14 days 21:34:54,support initialize tgi server within dspy support load tgi server directly within dspy prerequisite user already clone repository especially useful load finetuned model create bootstrapfinetune without manual intervention server initialization
424,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/150,unplanned,Unknown,"Update intro.ipynb I found that if not set model_type to chat, it generates many additional useless lines",2023-10-09T14:47:40Z,2023-10-16T18:19:48Z,7 days 03:32:08,update introipynb find set modeltype chat generate many additional useless line
426,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/148,unplanned,Unknown,"Pinecone retrieval improvements Firstly thank you for the great library. I was interested in using the Pinecone retrieval component and ran into several issues, including:

* Hardcoded api/env placeholders
* Undeclared dependency (reasonably to not force pinecone install for other users)
* Lack of support for multiple queries (used only results of first embedding)
* Deprecated openai embedding kwargs
etc.

This aims to address these issues by overhauling the pinecone retrieval class in many ways: fixing the hardcoded keys, including the dependency as optional so you can use `pip install dspy-ai[pinecone]`, and implementing similar behaviour to `dsp.retrieveEnsemble` to allow search against a query list, along with other code quality improvements.",2023-10-09T02:08:23Z,2023-10-09T16:24:01Z,0 days 14:15:38,pinecone retrieval improvement firstly thank great library interested use pinecone retrieval component run several issue include hardcoded apienv placeholder undeclared dependency reasonably force pinecone install user lack support multiple query use result first embed deprecate openai embed kwargs etc aim address issue overhaul pinecone retrieval class many way fix hardcoded key include dependency optional use pip install dspyai implement similar behaviour dspretrieveensemble allow search query list along code quality improvement
427,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/147,unplanned,Unknown,Program of Thought module merged past Program of Thought module [PR ](https://github.com/stanfordnlp/dspy/pull/116) to incorporate latest Signature changes [PR](https://github.com/stanfordnlp/dspy/pull/146).,2023-10-09T00:25:27Z,2023-10-24T19:00:24Z,15 days 18:34:57,program think module merge past program thought module incorporate late signature change
428,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/146,unplanned,Unknown,"modified signatures to handle new DSPy signature format -supports new DSPy signature declarations (`dspy.Signature(""{inputs}-> {outputs}""`)

methods added:
-`attach()`: defining field with prefix and description 
-`add_field()`: supports pretending and appending InputFields and OutputFields

(note for reference - currently, modifies signature in-place as the `attach()` functionality supports chaining) ",2023-10-08T17:33:07Z,2023-10-24T19:00:27Z,16 days 01:27:20,modified signature handle new dspy signature format support new dspy signature declaration dspysignatureinputs output method add attach define field prefix description addfield support pretend append inputfields outputfields note reference currently modify signature inplace attach functionality support chain
433,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/141,unplanned,Unknown,added optuna teleprompter optuna optimization integration with BootstrapFewShotWithRandomSearch,2023-09-28T02:20:36Z,2023-10-23T17:52:14Z,25 days 15:31:38,add optuna teleprompter optuna optimization integration bootstrapfewshotwithrandomsearch
435,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/137,unplanned,Unknown,"added knn module and teleprompter in-progress PR for [issue](https://github.com/stanfordnlp/dspy/issues/109)
-support for KNN Module to return topk train samples
-support for KNN Teleprompter 

",2023-09-23T06:39:54Z,2023-10-23T17:51:10Z,30 days 11:11:16,add knn module teleprompter inprogress pr support knn module return topk train sample support knn teleprompter
438,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/134,unplanned,Unknown,"modified threading handling in config for multithreaded flask handling multithreading within dsp.settings in setting config
modified relevant methods that involve thread->stack dictionary

refers to this [issue](https://github.com/stanfordnlp/dspy/issues/105)",2023-09-21T18:48:59Z,2023-10-23T17:49:14Z,31 days 23:00:15,modify thread handle config multithreaded flask handle multithreading within dspsettings set config modify relevant method involve threadstack dictionary refers
440,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/132,unplanned,Unknown,"modified peft and int8 code for finetuning -uncommented existing logic for peft 
-added handling for int8 in the model (prepare_model_for_int8_training)
",2023-09-21T04:57:38Z,2023-09-21T07:41:29Z,0 days 02:43:51,modify peft code finetuning uncommented exist logic peft add handle model
441,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/131,unplanned,Unknown,"Change PyseriniRetriever linear index search to lookup table retrieval when using your own data should be faster now, let me know if you have any other ideas/changes in mind",2023-09-20T18:03:55Z,2023-09-20T18:37:55Z,0 days 00:34:00,change pyseriniretriever linear index search lookup table retrieval use data faster let know ideaschanges mind
442,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/130,unplanned,Unknown,added stop_at_score kwarg to random_search added stop_at_score per [issue request ](https://github.com/stanfordnlp/dspy/issues/128),2023-09-20T06:20:16Z,2023-09-20T22:11:13Z,0 days 15:50:57,add stopatscore kwarg randomsearch add stopatscore per
446,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/126,unplanned,Unknown,"linked module docs onto README links added for LM clients, RM clients, relevant Modules and Teleprompters.
also updated PR link for dspy.ProgramOfThought",2023-09-18T00:10:47Z,2023-09-18T17:12:07Z,0 days 17:01:20,link module doc onto readme link add lm client rm client relevant module teleprompter also update pr link dspyprogramofthought
447,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/124,unplanned,Unknown,refactor: rename dsp module as internals None,2023-09-17T18:12:38Z,2023-09-17T18:41:48Z,0 days 00:29:10,refactor rename dsp module internals none
448,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/123,unplanned,Unknown,"enhancement(documentation): adding initial docs for dspy LM, RM, teleprompting, local models and generic modules. None",2023-09-17T17:28:37Z,2023-09-17T17:35:10Z,0 days 00:06:33,enhancementdocumentation add initial doc dspy lm rm teleprompting local model generic module none
449,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/122,unplanned,Unknown,added ReAct module -added ReAct module support within DSPy,2023-09-17T17:26:12Z,2023-10-08T18:00:20Z,21 days 00:34:08,add react module add react module support within dspy
450,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/121,unplanned,Unknown,"enhancement(documentation): Minor fixes Minor standardization changes:
- Add ToCs for quick navigations
- `Arguments` -> `Parameters`
- parameter type hints are in italic to improve readability
- `GPT3(...)` constructor -> `OpenAI(...)`
- add links to references for quick nav
- define `Quickstart` and `Usage` snippets at the top of doc modules and `Examples` at the end of doc modules. ",2023-09-17T17:11:26Z,2023-09-17T17:12:50Z,0 days 00:01:24,enhancementdocumentation minor fix minor standardization change add tocs quick navigation argument parameter parameter type hint italic improve readability constructor openai add link reference quick nav define quickstart usage snippet top doc module examples end doc module
451,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/120,unplanned,Unknown,Colab changes for Pyserini Continued from #119  ,2023-09-16T16:06:39Z,2023-09-16T22:47:36Z,0 days 06:40:57,colab change pyserini continue
452,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/119,unplanned,Unknown,Integrate Pyserini into DSPy Integrate pyserini to enable users to use their own data for retrieval in dspy. I've added a small notebook to demo how it works.,2023-09-16T01:12:57Z,2023-09-16T15:58:43Z,0 days 14:45:46,integrate pyserini dspy integrate pyserini enable user use data retrieval dspy ive add small notebook demo work
454,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/116,unplanned,Unknown,"added Program-of-Thought module -added Program-of-Thought module support within DSPy
-includes Python interpreter code adopted with modifications from [this repository.](https://github.com/camel-ai/camel) ",2023-09-13T18:11:49Z,2023-10-09T00:15:42Z,25 days 06:03:53,add programofthought module add programofthought module support within dspy include python interpreter code adopt modification
459,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/111,unplanned,Unknown,Add flake None,2023-09-11T15:28:08Z,2023-09-11T15:28:40Z,0 days 00:00:32,add flake none
463,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/107,unplanned,Unknown,Added support for using Pinecone None,2023-09-08T19:35:41Z,2023-09-08T19:44:54Z,0 days 00:09:13,added support use pinecone none
472,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/97,unplanned,Unknown,Fix typo in README.md quetion -> question,2023-08-27T15:42:39Z,2023-08-27T15:44:13Z,0 days 00:01:34,fix typo readmemd quetion question
474,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/93,unplanned,Unknown,"added support for vLLM server and docs for local models -updated DSPy init with vLLM support
-added vLLM to hf_client
-added docs for local model testing with HFModel, TGI, vLLM, and MLC
-added model initialization validation for HFClientTGI within latest changes",2023-08-23T15:02:04Z,2024-01-14T14:57:29Z,143 days 23:55:25,added support vllm server docs local model update dspy init vllm support add vllm hfclient add docs local model test hfmodel tgi vllm mlc add model initialization validation hfclienttgi within late change
475,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/92,unplanned,Unknown,"HFModel support for DSPy -added HFModel support within DSPy
-added postprocessing for HFModel",2023-08-23T14:37:25Z,2023-10-10T16:38:17Z,48 days 02:00:52,hfmodel support dspy add hfmodel support within dspy add postprocessing hfmodel
476,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/91,unplanned,Unknown,updated with mlc models post-processing updated DSPy internals with post-processing for MLC Llama models - `.split('\n')[0]`,2023-08-22T15:56:22Z,2023-10-10T16:51:45Z,49 days 00:55:23,update mlc model postprocessing updated dspy internals postprocessing mlc llama model splitn
477,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/90,unplanned,Unknown,V2 None,2023-08-21T22:37:10Z,2023-08-21T22:37:53Z,0 days 00:00:43,none
478,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/88,unplanned,Unknown,Minor updates so notebook works on Colab None,2023-08-15T18:50:37Z,2023-08-15T18:50:42Z,0 days 00:00:05,minor update notebook work colab none
479,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/87,unplanned,Unknown,V2++ None,2023-08-15T17:48:51Z,2023-08-15T17:50:51Z,0 days 00:02:00,none
480,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/86,unplanned,Unknown,Remove DSPv1 caches None,2023-08-15T17:47:56Z,2023-08-15T17:48:05Z,0 days 00:00:09,remove cache none
488,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/78,unplanned,Unknown,added wrapper for azure cognitive search retrieval added wrapper class to support retrieval model from Azure cognitive search.,2023-07-11T06:18:38Z,2023-07-22T21:59:01Z,11 days 15:40:23,add wrapper azure cognitive search retrieval add wrapper class support retrieval model azure cognitive search
490,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/76,unplanned,Unknown,Bird evaluation Sharing code for discussion with Omar,2023-07-06T23:40:24Z,2023-07-22T22:19:09Z,15 days 22:38:45,bird evaluation share code discussion omar
494,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/72,planned,Unknown,"feature(new-caching): sqlite caching + integration into gpt3 and colbert What the PR does:
- Creates a new caching logic using sqlite
- makes experiments reproducible using cached timeranges:
   - eg: if an example is not part of the specified time-range, it would error out instead of re-computing. This helps us to make the experiments consistent and re-producible.
- integrated the logic into gpt and colbert module.
   - usage: `dsp.settings.configure(lm=lm, rm=rm, experiment_end_timestamp=..., experiment_start_timestamp=...)`  # in timestamp
- Adds a testing notebook for understanding different behaviours of the cache. 
- adds a common logger which can be used for debugging along with custom logging level
- removes joblib caching logic",2023-06-29T19:36:51Z,2023-10-25T17:32:08Z,117 days 21:55:17,featurenewcaching sqlite cache integration colbert pr create new cache logic use sqlite make experiment reproducible use cached timeranges eg example part specify timerange would error instead recomputing help u make experiment consistent reproducible integrate logic gpt colbert module usage dspsettingsconfigurelmlm rmrm experimentendtimestamp experimentstarttimestamp timestamp add test notebook understand different behaviour cache add common logger use debug along custom log level remove joblib cache logic
498,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/68,unplanned,Unknown,Basic Adapter vs Chat Adapter None,2023-06-07T00:02:59Z,2023-10-10T16:55:41Z,125 days 16:52:42,basic adapter v chat adapter none
499,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/67,unplanned,Unknown,Incorporating reranker into thread safe settings.py None,2023-06-03T00:25:27Z,2023-09-18T21:26:51Z,107 days 21:01:24,incorporate reranker thread safe settingspy none
500,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/66,unplanned,Unknown,Fix typo Fix typo,2023-05-29T17:57:57Z,2023-05-29T18:18:46Z,0 days 00:20:49,fix typo fix typo
501,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/65,unplanned,Unknown,upgrade package version None,2023-05-28T20:05:42Z,2023-05-28T20:05:51Z,0 days 00:00:09,upgrade package version none
503,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/63,unplanned,Unknown,"Revert ""Making the DSP context window thread safe"" Reverts stanfordnlp/dsp#61",2023-05-27T01:59:52Z,2023-05-27T01:59:59Z,0 days 00:00:07,revert make dsp context window thread safe reverts
504,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/62,unplanned,Unknown,Add function inspect to readme None,2023-05-27T01:47:47Z,2024-02-03T18:54:33Z,252 days 17:06:46,add function inspect readme none
505,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/61,unplanned,Unknown,Making the DSP context window thread safe None,2023-05-27T00:16:42Z,2023-05-27T00:16:53Z,0 days 00:00:11,make dsp context window thread safe none
509,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/57,unplanned,Unknown,"feature(search): sbert re-ranker The PR does the below
- Introduces re-ranker that couples with existing retrieval. 
- updated the intro notebook for usage

```
sbert_reranker = dsp.SentenceTransformersCrossEncoder(""cross-encoder/ms-marco-MiniLM-L-12-v2"")

dsp.settings.configure(lm=lm, rm=rm, reranker=sbert_reranker)

# dsp.retrieve, and retrieveEnsemble already takes care of retrieving and re-ranking based on initiated rm, and reranker.

reranked_passages = dsp.retrieve(""When was the creator of Hadoop given an award?"", k=3)
```",2023-05-12T12:30:45Z,2023-05-12T13:31:36Z,0 days 01:00:51,featuresearch sbert reranker pr introduces reranker couple exist retrieval update intro notebook usage sbertreranker dspsettingsconfigurelmlm rmrm rerankersbertreranker dspretrieve retrieveensemble already take care retrieve reranking base initiated rm reranker rerankedpassages dspretrievewhen creator hadoop give award
513,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/53,planned,Unknown,bug(chatgpt-lm): remove unnecessary param passing addresses #52 bug,2023-05-01T06:39:42Z,2023-05-01T06:40:17Z,0 days 00:00:35,bugchatgptlm remove unnecessary param passing address bug
516,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/50,unplanned,Unknown,"feat: Unset Output Variable in `dsp.generate` to Prevent Leakage This PR addresses the issue raised in https://github.com/stanfordnlp/dsp/issues/47#issuecomment-1523430127.

The changes in this PR ensure that output variables in `dsp.generate` are unset if the template fails to parse the response corresponding to the output variables.

It is always recommended to pass only the necessary questions to `dsp.generate` or unset the output that users expect the function to produce. However, users often forget to do this when evaluating the dev set. In such cases, the gold answers remain in the output variable of the returned completion, which can lead to malformed output (e.g., a list) or an overestimation of performance. This PR serves as a temporary workaround for this issue.

closes: #47 ",2023-04-26T19:55:22Z,2023-09-18T21:27:46Z,145 days 01:32:24,feat unset output variable dspgenerate prevent leakage pr address issue raise change pr ensure output variable dspgenerate unset template fails parse response corresponding output variable always recommend pas necessary question dspgenerate unset output user expect function produce however user often forget evaluate dev set case gold answer remain output variable return completion lead malformed output eg list overestimation performance pr serve temporary workaround issue close
517,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/49,unplanned,Unknown,"Fix bug in predict.py:majority_vote_ Handle situations where pred[prediction_field] is List[str]
#47 ",2023-04-24T17:53:14Z,2023-04-26T14:19:44Z,1 days 20:26:30,fix bug predictpymajorityvote handle situation pred list
518,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/48,unplanned,Unknown,Fix bug in majority_vote_ Fix issue raised in #47 ,2023-04-24T17:15:07Z,2023-04-24T17:16:58Z,0 days 00:01:51,fix bug majorityvote fix issue raise
520,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/46,unplanned,Unknown,refactor(cohere-lm): retry wrapper should ignore optional coheere None,2023-04-24T12:35:12Z,2023-04-24T12:36:04Z,0 days 00:00:52,refactorcoherelm retry wrapper ignore optional coheere none
521,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/45,unplanned,Unknown,bug(cohere-lm): retry only for rate-limit errors None,2023-04-24T12:14:00Z,2023-04-24T12:14:27Z,0 days 00:00:27,bugcoherelm retry ratelimit error none
522,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/44,planned,Unknown,"bug(faiss): mismatch in types & args instead of kwargs addresses #41 
reference: https://github.com/facebookresearch/faiss/blob/main/faiss/IndexScalarQuantizer.cpp",2023-04-23T21:28:20Z,2023-04-23T21:49:23Z,0 days 00:21:03,bugfaiss mismatch type args instead kwargs address reference
523,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/43,unplanned,Unknown,enhancement(cohere-lm): add backoff retry None,2023-04-23T16:12:43Z,2023-04-23T16:12:50Z,0 days 00:00:07,enhancementcoherelm add backoff retry none
524,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/42,unplanned,Unknown,"refactor(hf-models): export hf module without the need for deps Addresses #40 and our previous issues of not wanting hf transformers to be a dependency for dsp.

minor bug fix on using hf models on cpu.",2023-04-23T14:51:31Z,2023-04-23T15:47:51Z,0 days 00:56:20,refactorhfmodels export hf module without need deps address previous issue want hf transformer dependency dsp minor bug fix use hf model cpu
526,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/40,unplanned,Unknown,Enable huggingface models None,2023-04-22T20:57:16Z,2023-04-23T16:04:24Z,0 days 19:07:08,enable huggingface model none
527,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/39,planned,Unknown,bug(type-hints): passage_match answers is a list of string None,2023-04-19T09:39:06Z,2023-04-19T09:39:56Z,0 days 00:00:50,bugtypehints passagematch answer list string none
528,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/38,unplanned,Unknown,bug(azure-api-support): minor fix None,2023-04-08T19:40:39Z,2023-04-08T19:41:02Z,0 days 00:00:23,bugazureapisupport minor fix none
529,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/37,planned,Unknown,enhancement(GPT-LLM): azure API support Adds support for Azure OpenAI API Service.,2023-04-08T19:33:31Z,2023-04-08T19:35:47Z,0 days 00:02:16,enhancementgptllm azure api support add support azure openai api service
530,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/36,unplanned,Unknown,Add Support for Multi-GPU Inference None,2023-04-04T21:31:33Z,2023-04-04T21:32:39Z,0 days 00:01:06,add support multigpu inference none
531,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/35,unplanned,Unknown,Fixed a minor typo in the README None,2023-04-03T20:03:40Z,2023-04-03T20:05:21Z,0 days 00:01:41,fix minor typo readme none
533,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/33,planned,Unknown,"refactor(cohere): cohere module abstraction & few bug fixes - Cohere uses the abstracted LM class (newly added) for consistency. 
- Base LM class breaks gpt3 module due to mismatch in abstracted class signature
- minor refactoring and bug fixes",2023-03-27T07:16:54Z,2023-03-27T11:22:46Z,0 days 04:05:52,refactorcohere cohere module abstraction bug fix cohere us abstract lm class newly add consistency base lm class break module due mismatch abstract class signature minor refactoring bug fix
534,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/32,unplanned,Unknown,Cohere integration This PR pertains to adding the class for Cohere models.,2023-03-24T01:13:28Z,2023-03-24T01:14:17Z,0 days 00:00:49,cohere integration pr pertain add class cohere model
535,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/31,unplanned,Unknown,"Bump webpack from 5.75.0 to 5.76.3 in /inspect-app/react-app Bumps [webpack](https://github.com/webpack/webpack) from 5.75.0 to 5.76.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/webpack/webpack/releases"">webpack's releases</a>.</em></p>
<blockquote>
<h2>v5.76.3</h2>
<h2>Bugfixes</h2>
<ul>
<li>Non-javascript files will correctly <strong>not</strong> be imported when using <code>experiments.outputModule</code> (ES Module Output) by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16809"">webpack/webpack#16809</a></li>
<li>Limit console output progress bar length to 40 when no columns provided by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16810"">webpack/webpack#16810</a></li>
<li>Add missing NodeJS Builtin Modules support for <code>inspector/promises</code>, <code>readline/promises</code>, and <code>stream/consumers</code> by <a href=""https://github.com/ShenHongFei""><code>@‚ÄãShenHongFei</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16841"">webpack/webpack#16841</a></li>
<li>webpack bin/cli now properly respects <code>NODE_PATH</code> env variable by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16808"">webpack/webpack#16808</a></li>
<li>Improve typos in <code>resolveResourceErrorHints</code> by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16806"">webpack/webpack#16806</a></li>
<li>Add missing <code>loaders</code> token support to <code>moduleFilenameTemplate</code> function call by <a href=""https://github.com/pgoldberg""><code>@‚Äãpgoldberg</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16756"">webpack/webpack#16756</a></li>
<li>Add gaurd condition for <code>enabledLibraryTypes</code> in internal <code>ContainerPlugin</code> by <a href=""https://github.com/PengBoUESTC""><code>@‚ÄãPengBoUESTC</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16635"">webpack/webpack#16635</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/ShenHongFei""><code>@‚ÄãShenHongFei</code></a> made their first contribution in <a href=""https://redirect.github.com/webpack/webpack/pull/16841"">webpack/webpack#16841</a></li>
<li><a href=""https://github.com/pgoldberg""><code>@‚Äãpgoldberg</code></a> made their first contribution in <a href=""https://redirect.github.com/webpack/webpack/pull/16756"">webpack/webpack#16756</a></li>
<li><a href=""https://github.com/PengBoUESTC""><code>@‚ÄãPengBoUESTC</code></a> made their first contribution in <a href=""https://redirect.github.com/webpack/webpack/pull/16635"">webpack/webpack#16635</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/webpack/webpack/compare/v5.76.2...v5.76.3"">https://github.com/webpack/webpack/compare/v5.76.2...v5.76.3</a></p>
<h2>v5.76.2</h2>
<h2>Bugfixes</h2>
<ul>
<li>Fix bug where a missing semicolon in generated bundle output for <code>publicPathRuntime</code> would cause concatenated runtime errors by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16811"">webpack/webpack#16811</a></li>
<li>Remove redundant semicolons generated in bundle runtime code after <code>onScriptComplete</code> function by <a href=""https://github.com/ahaoboy""><code>@‚Äãahaoboy</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16347"">webpack/webpack#16347</a></li>
<li>Fix bug where <code>RealContentHashPlugin</code> was not respecting <code>output.hashSalt</code>'s ability to cause a force recalculation of <code>[contenthash]</code> for emitted assets by <a href=""https://github.com/dmichon-msft""><code>@‚Äãdmichon-msft</code></a> <a href=""https://redirect.github.com/webpack/webpack/issues/16789"">#16789</a></li>
</ul>
<h2>Performance</h2>
<ul>
<li>Improve memory and runtime performance of sourcemaps via hoisting Regular Expression literals to stored variables by <a href=""https://github.com/TheLarkInn""><code>@‚ÄãTheLarkInn</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/15722"">webpack/webpack#15722</a></li>
<li>Correct v8 deoptimization in <code>ModuleGraph</code> due to instance property declarations occurring outside of constructor by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16830"">webpack/webpack#16830</a></li>
</ul>
<h2>Developer Experience</h2>
<ul>
<li>Improved internal typings to match <code>webpack-sources</code> typings for <code>Source</code> instances by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16805"">webpack/webpack#16805</a></li>
<li>Update repo examples to include missing quotation by <a href=""https://github.com/snitin315""><code>@‚Äãsnitin315</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16812"">webpack/webpack#16812</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a href=""https://github.com/ahaoboy""><code>@‚Äãahaoboy</code></a> made their first contribution in <a href=""https://redirect.github.com/webpack/webpack/pull/16347"">webpack/webpack#16347</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href=""https://github.com/webpack/webpack/compare/v5.76.1...v5.76.2"">https://github.com/webpack/webpack/compare/v5.76.1...v5.76.2</a></p>
<h2>v5.76.1</h2>
<h2>Fixed</h2>
<ul>
<li>Added <code>assert/strict</code> built-in to <code>NodeTargetPlugin</code></li>
</ul>
<h2>Revert</h2>
<ul>
<li>Improve performance of <code>hashRegExp</code> lookup by <a href=""https://github.com/ryanwilsonperkin""><code>@‚Äãryanwilsonperkin</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16759"">webpack/webpack#16759</a></li>
</ul>
<h2>v5.76.0</h2>
<h2>Bugfixes</h2>
<ul>
<li>Avoid cross-realm object access by <a href=""https://github.com/Jack-Works""><code>@‚ÄãJack-Works</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16500"">webpack/webpack#16500</a></li>
<li>Improve hash performance via conditional initialization by <a href=""https://github.com/lvivski""><code>@‚Äãlvivski</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16491"">webpack/webpack#16491</a></li>
<li>Serialize <code>generatedCode</code> info to fix bug in asset module cache restoration by <a href=""https://github.com/ryanwilsonperkin""><code>@‚Äãryanwilsonperkin</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16703"">webpack/webpack#16703</a></li>
<li>Improve performance of <code>hashRegExp</code> lookup by <a href=""https://github.com/ryanwilsonperkin""><code>@‚Äãryanwilsonperkin</code></a> in <a href=""https://redirect.github.com/webpack/webpack/pull/16759"">webpack/webpack#16759</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/webpack/webpack/commit/8ac9616280aebfb61ae7484f06ad1979d73e6a56""><code>8ac9616</code></a> 5.76.3</li>
<li><a href=""https://github.com/webpack/webpack/commit/a3f49f3c9faafea22778e0bd78c2bac116a30d87""><code>a3f49f3</code></a> Merge pull request <a href=""https://redirect.github.com/webpack/webpack/issues/16635"">#16635</a> from PengBoUESTC/chore/enabledLibraryTypes-opt</li>
<li><a href=""https://github.com/webpack/webpack/commit/39fbd73db516918b07f7e2371ffcdfa5906fc4fa""><code>39fbd73</code></a> Merge pull request <a href=""https://redirect.github.com/webpack/webpack/issues/16756"">#16756</a> from pgoldberg/pgoldberg/addLoadersToModuleFilename...</li>
<li><a href=""https://github.com/webpack/webpack/commit/7e5fba30c9360ebee39b942924034daf4fc3204d""><code>7e5fba3</code></a> Merge pull request <a href=""https://redirect.github.com/webpack/webpack/issues/16806"">#16806</a> from snitin315/fix-typo-in-error</li>
<li><a href=""https://github.com/webpack/webpack/commit/b604d7818241ebef02e4785cf94b10a9ec0f4ebf""><code>b604d78</code></a> Merge pull request <a href=""https://redirect.github.com/webpack/webpack/issues/16808"">#16808</a> from snitin315/fix-node-path</li>
<li><a href=""https://github.com/webpack/webpack/commit/4a4ba2c6e9f2f9b92efa77a6ce8ed3ed2c674330""><code>4a4ba2c</code></a> Merge pull request <a href=""https://redirect.github.com/webpack/webpack/issues/16841"">#16841</a> from ShenHongFei/complete-node-builtins</li>
<li><a href=""https://github.com/webpack/webpack/commit/eadbd7d5e73228729416c514893bf2fb2a82d832""><code>eadbd7d</code></a> Merge pull request <a href=""https://redirect.github.com/webpack/webpack/issues/16810"">#16810</a> from snitin315/fix/progress-length</li>
<li><a href=""https://github.com/webpack/webpack/commit/66fe018a61e545343dbbe5f7e96349c0ffa72b22""><code>66fe018</code></a> fix: complete the missing nodejs builtin modules</li>
<li><a href=""https://github.com/webpack/webpack/commit/f58ff9beb63bd9c8d5b56d73c55cbe5b2e8ce5c7""><code>f58ff9b</code></a> fix: do not import non javascript chunks</li>
<li><a href=""https://github.com/webpack/webpack/commit/dbf7bf39ab96e1462603435c150d601bb69ebe4e""><code>dbf7bf3</code></a> 5.76.2</li>
<li>Additional commits viewable in <a href=""https://github.com/webpack/webpack/compare/v5.75.0...v5.76.3"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~thelarkinn"">thelarkinn</a>, a new releaser for webpack since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=webpack&package-manager=npm_and_yarn&previous-version=5.75.0&new-version=5.76.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/stanfordnlp/dsp/network/alerts).

</details>",2023-03-23T14:00:56Z,2023-03-23T14:01:27Z,0 days 00:00:31,bump webpack inspectappreactapp bump notessourced file correctly import use experimentsoutputmodule e module output href hreflimit console output progress bar length column provide href hrefadd miss nodejs builtin module support inspectorpromises readlinepromises streamconsumers href hrefwebpack bincli properly respect nodepath env variable href hrefimprove typo resolveresourceerrorhints href hrefadd miss loader token support modulefilenametemplate function call href hrefadd gaurd condition enabledlibrarytypes internal containerplugin href hrefnew contributorsa href make first contribution hrefa href make first contribution hrefa href make first contribution hreffull changelog bug miss semicolon generate bundle output publicpathruntime would cause concatenated runtime error href hrefremove redundant semicolon generate bundle runtime code onscriptcomplete function href hreffix bug realcontenthashplugin respect outputhashsalts ability cause force recalculation emit asset href hrefperformanceimprove memory runtime performance sourcemaps via hoist regular expression literal store variable href hrefcorrect deoptimization modulegraph due instance property declaration occur outside constructor href hrefdeveloper experienceimproved internal typing match webpacksources typing source instance href hrefupdate repo example include miss quotation href hrefnew contributorsa href make first contribution hreffull changelog assertstrict builtin nodetargetpluginrevertimprove performance hashregexp lookup href crossrealm object access href hrefimprove hash performance via conditional initialization href hrefserialize generatedcode info fix bug asset module cache restoration href hrefimprove performance hashregexp lookup href href truncatedcommitsadditional commits viewable maintainer changesthis version push npm dependabot resolve conflict pr long dont alter also trigger rebase manually comment dependabot rebase dependabotautomergestart dependabotautomergeenddependabot command optionsyou trigger dependabot action comment pr dependabot rebase rebase pr dependabot recreate recreate pr overwrite edits make dependabot merge merge pr ci pass dependabot squash merge squash merge pr ci pass dependabot cancel merge cancel previously request merge block automerging dependabot reopen reopen pr close dependabot close close pr stop dependabot recreate achieve result closing manually dependabot ignore major version close pr stop dependabot create major version unless reopen pr upgrade dependabot ignore minor version close pr stop dependabot create minor version unless reopen pr upgrade dependabot ignore dependency close pr stop dependabot create dependency unless reopen pr upgrade yourselfyou disable automated security fix pr repo
536,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/30,unplanned,Unknown,Add code for supporting arbitrary Hugging Face models None,2023-03-21T03:12:55Z,2023-03-23T20:11:43Z,2 days 16:58:48,add code support arbitrary hugging face model none
538,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/28,planned,Unknown,"feature(new-LM): GPT turbo integration - documentation: adding type hints and doc-strings 
- GPT turbo model integration with simple single-turn user prompting strategy.",2023-03-12T19:26:04Z,2023-03-23T12:51:46Z,10 days 17:25:42,featurenewlm gpt turbo integration documentation add type hint docstrings gpt turbo model integration simple singleturn user prompting strategy
539,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/27,planned,Unknown,"documentation: adding type hints and doc-strings (1/n) - Almost all of the changes are just to improve the readability of the code. 
- few exception handling changes
- few refactors to help linters identify potential bugs in development",2023-03-11T19:19:50Z,2023-03-12T19:26:35Z,1 days 00:06:45,documentation add type hint docstrings almost change improve readability code exception handle change refactors help linters identify potential bug development
542,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/24,unplanned,Unknown,Add function inspection app None,2023-03-10T08:46:02Z,2023-03-23T13:02:33Z,13 days 04:16:31,add function inspection app none
550,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/16,unplanned,Unknown,Resolve import issues for FAISS @okhat resolves last comments in #12 ,2023-02-19T16:24:08Z,2023-02-19T16:39:49Z,0 days 00:15:41,resolve import issue faiss okhat resolve last comment
552,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/14,unplanned,Unknown,Merging compilationv1 to main None,2023-02-13T19:51:32Z,2023-02-13T19:56:16Z,0 days 00:04:44,merge main none
554,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/12,unplanned,Unknown,"Add KNN for DEMONSTRATE stage Hi Omar & the team,
I was impressed by the DSP paper. Very excited about future applications and research!

It turned out that some tools/primitives/transformations weren't released. The paper says ""Though most of the functionality in this section is implemented, the primitives branch, knn, and crossval are currently
work-in-progress"". 

So here's my vision and naive implementation for KNN op. It isn't finished though - I'm thinking rn about vectorization process. RN, it's implemented as follows: each Example in the train should have the `vectorized` field. But I have a draft of how it should be organized. I'm thinking about, like, hf/sentence-tranformers vectorization function, that will take a name of an embeddder, download and apply it. Also, it's easy to add OpenAI embeddings as an additional vectorization function. And the last one - read from the memmap-file. What do you think about these?

Any ideas and comments are appreciated!",2023-02-04T00:42:24Z,2023-02-18T11:42:57Z,14 days 11:00:33,add knn demonstrate stage hi omar team impress dsp paper excite future application research turn toolsprimitivestransformations werent release paper say though functionality section implement primitive branch knn crossval currently workinprogress here vision naive implementation knn op isnt finish though im think rn vectorization process rn implement follow example train vectorized field draft organize im think like hfsentencetranformers vectorization function take name embeddder download apply also easy add openai embeddings additional vectorization function last one read memmapfile think idea comment appreciate
557,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/9,unplanned,Unknown,Fix typo in template_v2.py seperator -> separator,2023-01-27T03:29:17Z,2023-01-31T14:11:41Z,4 days 10:42:24,fix typo seperator separator
559,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/7,unplanned,Unknown,"No longer assume we are always running in a notebook If notebook is set to False, it will return a string instead of displaying a styled dataframe.",2023-01-25T22:49:20Z,2023-01-26T03:17:34Z,0 days 04:28:14,long assume always run notebook notebook set false return string instead display style dataframe
560,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/6,unplanned,Unknown,"Create __init__.py The dsp/evaluation directory was not found by my script, but adding a blank __init__.py fixes it/",2023-01-25T20:04:43Z,2023-01-25T20:05:57Z,0 days 00:01:14,create initpy dspevaluation directory find script add blank initpy fix
563,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/3,unplanned,Unknown,"Update Python package info, release 0.1.1 None",2023-01-24T00:46:54Z,2023-01-24T16:55:33Z,0 days 16:08:39,update python package info release none
564,Direct Implementation,https://github.com/stanfordnlp/dspy/pull/2,unplanned,Unknown,Add link to ArXiv paper None,2023-01-23T21:24:10Z,2023-01-23T21:38:37Z,0 days 00:14:27,add link arxiv paper none
