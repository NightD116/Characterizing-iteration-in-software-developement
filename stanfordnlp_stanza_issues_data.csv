,created_at,html_url,name,body,published_at
0,2024-04-03T07:52:38Z,https://github.com/stanfordnlp/stanza/pull/1378,,"For languages where the MWT words exactly make up the text of the token, build the pieces of the MWT using the text from the original token we are splitting.  Should fix a bunch of the errors observed in https://github.com/stanfordnlp/stanza/issues/1371",
1,2024-04-01T22:18:26Z,https://github.com/stanfordnlp/stanza/issues/1377,,"**Describe the bug**
With the introduction of multi-word tokens (MWT) for english, we came across a test case where the tokens of a multi-word token are not linked correctly to associated token ids.  

**To Reproduce**
Steps to reproduce the behavior:
1. Run the sentence:
```
I dunno.
```
2. Check the Universal Dependencies, in particular the tokens for `dunno` reveal that one of the tokens for that word is not linked to by its multi-word token:

```json

[

// multi-word token here for ""dunno""
// id only links to 2 and 4, missing 3  
  {
    ""end_char"": 7,
    ""id"": [
      2,
      4
    ],
    ""misc"": ""SpaceAfter=No"",
    ""start_char"": 2,
    ""text"": ""dunno""
  },

// ""du"" is linked by this multi-word token
  {
    ""deprel"": ""root"",
    ""end_char"": 4,
    ""feats"": ""Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin"",
    ""head"": 0,
    ""id"": 2,
    ""lemma"": ""do"",
    ""start_char"": 2,
    ""text"": ""du"",
    ""upos"": ""AUX"",
    ""xpos"": ""VBP""
  },
  
  / ""n"" not linked by multi-word token
  {
    ""deprel"": ""advmod"",
    ""end_char"": 5,
    ""head"": 2,
    ""id"": 3,
    ""lemma"": ""not"",
    ""start_char"": 4,
    ""text"": ""n"",
    ""upos"": ""PART"",
    ""xpos"": ""RB""
  },
  
  // ""no"" is linked by multi-word token
  {
    ""deprel"": ""discourse"",
    ""end_char"": 7,
    ""head"": 2,
    ""id"": 4,
    ""lemma"": ""no"",
    ""start_char"": 5,
    ""text"": ""no"",
    ""upos"": ""INTJ"",
    ""xpos"": ""UH""
  },
]

   
```

**Expected behavior**
The MWT token links to all of the children tokens it encompasses. id: `[2, 3, 4]`

**Environment (please complete the following information):**
 - OS: Mac OS Ventura 
 - Python version: Python 3.12.2 using Poetry 1.8.2
 - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e

**Additional context**
I'm not sure if this behaviour is intended or not.  Are the IDs of the MWT token intended to act as a tuple, i.e. a range, or should they include every token that's a member of the multi-word token?  If it's the latter then I believe this is a bug.",
2,2024-03-26T06:40:55Z,https://github.com/stanfordnlp/stanza/pull/1375,,"Make a variant of the dataloader which limits a batch to 5000 words or less (by default) for the Pipeline.  Should help avoid OOM for things such as a few very long sentence soaking up too many resources.  https://github.com/stanfordnlp/stanza/issues/1372
",
3,2024-03-25T13:13:58Z,https://github.com/stanfordnlp/stanza/issues/1374,,"**Describe the bug**
When I download the 'grc' model the download reaches 100%, but I get the following error:
ValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a

**To Reproduce**
Steps to reproduce the behavior:
1. Open the Python interpreter
```
import stanza
stanza.download('grc')

Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]
2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json
2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...
Downloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download
    request_file(
  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file
    assert_file_exists(path, md5, alternate_md5)
  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists
    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))
ValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a
```

**Expected behavior**
No error at the end of download.

**Environment (please complete the following information):**
 - OS: Ubuntu for Windows
 - Python version: 3.8.10
 - Stanza version: 1.8.1

**Additional context**
I installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.
It does not happen if I install stanza 1.2 instead.
",
4,2024-03-02T15:53:49Z,https://github.com/stanfordnlp/stanza/issues/1359,,"**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS]
 - Python version: [e.g. Python 3.6.8 from Anaconda]
 - Stanza version: [e.g., 1.0.0]

**Additional context**
Add any other context about the problem here.
",
5,2024-02-29T22:04:09Z,https://github.com/stanfordnlp/stanza/issues/1357,,"**Describe the bug**
Trying to make a Stanza pipeline with any biomedical package and receive the error 'KeyError: bert_finetune' while loading the POS tagger (occurs with or without i2b2 NER processor). 

**To Reproduce**
Steps to reproduce the behavior:
1. Running `nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'i2b2'})` or `nlp = stanza.Pipeline('en', package='craft', processors={'ner': 'i2b2'})` or `nlp = stanza.Pipeline('en', package='genia', processors={'ner': 'i2b2'})`
2.  Error, example for 'mimic' package:
```
`2024-02-29 16:41:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:
373k/? [00:00<00:00, 53.2MB/s]
2024-02-29 16:41:08 INFO: Downloaded file to /home/idies/stanza_resources/resources.json
INFO:stanza:Downloaded file to /home/idies/stanza_resources/resources.json
2024-02-29 16:41:09 INFO: Loading these models for language: en (English):
==============================
| Processor | Package        |
------------------------------
| tokenize  | mimic          |
| pos       | mimic_charlm   |
| lemma     | mimic_nocharlm |
| depparse  | mimic_charlm   |
| ner       | i2b2           |
==============================

INFO:stanza:Loading these models for language: en (English):
==============================
| Processor | Package        |
------------------------------
| tokenize  | mimic          |
| pos       | mimic_charlm   |
| lemma     | mimic_nocharlm |
| depparse  | mimic_charlm   |
| ner       | i2b2           |
==============================

2024-02-29 16:41:09 INFO: Using device: cpu
INFO:stanza:Using device: cpu
2024-02-29 16:41:09 INFO: Loading: tokenize
INFO:stanza:Loading: tokenize
2024-02-29 16:41:09 INFO: Loading: pos
INFO:stanza:Loading: pos
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[95], line 2
      1 #nlp = stanza.Pipeline('nl', processors={'ner': 'conll02'})
----> 2 nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'i2b2'})

File ~/miniconda3/lib/python3.9/site-packages/stanza/pipeline/core.py:305, in Pipeline.__init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)
    302 logger.debug(curr_processor_config)
    303 try:
    304     # try to build processor, throw an exception if there is a requirements issue
--> 305     self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
    306                                                                               pipeline=self,
    307                                                                               device=self.device)
    308 except ProcessorRequirementsException as e:
    309     # if there was a requirements issue, add it to list which will be printed at end
    310     pipeline_reqs_exceptions.append(e)

File ~/miniconda3/lib/python3.9/site-packages/stanza/pipeline/processor.py:193, in UDProcessor.__init__(self, config, pipeline, device)
    191 self._vocab = None
    192 if not hasattr(self, '_variant'):
--> 193     self._set_up_model(config, pipeline, device)
    195 # build the final config for the processor
    196 self._set_up_final_config(config)

File ~/miniconda3/lib/python3.9/site-packages/stanza/pipeline/pos_processor.py:32, in POSProcessor._set_up_model(self, config, pipeline, device)
     29 args = {'charlm_forward_file': config.get('forward_charlm_path', None),
     30         'charlm_backward_file': config.get('backward_charlm_path', None)}
     31 # set up trainer
---> 32 self._trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], device=device, args=args, foundation_cache=pipeline.foundation_cache)
     33 self._tqdm = 'tqdm' in config and config['tqdm']

File ~/miniconda3/lib/python3.9/site-packages/stanza/models/pos/trainer.py:44, in Trainer.__init__(self, args, vocab, pretrain, model_file, device, foundation_cache)
     40 self.optimizers = utils.get_split_optimizer(self.args['optim'], self.model, self.args['lr'], betas=(0.9, self.args['beta2']), eps=1e-6, weight_decay=self.args.get('initial_weight_decay', None), bert_learning_rate=self.args.get('bert_learning_rate', 0.0), is_peft=self.args.get(""peft"", False))
     42 self.schedulers = {}
---> 44 if self.args[""bert_finetune""]:
     45     import transformers
     46     warmup_scheduler = transformers.get_linear_schedule_with_warmup(
     47         self.optimizers[""bert_optimizer""],
     48         # todo late starting?
     49         0, self.args[""max_steps""])

KeyError: 'bert_finetune'`
```

**Expected behavior**
No issues running basic Stanza model - i.e. `nlp = stanza.Pipeline('nl', processors={'ner': 'conll02'})`
Output: 
```
`2024-02-29 16:59:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:
373k/? [00:00<00:00, 37.7MB/s]
2024-02-29 16:59:36 INFO: Downloaded file to /home/idies/stanza_resources/resources.json
INFO:stanza:Downloaded file to /home/idies/stanza_resources/resources.json
2024-02-29 16:59:37 INFO: Loading these models for language: nl (Dutch):
===============================
| Processor | Package         |
-------------------------------
| tokenize  | alpino          |
| mwt       | alpino          |
| pos       | alpino_charlm   |
| lemma     | alpino_nocharlm |
| depparse  | alpino_charlm   |
| ner       | conll02         |
===============================

INFO:stanza:Loading these models for language: nl (Dutch):
===============================
| Processor | Package         |
-------------------------------
| tokenize  | alpino          |
| mwt       | alpino          |
| pos       | alpino_charlm   |
| lemma     | alpino_nocharlm |
| depparse  | alpino_charlm   |
| ner       | conll02         |
===============================

2024-02-29 16:59:37 INFO: Using device: cpu
INFO:stanza:Using device: cpu
2024-02-29 16:59:37 INFO: Loading: tokenize
INFO:stanza:Loading: tokenize
2024-02-29 16:59:37 INFO: Loading: mwt
INFO:stanza:Loading: mwt
2024-02-29 16:59:37 INFO: Loading: pos
INFO:stanza:Loading: pos
2024-02-29 16:59:39 INFO: Loading: lemma
INFO:stanza:Loading: lemma
2024-02-29 16:59:39 INFO: Loading: depparse
INFO:stanza:Loading: depparse
2024-02-29 16:59:40 INFO: Loading: ner
INFO:stanza:Loading: ner
2024-02-29 16:59:41 INFO: Done loading processors!
INFO:stanza:Done loading processors!`
```


**Environment (please complete the following information):**
 - OS: Windows 
 - Python version: 3.9.17
 - Stanza version:  1.8.0

Not sure if there are some obvious dependencies missing on my end. Appreciate any help you can provide. Thank you!",
6,2024-02-24T23:44:50Z,https://github.com/stanfordnlp/stanza/pull/1354,,"Fix TOP_DOWN parser for da_arboretum, which needs to look at the actual root labels rather than just assume ROOT
",
7,2024-02-24T23:43:24Z,https://github.com/stanfordnlp/stanza/pull/1353,,Update resource building to allow multiple languages in a single slice and to use finetuned transformer models for sentiment.  The latter is made reasonably efficient using peft,
8,2024-02-24T07:46:12Z,https://github.com/stanfordnlp/stanza/pull/1350,,"Initial attempt to chop up long inputs to a transformer into pieces that the transformer can digest, even if it isn't necessarily going to give great results for the later tokens in the sentence.  Addresses https://github.com/stanfordnlp/stanza/issues/1294
",
9,2024-02-22T01:57:13Z,https://github.com/stanfordnlp/stanza/pull/1348,,"Add some mechanisms for building and manipulating a silver dataset for the constituency parser.  Filtering the trees by number of matching parsers seems to help make a better silver dataset, whereas filtering by variance does not.  Will continue experimenting",
10,2024-02-21T16:47:19Z,https://github.com/stanfordnlp/stanza/pull/1347,,"Integrate peft with the constituency parser.  While we're here, add a feature to the dependency parser to only finetune N layers instead of all the layers, although this didn't help get better dependency parsers",
11,2024-02-17T05:52:11Z,https://github.com/stanfordnlp/stanza/pull/1346,,"Eliminate goeswith phrases from the lemmatizer training data.  Doesn't do anything to the lemmatizer in the case of eval data (dev or test sets, and more importantly, Pipelines)

Addresses https://github.com/stanfordnlp/stanza/issues/1345 once the models are retrained

",
12,2024-02-16T21:23:59Z,https://github.com/stanfordnlp/stanza/issues/1345,,"```
nlp = stanza.Pipeline(lang=""en"")
nlp(""Hi Andrea"")
```

gives

```
...
    {
      ""id"": 2,
      ""text"": ""Andrea"",
      ""lemma"": ""andreabertone@enron_development"",
      ""upos"": ""PROPN"",
      ""xpos"": ""NNP"",
      ""feats"": ""Number=Sing"",
      ""head"": 1,
      ""deprel"": ""vocative"",
      ""start_char"": 3,
      ""end_char"": 9,
      ""ner"": ""S-PERSON"",
      ""multi_ner"": [
        ""S-PERSON""
      ]
    }
...
```

causing incorrect lemmas to leak from the training data.",
13,2024-02-14T21:20:09Z,https://github.com/stanfordnlp/stanza/pull/1344,,"Add some various tooling such as a 2nd optimizer and integration with peft to the dependency parser.  Unfortunately, aside from Marathi for some reason, there seems to be little in the way of a big win for either full finetuning or using peft on the depparser.  Will continue to CV for better training parameters which do have a benefit, and integrating this tooling should make it so that any models we do find can be immediately released without a new code release.",
14,2024-02-06T21:42:15Z,https://github.com/stanfordnlp/stanza/pull/1342,,the original code only works in cases where `n_docs` is even for `self.config.bert_finetune_begin_epoch=0.5` due to rounding error.,
15,2024-02-03T01:15:42Z,https://github.com/stanfordnlp/stanza/pull/1341,,"Add an oracle for the TOP_DOWN transition scheme - seems to be effective in improving the scores for a top down parser

Includes unambiguous & ambiguous transitions

Also, keep track of stats for both the top_down and in_order oracles",
16,2024-01-29T23:29:12Z,https://github.com/stanfordnlp/stanza/issues/1339,,"**Describe the bug**
Depending on whether `tokenize_pretokenized` and `tokenize_no_ssplit` are each `True` or `False`,  the following sentence results in the coref processor yielding either the exception `ValueError: The coref model predicted a span that crossed two sentences!` or the exception `IndexError: list index out of range` error, on lines 120 and 119 of `stanza/pipeline/coref_processor.py`, respectively.

The sentence: `The son of Mr. and Mrs. X. He is four during the events of the first book . <eos>`

**To Reproduce**
Steps to reproduce the behavior:

Set up code:
```python
import stanza

s = ""The son of Mr. and Mrs. X. He is four during the events of the first book . <eos>""

pipeline = stanza.Pipeline(lang=""en"", processors=""tokenize,pos,lemma,depparse,coref"")
pipeline_no_ssplit = stanza.Pipeline(lang=""en"", processors=""tokenize,pos,lemma,depparse,coref"", tokenize_no_ssplit=True)
pipeline_pretok = stanza.Pipeline(lang=""en"", processors=""tokenize,pos,lemma,depparse,coref"", tokenize_pretokenized=True)
pipeline_pretok_no_ssplit = stanza.Pipeline(lang=""en"", processors=""tokenize,pos,lemma,depparse,coref"", tokenize_pretokenized=True, tokenize_no_ssplit=True)
```

Then the following line of code:

```py
a = pipeline(s)
```

produces the exception:

```py
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/apatil/anaconda3/envs/base_nlp/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 476, in __call__
    return self.process(doc, processors)
  File ""/Users/apatil/anaconda3/envs/base_nlp/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 427, in process
    doc = process(doc)
  File ""/Users/apatil/anaconda3/envs/base_nlp/lib/python3.9/site-packages/stanza/pipeline/coref_processor.py"", line 120, in process
    raise ValueError(""The coref model predicted a span that crossed two sentences!  Please send this example to us on our github"")
ValueError: The coref model predicted a span that crossed two sentences!  Please send this example to us on our github
```

whereas any of the following lines of code:

```py
b = pipeline_nossplit(s)
c = pipeline_pretok(s)
d = pipeline_pretok_nossplit(s)
```

produce the exception:

```py
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/apatil/anaconda3/envs/base_nlp/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 476, in __call__
    return self.process(doc, processors)
  File ""/Users/apatil/anaconda3/envs/base_nlp/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 427, in process
    doc = process(doc)
  File ""/Users/apatil/anaconda3/envs/base_nlp/lib/python3.9/site-packages/stanza/pipeline/coref_processor.py"", line 119, in process
    if sent_ids[span[1]] != sent_id:
IndexError: list index out of range
```

**Expected behavior**
All of these should just work. They should not throw any of the issues above.

**Environment (please complete the following information):**
 - OS: Reproduced on Mac (with CPU) and Oracle Linux (with GPU)
 - Python version: `Python 3.9.16 | packaged by conda-forge`
 - Stanza version: `1.7.0`

**Additional context**
I have also seen sporadic instances of the `coref model predicted a span that crossed two sentences!` error elsewhere, but previously only with a large group of sentences in a single doc, omitting any one of which, strangely, resulting in the error no longer surfacing. This is the first time I've been able to reproduce it with a single sentence, hence why I am reporting it. I can, however, provide other batches of sentences that result in the same issue, if it helps.
",
17,2024-01-28T18:19:29Z,https://github.com/stanfordnlp/stanza/pull/1338,,"This PR enables wandb support, including gradient tracking, for the coref tagger.",
18,2024-01-28T18:17:22Z,https://github.com/stanfordnlp/stanza/pull/1337,,"- Optional two-stage optimization scheme after first optim converged
- wandb gradient logging
- Model checkpointing with optimizer
",
19,2024-01-28T08:40:07Z,https://github.com/stanfordnlp/stanza/pull/1336,,"- PEFT and training ops (wandb) updates for NER
- optional two stage optimization (doesn't seem to help much)

depends on #1337 and #1338 (merged)",
20,2024-01-25T23:58:07Z,https://github.com/stanfordnlp/stanza/pull/1335,,"Add a PEFT wrapper for the Sentiment training.

Works quite well on English, actually, even without splitting the optimizer or implementing any form of scheduling.
With no finetuning, adding electra-large to the 3 class English dataset (SST plus a few other pieces) gets 70 Macro F1.
The base finetuning gets between 74-75 macro F1 on sstplus, but frequently fails to successfully train, getting somewhere around 60 F1
Training with PEFT gets in the 74-75 F1 range each time, with no failures observed so far.

Adds a chunk of test to the sentiment training which starts the Pipeline with a peft-trained model

Also included is adding a uses-charlm flag to the config, so that inadvertently passing a charlm (such as via Pipeline) to the sentiment model doesn't blow up if it was trained w/o a charlm",
21,2024-01-20T01:31:10Z,https://github.com/stanfordnlp/stanza/issues/1333,,"Hello

I got an error saying that the model predicted span that crosses two sentences and to send the example to github. Here is my code (pretty simple):

`import stanza

pipe = stanza.Pipeline(""en"", processors=""tokenize, coref"")
out = pipe(""""""If an electrical machine or equipment generates mechanical vibrations when in service, e.g. because it is out of balance, the vibration amplitude measured on the machine or the equipment on board shall not lie outside area A. For this evaluation, reference is made only to the self-generated vibration components. Area A may only be utilized if the loading of all components, with due allowance for local excess vibration, does not impair reliable long-term operation"""""")

print(out)` 

My guess is on the term ""Area A"". Is the model currently unable to process coreference that cross two sentence? What can I do about the sentence? 

Thank you
",
22,2024-01-17T19:18:40Z,https://github.com/stanfordnlp/stanza/issues/1332,,"**Describe the bug**
Whether I try and create a pipeline or download a model, I get mismatching md5 values

**To Reproduce**
```py
import stanza
stanza.download('en') # alternatively, stanza.Pipeline('en')
```
```
ValueError: md5 for C:\Users\vaavew\stanza_resources\en\default.zip is d788b1276f5eaa65c584543e2906db5f, expected d42b2d71cf57acd04ee9c4ef5b66a98f
# alternatively,
# ValueError: md5 for C:\Users\vaavew\stanza_resources\en\tokenize\combined.pt is d788b1276f5eaa65c584543e2906db5f, expected 10308f3db6c36e7c27aee30dea92c786
```

**Expected behavior**
I would have expected the models to download appropriately

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: 3.11
 - Stanza version: 1.7.0

**Additional context**
I'm working through a work VPN, and I turn the proxy on to download. The downloads reach 100%.
",
23,2024-01-14T21:25:48Z,https://github.com/stanfordnlp/stanza/pull/1331,,"Potentially lowercase the data in a lemmatizer if all of the training data (or a user flag) requested it

Testing additions:

Add a basic unit test of the all_lowercase function Add a test of the caseless lemmatizer in the Pipeline Test that the Latin ITTB lemmatizer is marked as caseless.  Check that the results for capitalized text is as expected

Addresses https://github.com/stanfordnlp/stanza/issues/1330
",
24,2024-01-10T17:48:02Z,https://github.com/stanfordnlp/stanza/issues/1329,,"I noticed the following note in the [docs](https://stanfordnlp.github.io/stanza/data_conversion.html).

```
Note:
convert_dict is now marked as deprecated, as internally we use the Document object everywhere. If you have a use case where you need it, please let us know!
```

I still use convert_dict sometimes because libraries for parsing coreference resolution data from conll files in the CoNLL-2012 Shared Task format, for example using `conll_transform.compute_chains` in [boberle/corefconversion](https://github.com/boberle/corefconversion), often require accessing the conll data as List[List[List]]. Some datasets as recent as 2020/2021 use this format. That being said I am not sure if this is a use case worth incentivizing, but maybe it would make sense to have some way to convert CoNLL-2012 coreference format into the Stanza format.",
25,2024-01-07T00:54:24Z,https://github.com/stanfordnlp/stanza/issues/1328,,"Hi,
I saw [Quote Attribution is supported in CoreNLP](https://stanfordnlp.github.io/CoreNLP/quote.html), is there any way to do it in Python?
Thanks!",
26,2023-12-20T15:31:45Z,https://github.com/stanfordnlp/stanza/issues/1324,,"In your webpage you report results on Greek **gud** model : https://stanfordnlp.github.io/stanza/performance.html
But when I'm trying to download available models only this model for Greek is available,  namely Greek **gdt** and  in the huggingface platform as well--
is the Greek GUD model available somewhere online or not?",
27,2023-12-15T10:03:36Z,https://github.com/stanfordnlp/stanza/pull/1322,,"Add SpacesAfter and SpacesBefore when processing a document with the Pipeline.

https://github.com/stanfordnlp/stanza/issues/1315",
28,2023-12-13T21:47:12Z,https://github.com/stanfordnlp/stanza/pull/1321,,"## Description
Resolve `DeprecationWarning`'s emitted for invalid escape sequences in python 3.11 and newer.

## Fixes Issues
Relates to #1293.

This fixes py3.11+ compatibility that arises from invalid escape literals in strings. Raw strings should be used for regexes and other strings that contain legitimate backslahes.

[Python library reference](https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals).
Or on [SO](https://stackoverflow.com/questions/50504500/deprecationwarning-invalid-escape-sequence-what-to-use-instead-of-d), if you prefer.

I have made sure that no other instances of this error are present in codebase (via `ruff check . --select W605`, here's [this rule](https://docs.astral.sh/ruff/rules/invalid-escape-sequence/)) 

## Unit test coverage
This issue should be addressed at the linter level. Ready to contribute a linting solution to catch this if authors are ready to accept some linting pipeline (which would be a good thing to do).

## Known breaking changes/behaviors
This change is fully backwards-compatible. Sorry for whitespace removed on autosave, I hope it was not so important for you, guys...
",
29,2023-12-13T05:37:42Z,https://github.com/stanfordnlp/stanza/pull/1320,,"This PR splits out the utility optimizer into separate parts for Bert and non-Bert; this method is backwards compatible, such that `get_optimizer` gets retained to be used for methods that have not yet been migrated, while a new `get_split_optimizer` is used for methods that desire an optimizer that has been split out. 

Further, the optimizer takes an `is_peft` option which stages the optimizer to tune `.parameters()` of the Bert model instead of filtering for`.named_parameters()` which is selected. This allows HF Peft to do weird shenanigans to the parameters value, and we will—when the weights trainable is being constrained by the Peft library—tune what it tells us to tune instead of tuning things that we select via its name. 

Taking advantage of this fact that we have a split optimizer now, Part of Speech tagging with Bert finetuning features a learning rate scheduler which a warmup and a linear decay if the user requests the Bert to be tuned. ",
30,2023-12-04T12:24:15Z,https://github.com/stanfordnlp/stanza/pull/1318,,"## Description
The code was updated to include a missing parameter in the function call download_resources_json(). The modification involved adding a new parameter resources_filepath=None to the function signature. This change allows for specifying a custom file path for the downloaded resources, while still providing a default value of None if no path is specified.

This modification ensures that the function call is now properly formatted and includes all required parameters. The bug has been fixed, and the code is now able to handle the download of resources correctly.

## Fixes Issues
#1317 
",
31,2023-12-04T12:10:54Z,https://github.com/stanfordnlp/stanza/issues/1317,,"The following is the code that triggers the bug
```
import stanza
proxies = {'http': 'http://ip:port', 'https': 'http://ip:port'}
stanza.download('en', proxies=proxies)  
```
exception
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
...
...
...
TypeError: expected str, bytes or os.PathLike object, not dict
```",
32,2023-12-04T02:17:55Z,https://github.com/stanfordnlp/stanza/issues/1316,,"**Describe the bug**
I was following the tutorial on the website trying to load a constituency parsing model using the following code

```
import stanza

pipe = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', package='default_accurate')
# nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')
doc = pipe('This is a test')
for sentence in doc.sentences:
    print(sentence.constituency)
```

However, the following key error appeared

```
Traceback (most recent call last):
  File ""/data/dzc/repo/Text2NKG-main/test.py"", line 3, in <module>
    pipe = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', package='default_accurate')
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 304, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/pipeline/processor.py"", line 193, in __init__
    self._set_up_model(config, pipeline, device)
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/pipeline/pos_processor.py"", line 32, in _set_up_model
    self._trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], device=device, args=args, foundation_cache=pipeline.foundation_cache)
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/models/pos/trainer.py"", line 33, in __init__
    self.load(model_file, pretrain, args=args, foundation_cache=foundation_cache)
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/models/pos/trainer.py"", line 124, in load
    self.model = Tagger(self.args, self.vocab, emb_matrix=emb_matrix, share_hid=self.args['share_hid'], foundation_cache=foundation_cache)
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/models/pos/model.py"", line 86, in __init__
    bert_model, bert_tokenizer = load_bert(self.args['bert_model'], foundation_cache)
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/models/common/foundation_cache.py"", line 95, in load_bert
    return foundation_cache.load_bert(model_name)
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/models/common/foundation_cache.py"", line 39, in load_bert
    model, tokenizer = bert_embedding.load_bert(transformer_name)
  File ""/home/dzc/miniconda3/envs/text2nkg/lib/python3.9/site-packages/stanza/models/common/bert_embedding.py"", line 52, in load_bert
    bert_model = AutoModel.from_pretrained(model_name)
  File ""/data/dzc/repo/Text2NKG-main/transformers/src/transformers/modeling_auto.py"", line 376, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File ""/data/dzc/repo/Text2NKG-main/transformers/src/transformers/configuration_auto.py"", line 187, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict[""model_type""]]
KeyError: 'electra'
```

I check the dict variable CONFIG_MAPPING and did not find the proper mapping for electra
```
CONFIG_MAPPING = OrderedDict(
    [
        (""t5"", T5Config,),
        (""distilbert"", DistilBertConfig,),
        (""albert"", AlbertConfig,),
        (""camembert"", CamembertConfig,),
        (""xlm-roberta"", XLMRobertaConfig,),
        (""bart"", BartConfig,),
        (""roberta"", RobertaConfig,),
        (""flaubert"", FlaubertConfig,),
        (""bert"", BertConfig,),
        (""openai-gpt"", OpenAIGPTConfig,),
        (""gpt2"", GPT2Config,),
        (""transfo-xl"", TransfoXLConfig,),
        (""xlnet"", XLNetConfig,),
        (""xlm"", XLMConfig,),
        (""ctrl"", CTRLConfig,),
    ]
)
```
",
33,2023-12-01T01:42:51Z,https://github.com/stanfordnlp/stanza/pull/1314,,Merge v1.7.0 from dev to main,
34,2023-11-28T05:45:45Z,https://github.com/stanfordnlp/stanza/issues/1312,,"but why there is no reference for below code for training? this repo seems more intuitive for people like me having worked with transformer lib. Could you please let me know if it is still under development?
https://github.com/stanfordnlp/stanza/tree/main/stanza/models/ner
  
           
              There's a couple documents on training NER models:

https://stanfordnlp.github.io/stanza/new_language_ner.html

https://stanfordnlp.github.io/stanza/retrain_ner.html

Training a new model on an existing language will be closer to the ""new_language_ner"" page than the retraining page, but you won't need to find new word vectors or build the charlm.  Feel free to ask if you have any questions, but I do suggest starting a new issue so we can keep this issue focused on checkpoint improvements.

_Originally posted by @AngledLuffa in https://github.com/stanfordnlp/stanza/issues/724#issuecomment-1829070158_
            ",
35,2023-11-22T10:05:33Z,https://github.com/stanfordnlp/stanza/issues/1310,,"**Describe the bug**

Pipeline creation for language ""si""
``` python
stanza.Pipeline(""si"", processors='tokenize', download_method=stanza.DownloadMethod.REUSE_RESOURCES)
```
causes KeyError 
``` python
File ~/miniconda3/lib/python3.8/site-packages/stanza/resources/common.py:182, in add_mwt(processors, resources, lang)
    175 """"""Add mwt if tokenize is passed without mwt.
    176 
    177 If tokenize is in the list, but mwt is not, and there is a corresponding
    178 tokenize and mwt pair in the resources file, mwt is added so no missing
    179 mwt errors are raised.
    180 """"""
    181 value = processors[TOKENIZE]
--> 182 if value == ""default"" and MWT in resources[lang]['default_processors']:
    183     logger.warning(""Language %s package default expects mwt, which has been added"", lang)
    184     processors[MWT] = 'default'

KeyError: 'default_processors'
```

**To Reproduce**
Steps to reproduce the behavior:
``` python
stanza.Pipeline(""si"", processors='tokenize', download_method=stanza.DownloadMethod.REUSE_RESOURCES)
```

**Expected behavior**
I expected all languages to (a) have some basic tokenizer or (b) have some 'default_processors' or (c) at the very least throw some nice user-facing error if no 'default_processors' are present. Further inspection of default `config.json` shows that there are 4 languages without 'default_processors' at the moment. Is this a correct behavior? Are there any guarantees regarding json.config fields? Maybe some schema?

Anyways at the moment this crash seems very user-unfriendly and I believe this could be fixed by either (a) some json schema enforced on config.json as part of CI/CD to eliminate the crash altogether or (b) some proper error thrown, explaining the lack of default processors for target language.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: 3.8.17
 - Stanza version: 1.5.0

",
36,2023-11-16T05:30:35Z,https://github.com/stanfordnlp/stanza/pull/1309,,"WIP: add English coref to the pipeline

- still need a script to rebuild data files
- also need to turn the coref results in the pipeline into objects of some sort",
37,2023-11-08T21:49:15Z,https://github.com/stanfordnlp/stanza/pull/1307,,"Make a dataset shuffler which can shuffle per batch without reading all of the batches into memory first.  Saves memory on some of the excessively large datasets, such as DE_HDT

@Jemoka ",
38,2023-11-06T07:05:13Z,https://github.com/stanfordnlp/stanza/pull/1306,,"## Description
Some languages, like German, OOM when training with the new PyTorch `Dataset` scheme as the overhead loading multiple datasets into separate `DataLoader` and then mixing them didn't work well. We did this because some entire input files wouldn't have upos/xpos/ufeats, and we don't want to calculate loss. 
Instead, this PR elects to create a `_ShadowDataset` object between them, and masks out loss (by turning the upos/xpos/etc. into padding tokens at *batch time*) with the exact *sentences* which came from datasets that doesn't have upos/xpos/ufeats masked out only.

## Unit test coverage
Passes all tests in `stanza.tests.pos.test_tagger`",
39,2023-11-03T21:03:54Z,https://github.com/stanfordnlp/stanza/issues/1305,,"**Describe the bug**
I have encountered a sentence where Stanza messes up and breaks a single sentence into two sentences.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to http://stanza.run/
2. Type in the sentence: `Just thinking about bedtime made Alice start to doze off.` and click Submit.
3. See error – sentence incorrectly broken into two separate sentences with separate roots.

![Screenshot 2023-11-03 at 2 00 01 PM](https://github.com/stanfordnlp/stanza/assets/126208852/19d80af2-6ab5-451e-80bc-db00228bf7aa)


**Expected behavior**
The parse returns a single sentence.  
**Environment (please complete the following information):**
 - Stanza version: 1.6.1

**Additional context**
Note that this sentence was parsing correctly before in Stanza 1.5 and earlier, so something with the model broke its behaviour in Stanza 1.6.x",
40,2023-10-31T11:20:59Z,https://github.com/stanfordnlp/stanza/issues/1304,,"Hi!
 
I have a couple of issues when training the tokenizer and lemmatizer, so I apologize in advance for the long text.

I managed to train the tokenizer, but when the script goes into predict mode I’m receiving the following error:

```
2023-10-20 16:14:14 INFO: Running dev step with args: ['--mode', 'predict', '--txt_file', '../data/processed/tokenize/ang_test.dev.txt', '--lang', 'ang', '--conll_file', '/var/folders/4c/zqzwjpjn42xbggh1sw1255nh0000gn/T/tmp897o2440', '--shorthand', 'ang_test', '--mwt_json_file', '../data/processed/tokenize/ang_test-ud-dev-mwt.json', '--step', '5000', '--save_name', 'ang_test_tokenizer.pt', '--save_dir', 'saved_models/tokenize']
2023-10-20 16:14:14 INFO: Running tokenizer in predict mode
2023-10-20 16:14:15 INFO: OOV rate:  0.000% (     0/  6925)
Traceback (most recent call last):
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/common/utils.py"", line 144, in ud_scores
    gold_ud = ud_eval.load_conllu_file(gold_conllu_file)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/conll18_ud_eval.py"", line 656, in load_conllu_file
    return load_conllu(_file,treebank_type)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/conll18_ud_eval.py"", line 241, in load_conllu
    process_word(word)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/conll18_ud_eval.py"", line 235, in process_word
    process_word(parent)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/conll18_ud_eval.py"", line 235, in process_word
    process_word(parent)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/conll18_ud_eval.py"", line 235, in process_word
    process_word(parent)
  [Previous line repeated 1 more time]
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/conll18_ud_eval.py"", line 227, in process_word
    raise UDError(""There is a cycle in the sentence that ends at line %d"" % line_idx)
stanza.utils.conll18_ud_eval.UDError: There is a cycle in the sentence that ends at line 876
```
What does that description error means? I have checked my data in that line, for the three splits, and I can’t find anything weird in there.

Related to this, how important is this step in the training process? I’m thinking on submitting this language model for consideration to be added as a new language, so I guess that the predict mode is important for evaluation, but not entirely sure. 

About the lemmatizer, I have the treebank ready prior to training the lemmatizer, no errors whatsoever. However I encounter the following error when running the lemmatizer training line.
 
```
(nlp) dario@192 stanza % python3 -m stanza.utils.training.run_lemma UD_Old_English-TEST --num_epoch 2
2023-10-20 16:34:01 INFO: Training program called with:
/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/training/run_lemma.py UD_Old_English-TEST --num_epoch 2
2023-10-20 16:34:01 DEBUG: UD_Old_English-TEST: ang_test
2023-10-20 16:34:01 INFO: UD_Old_English-TEST: saved_models/lemma/ang_test_nocharlm_lemmatizer.pt does not exist, training new model
2023-10-20 16:34:01 INFO: Running train lemmatizer for UD_Old_English-TEST with args ['--train_file', '../data/processed/lemma/ang_test.train.in.conllu', '--eval_file', '../data/processed/lemma/ang_test.dev.in.conllu', '--output_file', '/var/folders/4c/zqzwjpjn42xbggh1sw1255nh0000gn/T/tmpkbo6f5mj', '--gold_file', '../data/processed/lemma/ang_test.dev.gold.conllu', '--shorthand', 'ang_test', '--num_epoch', '60', '--mode', 'train', '--num_epoch', '2']
2023-10-20 16:34:01 INFO: Running lemmatizer in train mode
2023-10-20 16:34:01 INFO: [Loading data with batch size 50...]
2023-10-20 16:34:02 DEBUG: 453 batches created.
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/training/run_lemma.py"", line 149, in <module>
    main()
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/training/run_lemma.py"", line 146, in main
    common.main(run_treebank, ""lemma"", ""lemmatizer"", add_lemma_args, sub_argparse=lemmatizer.build_argparse(), build_model_filename=build_model_filename, choose_charlm_method=choose_lemma_charlm)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/training/common.py"", line 183, in main
    run_treebank(mode, paths, treebank, short_name,
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/training/run_lemma.py"", line 123, in run_treebank
    lemmatizer.main(train_args)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/lemmatizer.py"", line 109, in main
    train(args)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/lemmatizer.py"", line 132, in train
    dev_doc = CoNLL.conll2doc(input_file=args['eval_file'])
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/utils/conll.py"", line 114, in conll2doc
    return Document(doc_dict, text=None, comments=doc_comments)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/common/doc.py"", line 58, in __init__
    self._process_sentences(sentences, comments)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/common/doc.py"", line 137, in _process_sentences
    sentence = Sentence(tokens, doc=self)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/common/doc.py"", line 417, in __init__
    self._process_tokens(tokens)
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/common/doc.py"", line 457, in _process_tokens
    self.rebuild_dependencies()
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/common/doc.py"", line 663, in rebuild_dependencies
    if is_complete_dependencies and is_complete_words: self.build_dependencies()
  File ""/Users/dario/VSCode-Projects/stanza-train/stanza/stanza/models/common/doc.py"", line 677, in build_dependencies
    head = self.words[word.head - 1]
IndexError: list index out of range
```
What does that error mean? I mean, I know what it means, but I don’t know to where or what it refers to. Does it have something to do with the format of the data?

Thanks in advance for your help, and apologies again for the long text.
",
41,2023-10-25T07:34:50Z,https://github.com/stanfordnlp/stanza/pull/1303,,"In order to ensure long-term ability to augment the PoS tagging data against various types of augmentation (so far, punctuation removal on some sentences), and in order to make the DataLoader more efficient and compatible with current PyTorch semantics, we rename models.pos.data.DataLoader to models.pos.data.Dataset which has a to_loader() function that yields a torch.utils.data.DataLoader.
The data in this loader can dynamically be augmented per epoch (by changing the getitem function to perform any augmentation with a certain chance each time getitem is called), because the PyTorch DataLoader lazily fetches data only when it is accessed.

Debugged & squashed version of https://github.com/stanfordnlp/stanza/pull/1296",
42,2023-10-24T19:58:00Z,https://github.com/stanfordnlp/stanza/pull/1302,,"## Description
This is a followup to #1290 which allows manual control of MWT splitting via the `tokenize_postprocessor`. We implement this via a new attribute `MEXP=Yes` to denote pre-deliniated MWT that the MWT processor shouldn't touch.

For instance:

```python
nlp = stanza.Pipeline(lang=""fr"", processors=""tokenize"",
                                    tokenize_postprocessor=lambda draft: do_stuff_to_draft(draft))
```

whereby, the single argument passed to `tokenize_postprocessor` is a list of lists containing string sentence and word tokenizations

```
[['Le', 'prince', 'va', 'manger', ('du', True), 'poulet', ('aux', True), 'les', 
   'magasins', (""aujourd'hui"", [""aujourd'"", ""hui""]), '.']]
```

With this postprocessor return, `du`, `aux` are requested to be split via the traditional MWT splitter, whereas a user-defined split is provided for `aujourd'hui`.

## Unit Test Coverage
`test_postprocessor_mwt` is created to check for this functionality, and `pipeline/test_tokenizer.py` contains updates to support MWT override.
",
43,2023-10-18T14:40:05Z,https://github.com/stanfordnlp/stanza/issues/1300,,"Trying to run a gunicorn/Flask Stanza app in a Docker container.
Tried with different worker types and numbers but see each worker being killed.
Current ending of Dockerfile is:
```
EXPOSE 20900
ENTRYPOINT poetry run python -m gunicorn --worker-tmp-dir /dev/shm  -workers=2 --threads=4 --worker-class=sync 'isagog_api.nlp_api:app' -b 0.0.0.0:20900
```
docker compose up with a docker-compose.yml as follows:
```
version: ""3.8""
services:
  isagog-nlp-gu:
    build: .
    image: isagog-nlp-gu:v0.1
    container_name: isagog-nlp-gu
    restart: unless-stopped
    ports:
      - ""20900:20900/tcp""
```
yields repeating restarts of workers with the pytorch model being always stopped around 83-87%:
```
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Using device: cpu
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: tokenize
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Done loading processors!
isagog-nlp-gu  | 2023-10-18 16:36:55 WARNING: Language it package default expects mwt, which has been added
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading these models for language: it (Italian):
isagog-nlp-gu  | =================================
isagog-nlp-gu  | | Processor | Package           |
isagog-nlp-gu  | ---------------------------------
isagog-nlp-gu  | | tokenize  | combined          |
isagog-nlp-gu  | | mwt       | combined          |
isagog-nlp-gu  | | pos       | combined_charlm   |
isagog-nlp-gu  | | lemma     | combined_nocharlm |
isagog-nlp-gu  | | depparse  | combined_charlm   |
isagog-nlp-gu  | =================================
isagog-nlp-gu  | 
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Using device: cpu
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: tokenize
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: mwt
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: mwt
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: pos
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: pos
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: lemma
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: lemma
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: depparse
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Loading: depparse
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Done loading processors!
isagog-nlp-gu  | 2023-10-18 16:36:55 INFO: Done loading processors!
(…)s-summarization/resolve/main/config.json: 100%|██████████| 909/909 [00:00<00:00, 6.71MB/s]
pytorch_model.bin:  83%|████████▎ | 2.59G/3.13G [00:25<00:05, 104MB/s][2023-10-18 16:37:22 +0200] [7] [CRITICAL] WORKER TIMEOUT (pid:23)
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [7] [CRITICAL] WORKER TIMEOUT (pid:24)
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [23] [INFO] Worker exiting (pid: 23)
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [24] [INFO] Worker exiting (pid: 24)
pytorch_model.bin:  83%|████████▎ | 2.59G/3.13G [00:25<00:05, 101MB/s]
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [7] [ERROR] Worker (pid:23) exited with code 1
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [7] [ERROR] Worker (pid:23) exited with code 1.
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [102] [INFO] Booting worker with pid: 102
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [7] [ERROR] Worker (pid:24) exited with code 1
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [7] [ERROR] Worker (pid:24) exited with code 1.
isagog-nlp-gu  | [2023-10-18 16:37:22 +0200] [103] [INFO] Booting worker with pid: 103
isagog-nlp-gu  | 2023-10-18 16:37:24 INFO: Loading these models for language: it (Italian):
isagog-nlp-gu  | ========================
isagog-nlp-gu  | | Processor | Package  |
isagog-nlp-gu  | ------------------------
isagog-nlp-gu  | | tokenize  | combined |
isagog-nlp-gu  | | mwt       | combined |
isagog-nlp-gu  | | ner       | fbk      |
isagog-nlp-gu  | ========================
isagog-nlp-gu  | 
```
",
44,2023-10-17T15:44:33Z,https://github.com/stanfordnlp/stanza/pull/1299,,"# Description
In order to ensure long-term ability to augment the PoS tagging data against various types of augmentation (so far, punctuation removal on some sentences), and in order to make the DataLoader more efficient and compatible with current PyTorch semantics, we rename models.pos.data.DataLoader to models.pos.data.Dataset which has a to_loader() function that yields a torch.utils.data.DataLoader.
The data in this loader can dynamically be augmented per epoch (by changing the __getitem__ function to perform any augmentation with a certain chance each time __getitem__ is called), because the PyTorch DataLoader lazily fetches data only when it is accessed.
",
45,2023-10-16T22:49:25Z,https://github.com/stanfordnlp/stanza/pull/1298,,"Add the ability to Semgrex to process and print out treebanks with enhanced graphs.  There is also an option to use the Semgrex query over the enhanced graph, rather than using the basic dependencies.  Needs a new CoreNLP release to be functional",
46,2023-10-16T21:21:17Z,https://github.com/stanfordnlp/stanza/pull/1297,,"# Description
In order to ensure long-term ability to augment the PoS tagging data against various types of augmentation (so far, punctuation removal on some sentences), and in order to make the DataLoader more efficient and compatible with current PyTorch semantics, we rename models.pos.data.DataLoader to models.pos.data.Dataset which has a to_loader() function that yields a torch.utils.data.DataLoader.
The data in this loader can dynamically be augmented per epoch (by changing the __getitem__ function to perform any augmentation with a certain chance each time __getitem__ is called), because the PyTorch DataLoader lazily fetches data only when it is accessed.

Supersedes #1296",
47,2023-10-15T22:55:39Z,https://github.com/stanfordnlp/stanza/pull/1296,,"## Description
In order to ensure long-term ability to augment the PoS tagging data against various types of augmentation (so far, punctuation removal on some sentences), and in order to make the DataLoader more efficient and compatible with current PyTorch semantics, we rename `models.pos.data.DataLoader` to `models.pos.data.Dataset` which has a `to_loader()` function that yields a `torch.utils.data.DataLoader`. 
The data in this loader can dynamically be augmented per epoch (by changing the `__getitem__` function to perform any augmentation with a certain chance each time `__getitem__` is called), because the PyTorch `DataLoader` lazily fetches data only when it is accessed. 

## Unit test coverage
<img width=""645"" alt=""image"" src=""https://github.com/stanfordnlp/stanza/assets/28765741/cb2f2d98-6f49-48f6-9c90-4d20bd007bc0"">
<img width=""705"" alt=""image"" src=""https://github.com/stanfordnlp/stanza/assets/28765741/b37a5aad-3ff9-448f-9ce6-f367399a0276"">

",
48,2023-10-15T03:23:52Z,https://github.com/stanfordnlp/stanza/pull/1295,,"Use the networkx library to represent enhanced dependencies when reading in a UD data file

Also, correctly process extra words if `ignore_gapping` is `False`.  That includes reading them, attaching them as a second list to the sentence, and then outputting them back in the conll or dict formats

The networkx graph is a little slow, so we lazy initialize it (such as when using the Pipeline, which won't create enhanced dependencies)",
49,2023-10-05T00:17:45Z,https://github.com/stanfordnlp/stanza/issues/1292,,"
Hi there, 

In the page for constituency parsing pipeline, the scores for a Turkish parser on Starlang was introduced: https://stanfordnlp.github.io/stanza/constituency.html

However, when I loaded the model for Turkish:
nlp = stanza.Pipeline(lang='tr')

It showed:
=============================
| Processor | Package       |
-----------------------------
| tokenize  | imst          |
| mwt       | imst          |
| pos       | imst_charlm   |
| lemma     | imst_nocharlm |
| depparse  | imst_charlm   |
| ner       | starlang      |
=============================

No constituency parser is available, which made me confused. Where could I find the constituency parser as reported in the page?

FYI, I'm using stanza 1.6.0.

Thanks in advance. ",
50,2023-10-02T05:48:54Z,https://github.com/stanfordnlp/stanza/pull/1290,,"## Description
- creates a `tokenize_postprocessor` config element to `stanza.Pipeline` which allows a callable to be passed which can override default tokenization
- creates `stanza.models.tokenization.utils.reassemble_doc_from_tokens` which allows for reassemblage of a document from re-adjusted tokenizations

For instance:

```python
nlp = stanza.Pipeline(lang=""en"", processors=""tokenize"",
                                    tokenize_postprocessor=lambda draft: do_stuff_to_draft(draft))
```

whereby, the single argument passed to `tokenize_postprocessor` is a list of lists containing string sentence and word tokenizations

```
[['Joe', 'Smith', 'lives', 'in', 'California', '.'], 
 ['Joe', ""'s"", 'favorite', 'food', 'is', 'pizza', '.'], 
 ['He', 'enjoys', 'going', 'to', 'the', 'beach', '.']]
```

to mark a MWT, each element can optionally be a tuple with a Bool second element. If the word `dai` in the following sentence as an `MWT`, for instance, we will be passed

```
[['Diglielo', ('dai', True), 'venire', 'a', 'mangiare', 'un', ""po'"", 'di', 'margarina']]
```

The callable passed to `tokenize_postprocessor` must return a list of lists in the same format: adjusting word, sentence tokenizations and MWT designations as needed. 

## Unit test coverage
- integration test of postprocessor, and type check of the argument passed
- unit test of the `reassemble_doc_from_tokens` function against normal & OOV characters",
51,2023-09-30T02:53:08Z,https://github.com/stanfordnlp/stanza/pull/1289,,"Add an ability to label text with multiple types of NER tags using one classifier.  The idea is that a model can be trained to do both at the same time, and the information from both datasets should work together to make the overall model better.  The learning from the second dataset can help the model generalize, even if it isn't the same tagset as the first dataset.

This will let us do something like cross-train the same model on different datasets, such as OntoNotes and CoNLL at the same time, or OntoNotes and the 8 class WorldWide dataset.

In the training data for a mixed dataset, each word now has an entry for ""multi_ner"" which can support more than one NER tag.  Tags which aren't present for a sentence can be blank.  In the case of the OntoNotes & WorldWide mixed dataset, for example, text from the WorldWide dataset has its 8 class tag and a blank tag, and text from the OntoNotes dataset has the original 18 class tag and a downscaled version of the 8 class tag.  

There are two options for implementing this: one in which there is the original LSTM encoder, followed by a unique Linear for each tag class and a corresponding CRFLoss, and one in which the output of one of the Linears goes back into the input of the next output layer.

Old models are maintained by converting the original version of the tensors to the new tensors.  Old datasets with one NER tagset are converted when loaded at training time.  Therefore, no need to do anything to the existing models or datasets.


Results of running the OntoNotes model, with charlm but not transformer, on the OntoNotes and WorldWide test sets:

```
original ontonotes on worldwide:   88.71  69.29
simplify-separate                  88.24  75.75
simplify-connected                 88.32  75.47
```

Here, ""simplify"" means the 18 class OntoNotes model is converted to 8 classes, then that data is combined with the WorldWide data as the training data for the second output layer
",
52,2023-09-21T05:29:46Z,https://github.com/stanfordnlp/stanza/pull/1287,,"Add a `packages` map to the `resources.json` wherein `default` is what we recommend people use on a day to day basis, but there is also `default_fast` where no charlms are used for any model (good for CPU) and `default_best` for any language where we use transformers.  We also add packages for each of the UD datasets, which is useful since previous versions allowed `package=""gsd""` for French, for example, but that is actually not possible in version 1.5.1.  Some might even call that a bug...

https://github.com/stanfordnlp/stanza/issues/1259

https://github.com/stanfordnlp/stanza/issues/1284
",
53,2023-09-19T08:45:37Z,https://github.com/stanfordnlp/stanza/issues/1285,,"**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**

```
2023-09-19 16:26:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.1.json: 328kB [00:00, 1.08Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.1.json: 328kB [00:00, 1.08MB/s]
2023-09-19 16:26:48 INFO: ""zh"" is an alias for ""zh-hans""
2023-09-19 16:26:49 INFO: Loading these models for language: zh-hans (Simplified_Chinese):
===================================
| Processor    | Package          |
-----------------------------------
| tokenize     | gsdsimp          |
| pos          | gsdsimp_charlm   |
| lemma        | gsdsimp_nocharlm |
| constituency | ctb-51_charlm    |
| depparse     | gsdsimp_charlm   |
| sentiment    | ren              |
| ner          | ontonotes        |
===================================

2023-09-19 16:26:49 INFO: Using device: cpu
2023-09-19 16:26:49 INFO: Loading: tokenize
2023-09-19 16:26:49 INFO: Loading: pos
2023-09-19 16:26:49 INFO: Loading: lemma
2023-09-19 16:26:50 INFO: Loading: constituency
2023-09-19 16:26:50 INFO: Loading: depparse
2023-09-19 16:26:50 INFO: Loading: sentiment
2023-09-19 16:26:50 INFO: Loading: ner
Traceback (most recent call last):
  File ""run_700_Stanford.py"", line 8, in <module>
    nlp = stanza.Pipeline('zh', use_gpu=False, ner_pretrain_path=ner_pretrain_path)
  File ""C:\Users\lizhaolin\AppData\Roaming\Python\Python38\site-packages\stanza\pipeline\core.py"", line 296, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""C:\Users\lizhaolin\AppData\Roaming\Python\Python38\site-packages\stanza\pipeline\processor.py"", line 193, in __init__
    self._set_up_model(config, pipeline, device)
  File ""C:\Users\lizhaolin\AppData\Roaming\Python\Python38\site-packages\stanza\pipeline\ner_processor.py"", line 52, in _set_up_model
    trainer = Trainer(args=args, model_file=model_path, pretrain=pretrain, device=device, foundation_cache=pipeline.foundation_cache)
  File ""C:\Users\lizhaolin\AppData\Roaming\Python\Python38\site-packages\stanza\models\ner\trainer.py"", line 66, in __init__
    self.load(model_file, pretrain, args, foundation_cache)
  File ""C:\Users\lizhaolin\AppData\Roaming\Python\Python38\site-packages\stanza\models\ner\trainer.py"", line 161, in load
    self.model = NERTagger(self.args, self.vocab, emb_matrix=emb_matrix, foundation_cache=foundation_cache)
  File ""C:\Users\lizhaolin\AppData\Roaming\Python\Python38\site-packages\stanza\models\ner\model.py"", line 52, in __init__
    self.init_emb(emb_matrix)
  File ""C:\Users\lizhaolin\AppData\Roaming\Python\Python38\site-packages\stanza\models\ner\model.py"", line 123, in init_emb
    assert emb_matrix.size() == (vocab_size, dim), \
AssertionError: Input embedding matrix must match size: 250000 x 300, found torch.Size([100000, 300])
```

**Expected behavior**

""stanza_resources""  has been downloaded when running the code

**Environment (please complete the following information):**
 - OS: [Windows11]
 - Python version: [Python 3.8.6]
 - Stanza version: [1.5.1]

**Additional context**
Thank you very much for your help and look forward to your reply.
",
54,2023-09-18T09:25:44Z,https://github.com/stanfordnlp/stanza/issues/1284,,"I did `pip install stanza --upgrade --quiet` and now I have `1.5.1`. Configuring Multilingual Pipeline now returns 2 following warnings:

```
WARNING:stanza:Can not find lemma: ftb from official model list. Ignoring it.
WARNING:stanza:Can not find pos: ftb from official model list. Ignoring it.
```

**To Reproduce**
```
lang_id_config = {""langid_lang_subset"": ['fi', 'sv', 'de', 'ru', 'en', 'da',]}
lang_configs = {
    ""en"": {""processors"":""tokenize,lemma,pos"", ""package"":'lines',""tokenize_no_ssplit"":True},
	""sv"": {""processors"":""tokenize,lemma,pos"",""tokenize_no_ssplit"":True},
	""da"": {""processors"":""tokenize,lemma,pos"",""tokenize_no_ssplit"":True},
	""ru"": {""processors"":""tokenize,lemma,pos"",""tokenize_no_ssplit"":True},
	""fi"": {""processors"":""tokenize,lemma,pos,mwt"", ""package"":'ftb',""tokenize_no_ssplit"":True},
	""de"": {""processors"":""tokenize,lemma,pos"", ""package"":'hdt',""tokenize_no_ssplit"":True},
}
smp = MultilingualPipeline(
    lang_id_config=lang_id_config,
    lang_configs=lang_configs,
    download_method=DownloadMethod.REUSE_RESOURCES,
)

INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES

Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.1.json:
287k/? [00:00<00:00, 13.0MB/s]
Downloading https://huggingface.co/stanfordnlp/stanza-multilingual/resolve/v1.5.1/models/langid/ud.pt: 100%
9.07M/9.07M [00:00<00:00, 41.9MB/s]

INFO:stanza:Loading these models for language: multilingual ():
=======================
| Processor | Package |
-----------------------
| langid    | ud      |
=======================

INFO:stanza:Using device: cuda
INFO:stanza:Loading: langid
INFO:stanza:Done loading processors!
```

Then passing my document `d`:
```
d =""""""
I go to school everyday with majority of my best friends.
""""""
```
into multilingual pipeline:
`all_ = smp(d)
`
returns `None` for `.lemma` since it doen't exist as keys for my `<class 'stanza.models.common.doc.Document'>`:
```
for _, vsnt in enumerate(all_.sentences): 
    for _, vw in enumerate(vsnt.words):
        print(vw.lemma)
None
None
None
None
None
None
None
None
None
None
None
None
```
Here's my `all_`:
```
[
  [
    {
      ""id"": 1,
      ""text"": ""I"",
      ""start_char"": 1,
      ""end_char"": 2
    },
    {
      ""id"": 2,
      ""text"": ""go"",
      ""start_char"": 3,
      ""end_char"": 5
    },
    {
      ""id"": 3,
      ""text"": ""to"",
      ""start_char"": 6,
      ""end_char"": 8
    },
    {
      ""id"": 4,
      ""text"": ""school"",
      ""start_char"": 9,
      ""end_char"": 15
    },
    {
      ""id"": 5,
      ""text"": ""everyday"",
      ""start_char"": 16,
      ""end_char"": 24
    },
    {
      ""id"": 6,
      ""text"": ""with"",
      ""start_char"": 25,
      ""end_char"": 29
    },
    {
      ""id"": 7,
      ""text"": ""majority"",
      ""start_char"": 30,
      ""end_char"": 38
    },
    {
      ""id"": 8,
      ""text"": ""of"",
      ""start_char"": 39,
      ""end_char"": 41
    },
    {
      ""id"": 9,
      ""text"": ""my"",
      ""start_char"": 42,
      ""end_char"": 44
    },
    {
      ""id"": 10,
      ""text"": ""best"",
      ""start_char"": 45,
      ""end_char"": 49
    },
    {
      ""id"": 11,
      ""text"": ""friends"",
      ""start_char"": 50,
      ""end_char"": 57
    },
    {
      ""id"": 12,
      ""text"": ""."",
      ""start_char"": 57,
      ""end_char"": 58
    }
  ]
]
```

**Expected behavior**
As of Stanza `1.5.0` (`pip install stanza==1.5 --quiet`), here is my expected behavior which I used to get:
```
all_ = smp(d)
Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/tokenize/lines.pt: 100%
629k/629k [00:00<00:00, 8.91MB/s]
Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pos/lines.pt: 100%
33.3M/33.3M [00:00<00:00, 60.7MB/s]
Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/lemma/lines.pt: 100%
2.51M/2.51M [00:00<00:00, 21.5MB/s]
Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pretrain/lines.pt: 100%
107M/107M [00:01<00:00, 63.7MB/s]
Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/forward_charlm/1billion.pt: 100%
22.7M/22.7M [00:00<00:00, 45.8MB/s]
Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/backward_charlm/1billion.pt: 100%
22.7M/22.7M [00:00<00:00, 47.0MB/s]

INFO:stanza:Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | lines   |
| pos       | lines   |
| lemma     | lines   |
=======================

INFO:stanza:Using device: cuda
INFO:stanza:Loading: tokenize
INFO:stanza:Loading: pos
INFO:stanza:Loading: lemma
INFO:stanza:Done loading processors!

print(all_)
[
  [
    {
      ""id"": 1,
      ""text"": ""I"",
      ""lemma"": ""I"",
      ""upos"": ""PRON"",
      ""xpos"": ""PERS-P1SG-NOM"",
      ""feats"": ""Case=Nom|Number=Sing|Person=1|PronType=Prs"",
      ""start_char"": 1,
      ""end_char"": 2
    },
    {
      ""id"": 2,
      ""text"": ""go"",
      ""lemma"": ""go"",
      ""upos"": ""VERB"",
      ""xpos"": ""PRES"",
      ""feats"": ""Mood=Ind|Tense=Pres|VerbForm=Fin"",
      ""start_char"": 3,
      ""end_char"": 5
    },
    {
      ""id"": 3,
      ""text"": ""to"",
      ""lemma"": ""to"",
      ""upos"": ""ADP"",
      ""start_char"": 6,
      ""end_char"": 8
    },
    {
      ""id"": 4,
      ""text"": ""school"",
      ""lemma"": ""school"",
      ""upos"": ""NOUN"",
      ""xpos"": ""SG-NOM"",
      ""feats"": ""Number=Sing"",
      ""start_char"": 9,
      ""end_char"": 15
    },
    {
      ""id"": 5,
      ""text"": ""everyday"",
      ""lemma"": ""everyday"",
      ""upos"": ""ADV"",
      ""start_char"": 16,
      ""end_char"": 24
    },
    {
      ""id"": 6,
      ""text"": ""with"",
      ""lemma"": ""with"",
      ""upos"": ""ADP"",
      ""start_char"": 25,
      ""end_char"": 29
    },
    {
      ""id"": 7,
      ""text"": ""majority"",
      ""lemma"": ""majority"",
      ""upos"": ""NOUN"",
      ""xpos"": ""SG-NOM"",
      ""feats"": ""Number=Sing"",
      ""start_char"": 30,
      ""end_char"": 38
    },
    {
      ""id"": 8,
      ""text"": ""of"",
      ""lemma"": ""of"",
      ""upos"": ""ADP"",
      ""start_char"": 39,
      ""end_char"": 41
    },
    {
      ""id"": 9,
      ""text"": ""my"",
      ""lemma"": ""I"",
      ""upos"": ""PRON"",
      ""xpos"": ""P1SG-GEN"",
      ""feats"": ""Number=Sing|Person=1|Poss=Yes|PronType=Prs"",
      ""start_char"": 42,
      ""end_char"": 44
    },
    {
      ""id"": 10,
      ""text"": ""best"",
      ""lemma"": ""good"",
      ""upos"": ""ADJ"",
      ""xpos"": ""SPL"",
      ""feats"": ""Degree=Sup"",
      ""start_char"": 45,
      ""end_char"": 49
    },
    {
      ""id"": 11,
      ""text"": ""friends"",
      ""lemma"": ""friend"",
      ""upos"": ""NOUN"",
      ""xpos"": ""PL-NOM"",
      ""feats"": ""Number=Plur"",
      ""start_char"": 50,
      ""end_char"": 57
    },
    {
      ""id"": 12,
      ""text"": ""."",
      ""lemma"": ""."",
      ""upos"": ""PUNCT"",
      ""xpos"": ""Period"",
      ""start_char"": 57,
      ""end_char"": 58
    }
  ]
]

for _, vsnt in enumerate(all_.sentences): 
    for _, vw in enumerate(vsnt.words):
        print(vw.lemma)

I
go
to
school
everyday
with
majority
of
I
good
friend
.
```

**Environment (please complete the following information):**
 - OS: [Ubuntu 16.04]
 - Python version: [Python 3.10.12 from Colab: `!python --version`]
 - Stanza version: [1.5.1 vs. 1.5.0]

What am I actually doing wrong here?

Cheers,",
55,2023-09-10T00:30:01Z,https://github.com/stanfordnlp/stanza/pull/1283,,"For any language with a default code of 3 letters (as per universaldependencies), and an alternate code of 2 letters, we can add that langcode to the resources file to make an alias for people who expect the 2 letter code.

Currently that only applies to `se` / `sme` (that we know of, at least)",
56,2023-09-08T03:42:07Z,https://github.com/stanfordnlp/stanza/pull/1282,,1.5.1 release,
57,2023-09-03T14:49:04Z,https://github.com/stanfordnlp/stanza/pull/1281,,"Unroll the recursion in the tarjan cycles algorithm, which allows much larger graphs to finish successfully",
58,2023-08-31T21:05:33Z,https://github.com/stanfordnlp/stanza/issues/1280,,"**Describe the bug**
The [doc](https://stanfordnlp.github.io/stanza/available_models.html) lists Livvi as a supported language, but there are no available language model to be downloaded.

**To Reproduce**
```
import stanza
stanza.download('olo')

Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 36.0MB/s]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python\lib\site-packages\stanza\resources\common.py"", line 551, in download
    raise UnknownLanguageError(lang)
stanza.resources.common.UnknownLanguageError: Unknown language requested: olo
```

**Expected behavior**
Add a language model for Livvi, or remove Livvi as a supported language in the doc.

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS] Windows 11 x64
 - Python version: [e.g. Python 3.6.8 from Anaconda] Python 3.8.12
 - Stanza version: [e.g., 1.0.0] 1.5.0
",
59,2023-08-31T18:31:59Z,https://github.com/stanfordnlp/stanza/issues/1279,,"**Describe the bug**
It seems that `Stanza` uses 2-digit ISO 639-1 code, if available, as the default language code in the [doc](https://stanfordnlp.github.io/stanza/available_models.html#available-ner-models) and [JSON file](https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json). Otherwise, ISO 3-digit 639-3 language codes are used.

But for Northern Sámi, `sme` is used as the default language code while `se` is available as the 2-digit ISO 639-1 language code.

**Expected behavior**
To be consistent with other languages, use `se`  instead of `sme` as the default language code for Northern Sámi.

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS] Windows 11 x64
 - Python version: [e.g. Python 3.6.8 from Anaconda] Python 3.10.12
 - Stanza version: [e.g., 1.0.0] 1.5.0
",
60,2023-08-28T10:21:50Z,https://github.com/stanfordnlp/stanza/issues/1278,,"**Describe the bug**
If `bulk_process` is passed an empty list, it crashes with a misleading error message:

```
Traceback (most recent call last):
  File ""/Users/iikka/test.py"", line 9, in main
    docs = nlp.bulk_process(docs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/stanza/pipeline/core.py"", line 426, in bulk_process
    return self.process(docs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/stanza/pipeline/core.py"", line 415, in process
    doc = process(doc)
          ^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/stanza/pipeline/tokenize_processor.py"", line 69, in process
    raise ValueError(""If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.  Got %s"" % str(type(document)))
ValueError: If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.  Got <class 'list'>
```

**To Reproduce**
Call `nlp.bulk_process([])`

**Expected behavior**
An empty list should be returned back.

**Environment (please complete the following information):**
 - OS: MacOS 13.4.1
 - Python version: Python 3.11 from Homebrew
 - Stanza version: 1.5.0
",
61,2023-08-26T08:33:02Z,https://github.com/stanfordnlp/stanza/issues/1277,,"I'm attempting to test out the new Segrex client (released in V1.2.1) using [the documentation's](https://stanfordnlp.github.io/stanza/client_regex.html) example script. I'm on Mac, Python version 3.11.4, and stanza version 1.5.0.

```python
import stanza
from stanza.server.semgrex import Semgrex

nlp = stanza.Pipeline(""en"", processors=""tokenize,pos,lemma,depparse"")

doc = nlp(""Banning opal removed all artifact decks from the meta.  I miss playing lantern."")
with Semgrex(classpath=""$CLASSPATH"") as sem:
    # sem.process takes a single doc, which can have any number of
    # sentences and therefore any number of dependency graphs
    # it also takes a variable length list of semgrex expressions to run
    semgrex_results = sem.process(doc,
                                  ""{pos:NN}=object <obl {}=action"",
                                  ""{cpos:NOUN}=thing <obj {cpos:VERB}=action"")
    print(semgrex_results)
    print(semgrex_results.result[0].result[0])
```

I've tried on Mac and in Google Colab to get the demo script to work, but each time there's an issue with the `$CLASSPATH` variable. On Mac (and the Google Colab virtual machine), this variable is unassigned. (I haven't been able to test on linux) And because the environment variable is None, the script breaks.

```shell
Traceback (most recent call last):
  File ""test_script.py"", line 11, in <module>
    with Semgrex(classpath=""$CLASSPATH"") as sem:
  File ""python3.11/site-packages/stanza/server/java_protobuf_requests.py"", line 215, in __enter__
    self.open_pipe()
  File ""python3.11/site-packages/stanza/server/java_protobuf_requests.py"", line 204, in open_pipe
    self.pipe = subprocess.Popen([""java"", ""-cp"", self.classpath, self.java_main, ""-multiple""] + self.extra_args,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""python3.11/subprocess.py"", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""python3.11/subprocess.py"", line 1883, in _execute_child
    self.pid = _fork_exec(
               ^^^^^^^^^^^
TypeError: expected str, bytes or os.PathLike object, not NoneType
```

I've experimented with assigning the `$CLASSPATH` environment variable to various paths, such as where java is installed (`/usr/bin/java`), where the CoreNLP package is installed (`~/stanza_corenlp/`), and my `$JAVA_HOME` variable on Mac (`JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-18.jdk/Contents/Home`). I have java version 18.0.1.1 installed.
```shell
java 18.0.1.1 2022-04-22
Java(TM) SE Runtime Environment (build 18.0.1.1+2-6)
Java HotSpot(TM) 64-Bit Server VM (build 18.0.1.1+2-6, mixed mode, sharing)
```

But I'm not able to resolve this `$CLASSPATH` issue in the demo script.

I am, however, able to run CoreNLP's other Java clients, which connect to the java runtime without any problems.
```python
from stanza.server import CoreNLPClient

text = ""Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.""
with CoreNLPClient(
        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
        timeout=30000,
        memory='6G') as client:
    ann = client.annotate(text)
```

Does anyone have any suggestions about how to get around this `$CLASSPATH` issue in the new Semgrex demo script?",
62,2023-08-24T14:06:04Z,https://github.com/stanfordnlp/stanza/issues/1276,,"Can `stanza.server.CoreNLPClient` be used to get text annotations in the default (= `""serialized""`) output format and convert them into `""conllu""` data afterwards?

My issue is: I need to create `""conllu""` files but with entity coreference annotations in the misc column. As they are not provided by default, I have resorted to manually altering the created `""conllu""` string. Currently, I am calling `annotate` twice:

```python
GENERATE_CONLLU = {
    ""outputFormat"": ""conllu"",
    ""annotators"": ""tokenize,ssplit,pos,lemma,depparse"",
}

GENERATE_COREF = {""annotators"": ""coref""}

with CoreNLPClient() as client:
    conllu_text = client.annotate(text, properties=GENERATE_CONLLU)
    coref_ann = client.annotate(text, properties=GENERATE_COREF)
```

This seems pretty wasteful to me. Is there a way to call annotate just once? Something like:

```python
with CoreNLPClient() as client:
    ann = client.annotate(text)
    conllu_text = ann.magically_convert_into_different_output_format_somehow(""conllu"")
```

If there is an altogether better way to generate a `""conllu""` file with entity annotations, I would of course also be all ear :)

Thanks in advance!",
63,2023-08-23T15:04:13Z,https://github.com/stanfordnlp/stanza/issues/1275,,"
Hello, NLP experts!
Stanza seems by default, means using ´deprel´ parser, have  Chomsky deep syntax and not predicate syntax.
That has a tendency to make a root of a sentence to be not a verb, but something else, especially if verb is missing and stanza do not fill it.
Is there another peer relation builder in Stanza ( sorry, i have not found it in tutorials), what would make the depencency ree properly, means that the root of a clause of a sentence is always a verb, and direct children of it are proper verb clause members?  ideally filling the missing verb?",
64,2023-08-22T08:12:46Z,https://github.com/stanfordnlp/stanza/pull/1274,,"## Description
This PR removes dependency on six and cleans Python 2 compatibility-related codes.

## Fixes Issues
Fixes #1273.

## Unit test coverage
No.

## Known breaking changes/behaviors
No.
",
65,2023-08-19T20:40:14Z,https://github.com/stanfordnlp/stanza/issues/1273,,"**Describe the bug**
The latest version of `Stanza` does not officially support Python 2, so there're no reason to list `six` as a dependency. And there are several places in the codebase of `Stanza` where `six` is imported but not used. I suppose that these references to `six` could be safely removed, and then the six dependency be removed.

**Expected behavior**
Get rid of the `six` dependency.

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS] Windows 11 x64
 - Python version: [e.g. Python 3.6.8 from Anaconda] 3.10.12
 - Stanza version: [e.g., 1.0.0] 1.5.0
",
66,2023-08-17T06:33:06Z,https://github.com/stanfordnlp/stanza/pull/1271,,Update the names of the POS models to take into account which embedding they have,
67,2023-08-14T13:53:02Z,https://github.com/stanfordnlp/stanza/issues/1270,,"Hi everyone,

I want to build a service that uses stanza, but our Cloud Foundry provider only allows a maximum Docker container size of 4GB. Because torch is a huge package, which installs many cuda libaries etc. the container with the german model is over 6GB.

```
# syntax=docker/dockerfile:1
FROM python:3.11-slim-bullseye

RUN apt-get update && \
    apt-get upgrade -y && \
    apt install sudo -y

RUN useradd -ms /bin/bash worker
USER worker
WORKDIR /home/worker
ENV PATH=""/home/worker/.local/bin:${PATH}""

RUN python -m pip install --no-cache-dir --user --disable-pip-version-check --upgrade pip
COPY requirements.txt requirements.txt
RUN pip install --no-cache-dir --user -r requirements.txt

COPY download_model.py download_model.py
RUN python3 download_model.py

COPY app.py app.py
COPY app_logic.py app_logic.py
COPY utils.py utils.py

EXPOSE 8080
CMD [""gunicorn"", ""--bind"", "":8080"", ""--workers"", ""1"", ""--worker-class"", ""uvicorn.workers.UvicornWorker"", ""--threads"", ""10"", ""--timeout"", ""3600"", ""app:app""]
```

I only need a CPU version and I only want to do inference. Stanza provides better results for my use case than spaCy, but currently I'm forced to use spaCy, because the container is only 1.5GB with the de_core_news_lg model. Is there a smaller CPU inference version or a DockerFile that builds a smaller image for inference on CPU?",
68,2023-08-08T20:16:23Z,https://github.com/stanfordnlp/stanza/pull/1269,,"Adds the charlm to the lemmatizer.  Seems to help some languages some, whereas other languages are basically neutral.  Other possible improvements exist, such as learning specific edit patterns for the end of a word, but this will do for now.  By default, the resources will use the `nocharlm` version, as the gains aren't huge (but the slowdown isn't much, either, unless using CPU instead of GPU)",
69,2023-08-08T00:32:58Z,https://github.com/stanfordnlp/stanza/pull/1268,,Initial version of a tool to convert an ML dataset for tokenization purposes,
70,2023-08-02T17:02:15Z,https://github.com/stanfordnlp/stanza/issues/1267,,"I have a pipeline defined with:
```
processors = ""tokenize, mwt, pos, lemma, depparse, ner"" # NER and prerequisites
verbose = ""False""
GPU = ""False""
language = ""it""
```
after using it on 40K news articles in Italian, it hangs more or less indefintely on a long article. The only ""special"" characteristic I can see is its length:
```
Words: 1884
Chars (no spaces): 15621
Chars (with spaces): 17511
Lines: 235 

```
It is also quite full of HTML and some other Unicode chars but I am preprocessing it with BeautifulSoup and a replace regexp to clean the text of those.

Am appending the text here under to ask if you see other things that might hang the pipeline. Thanks for any idea ...

`""<p><em>Questo appello, <strong>Democratizing Work</strong>, esce oggi in simultanea in 25 lingue su 41 testate internazionali, tra cui El Comercio, <a href=\""https://www.bostonglobe.com/2020/05/15/opinion/lets-democratize-decommodify-work/?outputType=amp\"">Boston Globe</a>, <a href=\""https://www.theguardian.com/commentisfree/2020/may/15/humans-resources-coronavirus-democratise-work-health-lives-market\"">Guardian</a>, <a href=\""https://wyborcza.pl/7,75968,25950087,list-naukowcow-demokratyzujmy-uspoleczniajmy-i-uzdrawiajmy.html?disableRedirects=true\"">Gazeta Wyborcza</a>, <a href=\""https://www1.folha.uol.com.br/mercado/2020/05/folha-publica-manifesto-internacional-em-defesa-do-trabalho.shtml\"">La Folha de São Paulo</a>, <a href=\""https://thewire.in/economy/covid-19-crisis-3000-researchers-600-universities-op-ed\"">The Wire</a>, Cumhuriyet,<a href=\""https://plus.lesoir.be/301158/article/2020-05-15/plus-de-3000-academiques-signent-pour-un-autre-monde?referer=%2Farchives%2Frecherche%3Fdatefilter%3Dlastyear%26sort%3Ddate%2520desc%26word%3Dbattilana\""> Le Soir</a>, <a href=\""https://www.lemonde.fr/idees/article/2020/05/15/democratiser-pour-depolluer_6039777_3232.html\"">Le Monde</a>, <a href=\""https://www.zeit.de/zustimmung?url=https%3A%2F%2Fwww.zeit.de%2Fkultur%2F2020-05%2Fwirtschaften-nach-der-pandemie-demokratie-dekommodifizierung-nachhaltigkeit-manifest\"">Die Zeit</a>, <a href=\""https://blogs.publico.es/dominiopublico/32972/manifiesto-trabajo-democratizar-desmercantilizar-descontaminar/\"">Publico</a>, <a href=\""https://www.eldiario.es/tribunaabierta/Trabajo-Democratizar-desmercantilizar-descontaminar_6_1027207295.html\"">El Diario</a>, <a href=\""https://www.letemps.ch/opinions/travail-democratiser-demarchandiser-depolluer\"">Le Temps</a>, <a href=\""https://www.scmp.com/comment/opinion/article/3084387/covid-19-pandemic-shows-why-people-and-environment-should-be-heart\"">South China Morning Post</a>. In Italia gli autori hanno scelto <strong>il manifesto</strong>. </em></p>\n<p><em>L’appello è stato firmato da oltre 3.000 accademici e ricercatori di più di 650 università del mondo. Tra questi, Elisabeth Anderson, Thomas Piketty, Dani Rodrik, Jan Werner Mueller, Chantal Mouffe, Claus Offe, Julie Battilana, Joshua Cohen, Nancy Fraser, James K. Galbraith, Axel Honneth, Jan-Werner Müller, Benjamin Sachs, Debra Satz, Nadia Urbinati, Sarah Song, Lea Ypi, Isabelle Ferreras, Dominique Méda, Saskia Sassen, Lawrence Lessig.</em></p>\n<p>…</p>\n<p>Chi lavora è molto di più che una semplice risorsa. Questa è una delle lezioni principali che dobbiamo imparare dalla crisi in corso.</p>\n<p>Curare i malati; fare consegne di cibo, medicine e altri beni essenziali; smaltire i rifiuti; riempire gli scaffali e far funzionare le casse dei supermercati: le persone che hanno reso possibile continuare con la vita durante la pandemia di Covid-19 sono la prova vivente che il lavoro non può essere ridotto a una mera merce.</p>\n<p>La salute delle persone e la cura di chi è più vulnerabile non possono essere governati unicamente dalle leggi di mercato. Se affidiamo questi compiti esclusivamente al mercato, corriamo il rischio di esacerbare le diseguaglianze e di mettere a repentaglio le vite delle persone più svantaggiate.</p>\n<p>Come evitare che succeda questo? Implicando chi lavora nelle decisioni relative alle loro vite e al loro futuro nel luogo di lavoro. Democratizzando le imprese. De-mercificando il lavoro. Garantendo a tutti un impiego utile.</p>\n<p>Dinanzi al rischio spaventoso della pandemia e del collasso ambientale, optare per questi cambiamenti strategici ci permetterebbe non solo di assicurare la dignità di tutti i cittadini ma anche di riunire le forze collettive necessarie per poter preservare la vita sul nostro pianeta.</p>\n<h3>DEMOCRATIZZAZIONE.</h3>\n<p>Ogni mattina, donne e uomini si svegliano e vanno a lavorare per chi tra di noi può restare in casa in quarantena. La dignità del loro lavoro non ha bisogno di altra spiegazione se non quella contenuta nel termine di «lavoratore essenziale». Questo termine mette alla luce un fatto importante che il capitalismo ha sempre cercato di rendere invisibile, spingendoci a pensare alle persone come «risorse umane».</p>\n<p>Gli esseri umani non sono una risorsa tra le altre. Senza persone che vogliano investire il proprio lavoro non ci sarebbero produzione né servizi.</p>\n<p>Ogni mattina, si svegliano anche donne e uomini che, confinati in casa, si danno da fare per le imprese e ditte per le quali lavorano a distanza.</p>\n<p>Sono la dimostrazione che si sbaglia chi crede che senza supervisione non ci si possa fidare che i lavoratori si impegnino, che questi richiedano sorveglianza e disciplina esterna continua. Sono la dimostrazione, giorno e notte, che i lavoratori non sono solo una delle tante parti in gioco all’interno delle aziende: al contrario, sono loro la chiave per il successo dei datori di lavoro. Sono il nucleo costituente delle aziende; nonostante ciò, sono esclusi dalla partecipazione nella gestione dei luoghi di lavoro – un diritto, quest´ultimo, monopolizzato dagli investitori di capitale.</p>\n<p>Se ci chiediamo come le aziende e la società intera possono riconoscere il contributo dei lavoratori in tempo di crisi, la risposta è: democrazia.</p>\n<p>Certamente bisogna ridurre le enormi diseguaglianze salariali e assicurare che aumentino i redditi più bassi; ma questo non basta.</p>\n<p>Come, dopo le due Guerre Mondiali, si è riconosciuto il contributo innegabile delle donne alla società dando loro il diritto al voto, così oggi appare ingiustificato negare l’emancipazione di chi investe il suo lavoro e il riconoscimento dei suoi diritti di cittadinanza all’interno delle imprese.</p>\n<p>In Europa, la rappresentanza dei lavoratori sul luogo di lavoro esiste già a partire dalla fine della Seconda Guerra Mondiale, attraverso<a href=\""https://ec.europa.eu/social/main.jsp?catId=707&langId=en&intPageId=211\""> i Consigli di Lavoro</a>. Ma questi organi rappresentativi, nel migliore dei casi, hanno scarsa voce in capitolo nella gestione delle imprese, dove sono sempre subordinati alle decisioni dei direttori esecutivi scelti dagli azionisti.</p>\n<p>Questi Consigli non sono stati in grado di frenare o rallentare la spinta verso l’accumulazione del capitale, con effetti disastrosi per l’ambiente.</p>\n<p>Questi organi dovrebbero avere diritti simili ai Consigli di Amministrazione e i dirigenti aziendali dovrebbero avere l´obbligo di ottenere sempre un doppio consenso: sia da parte degli organi che rappresentano i lavoratori che da quelli che rappresentano gli azionisti.</p>\n<p>In Germania, Olanda e nei paesi scandinavi, vari tipi di co-gestione (<em>Mitbestimmung</em>) si sono stabiliti progressivamente dopo la Seconda Guerra Mondiale e hanno rappresentato un passo cruciale ma insufficiente verso la creazione di una vera e propria cittadinanza all’interno dell’impresa.</p>\n<p>Perfino negli Stati Uniti, dove le organizzazioni di lavoratori e sindacali sono state pesantemente indebolite, si alzano voci a favore del riconoscimento del diritto degli investitori di lavoro di eleggere rappresentanti con una maggioranza qualificata all’interno dei consigli di amministrazione.</p>\n<p>Questioni come la scelta di un amministratore delegato, le strategie principali e la distribuzione dei profitti sono troppo importanti per essere lasciate interamente nelle mani degli azionisti.</p>\n<p>Chi investe il proprio lavoro – ovvero, la propria mente e il proprio corpo, la propria salute o anche la propria vita – deve godere del diritto collettivo di appoggiare o respingere queste decisioni.</p>\n<h3>DE-MERCIFICAZIONE.</h3>\n<p>Questa crisi ci insegna anche che è sbagliato trattare il lavoro come mera merce e lasciare le scelte che incidono più profondamente sulle nostre comunità in mano interamente ai meccanismi di mercato.</p>\n<p>Da tempo le politiche di lavoro e di approvvigionamento nel campo sanitario sono state guidate dalla semplice analisi costi-benefici; la crisi della pandemia ci rivela come questo criterio ci abbia spinto a fare errori.</p>\n<p>Alcuni bisogni fondamentali e collettivi devono essere sottratti al criterio dell’analisi costi-benefici, come ci ricordano il numero crescente di morti di Covid in tutto il mondo. Chi sostiene il contrario ci mette in pericolo.</p>\n<p>Quando sono in gioco la salute e la nostra vita sul pianeta, ragionare in termini di costi e benefici è indifendibile.</p>\n<p>La de-mercificazione del lavoro significa proteggere alcuni settori dalla legge del cosiddetto «libero mercato»; significa inoltre assicurare che tutti abbiano accesso al lavoro e alla dignità che conferisce.</p>\n<p>Una possibile maniera per realizzare questo obiettivo è la creazione di una Garanzia di Impiego. L’articolo 23 della Dichiarazione Universale dei Diritti Umani afferma che ogni persona ha diritto al lavoro.</p>\n<p>Una Garanzia di Impiego non solo offrirebbe a ogni cittadino la possibilità di lavorare e vivere con dignità, ma rinforzerebbe anche la nostra capacità collettiva di far fronte alle tante sfide sociali e ambientali che ci troviamo davanti.</p>\n<p>Una Garanzia di Impiego permetterebbe ai governi, in collaborazione con le comunità locali, di creare lavoro degno e al contempo di contribuire agli sforzi per evitare il collasso ambientale.</p>\n<p>Davanti alla crescita della disoccupazione in tutto il mondo, i programmi per garantire l´impiego posso giocare un ruolo fondamentale per assicurare la stabilità sociale, economica e ambientale delle nostre società democratiche.</p>\n<p>Un tale programma deve essere adottato dall’Unione Europea come parte del suo <em>Green Deal</em>; al fine di assicurarlo, bisogna ridefinire la missione della Banca Centrale Europea, in modo che quest´ultima possa finanziarlo.</p>\n<p>Questo programma offrirebbe una soluzione anti-ciclica alla disoccupazione massiccia che sta per colpirci e sarà d’importanza fondamentale per la prosperità europea.</p>\n<h3>RISANAMENTO AMBIENTALE.</h3>\n<p>La nostra reazione alla crisi attuale non deve essere ingenua come lo fu quella alla crisi economica del 2008. Allora si adottò un piano di salvataggio senza condizioni che incrementò il debito pubblico senza pretendere nulla in cambio da parte del settore privato.</p>\n<p>Se i nostri governi si impegnano per salvare le imprese nella crisi attuale, anche queste ultime devono fare la loro parte, accettando alcune condizioni fondamentali della democrazia.</p>\n<p>I nostri governi, in nome delle società democratiche dai quali vengono scelti e alle quali devono rispondere, e in nome dell’obbligo che tutti abbiamo di assicurare l´abitabilità del nostro pianeta, devono appoggiare le imprese a condizione che queste adottino delle nuove pratiche, attendendosi a requisiti ambientali esigenti e introducendo strutture interne di governo democratico.</p>\n<p>Imprese governate democraticamente – all’interno delle quali avrà uguale peso, nelle decisioni strategiche, la voce di chi investe il suo lavoro e di chi investe capitale – saranno capaci di guidare la transizione dalla distruzione al risanamento e rigenerazione ambientali.</p>\n<p>Abbiamo avuto fin troppo tempo per costatare cosa succede, nel sistema corrente, quando il lavoro, il pianeta e i guadagni si scontrano: il lavoro e il pianeta ne escono perdenti.</p>\n<p>Sappiamo, grazie alle ricerche del Dipartimento di Ingegneria dell’<a href=\""https://pubs.acs.org/doi/abs/10.1021/es102641n\"">Università di Cambridge</a>, che «cambiamenti di progettazione realizzabili» possono ridurre il consumo globale di energia del 73%. Ma questi cambiamenti richiedono l´impiego di molta forza lavoro e per metterli in atto sono necessarie scelte che nell’immediato risultano costose.</p>\n<p>Finché le imprese saranno gestite con l’obiettivo di massimizzare il profitto in un mondo in cui l´energia è a basso costo, perché mai verrebbero adottati questi cambiamenti?</p>\n<p>Nonostante le sfide che questa transizione comporta, imprese sociali e aziende cooperative, guidate da obiettivi che tengono in conto tanto considerazioni finanziarie quanto sociali e ambientali e che danno spazio alla democrazia interna, hanno già dimostrato il loro potenziale come agenti dei cambiamenti desiderati.</p>\n<p>Non illudiamoci: gli investitori di capitale, potendo scegliere, non si cureranno della dignità degli investitori di lavoro e non si faranno carico di combattere la catastrofe ambientale.</p>\n<p>È possibile scegliere un’altra strada.</p>\n<p>Democratizziamo le imprese; de-mercifichiamo il lavoro; smettiamo di trattare le persone come risorse in modo da potere impegnarci insieme per sostenere la vita sul nostro pianeta.</p>\n<p>-*-</p>\n<p><strong>L’appello, tradotto in 25 lingue, è stato firmato da oltre 3.000 accademici e importanti ricercatori di oltre 650 università di tutto il mondo.<a href=\""https://democratizingwork.org/sign\""> Firma qui</a>.</strong></p>\n<p><em>Tradotto in italiano da Serena Olsaretti (ICREA-Universitat Pompeu Fabra), Riccardo Spotorno (Universitat Pompeu Fabra), Laura Cementeri (CNRS–Centre d’étude des Mouvements Sociaux (EHESS))</em></p>\n<h4>Primi firmatari</h4>\n<ol>\n<li><em>Isabelle Ferreras (University of Louvain/FNRS-Harvard LWP)</em></li>\n<li><em>Julie Battilana (Harvard University)</em></li>\n<li><em>Dominique Méda (University of Paris Dauphine PLS)</em></li>\n<li><em>Julia Cagé (Sciences Po-Paris)</em></li>\n<li><em>Lisa Herzog (University of Groningen)</em></li>\n<li><em>Sara Lafuente Hernandez (University of Brussels-ETUI)</em></li>\n<li><em>Hélène Landemore (Yale University)</em></li>\n<li><em>Pavlina Tcherneva (Bard College-Levy Institute)</em></li>\n<li><em>Serena Olsaretti (ICREA – Universitat Pompeu Fabra)</em></li>\n<li><em>Lea Ypi (London School of Economics)</em></li>\n<li><em>Massimo Maoret (IESE Business School)</em></li>\n<li><em>Laura Cementeri, (CNRS – Centre d’étude des Mouvements Sociaux (EHESS))</em></li>\n<li><em>Elizabeth Anderson (University of Michigan)</em></li>\n<li><em>Philippe Askénazy (CNRS-Paris School of Economics)</em></li>\n<li><em>Aurélien Barrau (CNRS et Université Grenoble-Alpes)</em></li>\n<li><em>Neil Brenner (Harvard University)</em></li>\n<li><em>Craig Calhoun (Arizona State University)</em></li>\n<li><em>Ha-Joon Chang (University of Cambridge)</em></li>\n<li><em>Erica Chenoweth (Harvard University)</em></li>\n<li><em>Joshua Cohen (Apple University, Berkeley, Boston Review)</em></li>\n<li><em>Christophe Dejours (CNAM)</em></li>\n<li><em>Olivier De Schutter (UCLouvain, UN Special Rapporteur on extreme poverty and human rights)</em></li>\n<li><em>Nancy Fraser (The New School for Social Research, NYC)</em></li>\n<li><em>Archon Fung (Harvard University)</em></li>\n<li><em>Javati Ghosh (Jawaharlal Nehru University)</em></li>\n<li><em>Stephen Gliessman (UC Santa Cruz)</em></li>\n<li><em>Stefan Gosepath (Freie Universität Berlin)</em></li>\n<li><em>Hans R. Herren (Millennium Institute)</em></li>\n<li><em>Axel Honneth (Columbia University)</em></li>\n<li><em>Eva Illouz (EHESS, Paris)</em></li>\n<li><em>Tim Jackson (University of Surrey)</em></li>\n<li><em>Sanford Jacoby (UCLA)</em></li>\n<li><em>Rahel Jäggi (Humboldt University)</em></li>\n<li><em>Pierre-Benoit Joly (INRA – National Institute of Agronomical Research, France)</em></li>\n<li><em>Michele Lamont (Harvard university)</em></li>\n<li><em>Lawrence Lessig (Harvard University)</em></li>\n<li><em>David Marsden (London School of Economics)</em></li>\n<li><em>Chantal Mouffe (University of Westminster)</em></li>\n<li><em>Jan-Werner Müller (Princeton University)</em></li>\n<li><em>Susan Neiman (Einstein Forum)</em></li>\n<li><em>Thomas Piketty (EHESS-Paris School of Economics)</em></li>\n<li><em>Michel Pimbert (Coventry University, Executive Director of Centre for Agroecology, Water and Resilience)</em></li>\n<li><em>Raj Patel (University of Texas)</em></li>\n<li><em>Katharina Pistor (Columbia University)</em></li>\n<li><em>Dani Rodrik (Harvard University)</em></li>\n<li><em>Hartmunt Rosa (Max-Weber-Kolleg, Erfut)</em></li>\n<li><em>Benjamin Sachs (Harvard University)</em></li>\n<li><em>Saskia Sassen (Columbia University)</em></li>\n<li><em>Debra Satz (Stanford University)</em></li>\n<li><em>Pablo Servigne PhD (in-Terre-dependent researcher)</em></li>\n<li><em>William Sewell (University of Chicago)</em></li>\n<li><em>Susan Silbey (MIT)</em></li>\n<li><em>Margaret Somers (University of Michigan)</em></li>\n<li><em>George Steinmetz (University of Michigan)</em></li>\n<li><em>Laurent Thévenot (EHESS)</em></li>\n<li><em>Nadia Urbinati (Columbia University)</em></li>\n<li><em>Jean-Pascal van Ypersele (UCLouvain)</em></li>\n<li><em>Judy Wajcman (London School of Economics)</em></li>\n<li><em>Lisa Wedeen (The University of Chicago)</em></li>\n<li><em>Gabriel Zucman (UC Berkeley)</em></li>\n</ol>\n<p><strong><em>e più di 3.000 studiosi da più di 650 università del pianeta… </em></strong></p>\n<p><strong>(<a href=\""https://www.ilmanifesto.it/democratizing-work-lista-dei-firmatari/\"" target=\""_blank\"" rel=\""noopener noreferrer\"">qui l’elenco aggiornato al 14 maggio 2020</a>)</strong></p>\n<p><strong>La lista completa con tutti i firmatari da oltre 650 università del pianeta è su <a href=\""https://democratizingwork.org/\"">democratizingwork.org</a></strong></p>\n<p><a href=\""https://global.ilmanifesto.it/democratizing-work/\""><em><strong>English version at il manifesto global</strong></em></a></p>\n""`
",
71,2023-07-28T02:31:42Z,https://github.com/stanfordnlp/stanza/issues/1266,,"**Describe the bug**
There was an [issue before ](https://github.com/stanfordnlp/stanza/issues/1231)where it was resolved, but now the online demo for the Biomedical models is not working in my browsers.
It returns a 502 Bad Gateway Error.

**Steps to reproduce the behavior**
1. Go to [https://stanfordnlp.github.io/stanza/biomed.html](https://stanfordnlp.github.io/stanza/biomed.html)
2. click on the 'Online Demo'
3. I get same error '502 Bad Gateway'

**Environment:**
OS: Windows 10
Browser: **Google Chrome**; Version 114.0.5735.110 (Official Build) (64-bit)
Browser: **Microsoft Edge**; Version 115.0.1901.183 (Official build) (64-bit)

**Additional context:**
I love the online demo's appearance and would like to reproduce it in google colab, any template available? Anyway, I do prefer the appearance of the web-service.",
72,2023-07-25T13:38:58Z,https://github.com/stanfordnlp/stanza/issues/1265,,"Hello,
I cannot currently use the Indonesian model due to a md5 mismatch on the /stanza_resources/id/pretrain/conll17.pt file
The file available on hugginface has a wrong md5. The error is:

`ValueError: md5 for /stanza_resources/id/pretrain/conll17.pt is 9d50fc3cf8267befb1fc25c0350f4589, expected 644c82e3c1f229e4b1a2133b5e55cb0d`
",
73,2023-07-17T11:35:29Z,https://github.com/stanfordnlp/stanza/issues/1264,,"Hello, I used to use Stanford CoreNLP for my tokenization work on Simplified Chinese text, and I was wondering if Stanza is using the same? I saw that in the introduction it says 'Stanza includes a Python interface to the [CoreNLP Java package](https://stanfordnlp.github.io/CoreNLP) and inherits additional functionality from there, such as constituency parsing, coreference resolution, and linguistic pattern matching.' but I don't know if this refers to what I am trying to get it. For Simplified Chinese text, what module is exactly used?",
74,2023-07-11T11:34:18Z,https://github.com/stanfordnlp/stanza/issues/1262,,I just wanted to ask about the availability of the http://stanza.run/ service. And thought it would be the best place to write about it.,
75,2023-07-07T08:23:58Z,https://github.com/stanfordnlp/stanza/issues/1261,,"Hello,

I am working with transcribed text. The general quality of the transcription is excellent but it contains a large number of misspelled entity names that are crucial for me.

For instance ""Swisscode"" or ""Swiss Gold"" instead of the correct ""Swissquote"" or ""Alex Hormital"" instead of the correct ""ArcelorMittal"". 

Prior to reinventing the wheel, I was wondering if anyone would be aware of an existing NER tool that could correct such mistakes?

Otherwise, any tips on the best approach for implementing such solution would be greatly appreciated. 

Best,

Ed
",
76,2023-06-27T07:27:21Z,https://github.com/stanfordnlp/stanza/issues/1260,,"**Describe the bug**
Download a new model using `stanza.download` provide AssertionError

**To Reproduce**
Steps to reproduce the behavior:
>>> import stanza
>>> stanza.download('ja')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 36.4MB/s]
2023-06-27 09:24:00 INFO: Downloading default packages for language: ja (Japanese)...
Downloading http://nlp.stanford.edu/software/stanza/1.1.0/ja/default.zip: 40.4kB [00:00, 28.1MB/s]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home//dev/.venv/lib/python3.8/site-packages/stanza/resources/common.py"", line 361, in download
    request_file(
  File ""/home/dev/.venv/lib/python3.8/site-packages/stanza/resources/common.py"", line 131, in request_file
    assert(not md5 or file_exists(path, md5))
AssertionError

**Expected behavior**
This was previously working. It seems that a switch in URL link ?

**Environment (please complete the following information):**
 - OS: Ubuntu 20.04 but also on Huggingface
 - Python version: 3.8.10
 - Stanza version: 1.1.1

",
77,2023-06-19T18:18:26Z,https://github.com/stanfordnlp/stanza/issues/1259,,"**Describe the bug**
We were using Stanza for quiet sometime on version 1.2 and a month ago we have upgraded it to version 1.5.0 keeping the same computer settings. We've noticed a considerable change in performance after that and thought it was not normal.
Did you notice the same performance degradation or have any steps to avoid this kind of issue?

**Environment (please complete the following information):**
 - OS: Ubuntu 20.4
 - Python version: 3.8.10
 - Stanza version: 1.5.0

",
78,2023-06-08T08:10:31Z,https://github.com/stanfordnlp/stanza/issues/1258,,"

**Describe the bug**
I get a RuntimeError when loading stanza whenever tensorflow is loaded as well. I am using python 3.7, tensorflow 2.4, torch 1.7 and stanza 1.2. I cannot use more recent versions of these packages because I need stanza as part of the pipeline for a model released in 2021, and newer versions of tensorflow and torch cause other issues. 

**To Reproduce**
Loading stanza using the following commands works fine:
```
import stanza
stanza.download('nl') 
nlp = stanza.Pipeline('nl', processors='tokenize') 
```
However whenever tensorflow is loaded I get a RuntimeError:

```
import tensorflow as tf

import stanza
stanza.download('nl')
nlp = stanza.Pipeline('nl', processors='tokenize')

2023-06-07 17:11:40.689662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49\] Successfully opened dynamic library libcudart.so.11.0
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB \[00:00, 87.3MB/s\]
2023-06-07 17:11:44 INFO: Downloading default packages for language: nl (Dutch)...
2023-06-07 17:11:45 INFO: File exists: /home/stanza_resources/nl/default.zip.
2023-06-07 17:11:49 INFO: Finished downloading models and saved to /home/stanza_resources.
2023-06-07 17:11:49 INFO: Loading these models for language: nl (Dutch):
=

| Processor | Package |
-

| tokenize  | alpino  |
=

2023-06-07 17:11:49 INFO: Use device: gpu
2023-06-07 17:11:49 INFO: Loading: tokenize
Traceback (most recent call last):
File ""predtest.py"", line 36, in \<module\>
nlp = stanza.Pipeline('nl', processors='tokenize')
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 130, in __init__
use_gpu=self.use_gpu)
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/stanza/pipeline/processor.py"", line 155, in __init__
self.\_set_up_model(config, use_gpu)
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py"", line 39, in \_set_up_model
self.\_trainer = Trainer(model_file=config\['model_path'\], use_cuda=use_gpu)
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py"", line 27, in __init__
self.model.cuda()
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 463, in cuda
return self.\_apply(lambda t: t.cuda(device))
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 359, in \_apply
module.\_apply(fn)
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 381, in \_apply
param_applied = fn(param)
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 463, in \<lambda\>
return self.\_apply(lambda t: t.cuda(device))
File ""/home/miniconda3/envs/june/lib/python3.7/site-packages/torch/cuda/__init__.py"", line 172, in \_lazy_init
torch.\_C.\_cuda_init()
RuntimeError: random_device could not be read
```

**Expected behavior**
I would expect the tokenizer to load normally

**Environment (please complete the following information):**
Ubuntu 20.04.5
python 3.7
tensorflow 2.4
torch 1.7 
stanza 1.2

**Additional context**
I have tried using different versions of tensorflow/torch/stanza, and I also tried reinstalling the packages, but that did not work. Any insights would be appreciated!
",
79,2023-06-05T19:40:24Z,https://github.com/stanfordnlp/stanza/issues/1256,,"**Describe the bug**
I cannot get the example from the docs to work.  See:
https://stanfordnlp.github.io/stanza/pipeline.html#building-your-own-processors-and-using-them-in-the-neural-pipeline

One issue is that the dependency is missing from the  documentation. I added the following line to the code to get it to compile:
```
from stanza.pipeline.processor import Processor, register_processor
```

But even after that, I get a error when trying to create the Pipeline:
```
processors = ""tokenize,lowercase""
nlp_en_parse = Pipeline(lang=""en"", processors=processors)
```
which outputs:
```
2023-06-05 12:21:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 41.9MB/s]
2023-06-05 12:21:20 INFO: Loading these models for language: en (English):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| lowercase | default  |
========================

2023-06-05 12:21:20 INFO: Using device: cpu
2023-06-05 12:21:20 INFO: Loading: tokenize
2023-06-05 12:21:20 INFO: Loading: lowercase
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/peter/opt/anaconda3/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 296, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
TypeError: __init__() got an unexpected keyword argument 'device'
```

I also tried modifying the __init__ call to take any args using ""__init__(self, *args)"".
This did result in a different error, referencing ""config"" instead of ""device"":
```
TypeError: __init__() got an unexpected keyword argument 'config'
```

**To Reproduce**
Run this python program:
```
from stanza.pipeline.core import Pipeline
from stanza.pipeline.processor import Processor, register_processor

@register_processor(""lowercase"")
class LowercaseProcessor(Processor):
    ''' Processor that lowercases all text '''
    _requires = set(['tokenize'])
    _provides = set(['lowercase'])
    # def __init__(self, *args):
    def __init__(self, config, pipeline, use_gpu):
        pass
    def _set_up_model(self, *args):
        pass
    def process(self, doc):
        doc.text = doc.text.lower()
        for sent in doc.sentences:
            for tok in sent.tokens:
                tok.text = tok.text.lower()
            for word in sent.words:
                word.text = word.text.lower()
        return doc

processors = ""tokenize,lowercase""
nlp_en_parse = Pipeline(lang=""en"", processors=processors)
nlp_results = nlp_en_parse(""a Few WoRdS"")
```

**Expected behavior**
The locally defined lowercase processor should be added to the pipelline.

**Environment (please complete the following information):**
 - MacOS Monterey 12.1 (21C52) 
 - Python 3.9.7 from Anaconda3 
 - stanza-1.5.0
 - java version ""1.8.0_371"" 


**Additional context**
none
",
80,2023-06-04T11:43:50Z,https://github.com/stanfordnlp/stanza/issues/1254,,"Hi ! 

I read through the documentation, closed issues, but couldn't find an issue similar to mine, so I opened this issue. Now I get that my question is a bit vague, but I'll try to explain the issue in the best way I could come up with.

For text annotations task, we can build a pipeline, which uses processors to define a particular task. Suppose for a task, normally if I were to do like : 

``` 
nlp = stanza.Pipeline('en', processors='tokenize,mwt, ner')
doc = nlp('Barack Obama was born in Hawaii.')
```
I will get a `Document` object. Suppose for some task, I want to isolate all of these into separate task. That means, I want to have separate pipeline object - each for Tokenizer, MWT, Dependency Parsing, or NER. Something like this : 

```
nlp1 = stanza.Pipeline('en', processors='tokenize')
nlp3 = stanza.Pipeline('en', processors='mwt')
# Maybe others such as dependency parsing, pos tagging etc.
nlp3 = stanza.Pipeline('en', processors='ner')
```   
As all of `nlp1, nlp2, nlp3` expect a string as an input, is it at all possible(by extending stanza/adding additional module, or any other references) to make these like a sequential (chain-like) structure? Like the output from `nlp1` being fed to `nlp2` and the output from `nlp2 ` being fed to `nlp3` so and so.. 

I want to use this type of architecture in a project, but I seem to have hit a roadblock, because of the way text annotation is done in Stanza. I get that this not an issue per say, but still I wanted to seek help if at all it is possible from your side.

Thanks,
Yash 

   
",
81,2023-06-01T13:55:33Z,https://github.com/stanfordnlp/stanza/issues/1253,,"**Describe the bug**
The website stanza.run is down. Not sure if this is the correct place to report this, but perhaps someone can point me to the right place if it isn't?

**To Reproduce**
Steps to reproduce the behavior:
1. Visit stanza.run in your browser.

**Expected behavior**
The site should load.

**Environment (please complete the following information):**
 - OS: MacOS
- Browser: Chrome",
82,2023-05-31T19:35:50Z,https://github.com/stanfordnlp/stanza/pull/1252,,"Add an fr_combined model out of GSD, Sequoia, ParisStories, and Rhapsodie

The overall results on GSD barely go down, whereas it significantly helps the results on the other three treebanks to include their data, so presumably this will be an improvement.  Will check with a known FR user first before making it the default",
83,2023-05-28T02:26:53Z,https://github.com/stanfordnlp/stanza/issues/1250,,"**Describe the bug**
I have not been able to get even the most simple stanza examples to work, including  just importing stanza.
I tried to follow the instructions for getting around segmentation faults as described at:
https://stanfordnlp.github.io/stanza/faq.html#troubleshooting-running-stanza

I reinstalled the torch and stanza packages by using:
`
pip uninstall torch stanza
pip install torch stanza
`
And I restarted my MacBook.
But I still get these errors.

**To Reproduce**

- launch python
- import stanza
This outputs the following:
`
OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.
zsh: abort      python
`


**Expected behavior**
(1) load stanza
`
import stanza
`
should succeed, and not abort with an error about libiomp5.dylib (OpenMP?).

I should not need to modify the environment in order load stanza as in:
`
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import stanza
`

(2) stanza.Pipeline() calls should compete.
After adding the code snippet above to use the hack of using KMP_DUPLICATE_LIB_OK, I can load stanza.
However, the first call fails with a segmentation fault.
`
nlp = stanza.Pipeline('en')
`
should succeed, and not exit with a segmentation fault.


**Environment (please complete the following information):**
 - MacOS Monterey 12.1 (21C52) 
 - Python 3.9.7 from Anaconda3 
 - stanza-1.5.0
 - java version ""1.8.0_371"" 

**Additional context**
none",
84,2023-05-21T13:41:36Z,https://github.com/stanfordnlp/stanza/pull/1249,,"

## Description
github -> GitHub
",
85,2023-05-17T12:18:12Z,https://github.com/stanfordnlp/stanza/issues/1248,,"**Describe the bug**
When I created a nlp pipeline with code ""nlp = stanza.Pipeline('zh', dir='./', download_method=None)"", I got this error

**To Reproduce**
Steps to reproduce the behavior:
1. download the zh model manually
2. make sure that the model and resource.json stay on the right path
3. run code
4. See the error

**Expected behavior**
it should create a pipeline and execute the following works

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: python 3.8
 - Stanza version: 1.5.0

**Additional context** 
More information about the error:

nlp = stanza.Pipeline('zh', dir='./', download_method=None) # initialize English neural pipeline
  File ""/usr/local/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 296, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""/usr/local/lib/python3.8/site-packages/stanza/pipeline/processor.py"", line 193, in __init__
    self._set_up_model(config, pipeline, device)
  File ""/usr/local/lib/python3.8/site-packages/stanza/pipeline/constituency_processor.py"", line 41, in _set_up_model
    trainer = Trainer.load(filename=config['model_path'],
  File ""/usr/local/lib/python3.8/site-packages/stanza/models/constituency/trainer.py"", line 144, in load
    model = Trainer.model_from_params(params, args, foundation_cache)
  File ""/usr/local/lib/python3.8/site-packages/stanza/models/constituency/trainer.py"", line 109, in model_from_params
    model = LSTMModel(pretrain=pt,
  File ""/usr/local/lib/python3.8/site-packages/stanza/models/constituency/lstm_model.py"", line 262, in __init__
    if args['constituent_stack'] == StackHistory.ATTN:
KeyError: 'constituent_stack'",
86,2023-05-15T13:53:01Z,https://github.com/stanfordnlp/stanza/issues/1247,,"Hi I understand that the F1 scores for each dependency below are for the development data, but are the evaluation results printed at the end for the test set? Or it is still for the dev set? Maybe I should use --score_test when training to get the test score?
```
2023-05-14 19:58:06 INFO: Running parser in predict mode
2023-05-14 19:58:06 INFO: Loading model from: saved_models/depparse/fr_goldgsd_parser.pt
2023-05-14 19:58:07 DEBUG: Loaded pretrain from /Users/yiminglu/stanza_resources/fr/pretrain/gsd.pt
2023-05-14 19:58:07 INFO: Loading data with batch size 5000...
2023-05-14 19:58:10 DEBUG: 8 batches created.
2023-05-14 19:58:10 INFO: Start evaluation...
2023-05-14 19:58:45 INFO: F1 scores for each dependency:
  Note that unlabeled attachment errors hurt the labeled attachment scores
         acl: p 0.0000 r 0.0000 f1 0.0000 (447 actual)
   acl:relcl: p 0.0000 r 0.0000 f1 0.0000 (293 actual)
       advcl: p 0.0000 r 0.0000 f1 0.0000 (308 actual)
 advcl:cleft: p 0.0000 r 0.0000 f1 0.0000 (23 actual)
      advmod: p 0.2787 r 0.0272 f1 0.0496 (1250 actual)
        amod: p 0.3534 r 0.5379 f1 0.4266 (1911 actual)
       appos: p 0.0000 r 0.0000 f1 0.0000 (592 actual)
         aux: p 0.0000 r 0.0000 f1 0.0000 (67 actual)
    aux:caus: p 0.0000 r 0.0000 f1 0.0000 (20 actual)
    aux:pass: p 0.0000 r 0.0000 f1 0.0000 (291 actual)
   aux:tense: p 0.0000 r 0.0000 f1 0.0000 (337 actual)
        case: p 0.6563 r 0.9080 f1 0.7619 (5345 actual)
          cc: p 0.0000 r 0.0000 f1 0.0000 (990 actual)
       ccomp: p 0.0000 r 0.0000 f1 0.0000 (118 actual)
        conj: p 0.0000 r 0.0000 f1 0.0000 (1235 actual)
         cop: p 0.3501 r 0.5105 f1 0.4153 (478 actual)
       csubj: p 0.0000 r 0.0000 f1 0.0000 (21 actual)
  csubj:pass: p 0.0000 r 0.0000 f1 0.0000 (2 actual)
         dep: p 0.0000 r 0.0000 f1 0.0000 (5 actual)
    dep:comp: p 0.0000 r 0.0000 f1 0.0000 (1 actual)
         det: p 0.9567 r 0.9614 f1 0.9591 (5469 actual)
   discourse: p 0.0000 r 0.0000 f1 0.0000 (74 actual)
  dislocated: p 0.0000 r 0.0000 f1 0.0000 (12 actual)
        expl: p 0.0000 r 0.0000 f1 0.0000 (14 actual)
   expl:pass: p 0.0000 r 0.0000 f1 0.0000 (55 actual)
     expl:pv: p 0.0000 r 0.0000 f1 0.0000 (102 actual)
   expl:subj: p 0.0000 r 0.0000 f1 0.0000 (86 actual)
       fixed: p 0.0000 r 0.0000 f1 0.0000 (342 actual)
flat:foreign: p 0.0000 r 0.0000 f1 0.0000 (71 actual)
   flat:name: p 0.0270 r 0.0015 f1 0.0028 (677 actual)
    goeswith: p 0.0000 r 0.0000 f1 0.0000 (8 actual)
        iobj: p 0.0000 r 0.0000 f1 0.0000 (113 actual)
  iobj:agent: p 0.0000 r 0.0000 f1 0.0000 (1 actual)
        mark: p 0.1354 r 0.0213 f1 0.0368 (611 actual)
        nmod: p 0.3715 r 0.6471 f1 0.4720 (3307 actual)
       nsubj: p 0.2175 r 0.5246 f1 0.3075 (1748 actual)
  nsubj:caus: p 0.0000 r 0.0000 f1 0.0000 (9 actual)
  nsubj:pass: p 0.0000 r 0.0000 f1 0.0000 (320 actual)
      nummod: p 0.0000 r 0.0000 f1 0.0000 (332 actual)
         obj: p 0.0000 r 0.0000 f1 0.0000 (1186 actual)
   obj:agent: p 0.0000 r 0.0000 f1 0.0000 (9 actual)
     obj:lvc: p 0.0000 r 0.0000 f1 0.0000 (46 actual)
         obl: p 0.0000 r 0.0000 f1 0.0000 (91 actual)
   obl:agent: p 0.0000 r 0.0000 f1 0.0000 (137 actual)
     obl:arg: p 0.0000 r 0.0000 f1 0.0000 (765 actual)
     obl:mod: p 0.1543 r 0.3585 f1 0.2158 (1431 actual)
      orphan: p 0.0000 r 0.0000 f1 0.0000 (17 actual)
   parataxis: p 0.0000 r 0.0000 f1 0.0000 (129 actual)
       punct: p 0.2146 r 0.2864 f1 0.2454 (3802 actual)
  reparandum: p 0.0000 r 0.0000 f1 0.0000 (3 actual)
        root: p 0.5971 r 0.5975 f1 0.5973 (1575 actual)
    vocative: p 0.0000 r 0.0000 f1 0.0000 (27 actual)
       xcomp: p 0.0000 r 0.0000 f1 0.0000 (401 actual)
2023-05-14 19:58:48 INFO: LAS	MLAS	BLEX
2023-05-14 19:58:48 INFO: 48.52	29.56	34.10
2023-05-14 19:58:48 INFO: Parser score:
2023-05-14 19:58:48 INFO: fr_goldgsd 48.52
2023-05-14 19:58:51 INFO: Finished running dev set on
UD_French-goldgsd
  UAS   LAS  CLAS  MLAS  BLEX
67.50 48.52 34.10 29.56 34.10
```

",
87,2023-05-14T10:17:34Z,https://github.com/stanfordnlp/stanza/issues/1246,,"Hi I was training a model for both tokenisation and multi-word expansions. And I get two sets of results for words. Are they the same concept (for multi-word expansions)? Sorry this is too basic a question. I just don`t know how to deal with the two ```Words``` with different scores trained on the same data. 
```
UD_Arabic-GOLD
   Tokens Sentences     Words
   100.00     79.17     99.15
```
```
UD_Arabic-GOLD
Words
99.90
```
Thanks!",
88,2023-05-12T15:06:47Z,https://github.com/stanfordnlp/stanza/issues/1245,,"Hi there, 

As you may know, I recently trained and submitted a NER model for the Armenian language, which your team approved and added to the main library.

However, I noticed that the Armenian language is not listed under the [available models and languages for NER on the Stanza website.](https://stanfordnlp.github.io/stanza/ner_models.html) 
I believe it would be helpful to include Armenian in the list to make it easier for users to find and use the model.

Thank you for your attention.",
89,2023-05-11T13:58:38Z,https://github.com/stanfordnlp/stanza/issues/1244,,"**Describe the bug**
When using python requests library in `download_file` function of common.py, we get at 500 Server Error even though the same URL can be accessed just fine through a browser.

If i set headers in the requests call, downloading the model is successful.  The headers variable I set is:

`    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}
`

**To Reproduce**
Steps to reproduce the behavior:
`stanza.download('en')`

**Expected behavior**
The model to be downloaded successfully

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS]
 - Python version: [e.g. Python 3.6.8 from Anaconda]
 - Stanza version: [e.g., 1.0.0]
Windows 10
Stanza 1.4
Python 3.9


**Additional context**
I believe running the same code on RHEL 8 does not require the headers to be set.
",
90,2023-05-07T07:32:11Z,https://github.com/stanfordnlp/stanza/issues/1243,,"Dear All,
I would like to ask you how to customize OpenIE pipeline [options](https://stanfordnlp.github.io/CoreNLP/openie.html#options) from the python interface.

That is to say,
import stanfordnlp.server as corenlp
text=""This is a text.""
c=corenlp.CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','parse','openie'], timeout=60000)

From all the [options](https://stanfordnlp.github.io/CoreNLP/openie.html#options) provided here, I want to set **max_entailments_per_clause to 500** and **triple.strict to True**.

Please suggest how do I go about it. 
Many thanks",
91,2023-05-01T05:56:37Z,https://github.com/stanfordnlp/stanza/pull/1241,,Finish the change to add the charlm to the depparse - we rebuild all the depparse models for which we have charlm (using Nov 2022 UD) and make them the published models for the 1.5.0 release.,
92,2023-04-28T06:44:15Z,https://github.com/stanfordnlp/stanza/issues/1240,,"Dear stanza developers,
I'm facing difficulties to work with the constituency parser output object:`<class 'stanza.models.constituency.parse_tree.Tree'>`. 

When I try to convert it to a dictionary (`dict(doc.sentences[0].constituency)`), then the error says: 
```
'Tree' object is not iterable
```
The [docs](https://stanfordnlp.github.io/stanza/data_objects.html#parsetree) do not say anything about how to use `parse_tree` objects for further processing.
__So how can I access the feature of this Tree being nested ?__ 
Ideally, the tree is converted to a nested dict, where the keys is the POS abbreviation modified with a number n corresponding to its n-th occurrence. Is it professional practice to define my own function using the string as an argument to do this?
I'm very surprised that other people haven't faced the same problem, so how completely wrong am I? 
Used code:
```py
import stanza
nlp_constituency = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')
doc = nlp_constituency('This is a test')
print(doc.sentences[0].consituency) # type = stanza.models.constituency.parse_tree.Tree
# result # (ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT a) (NN test)))))
```",
93,2023-04-25T07:39:16Z,https://github.com/stanfordnlp/stanza/issues/1239,,"**Describe the bug**
I meet this error when I run my .py file on my server. However, when running the same file on my local PC, no issue arises.

**Expected behavior**
First, I guess maybe it is an independency conflict issue.  I tried to uninstall and reinstall a lower version torch (torch=1.3.1) and the issue arises on my local PC. so I tied to update the package version on my server and fix the bug. Now on my server, the stanza version is 1.4.2 and the torch version is 1.13.1,  but the problem is still there. 

**Environment (please complete the following information):**
 - OS: CentOS(server), WINDOWS(local PC)
 - Python version: 3.7
 - Stanza version: 1.4.2
 - torch version: 1.13.1
 - stanza model language: 'zh-hans'

**Additional context**
  File ""/home/paddle/bert/main.py"", line 31, in <module>
    from extract_date_words import return_date
  File ""/home/paddle/bert/extract_date_words.py"", line 1, in <module>
    import stanza
  File ""/home/paddle/python3.7/lib/python3.7/site-packages/stanza/__init__.py"", line 1, in <module>
    from stanza.pipeline.core import DownloadMethod, Pipeline
  File ""/home/paddle/python3.7/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 27, in <module>
    from stanza.pipeline.constituency_processor import ConstituencyProcessor
  File ""/home/paddle/python3.7/lib/python3.7/site-packages/stanza/pipeline/constituency_processor.py"", line 12, in <module>
    import stanza.models.constituency.trainer as trainer
  File ""/home/paddle/python3.7/lib/python3.7/site-packages/stanza/models/constituency/trainer.py"", line 32, in <module>
    from stanza.models.constituency.lstm_model import LSTMModel
  File ""/home/paddle/python3.7/lib/python3.7/site-packages/stanza/models/constituency/lstm_model.py"", line 48, in <module>
    from stanza.models.constituency.utils import build_nonlinearity, initialize_linear, TextTooLongError
  File ""/home/paddle/python3.7/lib/python3.7/site-packages/stanza/models/constituency/utils.py"", line 13, in <module>
    from stanza.utils.get_tqdm import get_tqdm
ModuleNotFoundError: No module named 'stanza.utils.get_tqdm'",
94,2023-04-20T16:40:35Z,https://github.com/stanfordnlp/stanza/pull/1238,,"Add functionality to only backprop from batches if the batch actually has xpos, upos, or feats in it.  The missing elements are not included in the loss.  Allows for POS datasets with a mix of xpos tags, or allows adding constituency trees with only xpos as a data source, etc",
95,2023-04-18T04:12:46Z,https://github.com/stanfordnlp/stanza/issues/1237,,"**Describe the bug**
When I try to `import stanza`, I get the following error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""...\lib\site-packages\stanza\__init__.py"", line 1, in <module>
    from stanza.pipeline.core import DownloadMethod, Pipeline
  File ""...\lib\site-packages\stanza\pipeline\core.py"", line 26, in <module>
    from stanza.pipeline.constituency_processor import ConstituencyProcessor
  File ""...\lib\site-packages\stanza\pipeline\constituency_processor.py"", line 5, in <module>
    from stanza.models.constituency.trainer import Trainer
  File ""...\lib\site-packages\stanza\models\constituency\trainer.py"", line 32, in <module>
    from stanza.models.constituency.base_model import SimpleModel, UNARY_LIMIT
  File ""...\lib\site-packages\stanza\models\constituency\base_model.py"", line 32, in <module>
    from stanza.server.parser_eval import ParseResult, ScoredTree
  File ""...\lib\site-packages\stanza\server\__init__.py"", line 1, in <module>
    from stanza.protobuf import to_text
  File ""...\lib\site-packages\stanza\protobuf\__init__.py"", line 9, in <module>
    from .CoreNLP_pb2 import *
  File ""...\lib\site-packages\stanza\protobuf\CoreNLP_pb2.py"", line 20, in <module>
    _LANGUAGE = DESCRIPTOR.enum_types_by_name['Language']
AttributeError: 'NoneType' object has no attribute 'enum_types_by_name'
```

**To Reproduce**
Install stanza 1.5.0 and protobuf 3.14.0.
`import stanza`

**Expected behavior**
No error

**Environment (please complete the following information):**
 - OS: Windows 10
 - Python version: Python 3.7.16 from Anaconda
 - Stanza version: 1.5.0
 - Protobuf version: 3.14.0 (cannot be upgraded in the given environment)

**Additional context**
The only suggestion provided to others who have reported the same problem is to upgrade protobuf.  However, I cannot upgrade protobuf in the environment in which I am trying this, and this package does not provide a lower bound on the protobuf version that it claims to need, so I do not know if I should expect this to work or not.
",
96,2023-04-17T11:23:21Z,https://github.com/stanfordnlp/stanza/issues/1236,,"Hi when trying to train a new NER pipline , there is no ability to set a offline download flag I've noticed it with the 
stanza\utils\datasets\ner\convert_hy_armtdp.py 
script that there is no way to easily tell it not to automatically download this is it possible for a flag to be added ",
97,2023-04-14T20:08:07Z,https://github.com/stanfordnlp/stanza/issues/1235,,"Will the biomedical models be placed on https://huggingface.co/stanfordnlp? I am unable to install the models via the stanza.download('en', package='craft')"" route.
",
98,2023-04-14T12:21:51Z,https://github.com/stanfordnlp/stanza/issues/1234,,"Greeting,

I am new to stanza enviroment and trying to run the example code given on documentation. However, I got the error as follows:

[pool-1-thread-110] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52315] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-92] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52316] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:47 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-37] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52318] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:47 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:06:48 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:06:48 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:06:48 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:06:48 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-90] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52321] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-72] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52326] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-47] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52327] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-70] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52328] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:48 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-128] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52329] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:48 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-95] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52331] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-21] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52333] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:49 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-7] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52335] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:49 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-54] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52337] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:50 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-48] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52339] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:51 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-109] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52341] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:53 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-107] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52344] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:54 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-46] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52346] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:06:55 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-28] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52348] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:00 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-11] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52350] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:01 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:01 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-34] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52352] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:01 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:01 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-65] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52354] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-20] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52357] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-36] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52358] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:03 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:03 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-51] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52361] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-113] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52362] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:05 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-6] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52364] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:06 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:06 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:06 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-86] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52366] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-123] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52369] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-61] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52370] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:07 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-14] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52372] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:07 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:07 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-22] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52375] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-43] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52376] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:09 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:09 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-60] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52379] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-40] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52380] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:10 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-82] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52382] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:11 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:11 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-38] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52384] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-49] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52386] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:11 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-35] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52388] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:12 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:12 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-32] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52391] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:12 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:12 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-120] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52392] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:12 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-118] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52395] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:12 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-58] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52396] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:12 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-12] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52398] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-100] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52400] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-102] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52402] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:13 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-26] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52404] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:14 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:14 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:14 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-80] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52407] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-15] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52409] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:14 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-42] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52410] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:15 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-108] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52412] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-25] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52415] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:15 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-105] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52417] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:16 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:16 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-114] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52419] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:16 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-67] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52421] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:16 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-53] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52423] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:16 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-52] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52425] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-126] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52427] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:17 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-125] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52429] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:17 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:18 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:18 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-121] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52432] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-78] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52433] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-103] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52435] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:20 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-45] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52437] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:20 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:20 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-97] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52439] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:20 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:20 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:20 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:21 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-110] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52446] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-29] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52447] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-13] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52448] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-41] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52449] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-69] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52450] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:21 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-27] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52452] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:21 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-81] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52454] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:22 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-63] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52456] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:22 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-124] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52458] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:23 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:23 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-71] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52461] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-75] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52462] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:24 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-68] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52465] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:25 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:25 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-116] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52468] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-64] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52469] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:26 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-74] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52471] API call w/annotators tokenize,pos,lemma,ner,depparse
2023-04-14 20:07:27 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
2023-04-14 20:07:27 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.
[pool-1-thread-99] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52474] API call w/annotators tokenize,pos,lemma,ner,depparse
[pool-1-thread-117] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:52475] API call w/annotators tokenize,pos,lemma,ner,depparse




Environment:
Mac OS
Anaconda Jupyter Notebook 6.4.12
Stanza version: 4.5.2

Additional context
Please helps!!! Thank you so much!",
99,2023-04-11T19:25:21Z,https://github.com/stanfordnlp/stanza/issues/1233,,"I'm trying to set splitHyphenated (see setting here: https://stanfordnlp.github.io/CoreNLP/tokenize.html) to False instead of the default True, using Stanza's corenlp client. However, I cannot figure out how to pass this option to the Python wrapper of the original java library.

Any help would be much appreciated!",
100,2023-04-11T03:07:55Z,https://github.com/stanfordnlp/stanza/issues/1232,,"**Describe the bug**

Trying to use the stanza CoreNLPClient:

from stanza.server import CoreNLPClient

client_stanza = CoreNLPClient(
        timeout=150000000,
        annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'coref'],
        be_quiet=True,
        endpoint=""http://localhost:"" + new_port)

doc = client_stanza .annotate(text)

stanza.server.client.AnnotationException: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: 3.8
 - Stanza version: 1.4.2

**Additional context**
Please helps!!! Thank you so much!
",
101,2023-04-02T13:05:10Z,https://github.com/stanfordnlp/stanza/issues/1231,,"**Describe the bug**
The online demo for the Biomedical models is not working and returns a 502 Bad Gateway Error.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to https://stanfordnlp.github.io/stanza/biomed.html
2. Click on 'Online Demo'
3. See error '502 Bad Gateway'

**Expected behavior**
Returns online demo

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: [e.g. Python 3.6.8 from Anaconda]
 - Stanza version: [e.g., 1.0.0]

**Additional context**
Add any other context about the problem here.
",
102,2023-03-30T16:42:00Z,https://github.com/stanfordnlp/stanza/pull/1230,,"Use a networkx graph for enhanced dependencies (the 9th column in a conllu file) rather than burying them in a concatenated string

Use this networkx format to send enhanced requests to & from ssurgeon",
103,2023-03-30T06:35:51Z,https://github.com/stanfordnlp/stanza/issues/1229,,"I need to find the pos certain word from a sentence.
For example: sentence = ""For all models ~ITE+CKAD~ obtains the highest fluency of 1.68 and ~ITE+DD~ has the highest Knowledge Relevance of 0.56 and highest Context Coherence of 0.90""
 
From the above sentence, I need to find the pos before and after the word that is enclosed by ~ delimiter, I need to find pos of the bolded words from the same sentence. 
sentence = ""For all **models** ~ITE+CKAD~ **obtains** the highest fluency of 1.68 **and** ~ITE+DD~ **has** the highest Knowledge Relevance of 0.56 and highest Context Coherence of 0.90""

The issue in finding pos is the special character between the words,

When I pass the sentence into the model ~ITE+CKAD~  is separated into ITE, +, CKAD which is making me track pos of the adjacent words. 

What is want is how can I make a model to consider tokens as words that are separated by whitespace, the meaning model should consider ~ITE+CKAD~ as a single token. 

The end goal is to find the pos before and after of the text that is enclosed by ^ delimiter. 

Any other best approach is also appreciated

Note: there can be any special character then + like - or _  .

Thank you for your help in advance.



",
104,2023-03-28T12:52:43Z,https://github.com/stanfordnlp/stanza/issues/1227,,"Like in topic. 
I think about 2 use cases
Example in english for easier understanding, but want to use english and/or polish models

Example 1 filling mask
```
""The World War I started in <mask> year""
result: ""The World War I started in 1914 year""
```

ex2.  Next word suggestion/autocomplete/filling
```
""I"" - suggestions <am><was><think>
""I am"" - suggestions <writing><thinking><sure>
""I am writing"" - suggestions <this><about><something>
```
",
105,2023-03-27T14:01:08Z,https://github.com/stanfordnlp/stanza/issues/1225,,"I had maken my input text into `paragraph1\n\nparagraph2...`.
It's seems that  `\n\n` worke in   [#159](https://github.com/stanfordnlp/stanza/issues/159) ,[#660 ](https://github.com/stanfordnlp/stanza/issues/660).

Here is my code below:
```` python
with CoreNLPClient() as client:
    doc=client.annotate(content,properties=properties)

p=[]
for sent in doc.sentence:
    for j in sent.mentionsForCoref:
        p=p+[j.paragraph]
print(set(p)) #{1} 
````

The `paragraph` only show `1` in whole result.
Is there any way to get information about a paragraph? Thanks!
",
106,2023-03-24T02:49:04Z,https://github.com/stanfordnlp/stanza/issues/1224,,"How do I extract all VP across all branches and all trees for a given sentence. I did try to iterate through a few child nodes but I am not able to automate it or if there is an easier way then that would be helpful. I wasn't able to make full use of the help documentation.

```python
with open(f'./output/parse_output_{s}.txt', 'w') as f:
  for sentence in doc.sentences:
    tree = sentence.constituency
    f.write(tree.__format__(""P""))
    f.write('\n')
    f.write(str(tree.depth()))
    f.write('\n')
    if (tree.children[0].label=='VP'):
        f.write(str(tree.children[0].leaf_labels()))
    elif (tree.children[0].children[0].label=='VP'):
        f.write(str(tree.children[0].children[0].leaf_labels()))
    elif (tree.children[0].children[0].children[0].label=='VP'):
        f.write(str(tree.children[0].children[0].children[0].leaf_labels()))
    f.write('\n')
f.close()
```",
107,2023-03-22T16:38:00Z,https://github.com/stanfordnlp/stanza/issues/1223,,"In my code I use a Stanza pipeline twice on two sections of an input document, imagine a TITLE and a BODY sections.

I normally insert the resulting Stanza Document in a MongoDB document.

It can happen that either the TITLE or the BODY sections are missing and therefore the corresponding Documents are too.

To avoid complicating the MongoDB insert I'd like to simply insert an empty Stanza Document 

So imagine I use:
```
if titletext != """":
                    titlestanzadoc = do_stanza_nlp(
                        input_text=titletext, nlpipe=nlpipe
                    )
else:
                   titlestanzadoc = ?????????????

```
How would I create the empty Doc object? Thanks a lot

PS This seems an overkill, right? stanzadoc = nlp("""")",
108,2023-03-21T06:33:27Z,https://github.com/stanfordnlp/stanza/pull/1222,,"Add finetuning of the transformer, especially to the constituency parser",
109,2023-03-20T11:35:48Z,https://github.com/stanfordnlp/stanza/pull/1221,,"## Description
Remove empty bashrc file at root of repo.

This caught my eye when I was looking at files in the repo.

This appears to have been added in error in #1083 - skimming the code, it doesn't seem to invoke bash at all.

I may be wrong here and there's a subtlety for why you need this, in which case, sorry! I thought this was worth opening in case though, as it does seem to me to be accidental. (I see that PR does squash commits from a previous PR, but I couldn't find that looking at closed PRs, so maybe a squashed commit message had more detail). 

## Fixes Issues
I haven't bothered opening an issue for such a small change - but let me know if you would like me to and I'd be happy to.

## Unit test coverage
No - removing empty unused file so there's no additional code to test

## Known breaking changes/behaviors
No - but please correct me if I'm missing something!
",
110,2023-03-16T04:10:11Z,https://github.com/stanfordnlp/stanza/issues/1219,,"Does stanza support change the pattern of splitting words?

For example: non-alcohol into: 'non', '-', 'alcohol'; or renin-angiotensin into: 'renin', '-', 'angiotensin'.

",
111,2023-03-15T19:54:34Z,https://github.com/stanfordnlp/stanza/pull/1218,,"Data transformation (very minor) for Indonesian constituency, along with notes on the best running model",
112,2023-03-14T16:30:06Z,https://github.com/stanfordnlp/stanza/pull/1217,,As requested in #1214,
113,2023-03-14T16:00:27Z,https://github.com/stanfordnlp/stanza/issues/1216,,"**Describe the bug**
When launching my project on Apple M1 hardware I get the following error:

```
(isagog-ai-py3.10) (base) bob@Roberts-Mac-mini isagog-ai % python src/isagog_api/nlp_api.py
2023-03-14 16:05:29 INFO: Loading these models for language: it (Italian):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| mwt       | combined |
| ner       | fbk      |
========================

2023-03-14 16:05:29 INFO: Use device: cpu
2023-03-14 16:05:29 INFO: Loading: tokenize
2023-03-14 16:05:29 INFO: Loading: mwt
Traceback (most recent call last):
  File ""/Users/bob/Documents/work/code/isagog-ai/src/isagog_api/nlp_api.py"", line 14, in <module>
    from isagog_ai.nlp_it import StanzaLanguageProcessorIt
  File ""/Users/bob/Documents/work/code/isagog-ai/src/isagog_ai/nlp_it.py"", line 29, in <module>
    ""entity"": stanza.Pipeline(
  File ""/Users/bob/Documents/work/code/isagog-ai/.venv/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 278, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""/Users/bob/Documents/work/code/isagog-ai/.venv/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 173, in __init__
    self._set_up_model(config, pipeline, use_gpu)
  File ""/Users/bob/Documents/work/code/isagog-ai/.venv/lib/python3.10/site-packages/stanza/pipeline/mwt_processor.py"", line 21, in _set_up_model
    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
  File ""/Users/bob/Documents/work/code/isagog-ai/.venv/lib/python3.10/site-packages/stanza/models/mwt/trainer.py"", line 36, in __init__
    self.load(model_file, use_cuda)
  File ""/Users/bob/Documents/work/code/isagog-ai/.venv/lib/python3.10/site-packages/stanza/models/mwt/trainer.py"", line 149, in load
    self.model.load_state_dict(checkpoint['model'])
  File ""/Users/bob/Documents/work/code/isagog-ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1671, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Seq2SeqModel:
	Unexpected key(s) in state_dict: ""SOS_tensor"".
```
**Expected behavior**
Service launching just like with the exact same project on Intel.

**Environment (please complete the following information):**
 - OS: MacOS Ventura 13.2.1
 - Python version: 3.10.9
 - Stanza version: 1.4.2
Poetry managed environment:
```
/Users/bob/Documents/work/code/isagog-ai/.venv
astroid               2.14.2      An abstract syntax tree for Python with inference support.
attrs                 22.2.0      Classes Without Boilerplate
beautifulsoup4        4.11.2      Screen-scraping library
black                 22.12.0     The uncompromising code formatter.
certifi               2022.12.7   Python package for providing Mozilla's CA Bundle.
cfgv                  3.3.1       Validate configuration and produce human readable error messages.
charset-normalizer    3.0.1       The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.
click                 8.1.3       Composable command line interface toolkit
clickclick            20.10.2     Click utility functions
connexion             2.14.2      Connexion - API first applications with OpenAPI/Swagger and Flask
coverage              7.1.0       Code coverage measurement for Python
dill                  0.3.6       serialize all of python
distlib               0.3.6       Distribution utilities
emoji                 2.2.0       Emoji for Python
exceptiongroup        1.1.0       Backport of PEP 654 (exception groups)
filelock              3.9.0       A platform independent file lock.
flake8                6.0.0       the modular source code checker: pep8 pyflakes and co
flasgger              0.9.5       Extract swagger specs from your flask project
flask                 2.2.3       A simple framework for building complex web applications.
huggingface-hub       0.12.0      Client library to download and publish models, datasets and other repos on the huggingface.co hub
identify              2.5.15      File identification library for Python
idna                  3.4         Internationalized Domain Names in Applications (IDNA)
inflection            0.5.1       A port of Ruby on Rails inflector to Python
iniconfig             2.0.0       brain-dead simple config-ini parsing
isodate               0.6.1       An ISO 8601 date/time/duration parser and formatter
isort                 5.11.4      A Python utility / library to sort Python imports.
itsdangerous          2.1.2       Safely pass data to untrusted environments and back.
jinja2                3.1.2       A very fast and expressive template engine.
joblib                1.2.0       Lightweight pipelining with Python functions
jsonschema            4.17.3      An implementation of JSON Schema validation for Python
keybert               0.7.0       KeyBERT performs keyword extraction with state-of-the-art transformer models.
langcodes             3.3.0       Tools for labeling human languages with IETF language tags
lazy-object-proxy     1.9.0       A fast and thorough lazy object proxy.
markdown-it-py        2.1.0       Python port of markdown-it. Markdown parsing, done right!
markupsafe            2.1.2       Safely add untrusted strings to HTML/XML markup.
mccabe                0.7.0       McCabe checker, plugin for flake8
mdurl                 0.1.2       Markdown URL utilities
mistune               2.0.5       A sane Markdown parser with useful plugins and renderers
mypy                  0.991       Optional static typing for Python
mypy-extensions       0.4.3       Experimental type system extensions for programs checked with the mypy typechecker.
nltk                  3.8.1       Natural Language Toolkit
nodeenv               1.7.0       Node.js virtual environment builder
numpy                 1.24.1      Fundamental package for array computing in Python
packaging             23.0        Core utilities for Python packages
pathspec              0.11.0      Utility library for gitignore style pattern matching of file paths.
pillow                9.4.0       Python Imaging Library (Fork)
platformdirs          2.6.2       A small Python package for determining appropriate platform-specific dirs, e.g. a ""user data dir"".
pluggy                1.0.0       plugin and hook calling mechanisms for python
pre-commit            3.0.1       A framework for managing and maintaining multi-language pre-commit hooks.
protobuf              4.21.12
pycodestyle           2.10.0      Python style guide checker
pyflakes              3.0.1       passive checker of Python programs
pygments              2.14.0      Pygments is a syntax highlighting package written in Python.
pylint                2.16.2      python code static checker
pyparsing             3.0.9       pyparsing module - Classes and methods to define and execute parsing grammars
pyrsistent            0.19.3      Persistent/Functional/Immutable data structures
pytest                7.2.1       pytest: simple powerful testing with Python
pytest-cov            4.0.0       Pytest plugin for measuring coverage.
pyyaml                6.0         YAML parser and emitter for Python
rapidfuzz             2.13.7      rapid fuzzy string matching
rdflib                6.2.0       RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information.
regex                 2022.10.31  Alternative regular expression module, to replace re.
requests              2.28.2      Python HTTP for Humans.
rich                  13.2.0      Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal
scikit-learn          1.2.1       A set of python modules for machine learning and data mining
scipy                 1.9.3       Fundamental algorithms for scientific computing in Python
sentence-transformers 2.2.2       Multilingual text embeddings
sentencepiece         0.1.97      SentencePiece python wrapper
setuptools            66.1.1      Easily download, build, install, upgrade, and uninstall Python packages
six                   1.16.0      Python 2 and 3 compatibility utilities
soupsieve             2.3.2.post1 A modern CSS selector implementation for Beautiful Soup.
sparqlwrapper         2.0.0       SPARQL Endpoint interface to Python
stanza                1.4.2       A Python NLP Library for Many Human Languages, by the Stanford NLP Group
swagger-ui-bundle     0.0.9       swagger_ui_bundle - swagger-ui files in a pip package
threadpoolctl         3.1.0       threadpoolctl
tokenizers            0.13.2      Fast and Customizable Tokenizers
toml                  0.10.2      Python Library for Tom's Obvious, Minimal Language
tomli                 2.0.1       A lil' TOML parser
tomlkit               0.11.6      Style preserving TOML library
torch                 1.13.1      Tensors and Dynamic neural networks in Python with strong GPU acceleration
torchvision           0.14.1      image and video datasets and models for torch deep learning
tqdm                  4.64.1      Fast, Extensible Progress Meter
transformers          4.26.0      State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow
typing-extensions     4.4.0       Backported and Experimental Type Hints for Python 3.7+
urllib3               1.26.14     HTTP library with thread-safe connection pooling, file post, and more.
virtualenv            20.17.1     Virtual Python Environment builder
werkzeug              2.2.3       The comprehensive WSGI web application library.
wrapt                 1.14.1      Module for decorators, wrappers and monkey patching.
```",
114,2023-03-14T09:38:27Z,https://github.com/stanfordnlp/stanza/issues/1214,,"Hello,

I am finding Stanza very useful. Thanks to all the team! 

I have the following code:

```
main_lang = ['en', 'es', 'fr', 'pt', 'de', 'it', 'pl', 'nl', 'ro', 'cs', 'sv', 'hu', 'el', 'bg', 'da', 'hr', 
    'sk', 'fi', 'zh', 'hi', 'ar', 'ru', 'ur', 'bn', 'id', 'sw', 'ja', 'pa', 'vi', 'ko', 'tr', 'tl', 'fa']

lang_config = defaultdict(dict)
for lang in main_lang:
	lang_config[lang]['processors'] = 'tokenize', 'mwt', 'ner'
	lang_config[lang]['download_method'] = stanza.DownloadMethod.REUSE_RESOURCES

nlp = MultilingualPipeline(lang_id_config={
	'langid_clean_text': True, 
	'langid_lang_subset': main_lang},
	lang_configs=dict(lang_config),
	max_cache_size=16)
```

This works by removing the 'ner' model or by introducing an exception for certain languages.

Otherwise, it throws an error:

`stanza.pipeline.core.UnsupportedProcessorError: Processor ner is not known for language pt.  If you have created your own model, please specify the ner_model_path parameter when creating the pipeline.`

The problem is that the list 'main_lang' is dynamically generated according to certain criteria and so I do not know a priori which languages are going to be in this list at a given moment. In addition, I guess that new Stanza releases may include additional processors for certain languages.

Is there are way to find out programmatically which processors are implemented for which languages so that I can make my code more robust and less ad hoc?

Best regards,

Ed
",
115,2023-03-14T09:09:56Z,https://github.com/stanfordnlp/stanza/issues/1213,,"I have search alot about for a given implementation within NLP that can expect list of words as input variable and define the sequencing or some kind of dependency for execution. For instance:

There is a list of words as, ['delete', 'add', 'redeploy', 'create', 'cell', 'sector']

Now, I would like to re-arrange these words in a sequence that tells which word/action/event should occur first before the next one. Therefore, with that logic the list of words should then be re-arranged as, ['create', 'cell', 'sector', 'add', 'delete', 'redeploy']. Which means, 'create' as an operation/action should occur before 'add' or 'delete' operation/action.

Is there any specific library exist that can calculate this dependency/sequencing among a given list of words?",
116,2023-03-13T02:43:58Z,https://github.com/stanfordnlp/stanza/pull/1212,,,
117,2023-03-11T14:30:50Z,https://github.com/stanfordnlp/stanza/pull/1211,,"Ssurgeon requests now turn responses back into Document, although it currently doesn't handle MWT",
118,2023-03-10T22:42:19Z,https://github.com/stanfordnlp/stanza/pull/1210,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `main` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/main/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
119,2023-03-10T07:51:55Z,https://github.com/stanfordnlp/stanza/issues/1209,,"**Describe the bug**
Running Stanza NLP For the language of Indonesia and Vietnamese:
It came up this issue

**Expected behavior**
Using only one cuda device 

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: python3.8
 - Stanza version: 1.4.2

![image](https://user-images.githubusercontent.com/71269145/224282355-9364f39f-5bb0-4009-99bb-f240c6eb9350.png)

",
120,2023-03-08T08:40:46Z,https://github.com/stanfordnlp/stanza/issues/1207,,"Thanks for sharing. I wonder if you have included the implemantion of scene graph (https://nlp.stanford.edu/software/scenegraph-parser.shtml) in the current version of stanza?




If so, can you share a demo for how to transfer a sentence to a scene graph, thanks.",
121,2023-03-07T14:02:17Z,https://github.com/stanfordnlp/stanza/issues/1206,,"Hello! I have trained a NER model for the Armenian language using the[ ArmTDP dataset](https://github.com/myavrum/ArmTDP-NER) and the [xlm-roberta-base model](https://huggingface.co/xlm-roberta-base).

After that, I attempted to test the model using stanza.Pipeline:

```
import stanza

config = {
'processors': 'tokenize, ner',
'lang': 'hy',
'ner_model_path': '/Lab/Projects/ner/models/hy_armtdp_nertagger_bert_18.pt',
}

nlp = stanza.Pipeline(**config)

nlp(""some text in Arminian"")

```

While working with the same data, I observed that the outputs after loading the model were different each time. 
Although there was no such problem when testing the code using internal commands. Whenever I run the following code, I get the same output:

`python3 -m stanza.utils.training.run_ner hy_armtdp --score_test`

What could be the cause of this problem? 

Additionally, I have added data conversion and BERT code for Armenian in this [pull request](https://github.com/ShakeHakobyan/stanza/pull/2) (trained model can be downloaded from this [drive](https://drive.google.com/file/d/15Fc1BFj_Rlbio3Vt5c69QUH9YvS7dTzW/view?usp=sharing)).

If the problem is feasible, it would be great to integrate a NER model for Armenian in the main package


Thanks!

",
122,2023-03-07T08:09:15Z,https://github.com/stanfordnlp/stanza/pull/1205,,"Add a skeletal ssurgeon interface similar to the tsurgeon and semgrex interfaces.

This still needs a convenient conversion back from the ssurgeon response to the Doc format
Also, to function correctly, we need a CoreNLP release",
123,2023-03-05T14:46:43Z,https://github.com/stanfordnlp/stanza/issues/1203,,"I'm trying to get xpos for some Portuguese text, but it doesn't appear in the output. 

### code

`import stanza`
`nlp = stanza.Pipeline(lang='pt', processors='tokenize,mwt,pos,lemma')`

`text2 = 'Depois de o meu avião ter aterrado no aeroporto da minha terra natal no dia de janeiro, vi a notícia no telemóvel, que era “A transmissão de COVID-19 ocorre de humanos para humanos”.'`
`doc = nlp(text2)`

`print(*[f'word: {word.text}\tupos: {word.upos}\txpos: {word.xpos}\tfeats: {word.feats if word.feats else ""_""}' for sent in doc.sentences for word in sent.words], sep='\n')`

### output

> 2023-03-05 22:29:31 INFO: Use device: cpu
2023-03-05 22:29:31 INFO: Loading: tokenize
2023-03-05 22:29:31 INFO: Loading: mwt
2023-03-05 22:29:31 INFO: Loading: pos
2023-03-05 22:29:31 INFO: Loading: lemma
2023-03-05 22:29:31 INFO: Done loading processors!
word: Depois	upos: ADV	xpos: None	feats: _
word: de	upos: ADP	xpos: None	feats: _
word: o	upos: DET	xpos: None	feats: Definite=Def|Gender=Masc|Number=Sing|PronType=Art
word: meu	upos: DET	xpos: None	feats: Gender=Masc|Number=Sing|PronType=Prs
word: avião	upos: NOUN	xpos: None	feats: Gender=Masc|Number=Sing

### more information
According to #243, I checked documentation in [UD Portuguese Bosque](https://universaldependencies.org/treebanks/pt_bosque/index.html)

<html><body>
<!--StartFragment-->

Annotation | Source
-- | --
Lemmas | annotated manually in non-UD style, automatically converted to UD, with some manual corrections of the conversion
UPOS | annotated manually in non-UD style, automatically converted to UD, with some manual corrections of the conversion
XPOS | annotated manually
Features | annotated manually in non-UD style, automatically converted to UD, with some manual corrections of the conversion
Relations | annotated manually in non-UD style, automatically converted to UD, with some manual corrections of the conversion

<!--EndFragment-->
</body>
</html>

I'd like to understand whether it's a missing feature, a bug, or I'm missing something.
Thanks in advance!",
124,2023-03-03T20:21:20Z,https://github.com/stanfordnlp/stanza/issues/1202,,"Hi,

I am not using Python 2 and I am not under Jupyter Notebook.

I am under Windows and I already installed Java 19 and python -m pip install stanza as in the doc of Stanza.

I installed stanza into my .env folder (virtual environment), and when I type: stanza.install_corenlp()
 into my python script I recieve that Attribute Error message.

What could be the cause of this?


",
125,2023-03-01T00:29:01Z,https://github.com/stanfordnlp/stanza/pull/1201,,Add a visualization / semgrex webapp,
126,2023-02-28T19:33:23Z,https://github.com/stanfordnlp/stanza/pull/1200,,"## Description
Line 398 calls `get_ipython` but this function is never imported which throws error in console hidden applications. Now the import line is added. More details [here](https://stackoverflow.com/questions/75595323/stanza-based-auto-py-to-exe-gui-app-throws-exception-windows-10).


## Fixes Issues
Error while loading a GUI app involving the import of `stanza`, like the error below:
![image](https://user-images.githubusercontent.com/46898829/221959339-719d9159-33e0-4808-999e-da5c66c78afc.png)


## Unit test coverage
None

## Known breaking changes/behaviors
Don't know.
",
127,2023-02-28T13:41:06Z,https://github.com/stanfordnlp/stanza/issues/1199,,"Hello,

I struggle to find how to do not load all the processors in a multilingual context.

When I do:
```python
from stanza.pipeline.multilingual import MultilingualPipeline

nlp = MultilingualPipeline()
text = ""my piece of text in a random language.""
doc = nlp(text)
```
I get the following processors for the detected language:
```
============================
| Processor    | Package   |
----------------------------
| tokenize     | combined  |
| pos          | combined  |
| lemma        | combined  |
| depparse     | combined  |
| sentiment    | sstplus   |
| constituency | wsj       |
| ner          | ontonotes |
============================
```
Bust I only need to use  and load the ""tokenize"" processor. How can I avoid the usage of all the other processors?

I specify that I don't know the language of the input text in advance.

Thanks in advance for any help :)",
128,2023-02-27T17:14:06Z,https://github.com/stanfordnlp/stanza/issues/1198,,"Hello, I have a question concerning how to download or use a model.
When I try to load a model it never goes past 6% and I get messages that I don't understand, I am a beginner but i cannot understand what is going on. 

Until now I have pip installed stanza, imported it using 
>>>import stanza
and finally tried to use a model with : 
>>>stanza.download('en')

I am expecting it to just load the model so i can use it or create a pipeline but it stops at 6% and gives me this message : 

Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    stanza.download('en')
  File ""C:\Users\gaelleluciejeanne\AppData\Local\Programs\Python\Python310\lib\site-packages\stanza\resources\common.py"", line 562, in download
    request_file(
  File ""C:\Users\gaelleluciejeanne\AppData\Local\Programs\Python\Python310\lib\site-packages\stanza\resources\common.py"", line 154, in request_file
    assert_file_exists(path, md5, alternate_md5)
  File ""C:\Users\gaelleluciejeanne\AppData\Local\Programs\Python\Python310\lib\site-packages\stanza\resources\common.py"", line 107, in assert_file_exists
    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))
ValueError: md5 for C:\Users\gaelleluciejeanne\stanza_resources\en\default.zip is 99efe90ea83d42c9753e860a062fd228, expected b0eb259bd9a7a61fc23f3adb03d7b731

I am using The IDLE shell 3.10.10 and the latest version of stanza i think it's the 1.3

",
129,2023-02-24T23:19:38Z,https://github.com/stanfordnlp/stanza/pull/1196,,fix https://github.com/stanfordnlp/stanza/issues/1195,
130,2023-02-24T23:16:46Z,https://github.com/stanfordnlp/stanza/issues/1195,,"**Describe the bug**
It seems that there is a nasty bug with Language Identification when langid_clean_text=True (AttributeError: module 'emoji' has no attribute 'get_emoji_regexp') : https://stanfordnlp.github.io/stanza/langid.html#apply-text-cleaning-to-tweets

**To Reproduce**
Steps to reproduce the behavior: https://stanfordnlp.github.io/stanza/langid.html#basic-language-id-example


```python
from stanza.models.common.doc import Document
from stanza.pipeline.core import Pipeline

nlp = Pipeline(lang=""multilingual"", processors=""langid"", langid_clean_text=True)
docs = [""Hello world."", ""Bonjour le monde! #thisisfrench #ilovefrance""]
docs = [Document([], text=text) for text in docs]
nlp(docs)
print(""\n"".join(f""{doc.text}\t{doc.lang}"" for doc in docs))
```

**Expected behavior**
langid_clean_text=True should work

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS]
 - Python version: [e.g. Python 3.6.8 from Anaconda]
 - Stanza version: [e.g., 1.0.0]

**Additional context**
Add any other context about the problem here.
",
131,2023-02-24T02:58:28Z,https://github.com/stanfordnlp/stanza/issues/1194,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
132,2023-02-23T16:36:00Z,https://github.com/stanfordnlp/stanza/issues/1193,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
133,2023-02-07T14:57:20Z,https://github.com/stanfordnlp/stanza/issues/1191,,"I wish to execute the following code in my jupyter notebook `from stanza.server import CoreNLPClient`
However, the following error shows: `ModuleNotFoundError: No module named 'stanza.server'`
I have tried to install stanza, but when I run `pip3 install stanza`, I got `Requirement already satisfied: stanza in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.3)`.
I think the installed version of stanza(0.3) might be too old, but when I tried to reinstall stanza, the same version was downloaded. How can I solve this issue? My python version is 3.11.1.",
134,2023-02-06T20:19:41Z,https://github.com/stanfordnlp/stanza/pull/1190,,A version of the classifier that uses the constituency parser to build its embeddings,
135,2023-02-06T13:38:51Z,https://github.com/stanfordnlp/stanza/pull/1189,,,
136,2023-02-04T21:24:02Z,https://github.com/stanfordnlp/stanza/issues/1188,,"**Describe the bug**

Traceback (most recent call last):                                                                    
File ""<stdin>"", line 1, in <module>                                                                  
File ""/.../lib/python3.7/site-packages/stanza/pipeline/$ore.py"", line 408, in __call__                                                                         
  return self.process(doc, processors)                                                               
File ""/.../lib/python3.7/site-packages/stanza/pipeline/core.py"", line 397, in process                                                                          
  doc = process(doc)                                                                                 
File ""/.../lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py"", line 91, in process                                                             
  num_workers = self.config.get('num_workers', 0))                                                   
File ""/.../lib/python3.7/site-packages/stanza/models/tokenization/utils.py"", line 264, in output_predictions                                                   
  pred = np.argmax(trainer.predict(batch), axis=2)                                                   
File ""/.../lib/python3.7/site-packages/stanza/models/tokenization/trainer.py"", line 69, in predict                                                             
  pred = self.model(units, features)                                                                 
File ""/.../lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl                                                                    
  result = self.forward(*input, **kwargs)                                                            
File ""/.../lib/python3.7/site-packages/stanza/models/tokenization/model.py"", line 45, in forward                                                               
  emb = self.embeddings(x)                                                                           
File ""/.../lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl                                                                    
  result = self.forward(*input, **kwargs)                                                            
 File ""/.../lib/python3.7/site-packages/torch/nn/modules/sparse.py"", line 126, in forward                                                                      
  self.norm_type, self.scale_grad_by_freq, self.sparse)                                              
 File ""/.../lib/python3.7/site-packages/torch/nn/functional.py"", line 1852, in embedding                                                                        
  return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)                     
RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)


**To Reproduce**
Following the tokenization tutorial https://stanfordnlp.github.io/stanza/tokenize.html 

import stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize')
doc = nlp('This is a test sentence for stanza. This is another sentence.')



**Environment (please complete the following information):**
 - OS Ubuntu
 - Python   3.7.13
 - Pytorch 1.7.0+cu110
 - Stanza  1.4.2

**Additional context**
Change the stanza version to 1.4.0 will solve the problem
",
137,2023-02-03T09:02:32Z,https://github.com/stanfordnlp/stanza/issues/1187,,"I have a Django app hosted on ubuntu apache where am carrying out pos tagging with stanza. 
Every time I launch the application am getting this error
` Permission denied: 'home/ooglobe/tmpw_4rz8p9',` and when I refresh the page the temp file keeps changing  `Permission denied: 'home/ooglobe/tmp6nn3pf0o',`

This is my code
`import stanza`
`stanza.download('en',processors='tokenize,pos',model_dir='home/ooglobe/')`


`def extract_nouns_and_verbs(text):`
        `nlp = stanza.Pipeline(processors=""tokenize,pos"",dir=""/home/ooglobe/stanza_resources`

Not sure what am missing here",
138,2023-02-01T16:21:22Z,https://github.com/stanfordnlp/stanza/pull/1186,,"Includes basic test file using a supplementary folder (stored in a zip) for AWS labeling tracker
",
139,2023-01-31T19:51:17Z,https://github.com/stanfordnlp/stanza/pull/1185,,"
## Description
Add language Code for extremaduran language https://iso639-3.sil.org/code/ext
",
140,2023-01-23T19:04:07Z,https://github.com/stanfordnlp/stanza/issues/1184,,"When using a prebuilt pipeline, is there a way to access the original dictionary and find all variants of a specific word given its lemma?",
141,2023-01-20T14:50:44Z,https://github.com/stanfordnlp/stanza/issues/1183,,"Currently i am trying to train and evaluate sequioa and ftb corpus and respectively cross evaluate, ie Seq Vs ftb, ftb vs seq. But this is throwing down errors listed below, 

2023-01-20 14:50:16 DEBUG: UD_French-Sequoia: fr_sequoia
2023-01-20 14:50:16 INFO: Save file for fr_sequoia model: fr_sequoia_tagger.pt
2023-01-20 14:50:16 INFO: Using default pretrain for language, found in /home/lrec2022/stanza_resources/fr/pretrain/sequoia.pt  To use a different pretrain, specify --wordvec_pretrain_file
2023-01-20 14:50:16 INFO: Running dev POS for UD_French-Sequoia with args ['--wordvec_dir', 'extern_data/wordvec', '--eval_file', 'data/pos/fr_sequoia.dev.in.conllu', '--output_file', 'data/pos/fr_sequoia.dev.pred.conllu', '--gold_file', 'data/pos/fr_sequoia.dev.gold.conllu', '--lang', 'fr', '--shorthand', 'fr_sequoia', '--mode', 'predict', '--wordvec_pretrain_file', '/home/lrec2022/stanza_resources/fr/pretrain/sequoia.pt', '--save_name', 'fr_sequoia_tagger.pt']
2023-01-20 14:50:16 INFO: Running tagger in predict mode
2023-01-20 14:50:16 INFO: Loading model from: saved_models/pos/fr_sequoia_tagger.pt
2023-01-20 14:50:17 DEBUG: Loaded pretrain from /home/lrec2022/stanza_resources/fr/pretrain/sequoia.pt
2023-01-20 14:50:17 INFO: Loading data with batch size 5000...
2023-01-20 14:50:18 DEBUG: 8 batches created.
2023-01-20 14:50:18 INFO: Start evaluation...
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/utils/training/run_pos.py"", line 118, in <module>
    main()
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/utils/training/run_pos.py"", line 115, in main
    common.main(run_treebank, ""pos"", ""tagger"", add_charlm_args)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/utils/training/common.py"", line 240, in main
    temp_output_file.name, command_args, extra_args + save_name_args)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/utils/training/run_pos.py"", line 92, in run_treebank
    tagger.main(dev_args)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/models/tagger.py"", line 124, in main
    evaluate(args)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/models/tagger.py"", line 326, in evaluate
    _, _, score = scorer.score(system_pred_file, gold_file)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/models/pos/scorer.py"", line 12, in score
    evaluation = ud_scores(gold_conllu_file, system_conllu_file)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/models/common/utils.py"", line 125, in ud_scores
    gold_ud = ud_eval.load_conllu_file(gold_conllu_file)
  File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/utils/conll18_ud_eval.py"", line 655, in load_conllu_file
    _file = open(path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {}))
FileNotFoundError: [Errno 2] No such file or directory: 'data/pos/fr_sequoia.dev.gold.conllu'
 
In the above scenario I have trying renaming the ftb corpus as sequoia corpus as per stanza naming convention. I do have also tried mentioning the eval file as ftb but it gives following error,

   File ""/home/lrec2022/Documents/selixini/selexini-code/stanza-main/stanza/utils/conll18_ud_eval.py"", line 618, in evaluate
    """".join(map(_encode, system_ud.characters[index:index + 20]))
stanza.utils.conll18_ud_eval.UDError: The concatenation of tokens in gold file and in system file differ!
First 20 differing characters in gold file: 'L'associationachangé' and system file: 'Nousprionslescinéast'

The corpus are ud files which are different is there a restriction to evaluate and predict models on cross corpus.

",
142,2023-01-19T10:15:22Z,https://github.com/stanfordnlp/stanza/issues/1182,,"**Description**
I have read the official documentation of stanza and it says it has ""xpos"", ""upos"" and few other in **(POS)** and **Morphological** section. But, when I run the code, it gives me this error. 


**Code:**
```Python
def pos_tokenizer(text):
    # Use the pipeline to analyze the text
    doc = nlp(text)
    # Extract the part of speech for each token
    pos_tags = [token.xpos for token in doc.sentences[0].tokens]
    return pos_tags
```
**Code section that produces the error:**
```Python
text = ""The cat sat on the mat.""
pos_tags = pos_tokenizer(text)
```

**Error:**
```Python
AttributeError                            Traceback (most recent call last)
[<ipython-input-7-3db2d827bb78>](https://localhost:8080/#) in <module>
      1 text = ""The cat sat on the mat.""
----> 2 pos_tags = pos_tokenizer(text)

1 frames
[<ipython-input-6-91889fefb641>](https://localhost:8080/#) in <listcomp>(.0)
      3     doc = nlp(text)
      4     # Extract the part of speech for each token
----> 5     pos_tags = [token.upos for token in doc.sentences[0].tokens]
      6     return pos_tags

AttributeError: 'Token' object has no attribute 'upos'
```

**Note:** I am new in Stanza that's why, I might be missing something if anyone could help me or give pointers it'll be really helpful. 

**Documentation (I accessed):** https://stanfordnlp.github.io/stanza/pos.html",
143,2023-01-13T19:35:47Z,https://github.com/stanfordnlp/stanza/issues/1180,,"Names can be quite tricky:

Guido van der Valk -> https://en.wikipedia.org/wiki/Guido_van_der_Valk
Pedro Calderón de la Barca -> https://en.wikipedia.org/wiki/Pedro_Calder%C3%B3n_de_la_Barca
John F. Kennedy Jr. -> https://en.wikipedia.org/wiki/John_F._Kennedy_Jr.
Loredana De Petris -> https://en.wikipedia.org/wiki/Loredana_De_Petris

Could Stanza NER be leveraged in some ways to separate first and middle name(s) from lastname(s)?

There are python libs out there (eg nominally, nameparser etc) which attempt but fail too often.

I would need to identify this while parsing news articles where in the text you would tipically find the full name perhaps once but thereafter only the last or the firstname.
",
144,2023-01-12T02:29:53Z,https://github.com/stanfordnlp/stanza/pull/1178,,"When a comment is added to a sentence with a constituency tree, parse that comment into the tree object

This involves refactoring the StanzaObject, since it is used in both doc and parse_tree
",
145,2023-01-11T23:51:05Z,https://github.com/stanfordnlp/stanza/pull/1177,,"Add comments to the serialized format when saving a document.  This will keep any user added comments, but at the cost of making the files larger because they now have the text in a couple different places.
",
146,2023-01-11T08:46:19Z,https://github.com/stanfordnlp/stanza/issues/1176,,"**Describe the bug**
The List[List[Dict]] as produced by stanza .to_dict function cannot be fed to the opposite Document function to rebuild a Document object with the same content. See https://stanfordnlp.github.io/stanza/data_conversion.html

**To Reproduce**
```
nlp = stanza.Pipeline('en', processors='tokenize, pos, lemma, mwt, ner, depparse')
doc = nlp(""John rang the bell to Joe Biden's house in Washington."")
doc_dict = doc.to_dict()  # doc_dict is List[List[Dict]]
doc_anew = stanza.Document(doc_dict)
print(doc == doc_anew)
```
**False**

The fact that doc and doc_anew do not contain the same values can also be demonstrated by printing doc.entities (which would give me the recognized Named Entities) and printing doc_anew.entities which would produce an empty list.

**Expected behavior**
I need a reversible function to be able to serialize a Document object and expected these to perform this. So in the above example I would expect doc == doc_anew to be True.

**Environment (please complete the following information):**
MacOS Ventura
Python 3.11.0 from Anaconda
stanza                    1.4.2              pyhd8ed1ab_0    conda-forge

**Additional context**
I need to persist Document objects in a Redis Key-Value store. To store the Document I need to serialize it to a string or a byte sequence. I thought I could use the to_dict function as an input to a json.dumps to achieve this, but of course this needs to be symmetrically reversible. Happy 2023
",
147,2023-01-10T08:27:41Z,https://github.com/stanfordnlp/stanza/pull/1175,,Add a pipeline depparse variant which talks to the CoreNLP constituency converter,
148,2023-01-10T01:46:40Z,https://github.com/stanfordnlp/stanza/issues/1174,,"Hi Everyone,

Is there anyway to deal with this issue?Thank you~

Below is the issue shown:

requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/main/resources_1.4.0.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000028D0CF394C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))

",
149,2023-01-04T14:05:46Z,https://github.com/stanfordnlp/stanza/issues/1173,,"I am trying to retrian Arabic-PADT data with some corrections, but I get an error while preparing mwt
Tokenization is trained just fine, but mwt, after starting, stops with an error.

```
root@cb3e53da0984:/data# python3 -m stanza.utils.datasets.prepare_mwt_treebank UD_Arabic-PADT
2023-01-04 13:45:32 INFO: Datasets program called with:
/usr/local/lib/python3.8/dist-packages/stanza/utils/datasets/prepare_mwt_treebank.py UD_Arabic-PADT
Preparing data for UD_Arabic-PADT: ar_padt, ar
Reading from /data/UD_Arabic-PADT/ar_padt-ud-train.conllu and writing to /tmp/tmpyiumbfdm/ar_padt.train.gold.conllu
Reading from /data/UD_Arabic-PADT/ar_padt-ud-dev.conllu and writing to /tmp/tmpyiumbfdm/ar_padt.dev.gold.conllu
Reading from /data/UD_Arabic-PADT/ar_padt-ud-test.conllu and writing to /tmp/tmpyiumbfdm/ar_padt.test.gold.conllu
11766 unique MWTs found in data
2480 unique MWTs found in data
2426 unique MWTs found in data
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.8/dist-packages/stanza/utils/datasets/prepare_mwt_treebank.py"", line 63, in <module>
    main()
  File ""/usr/local/lib/python3.8/dist-packages/stanza/utils/datasets/prepare_mwt_treebank.py"", line 60, in main
    common.main(process_treebank)
  File ""/usr/local/lib/python3.8/dist-packages/stanza/utils/datasets/common.py"", line 257, in main
    process_treebank(treebank, paths, args)
  File ""/usr/local/lib/python3.8/dist-packages/stanza/utils/datasets/prepare_mwt_treebank.py"", line 49, in process_treebank
    source_filename = prepare_tokenizer_treebank.mwt_name(tokenizer_dir, short_name, shard)
AttributeError: module 'stanza.utils.datasets.prepare_tokenizer_treebank' has no attribute 'mwt_name'
root@cb3e53da0984:/data# python3 -m stanza.utils.datasets.prepare_mwt_treebank UD_Arabic-PADT
```

I am running Python 3.8 under Ubuntu 20.04 in a docker container.  Stanza is installed through pip.

Any hint?

Thank you,

Giuliano",
150,2023-01-04T07:35:43Z,https://github.com/stanfordnlp/stanza/issues/1172,,"Hi Stanza Developers,

Thanks for the super useful library!

**Is your feature request related to a problem? Please describe.**
Currently using Stanza for sentence segmentation. While Stanza mostly performs well, do encounter cases such as `Because There are issues` there the sentence segmentation gives `['Because', 'There are issues']`.

**Describe the solution you'd like**
In cases where there's unnecessary capitalization, keep as a single sentence.
Would this necessitate re-training or is there an easier workaround?

**Describe alternatives you've considered**
I think other libraries may handle as expected but would sacrifice performance in other cases.

**Additional context**
Using latest version (`1.4.2`).",
151,2022-12-31T06:08:48Z,https://github.com/stanfordnlp/stanza/pull/1171,,"Add a constituency comment to sentences in the doc (this makes the constituency tree output in the CoNLL format)

",
152,2022-12-26T17:45:16Z,https://github.com/stanfordnlp/stanza/pull/1170,,"
## Description
`prepare_depparse_treebank` does not use the pretrained file passed via `wordvec_pretrain_file` option.
`wordvec_args` expects the list of options as the last argument, but `args` is a namespace object.
Now if `wordvec_pretrain_file` is set we add it to the `base_args`, otherwise invoke `wordvec_args` function


",
153,2022-12-22T04:32:14Z,https://github.com/stanfordnlp/stanza/pull/1169,,"Add a `""{:C}""` format to `Document` and `Sentence` by rearranging some of the code from `conll`",
154,2022-12-20T11:08:33Z,https://github.com/stanfordnlp/stanza/issues/1168,,"Hi! 
I trained stanza with several treebanks, and I want to print the annotation results. When loading the self-trained models, pos and mwt always fail. To import the trained models, I did this:
```
# Implement the PADT model. 
config_PADT = {
	'processors': 'tokenize,mwt,pos,lemma,depparse',
    'lang': 'ar',
	'tokenize_model_path': './tokenize/ar_padt_tokenizer.pt',
    'mwt_model_path': './mwt/ar_padt_mwt_expander.pt',
	'pos_model_path': './pos/ar_padt_tagger.pt',
    'lemma_model_path': './lemma/ar_padt_lemmatizer.pt',
    'depparse_model_path': './depparse/ar_padt_parser.pt',
    'pos_pretrain_path': './ar/pretrain/padt.pt',
	'tokenize_pretokenized': True
}
nlp = stanza.Pipeline(**config_PADT)
```
For pos, the error signal was:
```
RuntimeError: Error(s) in loading state_dict for Tagger:
	size mismatch for drop_replacement: copying a param with shape torch.Size([2248]) from checkpoint, the shape in current model is torch.Size([325]).
	size mismatch for taggerlstm.lstm.0.lstm.weight_ih_l0: copying a param with shape torch.Size([800, 2248]) from checkpoint, the shape in current model is torch.Size([800, 325]).
	size mismatch for taggerlstm.lstm.0.lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([800, 2248]) from checkpoint, the shape in current model is torch.Size([800, 325]).
	size mismatch for taggerlstm.highway.0.weight: copying a param with shape torch.Size([400, 2248]) from checkpoint, the shape in current model is torch.Size([400, 325]).
	size mismatch for taggerlstm.gate.0.weight: copying a param with shape torch.Size([400, 2248]) from checkpoint, the shape in current model is torch.Size([400, 325]).
```
for mwt, the error was this:
```
RuntimeError: Error(s) in loading state_dict for Seq2SeqModel:
	Unexpected key(s) in state_dict: ""SOS_tensor"". 
```
I retrained the pos and mwt models, but still got the same error. The tokenizer, lemmatizer and depparser work fine. Is there anything I can do to make pos and mwt load?

I know that I can just download PADT from the stanza resources, but other treebanks, like NYUAD, also produce exactly the same error.

Thank you again for your help!
",
155,2022-12-19T17:25:50Z,https://github.com/stanfordnlp/stanza/issues/1167,,"Hi I am training stanza with the Arabic padt treebank. In training mwt, the evaluation of dev set failed and I got the following error.
```
2022-12-19 17:16:31 INFO: Training dictionary-based MWT expander...
2022-12-19 17:16:32 INFO: Evaluating on dev set...
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/utils/training/run_mwt.py"", line 113, in <module>
    main()
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/utils/training/run_mwt.py"", line 110, in main
    common.main(run_treebank, ""mwt"", ""mwt_expander"")
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/utils/training/common.py"", line 274, in main
    run_treebank(mode, paths, treebank, short_name,
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/utils/training/run_mwt.py"", line 79, in run_treebank
    mwt_expander.main(train_args)
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/models/mwt_expander.py"", line 94, in main
    train(args)
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/models/mwt_expander.py"", line 135, in train
    _, _, dev_f = scorer.score(system_pred_file, gold_file)
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/models/mwt/scorer.py"", line 8, in score
    evaluation = ud_scores(gold_conllu_file, system_conllu_file)
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/models/common/utils.py"", line 127, in ud_scores
    system_ud = ud_eval.load_conllu_file(system_conllu_file)
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/utils/conll18_ud_eval.py"", line 656, in load_conllu_file
    return load_conllu(_file,treebank_type)
  File ""/Users/yiminglu/Desktop/week_10/corpus/new_stanza/stanza-dev/stanza/utils/conll18_ud_eval.py"", line 256, in load_conllu
    parent = ud.words[sentence_start + hd -1] if hd else hd  # just assign '0' to parent for root cases
IndexError: list index out of range
```
However, the dev set passes the validation file in the ud github release. 
```
(env) (base) Toufig-Lu:tools-master yiminglu$ python validate.py --lang ar --level 2 ar_padt-ud-dev.conllu 
*** PASSED ***
```
I am also training other treebanks. None of them reported this problem. Is there anything I can do to fix it? Thank you.
",
156,2022-12-17T22:03:16Z,https://github.com/stanfordnlp/stanza/issues/1166,,"Hi, I am trying to preprocess mwt in UD Arabic treebanks, and train stanza with it using the following lines.
```
python3 -m stanza.utils.datasets.prepare_mwt_treebank UD_Arabic-PUD
python3 -m stanza.utils.training.run_mwt UD_Arabic-PUD --num_epoch 2
```
However, I have got the following traceback. Is there anything I can do with mwt_name? Thank you.
```
2022-12-17 22:48:37 INFO: Datasets program called with:
/Users/yiminglu/Desktop/week_8/corpus/stanza/stanza/stanza/utils/datasets/prepare_mwt_treebank.py UD_Arabic-PUD
Preparing data for UD_Arabic-PUD: ar_pud, ar
Reading from ../data/udbase/UD_Arabic-PUD/ar_pud-ud-train.conllu and writing to /var/folders/g_/r_bm_rwd0hx5qptpvpqsvb9c0000gn/T/tmpfilx_0u5/ar_pud.train.gold.conllu
Reading from ../data/udbase/UD_Arabic-PUD/ar_pud-ud-dev.conllu and writing to /var/folders/g_/r_bm_rwd0hx5qptpvpqsvb9c0000gn/T/tmpfilx_0u5/ar_pud.dev.gold.conllu
Reading from ../data/udbase/UD_Arabic-PUD/ar_pud-ud-test.conllu and writing to /var/folders/g_/r_bm_rwd0hx5qptpvpqsvb9c0000gn/T/tmpfilx_0u5/ar_pud.test.gold.conllu
0 unique MWTs found in data
0 unique MWTs found in data
16 unique MWTs found in data
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/Users/yiminglu/Desktop/week_8/corpus/stanza/stanza/stanza/utils/datasets/prepare_mwt_treebank.py"", line 63, in <module>
    main()
  File ""/Users/yiminglu/Desktop/week_8/corpus/stanza/stanza/stanza/utils/datasets/prepare_mwt_treebank.py"", line 60, in main
    common.main(process_treebank)
  File ""/Users/yiminglu/Desktop/week_8/corpus/stanza/stanza/stanza/utils/datasets/common.py"", line 257, in main
    process_treebank(treebank, paths, args)
  File ""/Users/yiminglu/Desktop/week_8/corpus/stanza/stanza/stanza/utils/datasets/prepare_mwt_treebank.py"", line 49, in process_treebank
    source_filename = prepare_tokenizer_treebank.mwt_name(tokenizer_dir, short_name, shard)
AttributeError: module 'stanza.utils.datasets.prepare_tokenizer_treebank' has no attribute 'mwt_name'
```",
157,2022-12-09T22:09:04Z,https://github.com/stanfordnlp/stanza/pull/1165,,Add a script to process a collection of English newswire from non-US sources into a 4 class dataset similar to the conll dataset.  Will allow for testing & retraining of those models using the new data.,
158,2022-12-09T21:44:00Z,https://github.com/stanfordnlp/stanza/pull/1164,,"Add charlm as an option to depparse, especially the en_combined model",
159,2022-12-08T21:51:51Z,https://github.com/stanfordnlp/stanza/pull/1162,,"
Avoid excessive copying of the full-text on each paragraph check.

`stanza.utils.datasets.prepare_tokenizer_treebank` took around 2 hours on my custom treebank (~60 MiB .txt file). 
After this fix it finished in 2 minutes.  

Results for other large UD treebanks:
|                      | before | after |
|----------------------|--------|---------|
| UD_Russian-SynTagRus | 46s    | 23s     |
| UD_German-HDT        | 6m 12s | 50s     |



",
160,2022-12-08T19:25:04Z,https://github.com/stanfordnlp/stanza/pull/1161,,"## Description
When using `stanza.utils.datasets.prepare_tokenizer_treebank` on custom treebank, I stumbled on an error

```
Traceback (most recent call last):
  File ""/nix/store/9srs642k875z3qdk8glapjycncf2pa51-python3-3.10.7/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/nix/store/9srs642k875z3qdk8glapjycncf2pa51-python3-3.10.7/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1140, in <module>
    main()
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1137, in main
    common.main(process_treebank, add_specific_args)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/common.py"", line 257, in main
    process_treebank(treebank, paths, args)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1127, in process_treebank
    process_ud_treebank(treebank, udbase_dir, tokenizer_dir, short_name, short_language, args.augment)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1023, in process_ud_treebank
    prepare_ud_dataset(treebank, udbase_dir, tokenizer_dir, short_name, short_language, ""train"", augment)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1012, in prepare_ud_dataset
    write_augmented_dataset(input_conllu, output_conllu, augment_punct)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 674, in write_augmented_dataset
    new_sents = augment_function(sents)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 657, in augment_punct
    new_sents = augment_comma_separations(new_sents)
  File ""/home/dvzubarev/installed_thirdparty/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 293, in augment_comma_separations
    if sentence[idx+1].split(""\t"")[1] != ',':
IndexError: list index out of range

``` 
## Fixes Issues
This PR fixes this issue

## Unit test coverage
I added a test for the sentence that caused this issue.


",
161,2022-12-06T05:58:44Z,https://github.com/stanfordnlp/stanza/pull/1159,,"All models now work via the pipeline.  Need to double check that they work everywhere

Temporary fix for constituency unit tests?  Need to come up with a good default for no device in args.  Need to double check other situations where device is passed in

",
162,2022-11-30T20:13:29Z,https://github.com/stanfordnlp/stanza/issues/1157,,"Could you kindly provide a list of bracket symbols you use in the constituency module? I know it's from Penn Treebank but it's very hard to find a complete list. E.g., most online sources don't have NML. And I'm not sure what the separated dashes are noted in the model output. 

",
163,2022-11-30T13:30:17Z,https://github.com/stanfordnlp/stanza/issues/1156,,"I am using Stanza to analyze Old Church Slavonic texts and extract lemmata and dependencies. Therefore, I wonder what resources (texts) were used to build pretrained models and how many. Is it possible to enhance lemmata manually, for example, if some changes are necessary ? 

There is a problem with how to encode Old Church Slavonic words -- there is not only an alphabet to consider but also diacritic symbols. What approach do you use?",
164,2022-11-16T15:58:41Z,https://github.com/stanfordnlp/stanza/issues/1155,,"I was trying to perform dependency parsing on Arabic text in conll format.

Sample file:

```
nw/n_0010   0    0                     الرَئِيسُ#ra}iys#Alr}ys#Al+ra}iys+u              DET+NOUN+CASE_DEF_NOM  (TOP(S(S(NP*         ra}iys   -   -   -           *       (ARG1*   (ARG0*         *            *    (10
nw/n_0010   0    1    الأَمِيرْكِيُّ#>amoriykiy~#Al>myrky#Al+>amiyrokiy~+u               DET+ADJ+CASE_DEF_NOM             *)   >amoriykiy~   -   -   -       (NORP)           *)       *)        *            *     10)
nw/n_0010   0    2                    تَعافَى#taEAfaY#tEAfY#taEAfaY+(null)                 PV+PVSUFF_SUBJ:3MS          (VP*        taEAfaY  01   -   -           *          (V*)       *         *            *      -
nw/n_0010   0    3                                      بِ-#clitics#b#bi-                                PREP          (PP*        clitics   -   -   -           *   (ARGM-MNR*        *         *            *      -
nw/n_0010   0    4                      -سُرْعَةٍ#suroEap#srEp#-suroE+ap+K   NOUN+NSUFF_FEM_SG+CASE_INDEF_GEN       (NP*))))       suroEap   -   -   -           *            *)       *         *            *      -
nw/n_0010   0    5                                      وَ-#clitics#w#wa-                                CONJ             *        clitics   -   -   -           *            *        *         *            *      -
nw/n_0010   0    6                            -بَدَأَ#bada>-a#bd>#-bada>+a                 PV+PVSUFF_SUBJ:3MS        (S(VP*        bada>-a  01   -   -           *            *      (V*)        *            *      -
nw/n_0010   0    7                        جَوْلَةً#jawolap#jwlp#jawol+ap+F   NOUN+NSUFF_FEM_SG+CASE_INDEF_ACC          (NP*        jawolap   -   -   -           *            *   (ARG1*         *            *    (11
nw/n_0010   0    8               داخِلِيَّةً#dAxiliy~#dAxlyp#dAxiliy~+ap+F    ADJ+NSUFF_FEM_SG+CASE_INDEF_ACC          *))))      dAxiliy~   -   -   -           *            *        *)        *            *     11)
nw/n_0010   0    9                                كَعْكُ#kaEok#kEk#kaEok+u                  NOUN+CASE_DEF_NOM      (S(S(NP*          kaEok   -   -   -   (PRODUCT*            *        *    (ARG0*            *      -
nw/n_0010   0   10                                           ""#DEFAULT#""#""                               PUNC          (NP*        DEFAULT   -   -   -           *            *        *         *            *     (8
nw/n_0010   0   11                     البريتزل#DEFAULT#Albrytzl#Al+brytzl                      DET+NOUN_PROP             *        DEFAULT   -   -   -           *            *        *         *            *      -
nw/n_0010   0   12                                           ""#DEFAULT#""#""                               PUNC            *))       DEFAULT   -   -   -           *)           *        *         *)           *      8)
nw/n_0010   0   13                         أَفْقَدَ#>afoqad#>fqd#>afoqad+a                 PV+PVSUFF_SUBJ:3MS          (VP*        >afoqad  01   -   -           *            *        *       (V*)           *      -
nw/n_0010   0   14                                      بُوش#buw$#bw$#buw$                          NOUN_PROP          (NP*)          buw$   -   -   -     (PERSON)           *        *    (ARG1*)           *    (10)
nw/n_0010   0   15                             وَعْيَ-#waEoy#wEy#waEoy+a-                   NOUN+CASE_DEF_ACC          (NP*          waEoy   -   -   -           *            *        *    (ARG2*            *      -
nw/n_0010   0   16                                       -هُ#clitics#h#-hu                      POSS_PRON_3MS       (NP*))))       clitics   -   -   -           *            *        *         *)           *    (10)
nw/n_0010   0   17                                      وَ-#clitics#w#wa-                                CONJ             *        clitics   -   -   -           *            *        *         *            *      -
nw/n_0010   0   18                       -سُقُوطُ-#suquwT#sqwT#-suquwT+u-                   NOUN+CASE_DEF_NOM        (S(NP*         suquwT   -   -   -           *            *        *         *       (ARG0*      -
nw/n_0010   0   19                                       -هُ#clitics#h#-hu                      POSS_PRON_3MS         (NP*))       clitics   -   -   -           *            *        *         *            *)   (10)
nw/n_0010   0   20                              تَرَكَ#tarak-u#trk#tarak+a                 PV+PVSUFF_SUBJ:3MS          (VP*        tarak-u  01   2   -           *            *        *         *          (V*)     -
nw/n_0010   0   21                        كَدْمَةً#kadomap#kdmp#kadom+ap+F   NOUN+NSUFF_FEM_SG+CASE_INDEF_ACC          (NP*)       kadomap   -   -   -           *            *        *         *       (ARG1*)     -
nw/n_0010   0   22                                   عَلَى#EalaY#ElY#EalaY                               PREP          (PP*          EalaY   -   -   -           *            *        *         *   (ARGM-LOC*      -
nw/n_0010   0   23                                 خَدِّ-#xad~#xd#xad~+i-                   NOUN+CASE_DEF_GEN          (NP*           xad~   -   -   -           *            *        *         *            *      -
nw/n_0010   0   24                                       -هِ#clitics#h#-hi                      POSS_PRON_3MS     (NP*))))))       clitics   -   -   -           *            *        *         *            *)   (10)
nw/n_0010   0   25                                           ""#DEFAULT#""#""                               PUNC          (NP*        DEFAULT   -   -   -           *            *        *         *            *     (8
nw/n_0010   0   26                     البريتزل#DEFAULT#Albrytzl#Al+brytzl                      DET+NOUN_PROP             *        DEFAULT   -   -   -    (PRODUCT)           *        *         *            *      -
nw/n_0010   0   27                                           ""#DEFAULT#""#""                               PUNC             *)       DEFAULT   -   -   -           *            *        *         *            *      8)
nw/n_0010   0   28                        جَوْلَةٌ#jawolap#jwlp#jawol+ap+N   NOUN+NSUFF_FEM_SG+CASE_INDEF_NOM          (NP*        jawolap   -   -   -           *            *        *         *            *    (11
nw/n_0010   0   29                مُقَرَّرَةٌ#muqar~ar#mqrrp#muqar~ar+ap+N    ADJ+NSUFF_FEM_SG+CASE_INDEF_NOM            *))      muqar~ar   -   -   -           *            *        *         *            *     11)

```

I obtained text from the 4th column, removed the transliteration text after '#' and got the following text:

الرَئِيسُ الأَمِيرْكِيُّ تَعافَى بِ- -سُرْعَةٍ وَ- -بَدَأَ جَوْلَةً داخِلِيَّةً كَعْكُ "" البريتزل "" أَفْقَدَ بُوش وَعْيَ- -هُ وَ- -سُقُوطُ- -هُ تَرَكَ كَدْمَةً عَلَى خَدِّ- -هِ "" البريتزل "" جَوْلَةٌ مُقَرَّرَةٌ
the number of space separated words in this sentence is 30 (which is visible in the 3rd column of the above conll format as well)

When I passed this to stanza, I get the following 50 tokens (instead of 30):

الرَئِيسُ الأَمِيرْكِيُّ تَعافَى بِ - - سُرْعَة ٍ وَ - - بَدَأَ جَوْلَة ً داخِلِيَّة ً ك َعْك "" البريتزل "" أَفْقَدَ بُوش و َعْيَ - - هُ وَ - - سُقُوطُ - - هُ تَرَكَ كَدْمَة ً عَلَى خَدِّ - - هِ "" البريتزل "" جَوْلَة ٌ مُقَرَّرَة ٌ

I do not know Arabic so I asked an expert. He said that there is some issue in tokenization. Tokenized sentence is problematic (with respect to original sentence).

I asked him to provide me a valid Arabic sentence. He sent the following.

Untokenized text: الرَئِيسُ الأَمريكِيُّ تَعافَى بِسُرْعَةٍ وبَدَأَ جَوْلَةً داخِلِيَّةً

Tokenized sentence from stanza: الرَئِيسُ الأَمريكِيُّ تَعافَى بِسُرْعَة ٍ و ب َدَأَ جَوْلَة ً داخِلِيَّة ً

The expert claimed that the tokenized sentence is incorrect. It is adding unnecessary spaces between the words. Is this because of the groundtruth treebank which was used to learn the tokenization? 

This is different than #797 which is about direction of writing. 

stanza == 1.4.2 <br> torch == 1.10.2",
165,2022-11-16T09:43:33Z,https://github.com/stanfordnlp/stanza/issues/1154,,"just following your steps:
AttributeError: 'NoneType' object has no attribute 'enum_types_by_name'
",
166,2022-11-14T04:50:00Z,https://github.com/stanfordnlp/stanza/pull/1153,,Added the Mirror MADGRAD Optimizer ,
167,2022-11-13T16:51:45Z,https://github.com/stanfordnlp/stanza/pull/1152,,The numbers for the optimizers will come out later.,
168,2022-11-13T05:13:25Z,https://github.com/stanfordnlp/stanza/pull/1151,,Update nonlinearity results,
169,2022-11-12T22:55:48Z,https://github.com/stanfordnlp/stanza/pull/1150,,"Add a few various options for customizing the tagger optimizer.  Doesn't seem to move the needle much, if at all, though",
170,2022-11-05T14:28:46Z,https://github.com/stanfordnlp/stanza/pull/1149,,"Here is a list of the additional nonlinearities that are present for our experiments: 
1. ELU 
2. Hardshrink
3. Hardsigmoid
4. Hardtanh
5. Hardswish
6. LogSigmoid
7. PReLU
8. ReLU6
9. RReLU
10. SELU
11. CELU
12. SiLU
13. Softplus
14. Softshrink
15. Softsign
16. Tanhshrink
17. GLU",
171,2022-10-30T05:56:26Z,https://github.com/stanfordnlp/stanza/pull/1148,,"Allow constituency training from a silver dataset

Probably should train some fraction of the words as delta words",
172,2022-10-27T11:39:29Z,https://github.com/stanfordnlp/stanza/issues/1147,,"I installed Stanza and I am using the standard NER package. I noticed that the ner package used by CoreNLP correctly identifies titles of persons but the one in Stanza does not.

Is it possible to somehow include titles in the ner package used by Stanza or can I use multiple NER packages?

Results from CoreNLP
![image](https://user-images.githubusercontent.com/38347144/198274618-40c57d78-9390-4660-a216-f7d0b4c09b72.png)

Results from Stanza, (notice missing titles)
![image](https://user-images.githubusercontent.com/38347144/198274714-bf337e1b-c278-436c-93da-ffeef44c574c.png)


",
173,2022-10-22T20:30:10Z,https://github.com/stanfordnlp/stanza/pull/1144,,A module to connect to the CoreNLP version of the lemmatizer,
174,2022-10-16T13:18:38Z,https://github.com/stanfordnlp/stanza/pull/1143,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `main` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/main/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
175,2022-10-13T04:08:38Z,https://github.com/stanfordnlp/stanza/issues/1142,,"Hi, 

I am experiencing a blocking issue with some multilingual pipeline code. My code is as follows: 

```
import stanza
from stanza.pipeline.multilingual import MultilingualPipeline

stanza.download(""ar"")
stanza.download(""vi"")
stanza.download(""multilingual"")
stanza.download(""bg"")
stanza.download(""be"")
stanza.download(""en"")
stanza.download(""es"")
stanza.download(""he"")
stanza.download(""id"")
stanza.download(""ko"")
stanza.download(""pt"")
stanza.download(""tr"")


nlp_multi = MultilingualPipeline(lang_id_config={
    ""langid_clean_text"": False, 
    ""langid_lang_subset"": [""en"",""ar"", ""es"", ""pt"", ""be"", ""bg"", ""ko"", ""id"", ""he"", ""ru"", ""th"", ""tr"", ""vi"" ],
    }, 
    lang_configs={
        ""en"": {""processors"": 'tokenize, pos, ner', ""download_method"": None},
        ""ar"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""es"": {""processors"": 'tokenize, pos, ner', ""download_method"": None},
        ""pt"": {""processors"": 'tokenize, pos, ner', ""download_method"": None},
        ""be"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""bg"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""he"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""id"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""ko"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""th"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""tr"": {""processors"": 'tokenize, ner', ""download_method"": None},
        ""vi"": {""processors"": 'tokenize, ner', ""download_method"": None}
    }, max_cache_size=15 )

# this is a Pandas Series FWIW
docs_series = fb_df['description'][fb_df['description'].notna()] 
docs_list = docs_series.to_list()


langed_docs = nlp_multi(docs_list)
```

This is the error I am getting: 
```
2022-10-12 22:05:56 INFO: Loading these models for language: en (English):
=========================
| Processor | Package   |
-------------------------
| tokenize  | combined  |
| pos       | combined  |
| ner       | ontonotes |
=========================

2022-10-12 22:05:56 INFO: Use device: cpu
2022-10-12 22:05:56 INFO: Loading: tokenize
2022-10-12 22:05:56 INFO: Loading: pos
2022-10-12 22:05:57 INFO: Loading: ner
FileNotFoundError                         Traceback (most recent call last)
e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\pipeline\core.py in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, proxies, **kwargs)
    279                                                                                           pipeline=self,
--> 280                                                                                           use_gpu=self.use_gpu)
    281             except ProcessorRequirementsException as e:

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\pipeline\processor.py in __init__(self, config, pipeline, use_gpu)
    172         if not hasattr(self, '_variant'):
--> 173             self._set_up_model(config, pipeline, use_gpu)
    174 

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\pipeline\ner_processor.py in _set_up_model(self, config, pipeline, use_gpu)
     48                     'charlm_backward_file': charlm_backward}
---> 49             trainer = Trainer(args=args, model_file=model_path, pretrain=pretrain, use_cuda=use_gpu, foundation_cache=pipeline.foundation_cache)
     50             self.trainers.append(trainer)

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\models\ner\trainer.py in __init__(self, args, vocab, pretrain, model_file, use_cuda, train_classifier_only, foundation_cache)
     70             # load everything from file
---> 71             self.load(model_file, pretrain, args, foundation_cache)
     72         else:

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\models\ner\trainer.py in load(self, filename, pretrain, args, foundation_cache)
    167         if pretrain is not None:
--> 168             emb_matrix = pretrain.emb
    169 

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\models\common\pretrain.py in emb(self)
     49         if not hasattr(self, '_emb'):
---> 50             self.load()
     51         return self._emb

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\models\common\pretrain.py in load(self)
     70             if not self._vec_filename and not self._csv_filename:
---> 71                 raise FileNotFoundError(""Pretrained file {} does not exist, and no text/xz file was provided"".format(self.filename))
     72             if self.filename is not None:

FileNotFoundError: Pretrained file E:\repos\stanza_resources\en\pretrain\fasttextcrawl.pt does not exist, and no text/xz file was provided

During handling of the above exception, another exception occurred:
TypeError                                 Traceback (most recent call last)
<ipython-input-15-c92758b16980> in <module>
      4 
      5 
----> 6 langed_docs = nlp_multi(docs_list)

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\pipeline\multilingual.py in __call__(self, doc)
    127 
    128     def __call__(self, doc):
--> 129         doc = self.process(doc)
    130         return doc
    131 

e:\Users\Eric\miniconda3\envs\arcgis_env\lib\site-packages\stanza\pipeline\multilingual.py in process(self, doc)
...
--> 183     p = os.fspath(p)
    184     seps = _get_bothseps(p)
    185     d, p = splitdrive(p)

TypeError: expected str, bytes or os.PathLike object, not list
```

After looking at several other closed issues referencing the FileNotFoundError exception, I did double-check and rerun `stanza.download(""en"")`. No effect. 

The only file present in the above-referenced `\stanza_resources\en\pretrain\` directory is `combined.pt`. 

Also, as a potentially important note, I first wrote this code back in late June/early July, and the above pipeline code ran successfully at that time (if not quite in the ways I wanted from a multilingual standpoint, but that's another matter). It is only now that I am returning to it (and after creating a new replacement conda environment) that this FileNotFoundError exception is being thrown. Perhaps a change in the last two minor releases is the reason for this exception? 

**Environment (please complete the following information):**

OS: Windows
Python version: 3.7.11
Stanza version: 1.4.1 and 1.4.2 (tried both, installed from both Miniconda and Pip). ",
176,2022-10-10T09:36:11Z,https://github.com/stanfordnlp/stanza/issues/1141,,"**Describe the bug**

For sentence `The company announced this week that it has filed lawsuits in the US and China over claims that Samsung is violating patents it holds related to 4G technology, operating systems, and user-interface designs.`, the result returned by constituency parser of stanza is different from what's returned by constituency parser of corenlp. And the result of corenlp looks better.

Result of stanza:

<img width=""1151"" alt=""stanza"" src=""https://user-images.githubusercontent.com/65410804/194836572-1c6baa52-c3e1-49ba-a0d4-b979fe749529.png"">

Result of corenlp
<img width=""1068"" alt=""corenlp"" src=""https://user-images.githubusercontent.com/65410804/194836095-6edb5183-831b-40bf-88cd-98238e7c7951.png"">

**To Reproduce**
Steps to reproduce the behavior:
1. Go to http://stanza.run/
2. Input the sentence
3. Select constituency parse under Annotations
4. Click on 'Submit'
5. Go to https://corenlp.run/ and do step 2 to 4

**Environment (please complete the following information):**
 - OS: [MacOS]
 - Python version: [3.7.4]
 - Stanza version: [1.4.2]

**Additional context**
Any way to make stanza return the same result as corenlp.
",
177,2022-10-06T03:32:20Z,https://github.com/stanfordnlp/stanza/pull/1140,,"## Description

Added a check to find & replace excessively-long tokens with ""UNK"", in order to avoid downstream GPU memory in `POS`.

See issue #1137 

## Approach

To avoid having to modify positions of downstream tokens and such, the easiest approach that came to
mind was to check for long whitespace-bound tokens and remove them prior to running the tokenizer.

this approach replaces the excessively long tokens with the string literal ""unk"", which may not be
desired. i 

## Questions

Some things that I wasn't sure about, and thought I would mention here:

1. should anything be done to handle the pre-tokenized case? the same token length issue could arise,
   but it's less clear to me whether we should be modifying tokens that the user has already
   manually created. (long term, this could also be an argument for addressing the issue at the
   pos level, although this is probably good enough for now..)

2. since the max length check is being handled in `tokenize_processor.py`, would it make sense to
   remove that logic from `output_predictions`?

3. in `stanza.models.tokenization.utils.output_predictions`, the max_seqlen is set to `max(1000,
   max_seqlen)`, which would ignore a users specified value if it's less than 1000.. is this
   necessary? and if so, would it make sense to at least emit a warning when the user's setting is
   overriden?

another potential long-term approach you could consider would be to add another processer that gets
run before others (~""preprocessor""?..), where logic along these lines could be included and tuned by
the user.

## Fixes Issues

- #1137 

## Unit test coverage

Yes. Just a simple test with a string with length >> `TokenizeProcessor.MAX_SEQ_LENGTH_DEFAULT`

## Known breaking changes/behaviors

I hope not!
",
178,2022-10-04T16:16:23Z,https://github.com/stanfordnlp/stanza/pull/1139,,"Some more classifier tests, along with small upgrades for readability",
179,2022-10-04T15:33:01Z,https://github.com/stanfordnlp/stanza/pull/1138,,"Remove some cuda() calls in favor of getting the device instead.  Will make it easier to use metal
",
180,2022-10-04T01:42:10Z,https://github.com/stanfordnlp/stanza/issues/1137,,"**Problem / motivation**

When processing large corpuses of text, the likelihood of encounter unexpected and ill-formatted inputs becomes large.

In my case, I was processing a collection of texts, and kept running into issues along the lines of:

```
RuntimeError: CUDA out of memory. Tried to allocate 4.97 GiB (GPU 0; 5.79 GiB total capacity; 236.60 MiB already allocated; 2.49 GiB free; 250.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

The typical suggestion is to adjust the batch size for the problematic processor(s), but this did not help.

Finally, I was able to track down the issue -- it turns out one of the input texts contained a binary-encoded image (e.g. `<img src=""data:image/png;base64,ivborw0kgg...`).

**Solution**

I would suggest adding a quick ""sanity"" check prior to calling the processors, and either:

1. Remove any tokens > `N` length, or,
2. Display a warning indicating the presence of a large token

From looking at the code, it seems like the `tokenize` processor does include a size constraint with a default value of ""1000"":

```
MAX_SEQ_LENGTH_DEFAULT = 1000
```

Running the pipeline with only the `tokenize` processor indeed runs without issue, so another option would be to have the downstream processors include the same constraints?

Let me know if any of these approaches seem reasonable, and I'd be happy to submit a PR.

**Alternatives**

The only other alternative that comes to mind would be to expand either/both the error messages and/or docs to include possible problem sources such as the above.

**Version info**

- Stanza 1.2.3 (via conda)
- Arch Linux (5.19.12)
- NVIDIA GeForce RTX 2060",
181,2022-10-03T22:45:35Z,https://github.com/stanfordnlp/stanza/pull/1136,,"Minor change to the pipeline to accommodate the usage of TokenizeProcessor, then add a script to process raw text files with an arbitrary tokenizer model",
182,2022-09-30T07:17:36Z,https://github.com/stanfordnlp/stanza/pull/1135,,Address one of the oldest remaining issues by adding a utility function to resplit tokens,
183,2022-09-26T10:00:59Z,https://github.com/stanfordnlp/stanza/issues/1134,,"Hi, I trained models and evaluated them with my own annotated treebank. I have some problems understanding the reported scores after evaluation. Sometimes there are several scores under one section, and I have no idea what they stand for. I did not find relevant information in the readme files, the stanza websites, the referenced articles, the ud2018 shared task website, and elsewhere, but I understand this is very basic and what I need probably exists somewhere. I would be grateful if you could direct me to the guideline. 

For example,  the dependency parsing scores:
```
2022-09-21 17:28:59 INFO: LAS	MLAS	BLEX
2022-09-21 17:28:59 INFO: 77.20	72.40	73.54
2022-09-21 17:28:59 INFO: Parser score:
2022-09-21 17:28:59 INFO: ar_padt 77.20
82.02 77.20 73.54 72.40 73.54
```
I understand LAS, MLAS and BLEX are all present in the final line, but what do the first and the third scores mean?

And also,
```
tokenization
98.65 85.58 92.96
```
I understand that these are scores for tokens, sentences, and words. Am I right in the sequence?

Thank you!",
184,2022-09-25T16:39:25Z,https://github.com/stanfordnlp/stanza/pull/1133,,"Make the number of hidden layers an option and start from zeros Generalize the num_layers for Phobert and XLNet.  Keep old models alive
",
185,2022-09-25T02:54:36Z,https://github.com/stanfordnlp/stanza/pull/1132,,"Add a transformer as an optional input to the POS model.  Would need to handle long sentences before making this the default.

Add a couple options to the constituency parser to allow for using a better POS model than the default.",
186,2022-09-20T22:30:54Z,https://github.com/stanfordnlp/stanza/pull/1130,,Turn the LSTM stack into an attention stack,
187,2022-09-19T19:05:26Z,https://github.com/stanfordnlp/stanza/pull/1129,,Add a script to convert from AWS annotator reports to a report on how much work each annotator did,
188,2022-09-19T04:22:35Z,https://github.com/stanfordnlp/stanza/pull/1128,,"use an LSTM over the max of tree inputs to build the next tree.  Some experiments show an improvement, others are neutral",
189,2022-09-19T03:28:26Z,https://github.com/stanfordnlp/stanza/pull/1127,,"Refactor the LSTM for transitions and constituents into a separate class.  The intention is to make it easier to build a Transformer Stack and then replace the existing LSTMs with one, as long as we make the Transformer use the same interface.",
190,2022-09-16T03:46:12Z,https://github.com/stanfordnlp/stanza/issues/1126,,"hello！When i try the sample：
```
>>> import stanza
>>> stanza.download('en')       # This downloads the English models for the neural pipeline
>>> nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English
>>> doc = nlp(""Barack Obama was born in Hawaii.  He was elected president in 2008."")
>>> doc.sentences[0].print_dependencies()

```


I meet a problem:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 408, in __call__
    return self.process(doc, processors)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 397, in process
    doc = process(doc)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py"", line 91, in process
    num_workers = self.config.get('num_workers', 0))
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/stanza/models/tokenization/utils.py"", line 264, in output_predictions
    pred = np.argmax(trainer.predict(batch), axis=2)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py"", line 69, in predict
    pred = self.model(units, features)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/stanza/models/tokenization/model.py"", line 45, in forward
    emb = self.embeddings(x)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/torch/nn/modules/sparse.py"", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File ""/home/users/.../anaconda3/envs/torchcpu/lib/python3.7/site-packages/torch/nn/functional.py"", line 1852, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)
```
What can I do to solve this problem？",
191,2022-09-15T03:14:37Z,https://github.com/stanfordnlp/stanza/pull/1125,,"The release that burns twice as bright burns half as long, and you have burned so very very brightly, 1.4.1",
192,2022-09-15T00:50:13Z,https://github.com/stanfordnlp/stanza/pull/1124,,"## Description

Breaks up all dependency groups onto separate lines.

Adds a `transformers` extra with a sanity bottom pin

## Fixes Issues

- addresses parts of #1120

## Unit test coverage

- n/a

## Known breaking changes/behaviors

- n/a",
193,2022-09-14T20:11:52Z,https://github.com/stanfordnlp/stanza/pull/1123,,"Switch the LRU to an OrderedDict instead of two separate items

Also, experiment some with pytest memory limitations",
194,2022-09-14T19:15:01Z,https://github.com/stanfordnlp/stanza/pull/1122,,"Address issues in #1120, specifically `pytest` and having upgraded `transformers`",
195,2022-09-14T13:32:54Z,https://github.com/stanfordnlp/stanza/issues/1120,,"Congratulations on the release!

Some dependency thoughts from re-packaging on [conda-forge](https://github.com/conda-forge/stanza-feedstock/pull/8):

- it looks like `pytest` was added
  - it is never imported in non-`test` code
- it looks like `pytorch <1.9` is no longer supported, as trying to `import stanza` with `torch <1.9` yields:
  ```
  AttributeError: module 'torch.nn' has no attribute 'Mish'
  ```
- `transformers` is now optional (and not documented in `setup.py`, even with an `[transformers]` extra)
  - installing with with `transformers<3` yields an inconsistent environment under `pip check`
    - this may be a `conda-forge` issue, haven't run it down

I'm still working through potentially adding the test suite (since it's just one `linux64` build), but would appreciate any feedback.",
196,2022-09-14T06:30:25Z,https://github.com/stanfordnlp/stanza/pull/1119,,"Bugfixes, constituency parser improvements, more NER models, a couple more Sentiment models",
197,2022-09-12T18:09:12Z,https://github.com/stanfordnlp/stanza/issues/1118,,"Hi,
I'm trying to reproduce the results mentioned [here](https://stanfordnlp.github.io/stanza/constituency.html#available-models) for constituency parser on Penn treebank data. I have access to wsj data and I downloaded the `wsj_bert.pt` model by calling the following command:
 
`stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', package={'constituency': 'wsj_bert'})`

The model is successfully downloaded and it is saved here: `~/stanza_resources/en/constituency`

Now, I want to get the performance of this model on wsj test data. I called this command: ( I renamed `test.trees`  to `en_wsj_bert_test.mrg` to keep the model's name and the data name consistents.)

`python -m stanza.utils.training.run_constituency en_wsj_bert --save_dir ~/stanza_resources/en/constituency --score_test`

This returns an awful score around 0.838210. I don't know where I make mistakes, but I would like to fix this. I'm going to use this as a baseline, so I need to replicate the scores exactly as mentioned [here](https://stanfordnlp.github.io/stanza/constituency.html#available-models) 

Thanks for your help

",
198,2022-09-08T06:22:46Z,https://github.com/stanfordnlp/stanza/pull/1117,,Process tokenizer files for Sindhi provided by Isra University,
199,2022-09-07T16:47:04Z,https://github.com/stanfordnlp/stanza/pull/1116,,"
## Description
In certain cases, it is possible that the first language in lang_request_history does not have a corresponding cached pipeline. In such cases, removing the nonexistent pipeline from cache will fail. Adjust the code to check for the oldest language that also has a currently cached pipeline before attempting cleanup.

## Fixes Issues
#1115 

## Unit test coverage
Not yet

## Known breaking changes/behaviors
None
",
200,2022-09-07T10:05:37Z,https://github.com/stanfordnlp/stanza/issues/1115,,"**Describe the bug**

When using the MultilingualPipeline in conjunction with the `max_cache_size` parameter the code crashes because it tries to call `remove(lang_id)` on a `dict` which is a method that is not available for dicts.

[Here's the part of the code where the problem occurs.](https://github.com/stanfordnlp/stanza/blob/011b6c4831a614439c599fd163a9b40b7c225566/stanza/pipeline/multilingual.py#L66)

**To Reproduce**
I've created a small script to reproduce the error. The script is configured with a `max_cache_size` of 2 and will crash once the third language is detected. 
```python
from multilingual import MultilingualPipeline

model_dir = ""./models""
nlp = MultilingualPipeline(
    model_dir=model_dir, max_cache_size=2
)

texts = [""english text for language detection"",
         ""deutscher text für spracherkennung"",
         ""texte en français pour la reconnaissance vocale""]

if __name__ == ""__main__"":
    for text in texts:
        nlp(text)
```

**Expected behavior**
Remove the first language (english) from the cache, load the model for french and do the analysis without crashing. 

**Environment (please complete the following information):**
 - OS: Linux (Manjaro)
 - Python version: Python 3.9.13
 - Stanza version: stanza-1.4.0

",
201,2022-09-06T04:18:54Z,https://github.com/stanfordnlp/stanza/pull/1113,,Add a unit test.  Remove click as a dependency & make lxml optional.,
202,2022-08-30T19:29:48Z,https://github.com/stanfordnlp/stanza/pull/1111,,"Add support for elmoformanylangs to sentiment

Inclues a matrix trained to connect the 3 layers of elmo instead of using the default averaging

Also, a projection from elmo dim to a lower dimension (although this was less useful)

Add a comment on how the sentiment processor doesn't load Elmo
",
203,2022-08-30T15:26:08Z,https://github.com/stanfordnlp/stanza/pull/1110,,"Co-authored-by: ryszardtuora <ryszardtuora@gmail.com>
Co-authored-by: Karol Saputa <32554739+k-sap@users.noreply.github.com>

## Description
This PR adds Polish NER dataset, I also link a trained model.

## Fixes Issues
I mentioned Polish NER in #1070.

## Model
BERT-based model is [here](http://mozart.ipipan.waw.pl/~ksaputa/stanza/pl_ner/ner/polish_ner_bert.tar.gz)
",
204,2022-08-25T01:07:37Z,https://github.com/stanfordnlp/stanza/pull/1108,,"A few minor improvements to the NER output, including sorting confusion matrices by type first, and outputting the results to a file if requested",
205,2022-08-22T17:54:54Z,https://github.com/stanfordnlp/stanza/issues/1107,,"I have built up a toy corpus with multiple dependencies that happen in Spanish, tagging the multiple dependencies in the corresponding column of the conllu files, but in the execution I can't get the multiple dependencies with the very test examples in the corpus. I suspected that Spanish wouldn't show multiple dependencies because AnCora corpus doesn't have anyone, but creating my own toy corpus, it doesn't work either. Is there any hidden parameter to show them, or are they skipped by design?",
206,2022-08-19T23:25:59Z,https://github.com/stanfordnlp/stanza/pull/1104,,process TASS2020,
207,2022-08-19T23:17:14Z,https://github.com/stanfordnlp/stanza/pull/1103,,Transliterate kazakh to a latin alphabet,
208,2022-08-18T03:51:22Z,https://github.com/stanfordnlp/stanza/issues/1101,,"**Describe the bug**
If I set lang_config in MultilingualPipeline for specific language , the Pipeline is always initialized as ""en""

**To Reproduce**
`lang_configs = {'hi': {'processors': 'tokenize,pos'}, 'ar': {'processors': 'tokenize,pos'}}`
`nlp = MultilingualPipeline(lang_configs=lang_configs)`

If I put hindi/arabic texts as input, the language identification works well, but the Pipeline is initialized with ""en"".
The problem seems that the ""lang"" param is not set in https://github.com/stanfordnlp/stanza/blob/main/stanza/pipeline/multilingual.py#L68

If I set lang explicitly in lang_configs, it works as expected, but I think this should be fixed to avoid misleading.

**Expected behavior**
Pipeline should be init with specific lang even if 'lang' is not set explicitly.

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS]
 - Python version: [e.g. Python 3.6.8 from Anaconda]
 - Stanza version: [e.g., 1.0.0]

**Additional context**
Add any other context about the problem here.
",
209,2022-08-17T00:54:01Z,https://github.com/stanfordnlp/stanza/pull/1100,,Add a preparation script for Masakhane,
210,2022-08-17T00:14:44Z,https://github.com/stanfordnlp/stanza/pull/1099,,A couple modifications to pattn & lattn - add a projection matrix to make the lattn fit in memory when using all inputs.  move the layer norm from the inputs from the timing to the transformer.  adjust the timing layers to use the exact size rather than size/2,
211,2022-08-15T05:06:26Z,https://github.com/stanfordnlp/stanza/pull/1098,,"Add a unit test and implement (some of?) the ideas in 
```Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling```
",
212,2022-08-15T01:52:06Z,https://github.com/stanfordnlp/stanza/issues/1097,,"**Describe the bug**
The word ""Uh"" in the sentences 
> Uh. Jon on a stool stealing from the cookie jar.  

and  

> Uh. Jackson on a stool stealing from the cookie jar.  

returns two separate POS labels even though the only differences are the names ""Jon"" and ""Jackson"" which are both syntactically the same things.

In the Jon sentence Uh is treated as ""Uh"" (INTJ) ""."" (PUNCT) whereas the Jackson sentence Uh is treated as ""Uh."" (PROPN)

**To Reproduce**
```python
lemmatizer = stanza.Pipeline(lang=""en"", use_gpu=False, processors='tokenize,mwt,pos,lemma')
a = [token.pos for token in lemmatizer(""Uh. Jon on a stool stealing from the cookie jar."").iter_words()]  # ['INTJ', 'PUNCT', 'PROPN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'NOUN', 'PUNCT']
b = [token.pos for token in lemmatizer(""Uh. Jackson on a stool stealing from the cookie jar."").iter_words()] # ['PROPN', 'PROPN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'NOUN', 'PUNCT']
```


**Expected behavior**
""Uh."" should be treated as ""Uh"" (INTJ) and ""."" (PUNCT) in both cases

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: 3.7.10
 - Stanza version: 1.4.0
",
213,2022-08-14T05:17:28Z,https://github.com/stanfordnlp/stanza/issues/1096,,"Hello! I'm using the constituency parsing of Stanza.
When I get a node in the parse_tree.Tree, how can I get the phrase under the node? There seems to be no such apis to get the text information.
Thanks for any reply!
",
214,2022-08-08T09:19:44Z,https://github.com/stanfordnlp/stanza/issues/1094,,"As my company already uses corenlp 4.4, I need use corenlp 4.4 for compatability.
I instanlled stanza and install corenlp from stanza 1.4, I extrally and definitely download 4.4 English-extra model.

stanza.install_corenlp()
stanza.download_corenlp_models(model='english-extra', version='4.4.0', dir=""/data/stanza_corenlp-4.4.0"")

from stanza.server import CoreNLPClient
with CoreNLPClient(
        server_id='second-server-name',
        annotators=['tokenize', 'pos'],
) as client:

How can I definitely assign corenlp english model 4.4 in stanza?
In a word, corenlp 4.4 is really very excellent on pos-tag and we hope defenitely use this version from stanza.
thanks in advance!!
",
215,2022-08-07T19:34:53Z,https://github.com/stanfordnlp/stanza/pull/1093,,,
216,2022-08-07T19:34:23Z,https://github.com/stanfordnlp/stanza/pull/1092,,wandb config logging & logging learning rate for charlm,
217,2022-08-07T05:12:30Z,https://github.com/stanfordnlp/stanza/pull/1091,,KK NER dataset,
218,2022-08-05T06:15:53Z,https://github.com/stanfordnlp/stanza/pull/1090,,"Add a trainer for the charlm - useful for saving and loading everything for checkpoints

Save checkpoint files as part of the eval iterations.  Load checkpoints back in when starting

Includes option to ignore checkpointing
",
219,2022-08-02T08:23:12Z,https://github.com/stanfordnlp/stanza/issues/1089,,"**Problem**
In the current version of Stanza, there is the `use_gpu` parameter which determines if we want to use the CUDA device instead of the CPU. Unfortunately, this option doesn't support multi-GPU environments, and we can't load the model on the specified device. For example, I have two GPUs and want to load some instances of the Stanza model on these two and distribute my inputs among these instances, so I can gain the most utilization out of the system and speed up my process.

**Solution**
When I checked the source code of the Stanza package, I found out that the Stanza uses the torch models in its core and `use_gpu` is simply a condition parameter alongside the `torch.cuda.is_available()` to determine if the `Pipeline` should use the default CUDA device. It is possible to change the `use_gpu` parameter to something like the `device` with the default value of `cuda` (instead of `True` for `use_gpu`). Then check if this `device` parameter was started with the `cuda` (for cases like `device=""cuda:1""`) and cuda was available, then use that specified device and load models on different devices.

**Another Solution**
It is also possible to add a parameter like `device_idx` to select the desired device and keep `use_gpu` as it is now.
",
220,2022-08-01T15:27:36Z,https://github.com/stanfordnlp/stanza/issues/1088,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
221,2022-07-25T18:50:39Z,https://github.com/stanfordnlp/stanza/issues/1087,,"Hi Stanza Devs,

I was wondering if http://stanza.run will be updated to Stanza 1.4?

Thanks,
John",
222,2022-07-20T17:12:52Z,https://github.com/stanfordnlp/stanza/pull/1086,,"Load the pretrained charlm, adds it as inputs to the POS model

This improves accuracy on almost all POS models

Doing the same thing for depparse would also make sense, but is
currently not done.  However, the downstream scores of depparse don't
seem to be negatively affected by using the different (better) POS
tags produced by models using the pretrained charlm

Add a pos-specific charlm map for the medical EN datasets and the one dataset which appears to be hurt by the charlm (tr_boun)

craft, genia -> None

Produces resources.json with pos charlms

Make the Pipeline pass in charlm paths if present in resources.json
TODO: use the foundation_cache to load them
",
223,2022-07-20T12:10:30Z,https://github.com/stanfordnlp/stanza/pull/1085,,Add feature to read .csv files when reading a pretrain,
224,2022-07-20T01:32:02Z,https://github.com/stanfordnlp/stanza/issues/1084,,"**Describe the bug**
The lemma of rose(rose flower) is rise in 1.4.0

**To Reproduce**
Steps to reproduce the behavior:
Take the sentence ""I gave her a rose"" as example, the POS of rose is right, which is NN, but the lemma of it is rise in 1.4.0

**Expected behavior**
The lemma of rose should be rose not rise, this behavior is normal in 1.3.0

**Environment**
 - OS: Debian
 - Python version: 3.8.10 from miniconda
 - Stanza version: 1.4.0
",
225,2022-07-18T23:23:31Z,https://github.com/stanfordnlp/stanza/pull/1083,,Replaces the original PR with a version where all the commits are squashed into one,
226,2022-07-18T16:26:49Z,https://github.com/stanfordnlp/stanza/pull/1082,,"Rewrite `xpos_vocab_factory` so that it looks at the raw data if the dataset isn't already known.

Open question: better to check a dataset against the known version of that dataset's xpos?  Might save some headache in the future",
227,2022-07-17T01:36:08Z,https://github.com/stanfordnlp/stanza/pull/1081,,"There is a small typo in stanza/utils/datasets/ner/convert_bsf_to_beios.py.

Should read `containing` rather than `cotaining`.


Semi-automated pull request generated by
https://github.com/timgates42/meticulous/blob/master/docs/NOTE.md",
228,2022-07-16T17:43:28Z,https://github.com/stanfordnlp/stanza/pull/1080,,"Document visualization with Spacy.  Processes completed docs or raw strings

Includes right-to-left support (in the NER viz in particular, tags are flipped, for example)

Added more documentation for usage, including necessary spaCy installations

Includes Jupyter examples for visualization; spacy.render() functions well here
Adding new Jupyter examples with support for new functions to visualize several strings with the same language pipeline

",
229,2022-07-16T06:56:44Z,https://github.com/stanfordnlp/stanza/pull/1079,,Build Thai NER from LST20,
230,2022-07-16T00:25:48Z,https://github.com/stanfordnlp/stanza/pull/1078,,"… the combined / test datasets automatically

**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `main` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/main/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
231,2022-07-14T06:35:14Z,https://github.com/stanfordnlp/stanza/pull/1077,,"…l languages can be chosen if all legal languages were negative

Addresses #1076
",
232,2022-07-13T17:22:30Z,https://github.com/stanfordnlp/stanza/issues/1076,,"**Describe the bug**
If you set a `lang_subset` to the langid processor, it gives as result a language that not always is in the subset

**To Reproduce**
Steps to reproduce the behavior:
```python
import stanza
langid = stanza.Pipeline(""multilingual"", langid_lang_subset = [""es""])
langid(""aaa"").lang
```

The result will be:
```
la
```

**Expected behavior**
The language should be `es` 100% since I use  `lang_subset = [""es""]`
**Environment (please complete the following information):**
 - Stanza version: 1.4.0

**Additional context**
I see that here:
https://github.com/stanfordnlp/stanza/blob/011b6c4831a614439c599fd163a9b40b7c225566/stanza/models/langid/model.py#L85is used the `self.lang_subset` variable that should be `[""es""]`.
While without this subset everything seems to work properly, if `self.lang_subset` is set not always.

I tried to add some print
```python
    def prediction_scores(self, x):
        prediction_probs = self(x)
        print(""prediction_probs"")
        print(prediction_probs)
        if self.lang_subset:
            print(""lang_mask"")
            print(self.lang_mask)
            prediction_batch_size = prediction_probs.size()[0]
            print(""prediction_batch_size"")
            print(prediction_batch_size)
            batch_mask = torch.stack([self.lang_mask for _ in range(prediction_batch_size)])
            print(""batch_mask"")
            print(batch_mask)
            prediction_probs = prediction_probs * batch_mask
            print(""prediction_probs"")
            print(prediction_probs)
        print(""argmax"")
        print(torch.argmax(prediction_probs, dim=1))
        print(self.idx_to_tag[torch.argmax(prediction_probs, dim=1)])
        return torch.argmax(prediction_probs, dim=1)
```

The result is:
```
prediction_probs
tensor([[-0.8162, -1.6429,  2.5455, -1.8278, -2.5117, -2.9207, -3.7888, -0.8804,
         -0.3428,  1.0926,  0.6126, -4.4035, -0.5995, -3.2369,  2.7545, -0.2619,
         -0.7622, -1.5564, -4.9973, -0.7835, -5.0576, -4.1394,  0.3663, -1.4397,
         -3.2930, -1.5079, -0.7348, -1.1671,  1.4471, -6.8030,  1.9239, -2.3856,
         -6.2493,  1.5562, -2.8086, -2.9353, -2.9437,  0.3282,  1.0697, -6.2935,
          0.5006, -0.3015, -0.4489, -0.4419,  6.0343, -2.1565,  0.9285, -2.3867,
         -2.4929, -0.8634, -3.6259, -4.6344, -0.1315, -4.4329,  0.6783,  0.3423,
         -1.2810, -3.3283, -1.4476, -6.0616, -2.5742, -4.2238, -0.5372, -6.5371,
         -2.5767, -1.5134, -3.3457,  1.6572]], device='cuda:0',
       grad_fn=<SumBackward1>)
lang_mask
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
       device='cuda:0')
prediction_batch_size
1
batch_mask
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],
       device='cuda:0')
prediction_probs
tensor([[-0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
         -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,
         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,
         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,
         -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,
          0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,
         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,
         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.5372, -0.0000,
         -0.0000, -0.0000, -0.0000,  0.0000]], device='cuda:0',
       grad_fn=<MulBackward0>)
argmax
tensor([0], device='cuda:0')
la
```

I think in this case the string `aaa` was too difficult for the model (indeed it doesn't mean anything) and so the probability of `es` was `-0.5372` (<0). All the other probability are set to `-0.0000` that is an higher value. So when you compute the argmax you'll get the first label which is probably `la`.

I know that this case can happen rarely, since with a real word the higher probability is usually positive, but I found some similar errors with models trained by me with your training script that sometimes take the first language of the label_list since the languages in the `lang_subset` have **negative probabilities**.",
233,2022-07-12T16:53:21Z,https://github.com/stanfordnlp/stanza/issues/1075,,"Hello, Im trying to add a new language in Stanza and for that im following **https://stanfordnlp.github.io/stanza/new_language.html** this link, so i already have language data in conllu format and using 
**python3 -m stanza.utils.charlm.conll17_to_text ./**  
this command i've successully converted my conllu file into .txt.xz file and now i want to convert it in a suitable dataset using following command:
**python3 -m stanza.utils.charlm.make_lm_data ./extern_data/charlm_raw ./extern_data/charlm**
so when I run this command im getting following output with the error, i dont know if im doing something wrong.

Output:
Processing files:
source root: ./extern_data/charlm_raw
target root: ./extern_data/charlm

1 total languages found:
['SINDHIDATA.txt.xz']

Traceback (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/msi/.local/lib/python3.10/site-packages/stanza/utils/charlm/make_lm_data.py"", line 139, in <module>
    main()
  File ""/home/msi/.local/lib/python3.10/site-packages/stanza/utils/charlm/make_lm_data.py"", line 69, in main
    data_dirs = os.listdir(lang_root)
NotADirectoryError: [Errno 20] Not a directory: 'extern_data/charlm_raw/SINDHIDATA.txt.xz'",
234,2022-07-10T16:43:42Z,https://github.com/stanfordnlp/stanza/pull/1074,,"Read in a pretrain even if it doesn't have a row/col header.  This applies to a lot of glove produced vectors, for example

Adds a test of the no-header pretrain
",
235,2022-07-06T16:51:58Z,https://github.com/stanfordnlp/stanza/pull/1073,,"## Description
It's my proposal for the returned value of `get_known_feats` - returning all values as a dictionary instead of only feats types as a list (omitting possible values).

## Fixes Issues
#1066 

## Unit test coverage
I extended the test of the method to test also the feat value - `Yes`.

## Known breaking changes/behaviors
The feature has not been added to main yet, so I think it's fine.
",
236,2022-07-06T07:01:48Z,https://github.com/stanfordnlp/stanza/pull/1071,,Add a Bangla NER model,
237,2022-07-04T09:29:43Z,https://github.com/stanfordnlp/stanza/issues/1070,,"I'd like to add NER model for Polish. For now, I wonder what else is needed.

**Datasets**
- Char-LM: [Wikipedia Subcorpus](http://clip.ipipan.waw.pl/PolishWikipediaCorpus)
- NER annotations: [NKJP Corpus](http://clip.ipipan.waw.pl/NationalCorpusOfPolish)

**Baseline models**
- [char-lm based](http://mozart.ipipan.waw.pl/~ksaputa/stanza/saved_models-base-wikipedia.tar.gz)
- [BERT-based](http://mozart.ipipan.waw.pl/~ksaputa/stanza/saved_models-herbert-large-without-charlm.tar.gz) ([herbert-large](https://huggingface.co/allegro/herbert-large-cased))

**Results**
For char-lm model:
```
2022-06-28 13:39:24 INFO: Running NER tagger in predict mode	
2022-06-28 13:39:25 INFO: Loading data with batch size 32...	
2022-06-28 13:39:26 DEBUG: 38 batches created.	
2022-06-28 13:39:26 INFO: Start evaluation...	
2022-06-28 13:39:37 INFO: Score by entity:	
Prec.   Rec.    F1	
85.55   87.69   86.61	
2022-06-28 13:39:37 INFO: Score by token:	
Prec.   Rec.    F1	
68.59   68.98   68.78	
2022-06-28 13:39:37 INFO: NER tagger score:	
2022-06-28 13:39:37 INFO: pl_nkjp 86.61	
```


I could definitely improve these models further and share an update in the coming weeks.
I'd like to ask is there something more I need to prepare to include these in the next Stanza release.

Especially I'm not sure about 
- BERT integration, for now I added only the training parameter in [my version](https://github.com/k-sap/stanza)
- to what extend sharing converted NER data & conversion code is needed

",
238,2022-07-02T17:41:56Z,https://github.com/stanfordnlp/stanza/pull/1069,,PR for WIP for the visualization of dependency trees using displacy,
239,2022-07-01T16:17:41Z,https://github.com/stanfordnlp/stanza/pull/1068,,"….  Thanks to Allan https://datatables.net/forums/discussion/73237/datatable-on-just-the-docs-github-page/ with a refinement from qipeng
",
240,2022-06-30T04:09:48Z,https://github.com/stanfordnlp/stanza/issues/1067,,"Hi,
I was checking your lemmatization for Hindi and Urdu and found that possessive [genitive]  case markers in Hindi and Urdu are wrongly lemmatized.
It refers to the Hindi possessive case markers:
का की के
I have noticed that your lemmatiser tends to reduce these [maybe because they are using builtin libraries] to a single form का 
लड़की के मामा की बहन
Lemmatized form लड़की का मामा का बहन
I personally do not agree with this approach not only because /ka/ is not the base form [lemma] of /ki/ and /ke/ but also because it is ""sexist"" in nature and reduces feminine and feminine/masc plural to a masculine singular form. I do no see 'her' or 'sa' or ihre' in En Fr Ger reduced to 'him', 'son','ihr' which it should by the same logic.
The same scenario is in Urdu
Stanford's Stanza reduces Urdu genitive case markers  /ka/ ke/ ki/ to  /ka/ 
Here is an output of the Urdu sentence
مھمد کی کتاب اور ھسن کے گھر
Lemma: مھمد کا کتاب اور ھسن کا گھر
I believe the library which does this is at fault. I consulted my colleagues who are linguists in Hindi and Urdu and alss work in the area of  NLP and we feel this approach is linguistically incorrect and worse still smacks of sexism. I do not think it is right to reduce a feminine form to a masculine. 
my email: raymond.doctor@gmail.com
I hope a more rational approach to this will be adopted. ",
241,2022-06-28T13:27:16Z,https://github.com/stanfordnlp/stanza/issues/1066,,"I'd like to, preferably programmatically as in Spacy, get tagsets for all processors for a given model/language in Stanza.

E.g. all NER tags, all dependency relations. (I know that NER tags are included in the documentation.)
I need to know what are the possible labels returned by Stanza and be able to update it easily.

Is it possible to get such information for a specific Stanza model?",
242,2022-06-28T11:46:30Z,https://github.com/stanfordnlp/stanza/issues/1065,,"
![image](https://user-images.githubusercontent.com/30179436/176170719-01d32930-f8fc-4017-bac3-b7544b94cce2.png)
![image](https://user-images.githubusercontent.com/30179436/176170792-849459e6-f8a2-43ae-ab67-6c4e3bbac0ff.png)

",
243,2022-06-28T10:34:32Z,https://github.com/stanfordnlp/stanza/issues/1064,,"When I parse a paper document, I get the following error, and only the first sentence parses the compound word
![image](https://user-images.githubusercontent.com/30179436/176158571-f2be8b7f-60f0-4b76-87a0-06b81daaad9d.png)
",
244,2022-06-27T16:13:40Z,https://github.com/stanfordnlp/stanza/issues/1063,,"Hello,

I tried using Stanza to lemmatize some simple German (i.e. ""Ich liebe dich [I love you]."" and ""Ich habe dich gehört [I heard you.]""  Stanza made mistakes with the verbs in both sentences. 

Here are screen shots from the Stanza demo for both of above sentences:

![image](https://user-images.githubusercontent.com/37348895/175982727-8d7892b6-6d5b-498f-aa11-329e8e56dac8.png)

![image](https://user-images.githubusercontent.com/37348895/175983147-05e080bf-e878-4d4d-8515-5c72be3ff23d.png)

In the first example, Stanza misidentified the lemma of 'liebe' as 'lassen' (instead of 'lieben', as it should be.) In the second example, Stanza misidentified the lemma of 'gehört' as 'gehören' (instead of 'hören', as it should be.) Given that the grammar of these sentences is fairly straightforward and that these are not uncommon verbs, I am surprised by these errors. (I have been using Stanza for lemmatization of French and have never had problems.)

Is there some way to fix this problem? Am I doing something wrong?

Thank you in advance for your time.
",
245,2022-06-27T15:56:02Z,https://github.com/stanfordnlp/stanza/issues/1062,,"Hi, a couple of questions that are related.

I'm trying to train a new model for a new language, but I'm first trying the data included in the packages to know more about how Stanza works when training data.

When I run the command 

`python3 -m stanza.utils.datasets.prepare_tokenizer_treebank UD_English-TEST` 

the following error appears:

`(nlp) dario@MacBook-Pro-de-Dario oe_lemmatizer_stanza % python3 -m stanza.utils.datasets.prepare_tokenizer_treebank UD_English-TEST
2022-06-27 16:45:52 INFO: Datasets program called with:
/Users/dario/virtual-environments/nlp/lib/python3.10/site-packages/stanza/utils/datasets/prepare_tokenizer_treebank.py UD_English-TEST
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/Users/dario/virtual-environments/nlp/lib/python3.10/site-packages/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1136, in <module>
    main()
  File ""/Users/dario/virtual-environments/nlp/lib/python3.10/site-packages/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1133, in main
    common.main(process_treebank, add_specific_args)
  File ""/Users/dario/virtual-environments/nlp/lib/python3.10/site-packages/stanza/utils/datasets/common.py"", line 134, in main
    process_treebank(treebank, paths, args)
  File ""/Users/dario/virtual-environments/nlp/lib/python3.10/site-packages/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1116, in process_treebank
    train_conllu_file = common.find_treebank_dataset_file(treebank, udbase_dir, ""train"", ""conllu"", fail=True)
  File ""/Users/dario/virtual-environments/nlp/lib/python3.10/site-packages/stanza/utils/datasets/common.py"", line 37, in find_treebank_dataset_file
    raise FileNotFoundError(""Could not find any treebank files which matched {}"".format(filename))
FileNotFoundError: Could not find any treebank files which matched extern_data/ud2/ud-treebanks-v2.8/UD_English-TEST/*-ud-train.conllu`

The path I am using is the exact one that comes with the package when cloning it from GitHub. My idea is to replace the files with my own ones. I have tried closed issues about some similar errors to this one, but the solutions are not applicable to my problem. 

Also, I'm following the documentation for this in [https://stanfordnlp.github.io/stanza/training.html#converting-ud-data](url), but no info is given about the train, test, and dev data. Is the script going to generate the dev and test ones? Do I need to generate them? I'm new to this, and the language I'm trying to add is not in the Universal Dependencies, I have found some datasets in .conll format, which I have converted to .conllu following Stanza documentation.

Any ideas?

Thanks!",
246,2022-06-26T20:20:18Z,https://github.com/stanfordnlp/stanza/pull/1061,,"## Description
Makes CoreNLPClient not check if the server is alive when start_server=StartServer.DONT_START

## Fixes Issues
#1059 

## Unit test coverage
test_external_server renamed to test_external_server_available (and modified)
test_external_server_timeout added
test_external_server_unavailable added

pytest executed successfully on stanza/tests/server/test_client.py

## Known breaking changes/behaviors
Now when start_server=StartServer.DONT_START the clients must be sure the server is running. Otherwise (if they launched a server instance but didn't wait for enough) they could get a connection error.
",
247,2022-06-25T11:46:00Z,https://github.com/stanfordnlp/stanza/issues/1060,,"![image](https://user-images.githubusercontent.com/30179436/175772133-0a5c58c9-8dd5-486e-a7df-13eff505cab6.png)
![image](https://user-images.githubusercontent.com/30179436/175772141-42b1b3d9-e3a3-4e68-8b3f-37ee641a5bf2.png)
i have installed java, and also get this error
",
248,2022-06-24T17:16:36Z,https://github.com/stanfordnlp/stanza/issues/1059,,"**Motivational problem**
We can use `CoreNLPClient` just as client interface to interact with an already existent CoreNLP standalone server by passing the argument: `start_server=StartServer.DONT_START`

But in this cases if the server is down, a request will hang in `self.ensure_alive() `: https://github.com/stanfordnlp/stanza/blob/011b6c4831a614439c599fd163a9b40b7c225566/stanza/server/client.py#L446

**Example**
```
import stanza.server as corenlp
from stanza.server import CoreNLPClient

client = CoreNLPClient(start_server=corenlp.StartServer.DONT_START, endpoint='http://non-existing-server.com:9050', annotators=['parse'], timeout=5000)
ann_sent = client.annotate('sarasa', output_format='json')
```

**Solution alternatives**
- (recommended) Do not check for ensure_alive at `CoreNLPClient` when `start_server=StartServer.DONT_START`. If the class was not responsible for creating the server instance, should not be responsible for wait it's alive. In this case we want to use an `CoreNLPClient` just as a client interface, and not using `RobustService` functionality.
- Make `CHECK_ALIVE_TIMEOUT` configurable at `RobustService.__init__` and let the user configure it on `CoreNLPClient.__init__`.
- Make `CHECK_ALIVE_TIMEOUT` configurable at `RobustService.ensure_alive` and let be affected by timeout value on `CoreNLPClient._request` (no recommended at all, it will change the semantic of timeout).

If you agree with the current behaviour must be changed, you can choose any solution and I will create the PR.",
249,2022-06-24T09:31:04Z,https://github.com/stanfordnlp/stanza/issues/1057,,"
Hi all!

First time posting a question, feel free to correct me if I'm not following conventions. I'm a Python newbie trying to start an NLP project, so all help is welcome!

I'm trying to add a new language (Old English) to Stanza, to train a model to automatically annotate OE texts. My data is converted into the corresponding format, I have word2vec word vectors, and I have a tokenized raw text file according to the documentation (https://stanfordnlp.github.io/stanza/new_language.html#data-format). My main issue is that the documentation is not clear for me. The case example used to explain how to add new languages to Stanza, and more concretely the section CharacterLM, assumes that the user is going to use data either from Wikipedia or from conll17 or OSCAR, so the terminal commands examples are fitting those scenarios. As my data is from other source, I'm using this command `python3 -m stanza.utils.charlm.make_lm_data extern_data/charlm_raw extern_data/charlm` from the third bulletpoint, giving my source directory and target directory.

This is where my problem starts, when I enter the command in the terminal, the following error appears:
<img width=""942"" alt=""Captura de Pantalla 2022-06-23 a las 14 52 40"" src=""https://user-images.githubusercontent.com/65161098/175506650-ef389dac-e2c7-4715-a325-013911cafbe8.png"">

The command automatically looks for a language, although the language parameter is optional in the command. It also appears that the command tries to create the target directory, although the target directory parameter is not optional for the command, and then the error NotADirectoryError appears.

Is there anything that I'm doing wrong that prevents me from progressing in this project? Any thoughts on how I can solve this problem? I have tried looking for info in the published issues in this repo and on the internet, but I haven't found any extra info about how to add new languages.

Thanks for your help, and sorry for the long post!
",
250,2022-06-24T04:06:12Z,https://github.com/stanfordnlp/stanza/pull/1056,,,
251,2022-06-23T09:10:43Z,https://github.com/stanfordnlp/stanza/issues/1055,,"For example to provide a list of abbreviations and tell the model to avoid splitting the token if it belongs to the list. This would be very useful to improve sentence splitting and tokenization for domain-specific vocabularies...
",
252,2022-06-21T06:58:08Z,https://github.com/stanfordnlp/stanza/pull/1054,,"Make a version of the lattn out of all the inputs, not just the pattn",
253,2022-06-20T18:39:48Z,https://github.com/stanfordnlp/stanza/pull/1053,,,
254,2022-06-20T09:19:11Z,https://github.com/stanfordnlp/stanza/issues/1052,,"I started a server using the following command line in a Ubuntu hyper-v server on winserver 2016:

java -Xmx16g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port 9009 -timeout 150000

When I try running the following python code:

requests.post('http://192.168.1.5:9009/tregex?pattern=NP|NN&filter=False&properties={""annotators"":""tokenize,ssplit,pos,ner,depparse,parse"",""outputFormat"":""json""}', data = {'data':tgt0}, headers={'Connection':'close'}).json()

The execution sticks there, and I see the following memssage on the server:

[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - Server default properties:
			(Note: unspecified annotator properties are English defaults)
			annotators = tokenize, ssplit, pos, lemma, ner, parse, coref
			coref.algorithm = hybrid
			coref.calculateFeatureImportance = false
			coref.defaultPronounAgreement = true
			coref.input.type = raw
			coref.language = zh
			coref.md.liberalChineseMD = false
			coref.md.type = RULE
			coref.path.word2vec = 
			coref.postprocessing = true
			coref.print.md.log = false
			coref.sieves = ChineseHeadMatch, ExactStringMatch, PreciseConstructs, StrictHeadMatch1, StrictHeadMatch2, StrictHeadMatch3, StrictHeadMatch4, PronounMatch
			coref.useConstituencyTree = true
			coref.useSemantics = false
			coref.zh.dict = edu/stanford/nlp/models/dcoref/zh-attributes.txt.gz
			depparse.language = chinese
			depparse.model = edu/stanford/nlp/models/parser/nndep/UD_Chinese.gz
			entitylink.wikidict = edu/stanford/nlp/models/kbp/chinese/wikidict_chinese.tsv.gz
			inputFormat = text
			kbp.language = zh
			kbp.model = none
			kbp.semgrex = edu/stanford/nlp/models/kbp/chinese/semgrex
			kbp.tokensregex = edu/stanford/nlp/models/kbp/chinese/tokensregex
			ner.applyNumericClassifiers = true
			ner.fine.regexner.mapping = edu/stanford/nlp/models/kbp/chinese/gazetteers/cn_regexner_mapping.tab
			ner.fine.regexner.noDefaultOverwriteLabels = CITY,COUNTRY,STATE_OR_PROVINCE
			ner.language = chinese
			ner.model = edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz
			ner.useSUTime = false
			outputFormat = json
			parse.model = edu/stanford/nlp/models/srparser/chineseSR.ser.gz
			pos.model = edu/stanford/nlp/models/pos-tagger/chinese-distsim.tagger
			prettyPrint = false
			segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
			segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
			segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
			segment.sighanPostProcessing = true
			ssplit.boundaryTokenRegex = [.。]|[!?！？]+
			tokenize.language = zh
[main] INFO CoreNLP - Threads: 12
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9009
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-2-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... done [21.0 sec].
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-2-thread-1] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/chinese-distsim.tagger ... done [1.3 sec].
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[pool-2-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/UD_Chinese.gz ... Time elapsed: 1.7 sec
[pool-2-thread-1] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 20000 vectors, elapsed Time: 2.301 sec
[pool-2-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [4.0 sec].
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[pool-2-thread-1] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/srparser/chineseSR.ser.gz ... done [5.7 sec].
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-2-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz ... done [3.2 sec].
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 21238 unique entries out of 21249 from edu/stanford/nlp/models/kbp/chinese/gazetteers/cn_regexner_mapping.tab, 0 TokensRegex patterns.
[pool-2-thread-1] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - numeric classifiers: true; SUTime: false [no docDate]; fine grained: true
[pool-2-thread-1] INFO edu.stanford.nlp.wordseg.ChineseDictionary - Loading Chinese dictionaries from 1 file:
[pool-2-thread-1] INFO edu.stanford.nlp.wordseg.ChineseDictionary -   edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
[pool-2-thread-1] INFO edu.stanford.nlp.wordseg.ChineseDictionary - Done. Unique words in ChineseDictionary is: 423200.
[pool-2-thread-1] INFO edu.stanford.nlp.wordseg.CorpusChar - Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese/dict/character_list [done].
[pool-2-thread-1] INFO edu.stanford.nlp.wordseg.AffixDictionary - Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb [done].
[pool-2-thread-1] INFO edu.stanford.nlp.wordseg.CorpusChar - Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese/dict/character_list [done].
[pool-2-thread-1] INFO edu.stanford.nlp.wordseg.AffixDictionary - Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb [done].
[pool-2-thread-1] WARN CoreNLP - java.lang.RuntimeException: Ate the whole text without matching.  Expected is ' CD-SS3.4', ate 'CD-SS3.4a'
  edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.advancePos(ChineseSegmenterAnnotator.java:296)
  edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.runSegmentation(ChineseSegmenterAnnotator.java:407)
  edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.doOneSentence(ChineseSegmenterAnnotator.java:133)
  edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.annotate(ChineseSegmenterAnnotator.java:127)
  edu.stanford.nlp.pipeline.TokenizerAnnotator.annotate(TokenizerAnnotator.java:379)
  edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
  edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:680)
  edu.stanford.nlp.pipeline.StanfordCoreNLPServer$TregexHandler.lambda$handle$7(StanfordCoreNLPServer.java:1332)
  java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
  java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
  java.base/java.lang.Thread.run(Thread.java:829)
",
255,2022-06-19T16:48:51Z,https://github.com/stanfordnlp/stanza/pull/1051,,"Train using AdaDelta for a while w/ no pattn, then switch to AdamW or some other optimizer with the full model",
256,2022-06-18T02:29:28Z,https://github.com/stanfordnlp/stanza/pull/1050,,"Load the pt directly in Trainer.load().  Will simplify other operations, such as loading or creating models from a foundation_cache
",
257,2022-06-16T22:01:17Z,https://github.com/stanfordnlp/stanza/pull/1049,,"Add a learning rate scheduler.  Do some futzing with the tests, especially the conparse tests",
258,2022-06-16T06:27:37Z,https://github.com/stanfordnlp/stanza/pull/1047,,"Finally, a method that successfully trains pattn layers (seriously, the scores go up compared to adadelta or adamw by themselves)",
259,2022-06-08T09:54:33Z,https://github.com/stanfordnlp/stanza/issues/1046,,"Hello ,

Could any let know what is the correct format of data to train sentiment mode and the way to train it?

how to train it for few scentences or words?
as i want different results for some scentences or words,

Thank you",
260,2022-06-07T18:10:46Z,https://github.com/stanfordnlp/stanza/issues/1045,,"python -m stanza.utils.datasets.prepare_${module}_treebank ${corpus} ${other_args}
what we have to pass in module,corpus,other_args

C:\Users\Kalpataru\Desktop\stanza-main\stanza\utils\datasets\ner>python -m stanza.utils.datasets.ner.prepare_ner_dataset fi_turku

FileNotFoundError: Cannot find train component of fi_turku in extern_data/ner\fi_turku\train.tsv

I want to train models as per our every time updated data
",
261,2022-06-07T11:13:09Z,https://github.com/stanfordnlp/stanza/issues/1044,,"Whenever [stanza.download](https://github.com/stanfordnlp/stanza/blob/main/stanza/resources/common.py#L514) method is called, it seems that resource_1_x.0.json file is assured to exist by calling [file_exists](https://github.com/stanfordnlp/stanza/blob/main/stanza/resources/common.py#L134). 
However, when this json file is examined, the md5 value of this file is set to be None so the _file_exists_ method returns False everytime.
![image](https://user-images.githubusercontent.com/29737578/172366341-896e5f9c-0a43-444d-9ca4-6c6e249ae8d4.png)

",
262,2022-06-05T00:18:59Z,https://github.com/stanfordnlp/stanza/pull/1043,,Use the recent L3Cube dataset release to add NER and sentiment for Marathi to Stanza,
263,2022-05-31T11:03:17Z,https://github.com/stanfordnlp/stanza/issues/1042,,"I am using lemmatization from Stanza (1.4.0). To disable the warning and send only critical messages to the console, I am using the following config

```
import warnings
warnings.filterwarnings('ignore')

import sys
logging.disable(sys.maxsize)

logger = logging.getLogger()
logger.setLevel(logging.CRITICAL)

```
While doing lemmatization, I receive the message below which says that it is downloading file ""resources_1.4.0.json"" on every call. This further increases the total runtime. How can I avoid this. 

> Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 612k Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 603kB/s]
> Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 648k Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 634kB/s]

Lemmatization function 
```
def lemmatize_text(text):
    import stanza
    nlp = stanza.Pipeline(lang='en', processors='tokenize, mwt, pos, lemma')
    doc = nlp(text)
    word_lemma = [sent.words[0].lemma for sent in doc.sentences]
    return word_lemma[0]
```",
264,2022-05-31T08:18:48Z,https://github.com/stanfordnlp/stanza/issues/1041,,"I am running stanza v1.3.0 but I have noticed this behavior on v1.1.1 as well.

The tokenizer splits one word into two and uses underscore instead of their text.

Take the following examples

पहा काय बरळला पाकड्यांचा पंतप्रधान इमरान खान
(_See what happened to Pakistan's Prime Minister Imran Khan_)

The dependency tree is
```
                 0_पहा                                  
                   root                                 
                   VERB                                 
    ________________|________                            
   |                     6_इमरान                        
   |                     parataxis                      
   |                        NOUN                        
   |         ________________|___________                
   |        |                      5_पंतप्रधान          
   |        |                                           
   |        |                          nsubj            
   |        |                           NOUN            
   |        |                            |               
   |        |                          3__                       <---------Notice
   |        |                        nmod:poss          
   |        |                           PRON            
   |        |                 ___________|__________     
1_काय    7_खान           2_बरळला                   4__        <---------Notice
vocative  punct            punct                   case 
  PRON    PUNCT            PUNCT                   ADP  

```

You can observe similar behavior on the following sentences

राज्यातलं सरकार आणि मुख्यमंत्री चांगलं काम करत आहेत.
(_The state government and the chief minister are doing a good job._)

```
                 8_करत                            
                   root                           
                   VERB                           
    ________________|_________                     
   |       |               7_काम                  
   |       |                advcl                 
   |       |                 VERB                 
   |       |                  |                    
   |       |              6_चांगलं                
   |       |                nsubj                 
   |       |                 NOUN                 
   |       |                  |                    
   |       |              2_सरकार                 
   |       |              nmod:poss               
   |       |                 NOUN                 
   |       |         _________|________            
   |       |      0__                4__          
   |       |    nmod:poss             conj        
   |       |       NOUN               NOUN        
   |       |        |          ________|______     
9_आहेत   10_.      1__     3_आणि             5__  
  aux    punct     case       cc             case 
  AUX    PUNCT     ADP      CCONJ            ADP  

```
Why is this happening? 

EDIT: I asked some native speakers of marathi, they told me that the tool is breaking those words that are a combination of words. For example, पाकड्यांचा (_of Pakistan_) is composed of पाक (_Pakistan_) and ड्यांचा (_of_). Similarly, in the second sentence, मुख्यमंत्री (_chief minister_) is composed of मुख्य (_chief_) and मंत्री (_minister_). However, this trend is not followed always. For example, पंतप्रधान (_prime minister_) is also composed of two words but it is not broken by stanza. And why their text is showing only underscore is still a mystery.",
265,2022-05-30T17:40:47Z,https://github.com/stanfordnlp/stanza/pull/1040,,Optional integration with wandb for NER,
266,2022-05-28T18:19:24Z,https://github.com/stanfordnlp/stanza/pull/1039,,"Try to generalize wikiner reading - currently the download format is a
bz2 file with one thing in it, but an older layout I have had the text
itself in a ""raw"" subdirectory

ignore leftover bz2 files

Some of the input files are Windows encoded ffs
",
267,2022-05-27T19:52:38Z,https://github.com/stanfordnlp/stanza/pull/1038,,Add a conversion of the Megagon GSD dataset,
268,2022-05-25T09:34:54Z,https://github.com/stanfordnlp/stanza/issues/1037,,added to request Japanese NER models  ,
269,2022-05-24T00:38:03Z,https://github.com/stanfordnlp/stanza/issues/1035,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
270,2022-05-22T10:49:24Z,https://github.com/stanfordnlp/stanza/issues/1034,,"### RuntimeError: input.size(-1) must be equal to input_size. Expected 45, got 37

The chinese-simple model has been download. I both tested the English model and Chinese model， only the English model can run righly.
When I try to run the Chinese-simpls demo using below code:

`import stanza
zh_nlp = stanza.Pipeline('zh')
doc = zh_nlp('我喜欢自然语言处理')
`
Below debug occurs:

`RuntimeError: input.size(-1) must be equal to input_size. Expected 45, got 37`

### environment：
System： windows 10
Python 3.7.13
",
271,2022-05-20T06:21:20Z,https://github.com/stanfordnlp/stanza/pull/1033,,Update NER to keep pretrain & delta word embeddings separate,
272,2022-05-19T15:33:59Z,https://github.com/stanfordnlp/stanza/issues/1032,,"I wanted to test Stanza on the recently released MPS backend for the M1 macs. However, I noticed looking in the code that Stanza consistently uses the .cuda() method instead of the more flexible .to(device), and .cuda() does not seem to be overwritten by the MPS backend. While I get that it is convenient for most users to just set use_gpu=True, I miss the option to alternatively set a specific device which would make the framework much more flexible.",
273,2022-05-18T03:32:33Z,https://github.com/stanfordnlp/stanza/pull/1031,,"Refactor the tokenizer data module to use a torch DataLoader at eval time.  Train time is not changed at all.

Between version 1.4.0 and dev branch as of this pull request, some optimizations were added to pass around numpy or torch items sooner.  This improved the runtime some already.  Switching to a pytorch dataloader further improves the runtime.  Timing on a 2080ti and a CPU that was top of the line a couple years ago:

```
main branch:
real    0m53.964s
user    0m51.178s
sys     0m4.205s

dev branch:
real    0m46.744s
user    0m44.220s
sys     0m4.028s

refactor_dataloader branch:
real    0m38.706s
user    0m46.471s
sys     0m6.985s
```

Most of the timing improvements wound up being portable to the dev branch without the Dataloader.  However, there are a few circumstances where lots of docs of mixed sizes can be faster by spawning multiple workers.  Also, in general it's basically the same speed if you go with num_workers=0, which is the default.  At least merging this in will make it easier to optimize the multiple worker case in the future...",
274,2022-05-16T23:13:15Z,https://github.com/stanfordnlp/stanza/issues/1030,,"The Kurmanji (Kurdish) model, although listed in ""Available Models & Languages"", cannot be downloaded.

When trying stanza.download('kmr') you get the error:

An error ValueError: Unsupported language: kmr.

I guess the model might have been retired from download list owing to sparse data, yet it might be useful to be able to use a model, albeit an unsatisfactory one.

Thanks!

Giuliano",
275,2022-05-16T03:44:58Z,https://github.com/stanfordnlp/stanza/pull/1029,,"Refactor some pieces of the tokenizer and make some optimizations on the way.  In particular, passing around numpy or torch arrays is faster than making lists of numbers.

Add some tests of the data object as well.

When parsing one large data file, this makes the tokenizer about 10% faster by simplifying some of the logic to go from raw letters -> numbers",
276,2022-05-12T23:59:38Z,https://github.com/stanfordnlp/stanza/pull/1028,,"Cache the charlms in a pipeline, especially for sentiment and conparse

Also, add bert models to the bottom layer of sentiment",
277,2022-05-12T20:36:46Z,https://github.com/stanfordnlp/stanza/pull/1027,,,
278,2022-05-11T22:33:01Z,https://github.com/stanfordnlp/stanza/pull/1026,,Refactor loading files for the charlm.  Cleaner and even slightly faster,
279,2022-05-02T03:17:34Z,https://github.com/stanfordnlp/stanza/pull/1025,,Reuse the conparser's charlm code in the sentiment model,
280,2022-04-30T08:21:39Z,https://github.com/stanfordnlp/stanza/pull/1024,,"More updates to sentiment: use json instead of text to store sentences, which allows for pretokenizing VI sentiment",
281,2022-04-29T07:37:26Z,https://github.com/stanfordnlp/stanza/pull/1023,,"Rewrite the sentiment conversion scripts to be entirely in python (except for the parts which use Java, of course)",
282,2022-04-28T06:34:59Z,https://github.com/stanfordnlp/stanza/pull/1022,,"Do some various refactoring of bert operations, compensate for a DE bert tokenizer issue, and use these changes to produce a German NER bert model.",
283,2022-04-27T02:05:44Z,https://github.com/stanfordnlp/stanza/pull/1021,,Refactor a bunch of the sentiment scripts.  Move all the scripts/sentiment stuff to stanza/utils,
284,2022-04-26T10:16:38Z,https://github.com/stanfordnlp/stanza/issues/1019,,"``Is there a list of all possible values of word.deprel can have. I am looking for something like [this](https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf) document right here, or at least a list.

**Example usage**: I want to extract transitive verbs from a document. So I wanted to check for direct or indirect objects of verbs, code below seems to find no dependency in my input file. This can be due to my input document lacking transitive verbs or due to depparser not using such tags (iobj,pobj,dobj,dative in this case). When i check the dependencies i can see obj, and obl to verb dependencies but I haven't seen dobj and others yet. I tried a few different input documents, and it made no difference.

```
transitive_verb_associations = []
for sentence in self._doc.sentences:
  print(sentence.text)
  for word in sentence.words:
    verb_id = None
    if word.upos==""VERB"":
      print(""Verb: ""+word.text)
      verb_id = word.id
    if verb_id is not None:
      for  reference_word in sentence.words:
        if(reference_word.head ==verb_id):
          print(reference_word.text+""-has-""+reference_word.deprel+""-dependency type-"")
          if(reference_word.deprel==""iobj"" or reference_word.deprel ==""pobj"" or reference_word.deprel == ""dobj"" or reference_word.deprel == ""dative""):
            print(""found one"")
            transitive_verb_associations.append(sentence.words[verb_id-1].text+""-->""+reference_word.text.title())

print(transitive_verb_associations)
```

The last part can be modified like this.
```
if(reference_word.deprel==""iobj"" or reference_word.deprel==""pobj""):
    indirect_object = True
if (reference_word.deprel==""dobj"" or reference_word.deprel== ""dative""):
    direct_object = True
if direct_object and not indirect_object:
    print(""found one"")
    transitive_verb_associations.append(sentence.words[verb_id-1].text+""-->""+reference_word.text.title())
```",
285,2022-04-26T06:24:08Z,https://github.com/stanfordnlp/stanza/issues/1018,,"For several languages (e.g. German) there are several UD models and also several NER models. Also there is an evaluation of the different NER models.

However, I cannot find any information about how the NER model depends on the UD model and how the choice of UD model may impact the performance of the NER model. The evaluation does not state which UD model was used for it. 

Is there anything known about this or is this documented somewhere and I have missed it?",
286,2022-04-25T09:36:43Z,https://github.com/stanfordnlp/stanza/issues/1017,,"This line: https://github.com/stanfordnlp/stanza/blob/main/stanza/pipeline/core.py#L169

Please export `stanza.DownloadMethod` so one could write:
````
stanza.Pipeline( ..., download_method=stanza.DownloadMethod.NONE, ... )
````",
287,2022-04-24T06:55:27Z,https://github.com/stanfordnlp/stanza/pull/1016,,"wrap files in tqdm so we don't stare at the screen for a while wondering what is happening
",
288,2022-04-23T06:49:19Z,https://github.com/stanfordnlp/stanza/pull/1014,,(do not merge - unfinished),
289,2022-04-23T03:58:27Z,https://github.com/stanfordnlp/stanza/pull/1012,,"V1.4.0 release, tons of changes big & small",
290,2022-04-23T01:55:40Z,https://github.com/stanfordnlp/stanza/pull/1011,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
291,2022-04-22T05:45:00Z,https://github.com/stanfordnlp/stanza/issues/1010,,"Using constituency parse function only consume 2G GPU mem, is there a way to fully utilize my GPU?",
292,2022-04-21T21:14:54Z,https://github.com/stanfordnlp/stanza/pull/1008,,"Use the EN handparsed treebank as a data source without augmentation.  Use the same mechanism to not augment the IT MWT, since that seems a little silly",
293,2022-04-19T11:01:25Z,https://github.com/stanfordnlp/stanza/issues/1005,,"I have been trying to replicate StanfordCoreNLP class's Morphology feature lemma(string word, string tag)
For that I guessed that LemmaProcessor might work since in the documentation it says 
`Perform [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) on a [Word](https://stanfordnlp.github.io/stanza/data_objects.html#word) using the Word.text and Word.upos values. The result can be accessed as Word.lemma.`

This is the following code that I have tried.

```
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma', lemma_pretagged=True)
pp = Document([[{'id': 1, 'text': 'Test', 'lemma': 'advanced', 'xpos': 'VB'}]])
doc = nlp(pp)
```

I get following error after executing the code.
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-42-d50e16622d44> in <module>
----> 1 doc = nlp(pp)

/usr/local/lib/python3.8/dist-packages/stanza/pipeline/core.py in __call__(self, doc)
    229         assert any([isinstance(doc, str), isinstance(doc, list),
    230                     isinstance(doc, Document)]), 'input should be either str, list or Document'
--> 231         doc = self.process(doc)
    232         return doc
    233 

/usr/local/lib/python3.8/dist-packages/stanza/pipeline/core.py in process(self, doc)
    223             if self.processors.get(processor_name):
    224                 process = self.processors[processor_name].bulk_process if bulk else self.processors[processor_name].process
--> 225                 doc = process(doc)
    226         return doc
    227 

/usr/local/lib/python3.8/dist-packages/stanza/pipeline/tokenize_processor.py in process(self, document)
     83         raw_text = '\n\n'.join(document) if isinstance(document, list) else document
     84         # set up batches
---> 85         batches = DataLoader(self.config, input_text=raw_text, vocab=self.vocab, evaluation=True, dictionary=self.trainer.dictionary)
     86         # get dict data
     87         _, _, _, document = output_predictions(None, self.trainer, batches, self.vocab, None,

/usr/local/lib/python3.8/dist-packages/stanza/models/tokenization/data.py in __init__(self, args, input_files, input_text, input_data, vocab, evaluation, dictionary)
     39         else:
     40             # set up text from file or input string
---> 41             assert txt_file is not None or input_text is not None
     42             if input_text is None:
     43                 with open(txt_file) as f:

AssertionError: 
```",
294,2022-04-19T08:22:25Z,https://github.com/stanfordnlp/stanza/issues/1004,,"**Describe the bug**
When I init a pipeline like nlp = stanza.Pipeline('en') in stanza 1.3.1, I got KeyError: 'open_nodes' 

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dzc/miniconda3/envs/torch1110/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 263, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""/home/dzc/miniconda3/envs/torch1110/lib/python3.9/site-packages/stanza/pipeline/processor.py"", line 159, in __init__
    self._set_up_model(config, pipeline, use_gpu)
  File ""/home/dzc/miniconda3/envs/torch1110/lib/python3.9/site-packages/stanza/pipeline/constituency_processor.py"", line 45, in _set_up_model
    self._model = trainer.Trainer.load(filename=config['model_path'],
  File ""/home/dzc/miniconda3/envs/torch1110/lib/python3.9/site-packages/stanza/models/constituency/trainer.py"", line 105, in load
    open_nodes=params['open_nodes'],
KeyError: 'open_nodes'",
295,2022-04-18T11:00:43Z,https://github.com/stanfordnlp/stanza/issues/1003,,"**Describe the bug**
Lemmatization does not appear to be working for Indonesian.

**To Reproduce**
nlp = stanza.Pipeline(lang='id', processors='tokenize,pos,lemma,depparse')`

doc = nlp.process('Ia menjadi Gubernur Bali menggantikan Anak Agung Bagus Sutedja.')



`
Output:

> word:  Ia lemma:  ia
> word:  menjadi lemma:  menjadi
> word:  Gubernur lemma:  gubernur
> word:  Bali lemma:  bali
> word:  menggantikan lemma:  mengantikan
> word:  Anak lemma:  anak
> word:  Agung lemma:  agung
> word:  Bagus lemma:  bagus
> word:  Sutedja lemma:  sutedja
> word:  . lemma:  .

**Expected behavior**

I am not an expert on Indonesian, but I am studying it now and am confident about the following: 

In Indonesian many words are built by adding affixes to lemmas. 

For example 

menjadi is men + jadi. Jadi is the lemma.
menggantikan is meng + ganti + kan. Ganti is the lemma.

In all example sentences I have tried, the words are not being correctly lemmatized.

So I expect, for words with lemmas like these two examples, to get output like:
> word:  menjadi lemma:  jadi
> word:  menggantikan lemma:  ganti


**Environment:**
 - OS: Ubuntu
 -  Python version: 3.9
 - Stanza version: 1.3

**Additional context**
I noticed the same behavior in the following libraries:
https://github.com/nlp-uoregon/trankit
https://github.com/TakeLab/spacy-udpipe

I understand that these projects also use UD Indonesian GSD. I note that when you look at this data, you can see the proper lemmatization - see the third and final columns in this row for an example:
`5	menggantikan	ganti	VERB	VSA	Mood=Ind|Voice=Act	2	advcl	_	MorphInd=^meN+ganti<v>+kan_VSA$`
https://github.com/UniversalDependencies/UD_Indonesian-GSD/blob/master/id_gsd-ud-test.conllu


Dictionary definition showing words and lemmas used in this example:
![image](https://user-images.githubusercontent.com/26212434/163791987-641db501-4927-4093-95a7-f0ec3d12ea21.png)
![image](https://user-images.githubusercontent.com/26212434/163792092-f4ba043f-635f-4584-af63-2090a3cb217c.png)

** When the bug does not appear**

I figured this out just before posting this issue - the bug does not appear when you use the (non default) package 'csui':
`
nlp = stanza.Pipeline(
    lang='id', processors='tokenize,pos,lemma,depparse', package='csui')
    
doc = nlp.process('Ia menjadi Gubernur Bali menggantikan Anak Agung Bagus Sutedja.')
`

When that runs, you get the expected output:

> word:  Ia lemma:  ia
> word:  menjadi lemma:  jadi
> word:  Gubernur lemma:  Gubernur
> word:  Bali lemma:  Bali
> word:  menggantikan lemma:  ganti
> word:  Anak lemma:  anak
> word:  Agung lemma:  Agung
> word:  Bagus lemma:  Bagus
> word:  Sutedja lemma:  Sutedja
> word:  . lemma:  .

Of interest to me is that both the GSD and CSUI do have the correct lemma in their lemma column, column 3.
See:
https://raw.githubusercontent.com/UniversalDependencies/UD_Indonesian-CSUI/master/id_csui-ud-train.conllu
and 
https://raw.githubusercontent.com/UniversalDependencies/UD_Indonesian-GSD/master/id_gsd-ud-train.conllu
for example, and search for the word menggantikan. In both files menggantikan appears correctly as the lemma ganti.

So why/how is it going wrong with the training of the GSD model?

Thanks for any insight, 

Xavier

",
296,2022-04-17T21:07:15Z,https://github.com/stanfordnlp/stanza/pull/1002,,"## Description
Moving a tensor to a particular device is not an in-place operation. To move `pe` to another device, it needs to be re-assigned with the returned tensor. 

## Fixes Issues
#989 

## Unit test coverage
```
nlp = stanza.Pipeline(lang=model, logging_level=""DEBUG"")
sent = ""Over and over again Dorian used to read this fantastic chapter, and the two chapters immediately following, in which, as in some curious tapestries or cunningly-wrought enamels, were pictured the awful and beautiful forms of those whom Vice and Blood and Weariness had made monstrous or mad: Filippo, Duke of Milan, who slew his wife, and painted her lips with a scarlet poison that her lover might suck death from the dead thing he fondled; Pietro Barbi, the Venetian, known as Paul the Second, who sought in his vanity to assume the title of Formosus, and whose tiara, valued at two hundred thousand florins, was bought at the price of a terrible sin; Gian Maria Visconti, who used hounds to chase living men, and whose murdered body was covered with roses by a harlot who had loved him; the Borgia on his white horse, with Fratricide riding beside him, and his mantle stained with the blood of Perotto; Pietro Riario, the young Cardinal Archbishop of Florence, child and minion of Sixtus IV., whose beauty was equalled only by his debauchery, and who received Leonora of Aragon in a pavilion of white and crimson silk, filled with nymphs and centaurs, and gilded a boy that he might serve at the feast as Ganymede or Hylas; Ezzelin, whose melancholy could be cured only by the spectacle of death, and who had a passion for red blood, as other men have for red wine--the son of the Fiend, as was reported, and one who had cheated his father at dice when gambling with him for his own soul; Giambattista Cibo, who in mockery took the name of Innocent, and into whose torpid veins the blood of three lads was infused by a Jewish doctor; Sigismondo Malatesta, the lover of Isotta, and the lord of Rimini, whose effigy was burned at Rome as the enemy of God and man, who strangled Polyssena with a napkin, and gave poison to Ginevra d'Este in a cup of emerald, and in honour of a shameful passion built a pagan church for Christian worship; Charles VI., who had so wildly adored his brother's wife that a leper had warned him of the insanity that was coming on him, and who, when his brain had sickened and grown strange, could only be soothed by Saracen cards painted with the images of Love and Death and Madness; and, in his trimmed jerkin and jewelled cap and acanthus-like curls, Grifonetto Baglioni, who slew Astorre with his bride, and Simonetto with his page, and whose comeliness was such that, as he lay dying in the yellow piazza of Perugia, those who had hated him could not choose but weep, and Atalanta, who had cursed him, blessed him.""
nlp(sent)
```

## Known breaking changes/behaviors
No
",
297,2022-04-08T17:55:52Z,https://github.com/stanfordnlp/stanza/pull/1001,,"Download files to a tempdir created underneath the expected destination, then use os.replace to move the files (atomically if supported by the file system).  The goal is to ensure that two processes downloading the same file don't clobber each other with partially downloaded junk.  https://github.com/stanfordnlp/stanza/issues/213
",
298,2022-04-08T17:24:00Z,https://github.com/stanfordnlp/stanza/issues/998,,"NER models start from a pretrained embedding, same as depparse, pos, etc

however, they finetune the embedding

the entire finetuned embedding is then saved in the model.  only the delta needs to be saved.  this will save disk space when downloading the entire pipeline, plus a little bit of gpu space when loading a pipeline with ner + any other annotator",
299,2022-04-08T10:52:43Z,https://github.com/stanfordnlp/stanza/issues/997,,"**Description of Bug**
Text and consitutuency tree of a sentence doesn't match properly.

When printing each sentence in a document and their corresponding constituency trees, the following output shows that constituencies of different sentences are printed instead of corresponding ones.

***Faulty Terminal Output***
```
User shall search documents using keywords.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB read) (NP (NNS documents)))) (. .)))
User shall search documents using multi-term keywords.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB create) (NP (NNS tags)))) (. .)))
User shall search documents by author name.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB tag) (NP (NNS documents)))) (. .)))
User shall search documents by tags.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB request) (NP (NN signup)))) (. .)))
System shall display description of semantic tags on search UI.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB follow) (NP (NNS tags)))) (. .)))
User shall make sub-class or super-class tag searches.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB browse) (NP (NN search) (NN history)))) (. .)))
User shall quick search using search queries from search history.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB list) (NP (VBN followed) (NNS tags)))) (. .)))
User shall sort results by date, ascending or descending.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB save) (NP (NN search) (NNS queries)))) (. .)))
User shall sort results by alphabetical order of titles, A-Z / Z-A.
(ROOT (S (NP (NNP Admin)) (VP (MD shall) (VP (VB display) (NP (JJ unresolved) (NNS reports)))) (. .)))
User shall sort results by alphabetical order of author's last name, A-Z / Z-A.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB list) (NP (VBN saved) (NN search) (NNS queries)))) (. .)))
User shall read documents.
(ROOT (S (VP (VB Utilize) (NP (DT the) (NN Entrez) (NN API)) (PP (IN in) (NP (NN design)))) (. .)))
User shall create tags.
(ROOT (S (VP (VB Utilize) (NP (NNP W3C) (NN Activity) (NN Stream)) (PP (IN in) (NP (NN design)))) (. .)))
User shall tag documents.
(ROOT (S (NP (NNP Admin)) (VP (MD shall) (VP (VB approve) (NP (NML (, /) (NN decline)) (NN signup) (NN request)))) (. .)))
User shall report misuse of a tag.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB have) (NP (ADJP (JJ private) (CC or) (JJ public)) (NN profile)))) (. .)))
User shall remove tags those they tagged from document.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NNS documents)) (PP (IN by) (NP (NNS tags))))) (. .)))
System shall inform the tagger first when reported.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB make) (NP (ADJP (JJ sub-class) (CC or) (JJ super-class)) (NN tag) (NNS searches)))) (. .)))
Admin shall remove tag from the document when reported.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB send) (NP (NN report)) (PP (IN to) (NP (NN admin))))) (. .)))
System shall display description of tag on-site.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NP (NNS documents)) (VP (VBG using) (NP (NNS keywords)))))) (. .)))
User shall request signup.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB display) (NP (NP (NN frequency)) (PP (IN of) (NP (NN term)))))) (. .)))
Admin shall approve/decline signup request.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB display) (NP (NP (NNS statistics)) (PP (IN for) (NP (NN document)))))) (. .)))
System shall inform applicant in case of signup request declination.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB switch) (PP (IN between) (NP (ADJP (JJ public) (CC and) (JJ private)) (NNS profiles))))) (. .)))
System shall email applicant in case of signup request approval.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NP (NNS documents)) (VP (VBG using) (NP (JJ multi-term) (NNS keywords)))))) (. .)))
System shall display frequency of term.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NP (NNS documents)) (PP (IN by) (NP (NN author) (NN name)))))) (. .)))
System shall display statistics for document.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB report) (NP (NP (NN misuse)) (PP (IN of) (NP (DT a) (NN tag)))))) (. .)))
System shall display trend over time for frequency of term.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB display) (NP (NP (NN description)) (PP (IN of) (NP (NN tag) (NN on-site)))))) (. .)))
User shall switch between public and private profiles.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB have) (NP (NP (JJ public) (NN profile)) (PP (IN by) (NP (NN default)))))) (. .)))
User shall browse search history.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB sort) (NP (NNS results)) (PP (IN by) (NP (NN date) (, ,) (VBG ascending) (CC or) (VBG descending))))) (. .)))
User shall send report to admin.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB have) (VP (VBG following) (NP (NP (NN user) (NNS types)) (PP (IN of) (NP (NN user) (CC or) (NN admin))))))) (. .)))
User shall list followed tags.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB inform) (NP (DT the) (NN tagger)) (NP (ADVP (RB first)) (SBAR (WHADVP (WRB when)) (S (VP (VBN reported))))))) (. .)))
User shall list saved search queries.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB inform) (NP (NN applicant)) (PP (IN in) (NP (NP (NN case)) (PP (IN of) (NP (NN signup) (NN request) (NN declination))))))) (. .)))
User shall follow tags.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB email) (NP (NN applicant)) (PP (IN in) (NP (NP (NN case)) (PP (IN of) (NP (NN signup) (NN request) (NN approval))))))) (. .)))
User shall save search queries.
(ROOT (S (NP (NN User) (NN activity)) (VP (MD shall) (VP (VB remain) (ADJP (JJ anonymous)) (SBAR (IN if) (S (NP (NN user)) (VP (VBZ has) (NP (JJ private) (NN profile))))))) (. .)))
System shall notify user when a followed tag is tagged on a document.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB display) (NP (NP (NN description)) (PP (IN of) (NP (NP (JJ semantic) (NNS tags)) (PP (IN on) (NP (NN search) (NN UI)))))))) (. .)))
System shall notify user when a new saved search query result is present.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB inform) (NP (NN user)) (SBAR (WHADVP (WRB when)) (S (NP (DT an) (NN admin)) (VP (VBZ resolves) (NP (DT the) (NN report))))))) (. .)))
Admin shall display unresolved reports.
(ROOT (S (NP (NN Author) (NN name)) (VP (MD shall) (VP (VB remain) (ADJP (JJ anonymous)) (SBAR (IN if) (S (NP (DT the) (NN tag) (NN author)) (VP (VBZ has) (NP (JJ private) (NN profile))))))) (. .)))
Admin shall mark report as solved, on-hold, or closed.
(ROOT (S (NP (NNP Admin)) (VP (MD shall) (VP (VB remove) (NP (NN tag)) (PP (IN from) (NP (NP (DT the) (NN document)) (SBAR (WHADVP (WRB when)) (S (VP (VBN reported)))))))) (. .)))
System shall inform user when an admin resolves the report.
(ROOT (S (NP (NN User)) (VP (MD shall) (ADVP (RB quick)) (VP (VB search) (S (VP (VBG using) (NP (NN search) (NNS queries)) (PP (IN from) (S (VP (NN search) (NP (NN history))))))))) (. .)))
Utilize the Entrez API in design.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB display) (NP (NN trend)) (PP (IN over) (NP (NP (NN time)) (PP (IN for) (NP (NP (NN frequency)) (PP (IN of) (NP (NN term))))))))) (. .)))
Utilize W3C Activity Stream in design.
(ROOT (S (NP (NNP Admin)) (VP (MD shall) (VP (VB mark) (NP (VB report)) (SBAR (IN as) (S (NP (NP (VBN solved)) (, ,) (PP (IN on) (NP (NN -hold))) (, ,) (CC or) (ADJP (VBN closed))) (. .)))))))
System shall have following user types of user or admin.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB remove) (NP (NP (NP (NNS tags)) (NP (DT those))) (SBAR (S (NP (PRP they)) (VP (VBD tagged) (PP (IN from) (NP (NN document))))))))) (. .)))
User shall have private or public profile.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB notify) (NP (NN user)) (SBAR (WHADVP (WRB when)) (S (NP (DT a) (JJ new) (VBN saved) (NML (NN search) (NN query)) (NN result)) (VP (VBZ is) (ADJP (JJ present))))))) (. .)))
User shall have public profile by default.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB notify) (NP (NN user)) (SBAR (WHADVP (WRB when)) (S (NP (DT a) (VBN followed) (NN tag)) (VP (VBZ is) (VP (VBN tagged) (PP (IN on) (NP (DT a) (NN document))))))))) (. .)))
User activity shall remain anonymous if user has private profile.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB sort) (NP (NP (NNS results)) (PP (IN by) (NP (NP (JJ alphabetical) (NN order)) (PP (IN of) (NP (NP (NNS titles)) (, ,) (NP (NN A-) (NN Z) (, /) (NN Z-A))))))))) (. .)))   
Author name shall remain anonymous if the tag author has private profile.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB sort) (NP (NNS results)) (PP (IN by) (NP (NP (JJ alphabetical) (NN order)) (PP (IN of) (NP (NP (NP (NN author) (POS 's)) (JJ last) (NN name)) (, ,) (NP (NNP A-) (NNP Z) (, /) (NNP Z-A)))))))) (. .)))
```


***requirements.txt***

```
User shall search documents using keywords.
User shall search documents using multi-term keywords.
User shall search documents by author name.
User shall search documents by tags.
System shall display description of semantic tags on search UI.
User shall make sub-class or super-class tag searches.
User shall quick search using search queries from search history.
User shall sort results by date, ascending or descending.
User shall sort results by alphabetical order of titles, A-Z / Z-A.
User shall sort results by alphabetical order of author's last name, A-Z / Z-A.
User shall read documents.
User shall create tags.
User shall tag documents.
User shall report misuse of a tag.
User shall remove tags those they tagged from document.
System shall inform the tagger first when reported.
Admin shall remove tag from the document when reported.
System shall display description of tag on-site.
User shall request signup.
Admin shall approve/decline signup request.
System shall inform applicant in case of signup request declination.
System shall email applicant in case of signup request approval.
System shall display frequency of term.
System shall display statistics for document.
System shall display trend over time for frequency of term.
User shall switch between public and private profiles.
User shall browse search history.
User shall send report to admin.
User shall list followed tags.
User shall list saved search queries.
User shall follow tags.
User shall save search queries.
System shall notify user when a followed tag is tagged on a document.
System shall notify user when a new saved search query result is present.
Admin shall display unresolved reports.
Admin shall mark report as solved, on-hold, or closed.
System shall inform user when an admin resolves the report.
Utilize the Entrez API in design.
Utilize W3C Activity Stream in design.
System shall have following user types of user or admin.
User shall have private or public profile.
User shall have public profile by default.
User activity shall remain anonymous if user has private profile.
Author name shall remain anonymous if the tag author has private profile.
```

***main.py***
```
import stanza
import os.path

# @inproceedings{qi2020stanza,
#     title={Stanza: A {Python} Natural Language Processing Toolkit for Many Human Languages},
#     author={Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
#     booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"",
#     year={2020}
# }

stanza.download('en')

nlp = stanza.Pipeline(lang='en', processors=""tokenize, pos, constituency"") 
# for purposes of processors, check https://stanfordnlp.github.io/stanza/pipeline.html

script_path = os.path.abspath(os.path.dirname(__file__))
document_path = os.path.join(script_path, ""..\\resources\\requirements\\requirements.txt"")
doc = nlp(open(document_path, ""r"").read()) # run annotation over a document

for i, sentence in enumerate(doc.sentences):
  print(sentence.text)
  print(sentence.constituency)
```

**To Reproduce**
Steps to reproduce the behavior:
1. Create requirements.txt file under resources/requirements/ folder in root project path
2. Copy content above inside the requirements.txt 
3. Run main.py
4. Check terminal output

**Expected behavior**
Following output is the expected behaviour where contituency trees nodes does not differ from the tokens of a sentence. Notice the correct order of constituency output matching the sentence content.

```
User shall search documents using keywords.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NP (NNS documents)) (VP (VBG using) (NP (NNS keywords)))))) (. .)))
User shall search documents using multi-term keywords.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NP (NNS documents)) (VP (VBG using) (NP (JJ multi-term) (NNS keywords)))))) (. .)))
User shall search documents by author name.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NP (NNS documents)) (PP (IN by) (NP (NN author) (NN name)))))) (. .)))
User shall search documents by tags.
(ROOT (S (NP (NN User)) (VP (MD shall) (VP (VB search) (NP (NNS documents)) (PP (IN by) (NP (NNS tags))))) (. .)))
System shall display description of semantic tags on search UI.
(ROOT (S (NP (NN System)) (VP (MD shall) (VP (VB display) (NP (NP (NN description)) (PP (IN of) (NP (NP (JJ semantic) (NNS tags)) (PP (IN on) (NP (NN search) (NN UI)))))))) (. .)))
```

**Environment:**
 - Windows 11 - 64 Bit
 - Python version: 3.9 
 - VsCode 
 - virtualenv -> pip list 
```
Package            Version
------------------ ---------
certifi            2021.10.8
charset-normalizer 2.0.12
colorama           0.4.4
emoji              1.7.0
idna               3.3
numpy              1.22.3
pip                22.0.4
protobuf           3.20.0
requests           2.27.1
setuptools         60.9.3
six                1.16.0
stanza             1.3.0
torch              1.11.0
tqdm               4.64.0
typing_extensions  4.1.1
urllib3            1.26.9
wheel              0.37.1
```

**Additional context**
- Input document contains '/' , '(' , ')' , and sentences finishes with . and seperated by '\n'.
- I also increased the terminal integrated scrollback setting in vscode to make sure output is not rolled-back by vscode.
",
300,2022-04-08T08:27:36Z,https://github.com/stanfordnlp/stanza/pull/996,,"Read/write NER on the Tokens

TODO: perhaps this needs to be on the Words instead",
301,2022-04-07T16:41:23Z,https://github.com/stanfordnlp/stanza/pull/995,,"Process sentence ids from the corpus, if available.  Change sentence.id to a string (and incidentally make it match the documentation regarding being 1 indexed)

",
302,2022-04-05T11:04:22Z,https://github.com/stanfordnlp/stanza/issues/992,,"sorry for flooding the board with my issues, but here's another one with Stanza main branch.
Please note that this is raised with English default model: I have processed files of the same size and of the same sentence length with models for a dozen of languages and it worked fine.

```
2022-04-05 10:46:47 DEBUG: Processing 5597 sentences
2022-04-05 10:48:26 ERROR: Went infinite!:
Final state:
```
Then there is the content of the buffer, which corresponds to a very long sentence (419 tokens as per UDpipe parsing). I repeat, I have the same sentence length in other languages (it's a parallel corpus of translations) and it worked smoothly.

```
Traceback (most recent call last):
  File ""/home/ec2-user/tools/process/ud-stanza.py"", line 238, in <module>
    main()
  File ""/home/ec2-user/tools/process/ud-stanza.py"", line 233, in main
    udparser(nlp,text,filename)
  File ""/home/ec2-user/tools/process/ud-stanza.py"", line 175, in udparser
    doc = nlp(text)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 231, in __call__
    doc = self.process(doc)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 225, in process
    doc = process(doc)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/pipeline/constituency_processor.py"", line 51, in process
    document.set(CONSTITUENCY, trees, to_sentence=True)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/common/doc.py"", line 237, in set
    assert len(self.sentences) == len(contents), \
AssertionError: Contents must have the same length as the sentences
```
Btw, how can I try with other English models, such as GUM?

System is an AWS EC2 machine with a Tesla n60: Deep Learning AMI GPU PyTorch 1.11.0 (Amazon Linux 2) , Python 3 .9.4
",
303,2022-04-04T17:50:09Z,https://github.com/stanfordnlp/stanza/pull/991,,Processing tool for the original VIT constituency treebank,
304,2022-04-04T15:10:27Z,https://github.com/stanfordnlp/stanza/issues/990,,"I was working on a project which involves dependency parsing of Hindi sentences and then performing ""further operations"" on it. I recently tried to replicate some of my previous results but I was surprised to see different results for the same sentences I used previously. My code for ""further operations"" is rule-based and unchanged, yet my results were different.

Fortunately I saved some of the dependency trees from my previous experiment, and I noticed that the stanza tool generates slightly different dependency trees now. It usually happens with long sentences. For example consider the following sentence:

```
कालांतर में इसके हजारों प्रदर्शन हुए और पड़ोसी देश rajsthan , बांग्लादेश और पाकिस्तान में भी इसकी अनेक प्रस्‍तुतियां हुईं ।
```
The old dependency tree (generated on September 2021) for this sentence was as follows:
```
                                                            5_हुए                                                                                               
                                                              root                                                                                              
                                                              VERB                                                                                              
   ____________________________________________________________|___________________________                                                                      
  |         |                    |                                                     19_हुईं                                                                  
  |         |                    |                                                        conj                                                                  
  |         |                    |                                                        VERB                                                                  
  |         |                    |                     ____________________________________|________________________________________________                     
  |         |                    |                    |                               9_rajsthan                                            |                   
  |         |                    |                    |                                                                                     |                   
  |         |                    |                    |                                   obl                                               |                   
  |         |                    |                    |                                  PROPN                                              |                   
  |         |                    |                    |         ___________________________|_____________                                   |                    
  |       0_कालांतर             4_प्रदर्शन                 |     8_देश         11_बांग्लादेश                  13_पाकिस्तान                           18_प्रस्‍तुतियां            
  |        obl                                        |       nmod                                                                                              
  |        NOUN               compound                |       NOUN          conj                        conj                             compound               
  |         |                   NOUN                  |        |           PROPN                       PROPN                               NOUN                 
  |         |          __________|___________         |        |             |              _____________|___________          _____________|______________      
20_।       1_में      2_इसके                  3_हजारों    6_और   7_पड़ोसी        10_,         12_और           14_में       15_भी   16_इसकी                      17_अनेक  
punct      case      nmod                  nummod     cc      amod         punct           cc           case        dep      nmod                         det   
PUNCT      ADP       PRON                   NUM     CCONJ     ADJ          PUNCT         CCONJ          ADP         PART     PRON                         DET   

```

But the new dependency tree (generated on 28 March 2022) is as follows:
```
                                                              5_हुए                                                                                             
                                                              root                                                                                             
                                                              VERB                                                                                             
   ____________________________________________________________|___________________________                                                                    
  |         |                    |                                                       19_हुईं                                                                 
  |         |                    |                                                        conj                                                                 
  |         |                    |                                                        VERB                                                                 
  |         |                    |                     ____________________________________|________________________________________________                   
  |         |                    |                    |                               9_rajsthan                                            |                  
  |         |                    |                    |                                                                                     |                  
  |         |                    |                    |                                   obl                                               |                  
  |         |                    |                    |                                  PROPN                                              |                  
  |         |                    |                    |         ___________________________|_____________                                   |                  
  |       0_कालांतर            4_प्रदर्शन                  |       8_देश         11_बांग्लादेश                13_पाकिस्तान                          18_प्रस्‍तुतियां              
  |        obl                                        |       nmod                                                                                             
  |        NOUN               compound                |       NOUN          conj                        conj                              nsubj                
  |         |                   NOUN                  |        |           PROPN                       PROPN                               NOUN                
  |         |          __________|___________         |        |             |              _____________|___________          _____________|______________    
20_।        1_में     2_इसके                3_हजारों      6_और   7_पड़ोसी        10_,            12_और         14_में      15_भी    16_इसकी                      17_अनेक 
punct      case      nmod                  nummod     cc      amod         punct           cc           case        dep      nmod                         det  
PUNCT      ADP       PRON                   NUM     CCONJ     ADJ          PUNCT         CCONJ          ADP         PART     PRON                         DET  

```

Notice the word with index=18 i.e. ```18_प्रस्‍तुतियां```, in the old dependency tree its dependency relation is ```compound``` but in the new dependency tree its dependency relation is ```nsubj```.

I downgraded my stanza library to the version which was released in Aug 2021. Yet it generates the new dependency tree.

What might be the reason behind this behavior? ",
305,2022-04-04T15:03:23Z,https://github.com/stanfordnlp/stanza/issues/989,,"This is a known problem with pytorch (I guess). 
I got this while working with several text files in different directory (different languages).  It occurs just with some directories, usually after having processed the first file, then my script just passes to another directory. I suspect there is something with the models, maybe some 'exotic' processors like NER or constituency. So far, problematic languages are da, en, it with their default models...

```
Traceback (most recent call last):
  File ""/home/ec2-user/tools/process/ud-stanza.py"", line 236, in <module>
    main()
  File ""/home/ec2-user/tools/process/ud-stanza.py"", line 231, in main
    udparser(nlp,text,filename)
  File ""/home/ec2-user/tools/process/ud-stanza.py"", line 173, in udparser
    doc = nlp(text)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 386, in __call__
    return self.process(doc, processors)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 382, in process
    doc = process(doc)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/pipeline/constituency_processor.py"", line 66, in process   
    trees = trainer.parse_tagged_words(self._model.model, words, self._batch_size)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/trainer.py"", line 717, in parse_tagged_wor
ds
    treebank = parse_sentences(sentence_iterator, build_batch_from_tagged_words, batch_size, model)
  File ""/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/trainer.py"", line 692, in parse_sentences
    horizon_batch = build_batch_fn(batch_size, data_iterator, model)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/trainer.py"", line 637, in build_batch_from
_tagged_words
    tree_batch = parse_transitions.initial_state_from_words(tree_batch, model)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/parse_transitions.py"", line 130, in initia
l_state_from_words
    return initial_state_from_preterminals(preterminal_lists, model, gold_trees=None)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/parse_transitions.py"", line 110, in initia
l_state_from_preterminals
    word_queues = model.initial_word_queues(preterminal_lists)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/lstm_model.py"", line 648, in initial_word_
queues
    partitioned_embeddings = self.partitioned_transformer_module(None, all_word_inputs)
  File ""/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/partitioned_transformer.py"", line 302, in 
forward
encoder_in = self.add_timing(self.pattention_morpho_emb_dropout(extra_content_annotations))
  File ""/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ec2-user/.local/lib/python3.9/site-packages/stanza/models/constituency/positional_encoding.py"", line 48, in forward
    out = torch.cat([x, timing], dim=-1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper___cat)
```
I am on a AWS EC2 machine with a Tesla n60: Deep Learning AMI GPU PyTorch 1.11.0 (Amazon Linux 2) , Python 3 .9.4

",
306,2022-04-04T02:37:17Z,https://github.com/stanfordnlp/stanza/issues/988,,"Hi, I want to use stanza to do coreference resolution? However, I can not find description about how to use stanza to do coreference resoluton? Can someone give me some advice? I just find two other project which can do this, such as Coreferee(in https://github.com/msg-systems/coreferee) and nerualcoref(https://github.com/huggingface/neuralcoref)",
307,2022-04-01T10:19:06Z,https://github.com/stanfordnlp/stanza/issues/987,,"Similarly to the start_char and end_char information, it would be useful to write down annotations such as NER in the MISC column while calling convert_token_dict()
Now the default is start_char/end_char, but maybe it would be better to have this as an option, something like

calling_convert_token_dict(misc)

where misc is a list of annotations such as ner, start_char/end_char or multi_ner
",
308,2022-03-31T16:50:17Z,https://github.com/stanfordnlp/stanza/issues/986,,"I got this error while running stanza

import stanza
from stanza.pipeline.core import Pipeline
from stanza.pipeline.constituency_processor import ConstituencyProcessor
import stanza.models.constituency.trainer as trainer
from stanza.server.parser_eval import EvaluateParser
from stanza.protobuf import to_text
from stanza.pipeline.core import Pipeline
from stanza.pipeline.constituency_processor import ConstituencyProcessor
mport stanza.models.constituency.trainer as trainer
from stanza.server.parser_eval import EvaluateParser
from stanza.protobuf import to_text
AttributeError: 'NoneType' object has no attribute 'enum_types_by_name'
",
309,2022-03-31T10:37:45Z,https://github.com/stanfordnlp/stanza/issues/985,,"I am comparing the corenlp wrapper with stanza for sentiment analysis but they output different results for many cases. 
Any Idea why?
The code- 

from pycorenlp import StanfordCoreNLP
text = 'Ein Neujahrsvorsatz könnte sein dass die Regierung nie vergißt, wofür das Digitale ist für alle Menschen, groß und klein.und dass sie Euch mit einbezieht und nicht nur auf die Wirtschaft schielt.'
nlp = StanfordCoreNLP('http://localhost:9000')
res = nlp.annotate(text,
                   properties={
                       'annotators': 'sentiment',
                       'outputFormat': 'json',
                       'timeout': 10000,
                           })

for s in res[""sentences""]:
    print(""%d: '%s': %s %s"" % (
        s[""index""],
        "" "".join([t[""word""] for t in s[""tokens""]]),
        s[""sentimentValue""], s[""sentiment""]))

Output - 
 'Ein Neujahrsvorsatz könnte sein dass die Regierung nie vergißt , wofür das Digitale ist für alle Menschen , groß und klein.und dass sie Euch mit einbezieht und nicht nur auf die Wirtschaft schielt .': 1 Negative

import stanza
text = 'Ein Neujahrsvorsatz könnte sein dass die Regierung nie vergißt, wofür das Digitale ist für alle Menschen, groß und klein.und dass sie Euch mit einbezieht und nicht nur auf die Wirtschaft schielt.'

# 0 is negative, 1 is neutral, 2 is positive https://stanfordnlp.github.io/stanza/sentiment.html

nlp = stanza.Pipeline(lang='de', processors='tokenize,sentiment')
doc = nlp(text)
for a, sentence in enumerate(doc.sentences):
    print(sentence.sentiment)

Output - 
1 




",
310,2022-03-25T07:14:13Z,https://github.com/stanfordnlp/stanza/issues/984,,"**Describe the bug**
Unable to load Constituency Parser

**To Reproduce**
Steps to reproduce the behavior:
`import stanza
STANZA_PIPELINE = stanza.Pipeline('en',processors='tokenize,lemma,pos,constituency',tokenize_pretokenized=True,tokenize_no_ssplit=True)

doc = STANZA_PIPELINE(tokens)`

**Expected behavior**
I expect a pipeline would be successfully initiated

**Environment (please complete the following information):**
 - OS: MacOS (Apple Silicon M1 Max)
 - Python version: Conda Python 3.9.7
 - Stanza version: Stanza 1.2.3

 - **Additional context**
Output:
`---------------------------------------------------------------------------
UnknownProcessorError                     Traceback (most recent call last)
/var/folders/f_/rfsknh_x4xq34cmy4ntsfgj40000gn/T/ipykernel_1032/920197552.py in <module>
      1 import stanza
----> 2 STANZA_PIPELINE = stanza.Pipeline('en',
      3                                   processors='tokenize,lemma,pos,constituency',
      4                                   tokenize_pretokenized=True,
      5                                   tokenize_no_ssplit=True)

~/miniconda3/envs/stanza/lib/python3.9/site-packages/stanza/pipeline/core.py in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)
    102             and MWT not in processors):
    103             add_mwt(processors, resources, lang)
--> 104         self.load_list = maintain_processor_list(resources, lang, package, processors) if lang in resources else []
    105         self.load_list = add_dependencies(resources, lang, self.load_list) if lang in resources else []
    106         self.load_list = self.update_kwargs(kwargs, self.load_list)

~/miniconda3/envs/stanza/lib/python3.9/site-packages/stanza/resources/common.py in maintain_processor_list(resources, lang, package, processors)
    178             assert(isinstance(key, str) and isinstance(value, str))
    179             if key not in PIPELINE_NAMES:
--> 180                 raise UnknownProcessorError(key)
    181             # check if keys and values can be found
    182             if key in resources[lang] and value in resources[lang][key]:

UnknownProcessorError: Unknown processor type requested: constituency`
",
311,2022-03-24T22:35:25Z,https://github.com/stanfordnlp/stanza/pull/983,,https://github.com/stanfordnlp/stanza/discussions/918,
312,2022-03-23T06:06:56Z,https://github.com/stanfordnlp/stanza/issues/982,,"
it would be nice if Stanza is made native to the apple silicon, especially if stanza can leverage on Apple's matrix co-processor(s) called AMX or even the GPU/neural engine.  Too bad PyTorch is not optimised to use all computing resources on Apple Silicon.

Thanks.",
313,2022-03-23T00:23:40Z,https://github.com/stanfordnlp/stanza/pull/981,,"Unrolls the recursion in the chu-liu-edmonds spanning tree

note: very large trees take a long time anyway",
314,2022-03-21T16:29:35Z,https://github.com/stanfordnlp/stanza/pull/980,,"Update all of the processors to get the EmbeddingCache from the pipeline

Cache the pretrain as well (this doesn't seem to change much in terms of memory usage, though)
",
315,2022-03-14T15:07:27Z,https://github.com/stanfordnlp/stanza/issues/977,,The table 'Available NER Models' https://stanfordnlp.github.io/stanza/available_models.html#available-ner-models shows dataset FBK for language Myanmar. I think it should be dataset: UCSY,
316,2022-03-11T23:16:17Z,https://github.com/stanfordnlp/stanza/pull/976,,"Refactor filter data code and extractions stanza/models/common/bert_embedding.py
this will make it easier to add bert embeddings to the bottom layer of other models

TODO: process long sentences rather than discarding or having an error
TODO: put bert tokenizers and models into a separate module so that when the same bert
is used in multiple models, it doesn't have to be loaded twice
",
317,2022-03-11T08:13:35Z,https://github.com/stanfordnlp/stanza/pull/975,,"Add an interface for turning pretokenized text into a Document

answers https://github.com/stanfordnlp/stanza/issues/967",
318,2022-03-09T07:02:03Z,https://github.com/stanfordnlp/stanza/pull/974,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
- Fixed based on feedback given on previous closed PR
- 
## Fixes Issues
- Refactor data filtering code and move it to bert_embedding.py
- Fixed small bugs
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
319,2022-03-09T06:45:49Z,https://github.com/stanfordnlp/stanza/pull/973,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
Fixed based on the feedback on previous PR. 

## Fixes Issues
- Refactor the data filter function
- Fixed small bugs

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
320,2022-03-08T20:28:39Z,https://github.com/stanfordnlp/stanza/pull/972,,"Simplify the unary transitions, since there doesn't seem to be any benefit to the more complicated method anyway",
321,2022-03-06T04:43:30Z,https://github.com/stanfordnlp/stanza/issues/971,,"**Describe the bug**
Slovak module does not handle multiwords such as ""naňho""

**To Reproduce**

```py
>>> import stanza
>>> stanza.download(""sk"")
>>> nlp=stanza.Pipeline(""sk"")
>>> doc=nlp(""Ten naňho spadol a zabil ho."")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""~/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 231, in __call__
    doc = self.process(doc)
  File ""~/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 225, in process
    doc = process(doc)
  File ""~/.local/lib/python3.9/site-packages/stanza/pipeline/depparse_processor.py"", line 51, in process
    sentence.build_dependencies()
  File ""~/.local/lib/python3.9/site-packages/stanza/models/common/doc.py"", line 555, in build_dependencies
    assert(word.head == head.id)
AssertionError
```

**Expected behavior**
""naňho"" should be split into two words ""na neho""

**Environment (please complete the following information):**
 - OS: Debian
 - Python version: Python 3.9.2
 - Stanza version: 1.3.0
",
322,2022-03-02T01:29:40Z,https://github.com/stanfordnlp/stanza/issues/970,,"**Describe the bug**
Running the constituency parser on a doc with `n` sentences can result in the `i`th Sentence object getting the constituency parse of the `n-i-1`th sentence.

**To Reproduce**
Steps to reproduce the behavior:
1. Run `stanza.download('en')`
2. Run this code (full code in  [this gist](https://gist.github.com/nnkennard/574bf87125fe6018197490d40ca5125a)) :

```
text = # Long text is in the gist
import stanza
STANZA_PIPELINE = stanza.Pipeline('en',
                                  processors='tokenize,lemma,pos,constituency',
                                  tokenize_pretokenized=True,
                                  tokenize_no_ssplit=True)
doc = STANZA_PIPELINE(text)
for sentence in doc.sentences:
    print("" "".join([token.text for token in sentence.tokens]))
    print(sentence.constituency)
    print()
```

3. Output:

```
CHAPTER I TREATS OF THE PLACE...
(ROOT (S (SBAR (IN Although) (S (NP (PRP I)) (VP (VBP am) (RB not) (VP (VBN disposed)...

For a long time after it was ushered into this world of sorrow and trouble...
(ROOT (S (S (PP (IN For) (NP (NP (DT a) (JJ long) (NN time)) (SBAR (IN after) (S (NP (PRP it)) (VP (VBD was) (VP (VBN ushered) (PP (IN into) (NP (NP (DT this) (NN world)) (PP (IN of) (NP (NN sorrow) (CC and) (NN trouble)))))))))))...

Although I am not disposed...
(ROOT (S (S (NP (NP (NN CHAPTER)) (SBAR (S (NP (PRP I)) (VP (VBZ TREATS) (PP (IN OF) (NP (NP (DT THE) (NN PLACE))...
```

**Expected behavior**
The parses should attach to the correct sentences; output should look like:

```
CHAPTER I TREATS OF THE PLACE...
(ROOT (S (S (NP (NP (NN CHAPTER)) (SBAR (S (NP (PRP I)) (VP (VBZ TREATS) (PP (IN OF) (NP (NP (DT THE) (NN PLACE))...

For a long time after it was ushered into this world of sorrow and trouble...
(ROOT (S (S (PP (IN For) (NP (NP (DT a) (JJ long) (NN time)) (SBAR (IN after) (S (NP (PRP it)) (VP (VBD was) (VP (VBN ushered) (PP (IN into) (NP (NP (DT this) (NN world)) (PP (IN of) (NP (NN sorrow) (CC and) (NN trouble)))))))))))...

Although I am not disposed...
(ROOT (S (SBAR (IN Although) (S (NP (PRP I)) (VP (VBP am) (RB not) (VP (VBN disposed)...

```

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: 3.9.7
 - Stanza version: 1.3.0

**Additional context**
* The problem persists even if I use a Stanza tokenization model, but I added `tokenize_pretokenized` and `tokenize_no_ssplit` because I want to use the coref labels from [LitBank](https://github.com/dbamman/litbank/blob/master/coref/conll/730_oliver_twist_brat.conll).
* I cannot reproduce this with shorter sentences.
* This does also happen with the texts below, so it is not a Charles Dickens problem or a LitBank/preprocessing problem.
  * `Phase the First : The Maiden I On an evening in the latter part of May a middle-aged man was walking homeward from Shaston to the village of Marlott , in the adjoining Vale of Blakemore , or Blackmoor .\n\nThe pair of legs that carried him were rickety , and there was a bias in his gait which inclined him somewhat to the left of a straight line .\n\nHe occasionally gave a smart nod , as if in confirmation of some opinion , though he was not thinking of anything in particular .`
  * `I will now proceed to write a very long sentence, one that contains many asides, with the hope of triggering whatever it is that caused the errors in the sentences I tried earlier, both of which came from novels in the LitBank dataset.\n\nAlas, I am not able to match the prolixity of Charles Dickens; nor, in fact, did I ever hope or intend to -- it is my desire merely to determine the exact conditions that cause the issues I have encountered, with the hope that these conditions are not somehow intrinsically linked to a problem with CoNLL formatting. It is very difficult to write long example sentences about nothing in particular; I have much to say about matters of importance, but none are relevant in this context.`",
323,2022-02-28T09:23:15Z,https://github.com/stanfordnlp/stanza/issues/969,,"I am writing to ask of a way to unload the stanza model from the GPU memory.
I want to load a separate pytorch model onto the GPU memory after running a stanza model. However this causes an out of memory error since it seems that the stanza model remains in the GPU memory after execution.
I have checked the documentation and github issues, but I could not find a way to do so. I apologies in advance if this issue is discussed in them.

Thank you",
324,2022-02-27T14:14:47Z,https://github.com/stanfordnlp/stanza/issues/968,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
325,2022-02-25T13:31:07Z,https://github.com/stanfordnlp/stanza/issues/967,,"Hi, thanks for the feature `tokenize_pretokenized=True` in the Pipeline.

My situation is that I have a **text** and the **tokens** of the text. I'd like to use my tokens and do the analysis on them. The problem is that the resulting text after using my tokens is different from my text in term of **whitespaces**. I'd like to respect my text and so the `start_char` and `end_char` of the tokens should be aligned wiht it. 

For example:
```python
import stanza

text = ""Hello.""
tokens = [[""Hello"","".""]]

nlp = stanza.Pipeline(""en"",tokenize_pretokenized=True)
doc = nlp(tokens)
print(doc.text)
print(text)
```

and the output is:
```
'Hello .'
'Hello.'
```

The text is re-created with whitespace between all tokens. I imagine that with tokens like `[[""Hello.""]]` this is fixed, but I have to keep tokens and text fixed. I could use my text, but the character indexes of the words should be wrong.

How can I solve this problem? ",
326,2022-02-23T06:54:23Z,https://github.com/stanfordnlp/stanza/pull/966,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
327,2022-02-23T03:43:59Z,https://github.com/stanfordnlp/stanza/pull/965,,"Check corenlp model file existence before downloading, without just re-download it.",
328,2022-02-22T20:25:58Z,https://github.com/stanfordnlp/stanza/pull/964,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
329,2022-02-22T05:52:06Z,https://github.com/stanfordnlp/stanza/issues/963,,"After installing `stanza`, I'm just trying to download the `en` model using the following code:
```
stanza.download('en')  # download English model
```

And I keep getting the following error.

```
/Users/[my_username]/anaconda3/envs/bionlp/bin/python /Users/[my_username]/PycharmProjects/bio-nlp/src/stanza_class.py
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json: 142kB [00:00, 5.23MB/s]                    
2022-02-22 00:44:57 INFO: Downloading default packages for language: en (English)...
Traceback (most recent call last):
  File ""/Users/[my_username]/PycharmProjects/bio-nlp/src/stanza_class.py"", line 3, in <module>
    stanza.download('en')  # download English model
  File ""/Users/[my_username]/anaconda3/envs/bionlp/lib/python3.7/site-packages/stanza/resources/common.py"", line 435, in download
    md5=resources[lang]['default_md5'],
  File ""/Users/[my_username]/anaconda3/envs/bionlp/lib/python3.7/site-packages/stanza/resources/common.py"", line 142, in request_file
    download_file(url, path, proxies, raise_for_status)
  File ""/Users/[my_username]/anaconda3/envs/bionlp/lib/python3.7/site-packages/stanza/resources/common.py"", line 118, in download_file
    with open(path, 'wb') as f:
PermissionError: [Errno 13] Permission denied: '/Users/[my_username]/stanza_resources/en/default.zip'
```
I know this is obviously due to the permission access issue, but I wonder what would be a proper way to resolve this issue when downloading the models?",
330,2022-02-22T00:13:19Z,https://github.com/stanfordnlp/stanza/issues/962,,"**Describe the bug**
During the parsing of [this wiki page](https://en.wikipedia.org/wiki?curid=52074018), the parser fails with the following traceback:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 231, in __call__
    doc = self.process(doc)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 225, in process
    doc = process(doc)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/pipeline/depparse_processor.py"", line 45, in process
    preds += self.trainer.predict(b)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/models/depparse/trainer.py"", line 75, in predict
    head_seqs = [chuliu_edmonds_one_root(adj[:l, :l])[1:] for adj, l in zip(preds[0], sentlens)] # remove attachment for the root
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/models/depparse/trainer.py"", line 75, in <listcomp>
    head_seqs = [chuliu_edmonds_one_root(adj[:l, :l])[1:] for adj, l in zip(preds[0], sentlens)] # remove attachment for the root
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/models/common/chuliu_edmonds.py"", line 132, in chuliu_edmonds_one_root
    tree = chuliu_edmonds(scores)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/models/common/chuliu_edmonds.py"", line 98, in chuliu_edmonds
    contracted_tree = chuliu_edmonds(subscores)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/models/common/chuliu_edmonds.py"", line 98, in chuliu_edmonds
    contracted_tree = chuliu_edmonds(subscores)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/models/common/chuliu_edmonds.py"", line 98, in chuliu_edmonds
    contracted_tree = chuliu_edmonds(subscores)
  [Previous line repeated 977 more times]
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/stanza/models/common/chuliu_edmonds.py"", line 91, in chuliu_edmonds
    subscores = np.pad(subscores, ( (0,1) , (0,1) ), 'constant')
  File ""<__array_function__ internals>"", line 180, in pad
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/numpy/lib/arraypad.py"", line 743, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/numpy/lib/arraypad.py"", line 489, in _as_pairs
    x = np.round(x).astype(np.intp, copy=False)
  File ""<__array_function__ internals>"", line 180, in round_
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/numpy/core/fromnumeric.py"", line 3773, in round_
    return around(a, decimals=decimals, out=out)
  File ""<__array_function__ internals>"", line 180, in around
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/numpy/core/fromnumeric.py"", line 3348, in around
    return _wrapfunc(a, 'round', decimals=decimals, out=out)
  File ""/path/pyenv/versions/3.10.2/lib/python3.10/site-packages/numpy/core/fromnumeric.py"", line 52, in _wrapfunc
    bound = getattr(obj, method, None)
RecursionError: maximum recursion depth exceeded while calling a Python object
```
This is probably due to trying to process a dependency tree which is too deep.

**To Reproduce**
See [this gist](https://gist.github.com/taiqihe/f4ed32965a06a3e3899b9c383f31add3)

**Expected behavior**
Realistically, this document is not that useful for NLP purposes, but it would be nice to have an option to limit the max dependency depth or something else that does not cause a run-time error.

**Environment**
 - OS: Ubuntu 20.04
 - Python version: 3.10.2
 - Stanza version: 1.3.0

",
331,2022-02-20T19:01:21Z,https://github.com/stanfordnlp/stanza/pull/961,,"Fix a variety of tagging errors which can occur when tag sequences start or end with I-
",
332,2022-02-20T16:49:22Z,https://github.com/stanfordnlp/stanza/issues/960,,"**Describe the bug**
http://stanza.run is down

**To Reproduce**
Steps to reproduce the behavior:
1. Go to http://stanza.run
2. See 502 Bad Gateway

I've tried with both Safari and Firefox.
",
333,2022-02-18T20:19:01Z,https://github.com/stanfordnlp/stanza/issues/959,,"Hi. I want to use Stanza to extract triples from my dataset dynamically. I am using PyTorch and its facilities, Dataset and DataLoader. I call the following function inside __getitem__ from PyTorch's Dataset class:
```
def extract_triples(text, annotators=[""openie""], properties={}):
    with CoreNLPClient(
        annotators=annotators, properties=properties, be_quiet=True,
    ) as client:
        ann = client.annotate(text)
        triples = []
        for sentence in ann.sentence:
            for triple in sentence.openieTriple:
                triples.append(
                    {
                        ""subject"": triple.subject,
                        ""relation"": triple.relation,
                        ""object"": triple.object,
                    }
                )

    return triples
```
I then add this information to my batch. The problem is that I always get ""Error: unable to start the CoreNLP server on port 9000 (possibly something is already running there)"". If I use the DataLoader with num_workers=0, the processing takes TOO long. I if use more workers, I  will turn on multi-process data and get the previously mentioned error.
Is there a way to reuse the same client instance set on port 9000 or to use a different port if that one is being used? Do you have a better solution? Thanks in advance for your help.
",
334,2022-02-18T08:23:11Z,https://github.com/stanfordnlp/stanza/issues/956,,"Hello Team,
I'd like to know that if I'm using `i2b2` NER model with `mimic` package for clinical notes in a commercial application, what kind of license is applicable?
I could see that the `stanza` library is licensed under [Apache license](https://github.com/stanfordnlp/stanza/blob/main/LICENSE), are all models under the same license?

Thanks in advance",
335,2022-02-17T04:11:08Z,https://github.com/stanfordnlp/stanza/pull/955,,https://github.com/stanfordnlp/stanza/issues/928#issuecomment-1030820465,
336,2022-02-14T22:18:18Z,https://github.com/stanfordnlp/stanza/issues/954,,"I'm using the CoreNLP server inside Stanza for coreference resolution. After I make a successful request to the server in Python, I'd like to make use of the result. Can someone explain to me what the result means? Is there a similar API documentation to the native Stanza [objects](https://stanfordnlp.github.io/stanza/data_objects.html) for Python?",
337,2022-02-11T10:47:57Z,https://github.com/stanfordnlp/stanza/issues/952,,"Hi everyone!
I noticed that since v1.2.0 (https://github.com/stanfordnlp/stanza/releases) English and Italian models combine different treebanks ""and a custom dataset including MWT tokens"". Now, italian pronouns - both MWT and single tokens - are lemmatized, using combined model, differently than in treebanks (i.e. ""lo"" > ""lui"", where ISDT, VIT etc. have ""lo"" > ""lo""). Therefore, when you try to train a new model using UD treebanks you will have a lot of discrepancies, about pronouns, between training corpus and model to be trained. The question is: when you created combined models, have you thought about how to get around this problem?

Thank you in advance for your reply!",
338,2022-02-10T21:40:48Z,https://github.com/stanfordnlp/stanza/issues/951,,"**Describe the bug**
I'm trying to install stanza with pip. The default installed version is 0.3, but the latest version is 1+.

**To Reproduce**
```
$ pip install stanza -U
Defaulting to user installation because normal site-packages is not writeable
Collecting stanza
  Using cached stanza-1.3.0-py3-none-any.whl (432 kB)
Requirement already satisfied: requests in c:\users\songy\appdata\roaming\python\python310\site-packages (from stanza) (2.27.1)
Requirement already satisfied: tqdm in c:\users\songy\appdata\roaming\python\python310\site-packages (from stanza) (4.62.3)
Collecting protobuf
  Downloading protobuf-3.19.4-cp310-cp310-win_amd64.whl (895 kB)
     ---------------------------------------- 895.5/895.5 KB 1.2 MB/s eta 0:00:00
Collecting emoji
  Downloading emoji-1.6.3.tar.gz (174 kB)
     ---------------------------------------- 174.2/174.2 KB 1.1 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting stanza
  Using cached stanza-1.2.3-py3-none-any.whl (342 kB)
  Using cached stanza-1.2.2-py3-none-any.whl (337 kB)
  Using cached stanza-1.2.1-py3-none-any.whl (334 kB)
  Using cached stanza-1.2-py3-none-any.whl (282 kB)
  Using cached stanza-1.1.1-py3-none-any.whl (227 kB)
  Using cached stanza-1.0.1-py3-none-any.whl (193 kB)
  Using cached stanza-1.0.0-py3-none-any.whl (189 kB)
  Using cached stanza-0.3-py2.py3-none-any.whl (76 kB)
Installing collected packages: stanza
Successfully installed stanza-0.3
```

**Expected behavior**
The newest version should be installed.

**Environment (please complete the following information):**
 - OS: Win 11
 - Python version: Python 3.10
 - Stanza version: 0.3 but should be the latest 1+
",
339,2022-02-09T12:04:29Z,https://github.com/stanfordnlp/stanza/issues/950,,"**Describe the bug**

I am trying to use CoreNLP server using python to analize sentences in spanish.

**To Reproduce**
Steps to reproduce the behavior:


`from stanza.server import CoreNLPClient`

Now I set up the settings:

``` 
propiedades = {
      'tokenize.language': 'es',
      'mwt.mappingFile': 'edu/stanford/nlp/models/mwt/spanish/spanish-mwt.tsv',
      'pos.model': 'edu/stanford/nlp/models/pos-tagger/spanish-ud.tagger',
      'ner.model': 'edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz'
}
```

Launch CoreNLPClient:

```
client = CoreNLPClient(
    annotators=['tokenize', 'ssplit', 'mwt', 'pos', 'lemma', 'ner'], 
    memory='4G', 
    endpoint='http://localhost:9008',
    properties=propiedades,
    be_quiet=False)

client.start()
```

```
text = ""tomar una cerveza en Madrid""
document = client.annotate(text)
```

When I do this, I have the following error:

```
HTTPError                                 Traceback (most recent call last)
~\Anaconda3\lib\site-packages\stanza\server\client.py in _request(self, buf, properties, reset_default, **kwargs)
    454                               timeout=(self.timeout*2)/1000, **kwargs)
--> 455             r.raise_for_status()
    456             return r

~\Anaconda3\lib\site-packages\requests\models.py in raise_for_status(self)
    952         if http_error_msg:
--> 953             raise HTTPError(http_error_msg, response=self)
    954 

HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9008/?properties=%7B%27annotators%27%3A+%27tokenize%2Cssplit%2Cmwt%2Cpos%2Clemma%2Cner%27%2C+%27outputFormat%27%3A+%27serialized%27%7D&resetDefault=false

During handling of the above exception, another exception occurred:

AnnotationException                       Traceback (most recent call last)
C:\Users\JULIAN~1.GAR\AppData\Local\Temp/ipykernel_20888/1505391084.py in <module>
      1 text = ""tomar una cerveza en Madrid""
----> 2 document = client.annotate(text)

~\Anaconda3\lib\site-packages\stanza\server\client.py in annotate(self, text, annotators, output_format, properties, reset_default, **kwargs)
    521         if reset_default is None:
    522             reset_default = False
--> 523         r = self._request(text.encode('utf-8'), request_properties, reset_default, **kwargs)
    524         if request_properties[""outputFormat""] == ""json"":
    525             return r.json()

~\Anaconda3\lib\site-packages\stanza\server\client.py in _request(self, buf, properties, reset_default, **kwargs)
    459                 raise TimeoutException(r.text)
    460             else:
--> 461                 raise AnnotationException(r.text)
    462 
    463     def annotate(self, text, annotators=None, output_format=None, properties=None, reset_default=None, **kwargs):

AnnotationException: java.lang.NullPointerException: null
```

I suppose that there is an error with the annotator, but I don't know how to fix this problem.

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: Python 3.7 from Anaconda

**Additional context**
Add any other context about the problem here.
",
340,2022-02-08T23:03:08Z,https://github.com/stanfordnlp/stanza/issues/949,,"Hi Stanza Developers,

Apologies if this is a conda issue I'm having and not a Stanza issue. BTW, I'm a python/conda newbie.

I get this error when trying to install stanza version 1.3.0 with conda.

```
$ conda-lock -f environment.yml -p osx-64 -p linux-64
Generating lockfile(s) for osx-64...
Could not lock the environment for platform osx-64
The following packages are not available from current channels:

  - stanza==1.3.0

Current channels:

  - https://conda.anaconda.org/conda-forge/osx-64
  - https://conda.anaconda.org/conda-forge/noarch
  - file:///var/folders/tr/1gk6c0011y7_gyx_x7f6qs3hcm9cyq/T/tmpkd0ivcds/osx-64
  - file:///var/folders/tr/1gk6c0011y7_gyx_x7f6qs3hcm9cyq/T/tmpkd0ivcds/noarch

```

Is there a different channel I should be using?

I see the package listed here for both platforms:
https://anaconda.org/stanfordnlp/stanza

Thanks for you help,
John
",
341,2022-02-08T12:45:31Z,https://github.com/stanfordnlp/stanza/issues/948,,"**Describe the bug**

I am trying to use in a parallel way the Stanford NLP library as I have 4,500 Millions of words to lemmatize, so in python is about 100 days calculating.

**To Reproduce**
Steps to reproduce the behavior:

I have the following data:

```
data = ""dog, cat, house""

distData = spark.sparkContext.parallelize(data)
```

Then I import the library and the function is defined to be parallelized

```
import stanza
stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=True)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True)

def stanza(x):
   return stNLP([x])

words = distData.flatMap(stanza)
```

When I execute that code, there is no error, but when I try to print the information of words is not working.

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment (please complete the following information):**
 - OS: Windows 10
 - Python version: Python 3.7 running in a Cluster of AWS
 - Stanza version: 1.3.0

**Additional context**


",
342,2022-02-08T06:16:13Z,https://github.com/stanfordnlp/stanza/pull/947,,,
343,2022-02-07T16:47:46Z,https://github.com/stanfordnlp/stanza/issues/946,,"I am not sure this is a bug, but I am trying to build dependencies for some german text, and what I noticed, is in some instances instead of having a clear ID there is a group (tuple)` {'id': (5, 6), 'text': 'zur', 'start_char': 20, 'end_char': 23},` containing the real word from the sentence following by the calculation, however, these words are not in the original sentence 
```
 {'id': 5,
  'text': 'zu',
  'lemma': 'zu',
  'upos': 'ADP',
  'xpos': 'APPR',
  'head': 7,
  'deprel': 'case'},
 {'id': 6,
  'text': 'der',
  'lemma': 'der',
  'upos': 'DET',
  'xpos': 'ART',
  'feats': 'Case=Dat|Definite=Def|Gender=Fem|Number=Sing|PronType=Art',
  'head': 7,
  'deprel': 'det'},

```
Steps to reproduce the behavior:


```

stanza.download('de')

sentence =  'heute habe Ida erst zur sport'

nlp = stanza.Pipeline('de', processors = ""tokenize,mwt,pos,lemma,depparse"") 
doc = nlp(sentence)
doc.sentences[0].print_dependencies()
sent_dict = doc.sentences[0].to_dict()
```

the full output 


```
[{'id': 1,
  'text': 'heute',
  'lemma': 'heute',
  'upos': 'ADV',
  'xpos': 'ADV',
  'head': 2,
  'deprel': 'advmod',
  'start_char': 0,
  'end_char': 5},
 {'id': 2,
  'text': 'habe',
  'lemma': 'haben',
  'upos': 'AUX',
  'xpos': 'VAFIN',
  'feats': 'Mood=Sub|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',
  'head': 0,
  'deprel': 'root',
  'start_char': 6,
  'end_char': 10},
 {'id': 3,
  'text': 'ida',
  'lemma': 'ida',
  'upos': 'PROPN',
  'xpos': 'NE',
  'feats': 'Case=Nom|Gender=Masc|Number=Sing',
  'head': 2,
  'deprel': 'nsubj',
  'start_char': 11,
  'end_char': 14},
 {'id': 4,
  'text': 'erst',
  'lemma': 'erst',
  'upos': 'ADV',
  'xpos': 'ADV',
  'head': 2,
  'deprel': 'advmod',
  'start_char': 15,
  'end_char': 19},
 {'id': (5, 6), 'text': 'zur', 'start_char': 20, 'end_char': 23},
 {'id': 5,
  'text': 'zu',
  'lemma': 'zu',
  'upos': 'ADP',
  'xpos': 'APPR',
  'head': 7,
  'deprel': 'case'},
 {'id': 6,
  'text': 'der',
  'lemma': 'der',
  'upos': 'DET',
  'xpos': 'ART',
  'feats': 'Case=Dat|Definite=Def|Gender=Fem|Number=Sing|PronType=Art',
  'head': 7,
  'deprel': 'det'},
 {'id': 7,
  'text': 'sport',
  'lemma': 'sport',
  'upos': 'NOUN',
  'xpos': 'NN',
  'feats': 'Case=Dat|Gender=Fem|Number=Sing',
  'head': 2,
  'deprel': 'obl',
  'start_char': 24,
  'end_char': 29}]
```

is there any way of fixing this? ",
344,2022-02-07T16:22:39Z,https://github.com/stanfordnlp/stanza/issues/945,,"I'd like to save a model in memory and **dynamically decide to disable a processor during the execution**. 
For example, I want to have in memory the full model for English with all the processors like this:
```python
import stanza
nlp = stanza.Pipeline(""en"")
```
```
2022-02-07 11:52:01 INFO: Loading these models for language: en (English):
============================
| Processor    | Package   |
----------------------------
| tokenize     | combined  |
| pos          | combined  |
| lemma        | combined  |
| depparse     | combined  |
| sentiment    | sstplus   |
| constituency | wsj       |
| ner          | ontonotes |
============================

2022-02-07 11:52:01 INFO: Use device: cpu
2022-02-07 11:52:01 INFO: Loading: tokenize
2022-02-07 11:52:01 INFO: Loading: pos
2022-02-07 11:52:01 INFO: Loading: lemma
2022-02-07 11:52:01 INFO: Loading: depparse
2022-02-07 11:52:01 INFO: Loading: sentiment
2022-02-07 11:52:02 INFO: Loading: constituency
2022-02-07 11:52:03 INFO: Loading: ner
2022-02-07 11:52:03 INFO: Done loading processors!
```

Then during the execution of the script, **given some conditions**, I want to execute **only POS**, so disabling NER, sentiment and so on, in a way similar to this:

```python
text = ""Hello world""
nlp(text, disable= ""ner,sentiment"") #not existing now
```

At the moment the only way I found was to re-load the model disabling those processors in the beginning, but my need is to execute the full model always but in some occasional times, only POS. So it would be very helpful to put the processors as parameters in the `__call__` function without re-loading the model.

https://github.com/stanfordnlp/stanza/blob/b24d124156911f95e3c5715e9dc9f75c6076619c/stanza/pipeline/core.py#L228-L232",
345,2022-02-07T08:48:18Z,https://github.com/stanfordnlp/stanza/issues/944,,"**Describe the bug**
When I was visualizing SQuAD 1.1 questions' dependency trees, I found that root could have two children.

**To Reproduce**
Steps to reproduce the behavior:
1. Open Google Colab
2. Install stanza in the first cell:
```
!pip install stanza
import stanza
stanza.download('en')
nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')
```
3. Generate sample sentence in the next:
```
import pandas as pd
doc = nlp(""Which Carolina Panthers team member was picked as the team's MVP in 2015?"")
df = pd.DataFrame(data=[[word.head, word.id, word.text, word.deprel] for sent in doc.sentences for word in sent.words], columns=[""head"", ""id"", ""text"", ""deprel""])
display(df)
```
![weird_deptree](https://user-images.githubusercontent.com/36724603/152753271-1ff2de32-1890-40cd-9398-4802cec7d7b1.png)


**Expected behavior**
Root should have only one child to the best of my knowledge.

**Environment (please complete the following information):**
 - OS: Google Colab (Linux?)
 - Python version: 3.7
 - Stanza version: 1.3.0
",
346,2022-02-05T08:25:35Z,https://github.com/stanfordnlp/stanza/pull/943,,"Addresses https://github.com/stanfordnlp/stanza/issues/486

download dependencies automatically when building a pipeline",
347,2022-02-04T05:23:00Z,https://github.com/stanfordnlp/stanza/pull/942,,"Converts the run_charlm.sh script to python (should work under Windows, better fits the rest of the ecosystem)",
348,2022-02-03T14:27:31Z,https://github.com/stanfordnlp/stanza/issues/941,,"Testcase:
````python
import stanza
stanza.download('en')
nlp_pipeline = stanza.Pipeline('en', batch_size=3000, logging_level='WARN', tokenize_pretokenized=True)
nlp_pipeline('Gloria patriit Filio et Spiritui Sancto Sicuteratin Principio Nucat Sempe Etin Cecula Secodorumamen')
````

Console:
````
Traceback (most recent call last):
  File ""/home/login/stz.py"", line 4, in <module>
    nlp_pipeline('Gloria patriit Filio et Spiritui Sancto Sicuteratin Principio Nucat Sempe Etin Cecula Secodorumamen')
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 231, in __call__
    doc = self.process(doc)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 225, in process
    doc = process(doc)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/pipeline/constituency_processor.py"", line 50, in process
    trees = trainer.parse_tagged_words(self._model.model, words, self._batch_size)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/models/constituency/trainer.py"", line 545, in parse_tagged_words
    treebank = parse_sentences(sentence_iterator, build_batch_from_tagged_words, batch_size, model)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/models/constituency/trainer.py"", line 501, in parse_sentences
    tree_batch = build_batch_fn(batch_size, data_iterator, model)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/models/constituency/trainer.py"", line 485, in build_batch_from_tagged_words
    tree_batch = parse_transitions.initial_state_from_words(tree_batch, model)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/models/constituency/parse_transitions.py"", line 139, in initial_state_from_words
    return initial_state_from_preterminals(preterminal_lists, model, gold_trees=None)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/models/constituency/parse_transitions.py"", line 112, in initial_state_from_preterminals
    word_queues = model.initial_word_queues(preterminal_lists)
  File ""/home/login/.local/lib/python3.9/site-packages/stanza/models/constituency/lstm_model.py"", line 289, in initial_word_queues
    raise KeyError(""Constituency parser not trained with tag {}"".format(str(e))) from e
KeyError: ""Constituency parser not trained with tag 'GW'""
````",
349,2022-02-02T15:09:46Z,https://github.com/stanfordnlp/stanza/issues/940,,"I feel caught between wanting to use the new neural models and the java CoreNLP processes:

* I would like to use the quote extraction which is available only in CoreNLP.
* I would like to see the verb features available only in the neural pipeline.

Do you have any thoughts or plans on how to combine these?

### Supporting documentation

#### Quote annotator is not available in neural pipeline

```
In [23]: nlp = stanza.Pipeline(
    ...:     ""en"",
    ...:     processors="","".join(
    ...:         [""tokenize"", ""pos"", ""lemma"", ""depparse"", ""constituency"", ""ner"", ""quote""],
    ...:     ),
    ...: )
---------------------------------------------------------------------------
UnknownProcessorError                     Traceback (most recent call last)
Input In [23], in <module>
----> 1 nlp = stanza.Pipeline(
      2     ""en"",
      3     processors="","".join(
      4         [""tokenize"", ""pos"", ""lemma"", ""depparse"", ""constituency"", ""ner"", ""quote""],
      5     ),
      6 )

File ~/miniforge3/lib/python3.9/site-packages/stanza/pipeline/core.py:106, in Pipeline.__init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)
    102 if (not kwargs.get(""tokenize_pretokenized"")
    103     and TOKENIZE in processors
    104     and MWT not in processors):
    105     add_mwt(processors, resources, lang)
--> 106 self.load_list = maintain_processor_list(resources, lang, package, processors) if lang in resources else []
    107 self.load_list = add_dependencies(resources, lang, self.load_list) if lang in resources else []
    108 self.load_list = self.update_kwargs(kwargs, self.load_list)

File ~/miniforge3/lib/python3.9/site-packages/stanza/resources/common.py:180, in maintain_processor_list(resources, lang, package, processors)
    178 assert(isinstance(key, str) and isinstance(value, str))
    179 if key not in PIPELINE_NAMES:
--> 180     raise UnknownProcessorError(key)
    181 # check if keys and values can be found
    182 if key in resources[lang] and value in resources[lang][key]:

UnknownProcessorError: Unknown processor type requested: quote
```

It works great, of course, in CoreNLP:
```
In [24]: text = ""Bob said, ’What is going on?’""

In [44]: props = {
    ...:     ""quote.singleQuotes"": True,
    ...:     ""quote.asciiQuotes"": True,
    ...:     ""quote.attributeQuotes"": True,
    ...: }

In [44]: with CoreNLPClient(
    ...:     properties=props,
    ...:     annotators=""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,quote"",
    ...:     output_format=""json"",
    ...: ) as client:
    ...:     ann = client.annotate(text)
2022-02-02 15:12:08 INFO: Writing properties to tmp file: corenlp_server-fada384f0d5647df.props
2022-02-02 15:12:08 INFO: Starting server with command: java -Xmx5G -cp /Users/ryan/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-fada384f0d5647df.props -annotators tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,quote -preload -outputFormat json
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - Server default properties:
			(Note: unspecified annotator properties are English defaults)
			annotators = tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,quote
			inputFormat = text
			outputFormat = json
			prettyPrint = false
			quote.asciiQuotes = True
			quote.attributeQuotes = True
			quote.singleQuotes = True
			threads = 5
[...]
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.

In [43]: ann[""quotes""]
Out[43]: 
[{'id': 0,
  'text': '’What is going on?’',
  'beginIndex': 10,
  'endIndex': 29,
  'beginToken': 3,
  'endToken': 9,
  'beginSentence': 0,
  'endSentence': 0,
  'mention': 'Bob',
  'mentionBegin': 0,
  'mentionEnd': 0,
  'mentionType': 'name',
  'mentionSieve': 'trigram CVQ',
  'speaker': 'Bob',
  'speakerSieve': 'automatic name',
  'canonicalSpeaker': 'Bob',
  'canonicalMentionBegin': 0,
  'canonicalMentionEnd': 0}]
```

#### Verb features are not in CoreNLP output

CoreNLP shows for a given verb:

```
In [28]: ann[""sentences""][0][""tokens""][1]
Out[28]: 
{'index': 2,
 'word': 'said',
 'originalText': 'said',
 'lemma': 'say',
 'characterOffsetBegin': 4,
 'characterOffsetEnd': 8,
 'pos': 'VBD',
 'ner': 'O',
 'speaker': 'PER0',
 'before': ' ',
 'after': ''}

In [40]: ann[""sentences""][0][""tokens""][-5]
Out[40]: 
{'index': 6,
 'word': 'is',
 'originalText': 'is',
 'lemma': 'be',
 'characterOffsetBegin': 16,
 'characterOffsetEnd': 18,
 'pos': 'VBZ',
 'ner': 'O',
 'speaker': 'PER0',
 'before': ' ',
 'after': ' '}
```

By comparison, the neural pipeline shows for a given verb:

```
In [29]: nlp = stanza.Pipeline(""en"")
2022-02-02 15:03:41 INFO: Loading these models for language: en (English):
============================
| Processor    | Package   |
----------------------------
| tokenize     | combined  |
| pos          | combined  |
| lemma        | combined  |
| depparse     | combined  |
| sentiment    | sstplus   |
| constituency | wsj       |
| ner          | ontonotes |
============================

2022-02-02 15:03:41 INFO: Use device: cpu
2022-02-02 15:03:41 INFO: Loading: tokenize
2022-02-02 15:03:41 INFO: Loading: pos
2022-02-02 15:03:42 INFO: Loading: lemma
2022-02-02 15:03:42 INFO: Loading: depparse
2022-02-02 15:03:42 INFO: Loading: sentiment
2022-02-02 15:03:42 INFO: Loading: constituency
2022-02-02 15:03:42 INFO: Loading: ner
2022-02-02 15:03:43 INFO: Done loading processors!

In [30]: doc = nlp(text)

In [34]: doc.sentences[0].tokens[1]

Out[34]: 
[
  {
    ""id"": 2,
    ""text"": ""said"",
    ""lemma"": ""say"",
    ""upos"": ""VERB"",
    ""xpos"": ""VBD"",
    ""feats"": ""Mood=Ind|Tense=Past|VerbForm=Fin"",
    ""head"": 0,
    ""deprel"": ""root"",
    ""start_char"": 4,
    ""end_char"": 8,
    ""ner"": ""O""
  }
]

In [39]: doc.sentences[0].tokens[-5]
Out[39]: 
[
  {
    ""id"": 6,
    ""text"": ""is"",
    ""lemma"": ""be"",
    ""upos"": ""AUX"",
    ""xpos"": ""VBZ"",
    ""feats"": ""Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin"",
    ""head"": 7,
    ""deprel"": ""aux"",
    ""start_char"": 16,
    ""end_char"": 18,
    ""ner"": ""O""
  }
]
```

It's those great verb features (mood, number, tense, person, verb form, etc) that I'd love to be able to access. (Or, alternatively, I'd love to access the quote extractor in the neural pipeline.)

Thanks for your help and all your work on this fantastic software!
",
350,2022-02-02T10:45:11Z,https://github.com/stanfordnlp/stanza/issues/939,,"Hi, currently i have a dataset with 1 Lakh sentences. I am trying to get the annotations for each sentences with the following code:

```
import stanza
import os
import psutil


data = open(""1l_test.txt"",""r"").read().splitlines()
#print(len(data))
model =  stanza.Pipeline(""en"", dir=""/home/rkoy/stanza_resources1"", processors=""tokenize,pos,lemma,depparse,ner"")


for i in data:
    doc = model(i)
    for sent in doc.sentences:
      for word in sent.words:
           l = word.xpos    #do something operations here
    with open(""memory_occupied.txt"",""a"") as writer:  # writing the ram used by the process after every sentence inference
         process = psutil.Process(os.getpid())
         writer.write(str(process)+""\t""+str(process.memory_info().rss/1000000)+""\n"")`
```
So while getting the annotations for each sentences, i am also writing the ram occupied by the process to  `memory_occupied.txt`. In the `memory_occupied.txt` i could see that the the ram used by the process is increasing. Is this an expected behaviour of stanza and if Yes then i would like to know the reason for the process occuping more space in ram and is there a way to clear the additional memory occupied by the process.

",
351,2022-02-02T10:26:29Z,https://github.com/stanfordnlp/stanza/issues/938,,I couldn't find Polarity->Negative in features. Where can we get the negation information from ?,
352,2022-02-02T08:05:28Z,https://github.com/stanfordnlp/stanza/pull/937,,What would you do if you had a million dollars? I'll tell you what I'd do: two NER models at the same time,
353,2022-02-02T06:22:24Z,https://github.com/stanfordnlp/stanza/issues/936,,"Hello,

I noticed this https://github.com/stanfordnlp/stanza/commit/b6d83e20a65a8cd46005f96dfa3a6d49d863759b commit, where the relation 'dobj' has been changed to 'obj'. Are there any other relations that have been changed with stanza compared to the previous corenlp java based server?

Thanks.",
354,2022-01-26T08:35:25Z,https://github.com/stanfordnlp/stanza/pull/934,,,
355,2022-01-24T22:01:31Z,https://github.com/stanfordnlp/stanza/issues/933,,"for https://github.com/stanfordnlp/stanza/blob/main/stanza/utils/datasets/prepare_tokenizer_treebank.py,

""else:"" in line 485 is aligned with ""for line in sent:"", really?",
356,2022-01-24T01:25:36Z,https://github.com/stanfordnlp/stanza/pull/932,,"Add a flag which changes the method for composing children into a parent tree.  Add an additional method for doing this, MAX, which simply takes the max value of all the children.  This is surprisingly effective...
",
357,2022-01-19T17:14:36Z,https://github.com/stanfordnlp/stanza/pull/930,,Adds a Turkish NER and con dataset,
358,2022-01-19T09:41:43Z,https://github.com/stanfordnlp/stanza/issues/929,,"Hi, 
first of thanks for the great tool. Helps me a lot! 

Second I found an irregularity or bug or I do something wrong. 
I am trying to do NER on some chinese text. I use the CoreNLPClient for this. 

```python
from stanza.server import CoreNLPClient
teststr = ""告诉 华商 报 记者 : “ 早上 9 %点 多 , 孩子 被 120 急救车 送到 医院 后 , 我们 就 对 他 进行 了 检查 。 发现 他 出生 才 两 天 , 而且 在 前 一 天 , "" \

properties = {""language"": ""zh"",
             # These are hardcoded for English, so if using a different
             # language, this should be set to false.
             ""ner.applyNumericClassifiers"": False,
             ""ner.useSUTime"": False,
                               }
nlp = CoreNLPClient(annotators=['tokenize', 'ssplit', 'pos', 'lemma','ner'],
                    properties=properties,
                    output_format=""json"")
nlp_res = nlp.annotate(teststr)
ents = []
for s in nlp_res[""sentences""]:
    ents.append(s)
print(ents)
```
If you run this code you will see that the ""9 %"" gets annotated as PERCENT. Even though I disabled it with ` ""ner.applyNumericClassifiers"": False`and ` ""ner.useSUTime"": False,`. The other tags seem to get filtered mostly. 

I don't know why that is the case since the [documentation](https://stanfordnlp.github.io/CoreNLP/ner.html) says this should work. 

Also if I discard the properties dictionary and only use ""zh"" the Server call is multiple times faster. 
Any ideas?  
",
359,2022-01-18T08:05:37Z,https://github.com/stanfordnlp/stanza/issues/928,,"I want to run multiple stanza NER models, but i want to run them parallel to each other? how can I do so?
I tried to do this using torch multiprocessing by creating multiple processes and each process run each models but it doesn't seem to go well

`processes = []
for i in range(4): # No. of processes
    p = mp.Process(target=test, args=(model,))
    p.start()
    processes.append(p)
for p in processes: p.join()`",
360,2022-01-15T04:55:44Z,https://github.com/stanfordnlp/stanza/issues/927,,"I am trying to use stanza with spacy to preprocess long text and encounter this error 

```

# -*- coding: UTF-8 -*-
# !/usr/bin/env python3

import random, pickle, os, csv
import re, string
import string
#import stanza
import spacy_stanza
import warnings
warnings.filterwarnings(""error"")
from random import shuffle

# stanza.download('fr')
nlp = spacy_stanza.load_pipeline('fr', processors='tokenize,mwt,pos,lemma')
random.seed(1)

def tokenizer(sentence):

	sent_doc = nlp(sentence)
	wds = [token.text for token in sent_doc if token.pos_ != 'SPACE']
	return wds
	
def lemmatizer(token):

	tok = [token.lemma_ for token in nlp(token)]
	tok_lemme = tok[0]
	#print(tok_lemme)
	
	return tok_lemme

test = ""Là où les vêtements de sport connectés actuels sont axés sur la performance des sportifs, ici, on aura l'occasion pour des amateurs de se rassurer que les mouvements que nous effectuons sont justes. Cela nous évitera bien des mauvaises surprises (douleurs et autres...) au lendemain d'une activité.""

tokenizer(test)

```

error

```
   Là où les vêtements de sport connectés actuels sont axés sur la performance des sportifs, ici, on aura l'occasion pour des amateurs de se rassurer que les mouvements que nous effectuons sont justes. Cela nous évitera bien des mauvaises surprises (douleurs et autres...) au lendemain d'une activité.
Traceback (most recent call last):
  File ""/gpfs7kw/linkhome/rech/genlig01/umg16uw/test/expe_5/substitution/augment.py"", line 93, in <module>
    gen_eda(args.input, output, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, alpha_rd=alpha_rd, num_aug=num_aug)
  File ""/gpfs7kw/linkhome/rech/genlig01/umg16uw/test/expe_5/substitution/augment.py"", line 80, in gen_eda
    aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)
  File ""/gpfs7kw/linkhome/rech/genlig01/umg16uw/test/expe_5/substitution/substitution.py"", line 229, in eda
    words = tokenizer(sentence)
  File ""/gpfs7kw/linkhome/rech/genlig01/umg16uw/test/expe_5/substitution/substitution.py"", line 60, in tokenizer
    sent_doc = nlp(sentence)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/spacy/language.py"", line 998, in __call__
    doc = self.make_doc(text)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/spacy/language.py"", line 1081, in make_doc
    return self.tokenizer(text)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/spacy_stanza/tokenizer.py"", line 83, in __call__
    snlp_doc = self.snlp(text)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 231, in __call__
    doc = self.process(doc)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 225, in process
    doc = process(doc)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/stanza/pipeline/mwt_processor.py"", line 33, in process
    preds += self.trainer.predict(b)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/stanza/models/mwt/trainer.py"", line 79, in predict
    preds, _ = self.model.predict(src, src_mask, self.args['beam_size'])
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/stanza/models/common/seq2seq_model.py"", line 296, in predict
    is_done = beam[b].advance(log_probs.data[b])
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/stanza/models/common/beam.py"", line 86, in advance
    prevK = bestScoresId // numWords
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/torch/_tensor.py"", line 29, in wrapped
    return f(*args, **kwargs)
  File ""/linkhome/rech/genlig01/umg16uw/.conda/envs/bert/lib/python3.9/site-packages/torch/_tensor.py"", line 575, in __floordiv__
    return torch.floor_divide(self, other)
UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)
Exception ignored in: <_io.FileIO name='Test_dolo_augmented.txt' mode='wb' closefd=True>
ResourceWarning: unclosed file <_io.TextIOWrapper name='Test_dolo_augmented.txt' mode='w' encoding='utf-8'>

```

version

```

python : 3.9
torch 1.9
tokenizers 0.10.3 pypi_0 pypi
torchaudio 0.9.0 py39 pytorch
torchvision 0.10.0 py39_cu102 pytorch
spacy 3.1.4 pypi_0 pypi
spacy-legacy 3.0.8 pypi_0 pypi
spacy-stanza 1.0.1 pypi_0 pypi
sparqlwrapper 1.8.5 py39hf3d152e_1005 conda-forge
sqlite 3.35.4 hdfb4753_0
srsly 2.4.2 pypi_0 pypi
stanza 1.3.0 pypi_0 pypi
tensorboard 2.6.0 pyhd8ed1ab_0 conda-forge


```",
361,2022-01-14T11:52:38Z,https://github.com/stanfordnlp/stanza/issues/926,,"**Describe the bug**
When performing lemmatization of certain Finnish expressions, PyTorch emits a UserWarning about the deprecated `__floordiv__` operation. The lemmatization is still working. The UserWarning is only shown once per process/session.

This appears to be quite rare, only certain combinations of words will trigger this. But when processing a large file in Finnish, it will eventually be triggered. I've also done similar lemmatization for long documents in Swedish and English, but never saw this warning with those languages.

**To Reproduce**

This code will trigger the warning for me:

```python
import stanza
nlp = stanza.Pipeline(lang='fi', processors='tokenize,mwt,pos,lemma')
doc = nlp(""ettei se"")
```

Output:

```
2022-01-14 13:39:50 INFO: Loading these models for language: fi (Finnish):
=======================
| Processor | Package |
-----------------------
| tokenize  | tdt     |
| mwt       | tdt     |
| pos       | tdt     |
| lemma     | tdt     |
=======================

2022-01-14 13:39:50 INFO: Use device: cpu
2022-01-14 13:39:50 INFO: Loading: tokenize
2022-01-14 13:39:50 INFO: Loading: mwt
2022-01-14 13:39:50 INFO: Loading: pos
2022-01-14 13:39:51 INFO: Loading: lemma
2022-01-14 13:39:51 INFO: Done loading processors!
[REDACTED]/lib/python3.8/site-packages/stanza/models/common/beam.py:86: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
```

**Expected behavior**
Expected no UserWarning.

**Environment (please complete the following information):**
 - OS: Ubuntu 20.04
 - Python version: 3.8.10 from Ubuntu system package 3.8.10-0ubuntu1~20.04.2
 - Stanza version: 1.3.0 (installed from PyPI in a virtual environment)
 - PyTorch version: 1.10.1 (installed from PyPI in a virtual environment)

**Additional context**

According to the warning message, the problem seems to be this line: https://github.com/stanfordnlp/stanza/blob/e44d1c88340e33bf9813e6f5a6bd24387eefc4b2/stanza/models/common/beam.py#L86

Here is a PR fixing the same warning in another codebase: https://github.com/NVIDIA/MinkowskiEngine/pull/407
",
362,2022-01-12T20:30:06Z,https://github.com/stanfordnlp/stanza/pull/925,,"Add a tsurgeon interface to CoreNLP (will need a 4.3.3 release of CoreNLP)

Also, add a conversion script for the paid licensed Danish Arboretum treebank",
363,2022-01-12T18:52:59Z,https://github.com/stanfordnlp/stanza/pull/924,,"## Description
The start offset for tokens is wrong when the tokenizer has `skip_newline` enabled (e.g., in Chinese) and a token is preceded by whitespace.

## Fixes Issues
N/A

## Unit test coverage
New unit test `test_zh_tokenizer_skip_newline_offsets` added in `stanza/tests/pipeline/test_tokenizer.py`, also reduced the scope of the original `test_zh_tokenizer_skip_newline` to only consider tokenizers.

## Known breaking changes/behaviors
No breaking changes, but the behavior of char offsets should improve for languages that use `skip_newline` in their tokenizers.",
364,2022-01-11T14:14:35Z,https://github.com/stanfordnlp/stanza/issues/922,,"For verb ""نیازمود"" the correct lemma is ""آزما"" and now version 1.2.3 said it's ""نیازمود""
python: 3.8
stanza: 1.2.3
language: 'fa'

So, Please correct it",
365,2022-01-10T20:58:08Z,https://github.com/stanfordnlp/stanza/pull/921,,Recompile the proto with a newer version of the protoc compiler.  The file gets suspiciously smaller...,
366,2022-01-10T12:35:19Z,https://github.com/stanfordnlp/stanza/issues/920,,"I'm finding a strange division of sentences on a Chinese example when there are **double newlines** `\n\n`.

```python
import stanza

nlp = stanza.Pipeline(""zh"")
text = """"""“今日話題 Today’s Topic”是美國洛杉磯AM1300中文廣播電台的王牌節目。自1994年開播以來廣受好評，成為了家喻戶曉、歷史最悠久的海外華語節目之一。“今日話題”的內容包羅萬象，如：與時事相關的政治、法律、社會、文化、家庭、 婚姻、電影、新書等五花八門的內容；同時還有兩位主持人中迅、高寧的分析與點評，他們的獨特風格令節目輕松又有趣！只需要花一點點時間，就能掌握天下大事，而且是必須知道的時事話題！不誇張的說，聽了節目秒變半個時事專家！節目的官方粉絲頁每日更新話題和資訊 

 歡迎各種圍觀轉發！(節目版權所屬MRBI KAZN AM1300中文廣播電台 

 ）""""""

doc = nlp(text)

for i, sent in enumerate(doc.sentences):
      print(f""Sentence {i}: '{sent.text}'"")
```

and the ouptut is:
```
Sentence 0: '“今日話題 Today’s Topic”是美國洛杉磯AM1300中文廣播電台的王牌節目。'
Sentence 1: '自1994年開播以來廣受好評，成為了家喻戶曉、歷史最悠久的海外華語節目之一。'
Sentence 2: '“今日話題”的內容包羅萬象，如：與時事相關的政治、法律、社會、文化、家庭、 婚姻、電影、新書等五花八門的內容；同時還有兩位主持人中迅、高寧的分析與點評，他們的獨特風格令節目輕松又有趣！只需要花一點點時間，就能掌握天下大事，而且是必須知道的時事話題！不誇張的說，聽了節目秒變半個時事專家！節目的官方粉絲頁每日更新話題和資訊'
Sentence 3: '

 歡迎各種圍觀轉發！(節目版權所屬MRBI KAZN AM1300中文廣播電'
Sentence 4: '台'
Sentence 5: '）'
```

I usually didn't find the `\n` in the text of the sentences. I mean: the indexes of the sentences obviously consider them, but in the text, they are not shown in my experience. In this example only in sentence 3 I find them, but for example not in sentences 4 or 5 (even if a double newline was present there too).

For example trying with the English text:
```python
nlp = stanza.Pipeline(""en"")
text=""""""Hello

How are you?

Fine.""""""
doc = nlp(text)

for i, sent in enumerate(doc.sentences):
      print(f""Sentence {i}: '{sent.text}'"")
```

The output should be:
```
Sentence 0: 'Hello'
Sentence 1: 'How are you?'
Sentence 2: 'Fine.'
```
as expected (no `\n\n`)

Am I making a mistake?",
367,2022-01-09T15:27:27Z,https://github.com/stanfordnlp/stanza/issues/919,,"**Describe the bug**
English Constituency Parser does not produce output for every line.

**To Reproduce**
I am trying to compare the trees of a pair of documents.
`refdoc = open(os.path.join('.', 'drive', 'MyDrive',  'en.devtest'), 'r').read().split('\n')`
`hypdoc = open(os.path.join('.', 'drive', 'MyDrive', 'sw-en-hyp.txt'), 'r').read().split('\n')`
`print(len(refdoc), len(hypdoc))`
`1013 1013`
They are both English and they are both the same length in lines.
However, when I run:
`stanza.download('en')`
`nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', tokenize_no_ssplit=True)`
`refdoc = nlp(refdoc)`
`hypdoc = nlp(hypdoc)`
`print(len(refdoc.sentences), len(hypdoc.sentences))`
`1012 1010`

**Expected behavior**
I expect doc.sentences to be the same length as the original unparsed doc, and I expect doc.sentences to be the same length for both docs.  This is critical. 

**Environment:**
 - OS: Google Colab
 - Python version: 3.7
 - Stanza version: 1.3.0

**Additional context**
Each line may contain more or less than 1 complete sentence.
",
368,2022-01-06T14:09:50Z,https://github.com/stanfordnlp/stanza/issues/917,,"Hi,
I followed the documents to use spaCy for Fast Tokenization and Sentence Segmentation as the following setting:
`nlp = stanza.Pipeline(lang='en', processors={'tokenize': 'spacy'})`
Then I found the processors will load all processors (i.e., Loading: tokenize, pos, lemma, and depparse). If I only want to use the tokenize with spacy and pos processors, what should I do?
Thanks!",
369,2022-01-04T00:59:42Z,https://github.com/stanfordnlp/stanza/pull/916,,"Change pattn to be a separate module

Add a module for labeled attention",
370,2022-01-03T23:43:52Z,https://github.com/stanfordnlp/stanza/pull/915,,Reorganize Hong's labeled attention changes,
371,2022-01-03T23:34:24Z,https://github.com/stanfordnlp/stanza/pull/914,,,
372,2022-01-03T22:21:45Z,https://github.com/stanfordnlp/stanza/pull/913,,Update public suc3 processing to latest version of suc_to_iob.py,
373,2022-01-03T11:55:09Z,https://github.com/stanfordnlp/stanza/issues/912,,"I was hoping that stanza would be willing to add support for NER in Swedish. 

For this, you need a **Swedish dataset in IOB2 format**. And I have a way to get a file like that using the SUC 3.0 corpus, and a small converter script that I wrote: https://github.com/EmilStenstrom/suc_to_iob

The license for the corpus us CC BY-SA, and it does not affect the license of your model file (see the repo above for licensing clarifications).

How can I help to get this done? :)",
374,2022-01-03T01:49:36Z,https://github.com/stanfordnlp/stanza/pull/911,,"Adding the PartitionedTransformerModule and the LabelAttentionModule, both of which can be used for further testing with other layers of the parser. It would be easy to import these modules and plug them into other Stanza tools, such as the NER, or any other tools that have not incorporated attention.

The two modules, as of now, require an embedding and the appropriate maskings for the embedding. The PartitionedTransformerModule returns a partitioned representation that takes into account both content and positional information. The LabelAttentionModule returns a labeled representation that takes into account the appropriate labels. ",
375,2022-01-01T08:28:12Z,https://github.com/stanfordnlp/stanza/pull/910,,"Originally named da_ner for the DDT dataset, but has since expanded to include NB and NN",
376,2021-12-27T08:00:49Z,https://github.com/stanfordnlp/stanza/issues/909,,"**Describe the bug**
I installed stanza version 1.1.1 using pip. I am trying to use stanza tokenizer for English language. When I execute, the error says : 
FileNotFoundError: [Errno 2] No such file or directory: '/stanza_resources/en/tokenize/combined.pt'


**To Reproduce**
Steps to reproduce the behavior:
1. Open a new Jupter Notebook
2. Execute the following :   ! pip install stanza==1.1.1
3. Execute the following code : 
   import stanza
   nlp = stanza.Pipeline(""en"", processors='tokenize', package = 'default')
4. See error : FileNotFoundError: [Errno 2] No such file or directory: '/stanza_resources/en/tokenize/combined.pt'

Stack trace : 

File ""process_mpqa.py"", line 397, in main
    nlp = stanza.Pipeline(""en"", processors='tokenize', package = 'default')
  File ""sg/.local/lib/python3.6/site-packages/stanza/pipeline/core.py"", line 113, in __init__
    use_gpu=self.use_gpu)
  File ""sg/.local/lib/python3.6/site-packages/stanza/pipeline/processor.py"", line 146, in __init__
    self._set_up_model(config, use_gpu)
  File ""sg/.local/lib/python3.6/site-packages/stanza/pipeline/tokenize_processor.py"", line 38, in _set_up_model
    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
  File ""sg/.local/lib/python3.6/site-packages/stanza/models/tokenize/trainer.py"", line 19, in __init__
    self.load(model_file)
  File ""sg/.local/lib/python3.6/site-packages/stanza/models/tokenize/trainer.py"", line 85, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""sg/.local/lib/python3.6/site-packages/torch/serialization.py"", line 594, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""sg/.local/lib/python3.6/site-packages/torch/serialization.py"", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""sg/.local/lib/python3.6/site-packages/torch/serialization.py"", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'sg/stanza_resources/en/tokenize/combined.pt'


**Expected behavior**
Expected the tokenizer to be implemented and to obtain an instance of the same.

**Environment (please complete the following information):**
 - OS: Linux
 - Python version:  3.6.8 from Anaconda
 - Stanza version: 1.1.1


",
377,2021-12-24T21:57:15Z,https://github.com/stanfordnlp/stanza/pull/908,,"The Label Attention Layer comes after the Self-Attention Layer. After verifying that this works with our parser, I think we can squeeze everything (all the layers and the functions) into one Self-Attention Layer (like you mentioned) and one Label Attention Layer, which would make it easy for you to experiment with the subtrees

Merry Christmas! ",
378,2021-12-23T07:02:32Z,https://github.com/stanfordnlp/stanza/pull/907,,Add a conversion for the official licensed SUC3 dataset,
379,2021-12-23T06:01:14Z,https://github.com/stanfordnlp/stanza/pull/906,,"This is the current state of the Label Attention implementation. 

After getting the dimension mismatch fixed, the parser should be good to go",
380,2021-12-21T23:06:52Z,https://github.com/stanfordnlp/stanza/issues/905,,"I want to extract NER tags for each token of a sentence but not in the BIOES format.
Is there any way through which I can get normal NER tags for the tokens with stanza?
",
381,2021-12-20T12:11:34Z,https://github.com/stanfordnlp/stanza/issues/904,,"Hi! I'm using Stanza NER models for a long time. After using them in different languages, I have seen that some models have **slightly different names of tags**. For example I have seen that:

- **Ukrainian** NER has `PERS` instead of the common `PER`
- **Vietnamese** NER model has `ORGANIZATION` instead of `ORG`.

Until some months ago there were only 2 possibilities: **4-tags models** (with `PER`, `ORG`, `LOC` and `MISC`) and **18-tags models** (like for English and Chinese).

It would be helpful to have a sort of mapping between the most common labels and the language-specific labels. And also to have a clear list of labels for each language",
382,2021-12-20T01:30:53Z,https://github.com/stanfordnlp/stanza/issues/903,,"I have download the latest 'en' package, and try to build a pipeline:
'''
nlp = stanza.Pipeline('en',use_gpu=False)
'''
but i got an error:
'''
AttributeError: Can't get attribute 'SentenceBoundary' on <module 'stanza.models.constituency.lstm_model' from '/Users/didi/opt/anaconda3/lib/python3.8/site-packages/stanza/models/constituency/lstm_model.py'>
'''

I don't know how to solve it ,(all the model and package are latest version.",
383,2021-12-16T09:10:55Z,https://github.com/stanfordnlp/stanza/issues/902,,"Which version of Stanza was used in the model performance in the current docs?

https://stanfordnlp.github.io/stanza/performance.html

If this isn't for the most recent release, are there more up-to-date tables for UD v2.5 for newer releases?

Thanks!",
384,2021-12-15T21:26:14Z,https://github.com/stanfordnlp/stanza/issues/901,,Is there a way to get the confidence score for the Biomedical NER model in stanza ?,
385,2021-12-15T10:56:22Z,https://github.com/stanfordnlp/stanza/issues/900,,"The docs say it is possible to download only specific processors with the corresponding argument to `stanza.download`. This argument can either be a string of comma-separated processor names, or it can be a dict of string processor names to string package names. Based on this I would expect the following to only download the `tokenize` processor.

```python
import stanza

stanza.download(lang='en', model_dir='.', processors={'tokenize': 'default'})
```
However, it proceeds to download all the processors.

This behaves as expected:
```python
stanza.download(lang='en', model_dir='.', processors='tokenize')
```
Am I missing something or is this a bug?

Thanks in advance for your time.",
386,2021-12-14T12:19:38Z,https://github.com/stanfordnlp/stanza/issues/899,,"Hi，it's very excited when i know there‘s a NER model for  clinical reports！
I had used this demo changed 'i2b2' to 'radiology'：
![image](https://user-images.githubusercontent.com/48993553/145995290-9d41fa24-82af-434a-a69b-69d1ff48753c.png)
and got these files:
![image](https://user-images.githubusercontent.com/48993553/145996156-5d8141a6-aeef-458a-be08-00a733109f20.png)
due to the ternet reasons, i want to load the model  locally through configuring config like this:
![image](https://user-images.githubusercontent.com/48993553/145996616-d710bd06-8b76-4ccf-b63c-8508c1e00e07.png)
but got the bug:
![image](https://user-images.githubusercontent.com/48993553/145996798-c95ab5ee-8d67-44ed-8e87-d921090b21a8.png)
it seems i should download the 1billion.pt? while files existed are \forward_charlm\mimic.pt and \backward_charlm\mimic.pt what should i do to solve this problem?  
thanks",
387,2021-12-14T10:55:25Z,https://github.com/stanfordnlp/stanza/issues/898,,"**Describe the bug**
When running Stanza on the Italian expression ""la foglia gialla"" (the yellow leaf), the article ""la"" is classified as a MWT:

```
[
    {
        ""id"": [
            1,
            2
        ],
        ""misc"": ""start_char=0|end_char=2"",
        ""text"": ""la"",
        ""textPageId"": ""id-wo:en-it-f-la""
    },
    {
        ""id"": 1,
        ""deprel"": ""det"",
        ""feats"": ""Definite=Def|Gender=Fem|Number=Sing|PronType=Art"",
        ""head"": 3,
        ""lemma"": ""il"",
        ""text"": ""la"",
        ""upos"": ""DET"",
        ""xpos"": ""RD"",
        ""textPageId"": ""id-wo:en-it-f-la"",
        ""lemmaPageId"": ""id-wo:en-it-f-il""
    },
    {
        ""id"": 2,
        ""deprel"": ""det"",
        ""feats"": ""Definite=Def|Gender=Fem|Number=Sing|PronType=Art"",
        ""head"": 3,
        ""lemma"": ""il"",
        ""text"": ""la"",
        ""upos"": ""DET"",
        ""xpos"": ""RD"",
        ""textPageId"": ""id-wo:en-it-f-la"",
        ""lemmaPageId"": ""id-wo:en-it-f-il""
    },
    {
        ""id"": 3,
        ""deprel"": ""root"",
        ""feats"": ""Gender=Fem|Number=Sing"",
        ""head"": 0,
        ""lemma"": ""foglia"",
        ""misc"": ""start_char=3|end_char=9"",
        ""text"": ""foglia"",
        ""upos"": ""NOUN"",
        ""xpos"": ""S"",
        ""textPageId"": ""id-wo:en-it-f-foglia""
    },
    {
        ""id"": 4,
        ""deprel"": ""amod"",
        ""feats"": ""Gender=Fem|Number=Sing"",
        ""head"": 3,
        ""lemma"": ""giallo"",
        ""misc"": ""start_char=10|end_char=16"",
        ""text"": ""gialla"",
        ""upos"": ""ADJ"",
        ""xpos"": ""A"",
        ""textPageId"": ""id-wo:en-it-f-gialla"",
        ""lemmaPageId"": ""id-wo:en-it-f-giallo""
    }
]
```

Note that the two ""words"" of which ""la"" is composed are both ""la"", and all their properties are the same. Maybe Stanza needs a mitigation which checks the MWT expansions. If the MWT expansion says that the MWT is composed of twice the same word, which are identical to the MWT itself, then the MWT expansion must be wrong and can be removed (""la"" !== ""la"" + ""la"").

**Expected behavior**
""la"" ist just an article. It should not be classified as composed of more than one word.

**Environment (please complete the following information):**
 - Stanza version: 1.1.1
",
388,2021-12-13T12:19:55Z,https://github.com/stanfordnlp/stanza/issues/897,,i installed jdk and netbeans ide then i imported the java project and i am trying to run it so that i can access the confidence score ,
389,2021-12-11T22:15:33Z,https://github.com/stanfordnlp/stanza/issues/896,,"I am trying to train model for dependency parsing. When I try to do the converting UD data on the documentation for model training and evaluation after this command:
`python -m stanza.utils.datasets.prepare_depparse_treebank UD_English-TEST --wordvec_pretrain_file /Users/yagmurakarken/stanza_resources/en/depparse/combined.pt --gold
`
I get this error:
```
2021-12-12 01:03:15 INFO: Datasets program called with:
/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_depparse_treebank.py UD_English-TEST --wordvec_pretrain_file /Users/yagmurakarken/stanza_resources/en/depparse/combined.pt --gold
Preparing data for UD_English-TEST: en_test, en
Traceback (most recent call last):
  File ""/Users/yagmurakarken/opt/anaconda3/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Users/yagmurakarken/opt/anaconda3/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_depparse_treebank.py"", line 69, in <module>
    main()
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_depparse_treebank.py"", line 66, in main
    common.main(process_treebank, add_specific_args)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/common.py"", line 141, in main
    process_treebank(treebank, paths, args)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_depparse_treebank.py"", line 36, in process_treebank
    prepare_tokenizer_treebank.copy_conllu_treebank(treebank, paths, paths[""DEPPARSE_DATA_DIR""])
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 67, in copy_conllu_treebank
    process_treebank(treebank, paths, args)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1071, in process_treebank
    process_ud_treebank(treebank, udbase_dir, tokenizer_dir, short_name, short_language, args.augment)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 979, in process_ud_treebank
    prepare_ud_dataset(treebank, udbase_dir, tokenizer_dir, short_name, short_language, ""train"", augment)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 969, in prepare_ud_dataset
    write_augmented_dataset(input_conllu, output_conllu, augment_punct)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 730, in write_augmented_dataset
    new_sents = augment_function(sents)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 710, in augment_punct
    new_sents = augment_apos(sents)
  File ""/Users/yagmurakarken/Desktop/stanza-train/stanza/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 486, in augment_apos
    raise ValueError(""Cannot find '# text'"")
ValueError: Cannot find '# text'

```

When I look into the code  augment_apos(sents) in prepare_tokenizer_treebank.py I realized that the line with punctuation is not startswith(""# text"") and it raises an error for this. When comment these two lines:
```
else:
            raise ValueError(""Cannot find '# text'"")
```

it works, but I am not sure  if it affects the training results or not.


Environment :
 - OS:  MacOS
 - Python version: Python 3.8.5
 - Stanza version:  1.3.0

I am using the toy data from your stanza-train, so that I am sure about the format of the data.

Am I doing something wrong for preparation of the data or the problem is about sth else?
Thank you in advance.
",
390,2021-12-08T11:37:27Z,https://github.com/stanfordnlp/stanza/issues/894,,"**When I run the following python code:**

import stanza
from stanza.server import CoreNLPClient
text = ""中国是一个伟大的国家。""
print(text)
with CoreNLPClient(
        properties='chinese',
        classpath=r'F:\StanfordCoreNLP\stanford-corenlp-4.2.2\*',
        strict=False,
        start_server=stanza.server.StartServer.TRY_START ,
        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse'],
        timeout=30000,
        memory='16G') as client:

    pattern = 'NP'
    matches = client.tregex(text, pattern)
    # You can access matches similarly
    print(matches['sentences'][0]['0']['match'])

**I got:**
中国是一个伟大的国家。
2021-12-08 19:35:10 INFO: Using CoreNLP default properties for: chinese.  Make sure to have chinese models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH
2021-12-08 19:35:10 INFO: Connecting to existing CoreNLP server at localhost:9000
2021-12-08 19:35:10 INFO: Connecting to existing CoreNLP server at localhost:9000
(NP (NNP �й���һ��ΰ��Ĺ���) (SYM ��))

Any idea about the garbage characters?

Process finished with exit code 0
",
391,2021-12-07T11:50:22Z,https://github.com/stanfordnlp/stanza/issues/891,,"Hi, I'm seeing a different output running Stanza in my pc and on [Stanza run](http://stanza.run/).

```python
import stanza

text = ""Citizen lab reported this to WhatsApp but WhatsApp was already investigating similar attacks through its protocol.""

nlp = stanza.Pipeline(""en"")
doc = nlp(text)
print(doc.ents)
```

The output is:
```python
[{
  ""text"": ""App"",
  ""type"": ""ORG"",
  ""start_char"": 47,
  ""end_char"": 50
}]
```

Since this output was strange, I checked on [Stanza run](http://stanza.run/) and the result is:

![image](https://user-images.githubusercontent.com/38130299/145023631-a5eb6971-7017-456e-b5aa-07f6a86f82fa.png)

That was the result I expected. 

I have just downloaded the last version of the English model, so I think it's updated.

Another question related to the output I'm seeing: can an entity be a substring of a word? In my case with `WhatsApp` only `App` is taken (I would like to have only ""complete"" words).

Thank you ",
392,2021-12-06T16:07:35Z,https://github.com/stanfordnlp/stanza/issues/890,,"Hi, 
I'm using the CoreNLP client from Stanza to get coreference chains. Here is what I have done. 
```
client = CoreNLPClient(
    annotators=['dcoref'], 
    memory='6G', 
    endpoint='http://localhost:9001',
    outputFormat=""json"",
    be_quiet=False)

annotated_doc = client.annotate(text)
```
However, I realized that the tokenizer used by this client is different from the tokenizer for the gold dataset. I wonder if there is a way for me to replace the tokenizer or just change the sentence separator token. Or, is there a way for me to use tokenized text? Thank you.
",
393,2021-12-05T16:16:41Z,https://github.com/stanfordnlp/stanza/issues/889,,"**Is your feature request related to a problem? Please describe.**
I'm testing a trained NER model for Armenian for my project. After obtaining tangible results of its performance, this might be a valuable addition to the library. 

It follows the dataset format and model characteristics defined by you.

Please, let me know if you have further testing guidelines and performance measures to validate the model's compatibility.",
394,2021-11-30T08:27:04Z,https://github.com/stanfordnlp/stanza/issues/888,,"According to the [documentation](https://stanfordnlp.github.io/stanza/pipeline.html#build-pipeline-from-a-config-dictionary), it is possible to pass the pipeline keyword arguments in a dictionary. Using the example from the documentation as input,
```
import stanza

config = {
	'processors': 'tokenize,mwt,pos', # Comma-separated list of processors to use
	'lang': 'fr', # Language code for the language to build the Pipeline in
	'tokenize_model_path': './fr_gsd_models/fr_gsd_tokenizer.pt', # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""
	'mwt_model_path': './fr_gsd_models/fr_gsd_mwt_expander.pt',
	'pos_model_path': './fr_gsd_models/fr_gsd_tagger.pt',
	'pos_pretrain_path': './fr_gsd_models/fr_gsd.pretrain.pt',
	'tokenize_pretokenized': True # Use pretokenized text as input and disable tokenization
}
nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict
doc = nlp(""Van Gogh grandit au sein d'une famille de l'ancienne bourgeoisie ."") # Run the pipeline on the pretokenized input text
print(doc) # Look at the result
```
an error is returned that the `expected model directory  is missing`. 

But: The selected models are present in the default model path, `~/stanza_resources/`, and this works when giving the full path.

In addition to this, even if the model was not present in the default path, according to the [pipeline keywords in the documentation](https://stanfordnlp.github.io/stanza/pipeline.html), it should be possible to set the `dir` or `model_dir` for the selected processors using the `dir` keyword. But that also does not work. 

Initially, the directories are set (semi-)correctly in `core.py` in `build_default_config`, but that then gets overwritten by updating the config with the keyword arguments:
https://github.com/stanfordnlp/stanza/blob/e44d1c88340e33bf9813e6f5a6bd24387eefc4b2/stanza/pipeline/core.py#L115

Right now, I don't see any other option than setting the full path manually by hand, when passing the input as a dictionary, ie.
```
'tokenize_model_path': '/home/user/stanza_resources/fr_gsd_models/fr_gsd_tokenizer.pt'
```
 Possibly one needs to define another method in the `Pipeline` class that ensures `dir` and the model paths are set correctly when `dir` is given.

This was with stanza Version: 1.3.0 on Ubuntu 20.04.3 LTS, Python 3.8",
395,2021-11-29T11:06:48Z,https://github.com/stanfordnlp/stanza/issues/887,,"**Describe the bug**

While trying to download a french model during a Docker build, an HTTP exception occurs

**To Reproduce**
Steps to reproduce the behavior:

```
FROM python:3.7-slim-buster

RUN apt-get -y clean \
    && apt-get -y update \
    && apt-get -y install python-dev \
    && apt-get -y install build-essential \
    && apt-get -y install curl \
    && apt-get -y install wget \
    && apt-get -y install unzip \
    && pip install --no-cache stanza
    && python -c 'import stanza; stanza.download(""fr"")' \
    && rm -r /root/.cache \
    && apt-get -y clean \
    && apt-get -y autoremove

CMD [""python"", ""-c"", ""import stanza""]
```
This is the traceback : 
```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|          | 0.00/24.1k [00:00<?, ?B/s][0m[91m
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 140kB [00:00, 59.1MB/s]                    [0m[91m
2021-11-29 08:57:06 INFO: Downloading default packages for language: fr (French)...
[0m[91mTraceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 706, in urlopen
    chunked=chunked,
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 382, in _make_request
    self._validate_conn(conn)
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 1010, in _validate_conn
    conn.connect()
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connection.py"", line 426, in connect
    tls_in_tls=tls_in_tls,
  File ""/usr/local/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 450, in ssl_wrap_socket
    sock, context, tls_in_tls, server_hostname=server_hostname
  File ""/usr/local/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/local/lib/python3.7/ssl.py"", line 412, in wrap_socket
    session=session
  File ""/usr/local/lib/python3.7/ssl.py"", line 853, in _create
    self.do_handshake()
  File ""/usr/local/lib/python3.7/ssl.py"", line 1117, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File ""/usr/local/lib/python3.7/site-packages/urllib3/util/retry.py"", line 532, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/usr/local/lib/python3.7/site-packages/urllib3/packages/six.py"", line 769, in reraise
    raise value.with_traceback(tb)
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 706, in urlopen
    chunked=chunked,
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 382, in _make_request
    self._validate_conn(conn)
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 1010, in _validate_conn
    conn.connect()
  File ""/usr/local/lib/python3.7/site-packages/urllib3/connection.py"", line 426, in connect
    tls_in_tls=tls_in_tls,
  File ""/usr/local/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 450, in ssl_wrap_socket
    sock, context, tls_in_tls, server_hostname=server_hostname
  File ""/usr/local/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/local/lib/python3.7/ssl.py"", line 412, in wrap_socket
    session=session
  File ""/usr/local/lib/python3.7/ssl.py"", line 853, in _create
    self.do_handshake()
  File ""/usr/local/lib/python3.7/ssl.py"", line 1117, in do_handshake
    self._sslobj.do_handshake()
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/stanza/resources/common.py"", line 431, in download
    md5=resources[lang]['default_md5'],
  File ""/usr/local/lib/python3.7/site-packages/stanza/resources/common.py"", line 142, in request_file
    download_file(url, path, proxies, raise_for_status)
  File ""/usr/local/lib/python3.7/site-packages/stanza/resources/common.py"", line 117, in download_file
    r = requests.get(url, stream=True, proxies=proxies)
  File ""/usr/local/lib/python3.7/site-packages/requests/api.py"", line 76, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 677, in send
    history = [resp for resp in gen]
  File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 677, in <listcomp>
    history = [resp for resp in gen]
  File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 245, in resolve_redirects
    **adapter_kwargs
  File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/adapters.py"", line 498, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
```

This is probably because models are served via HTTP and not HTTPS. We could of course download them and store them elsewhere but i'm wondering if supporting https wouldn't be more secure as I'm sure others are using stanza in production.


**Expected behavior**

Sucessful model download and loading.

**Environment (please complete the following information):**
 - OS: [CentOS, MacOS]
 - Python version: 3.7.2 
 - Stanza version: 1.2.3

**Additional context**

",
396,2021-11-26T03:53:18Z,https://github.com/stanfordnlp/stanza/pull/886,,"Refactor the wiki selftrain script so most of the pieces can be reused for vi quad

Adjust maxlen to be smaller?  Lots of files are causing cuda errors

Turn off shuffle of docs and files using a flag

Also, throw an exception for text too long in the parser, skipping those sentences when building a silver dataset

VI quad dataset filter non-SQ sentences.

Adds a simple test for parsing the vi_quad data
",
397,2021-11-22T00:17:26Z,https://github.com/stanfordnlp/stanza/pull/885,,"Merge in Hung's attn changes, with a couple additions for backwards compatibility and a sin/cos position embedding option",
398,2021-11-19T22:16:10Z,https://github.com/stanfordnlp/stanza/pull/884,,"Attempt AdaBelief and madgrad optimizers (requires a separate install)

Also, add eps as an argument for the adabelief optimizer.  Could use it elsewhere as well
",
399,2021-11-19T21:38:04Z,https://github.com/stanfordnlp/stanza/pull/883,,,
400,2021-11-18T22:47:12Z,https://github.com/stanfordnlp/stanza/issues/882,,"**Describe the bug**
constituency parse tree cannot align with original sentence

**To Reproduce**
```
import requests
import stanza

def main():
    pipeline = stanza.Pipeline(lang=""en"")
    en_text = requests.get(""https://gist.github.com/qiuhaoling/ad3d32c683742bae63578792e2b311b4/raw/e4fc4a0f0f6d550b1796668ca9f9a0b3b65eb4be/gistfile1.txt"").text
    stanza_doc = pipeline(en_text)
    for stanza_sentence in stanza_doc.sentences:
        stanza_sentence.print_tokens()
        print(repr(stanza_sentence.constituency))

if __name__ == ""__main__"":
    main()
```

**Expected behavior**
Constituency parse tree should anchor exactly on original sentence.(token to token)

**Environment (please complete the following information):**
 - OS: CentOS7.8/ArchLinux
 - Python version: Python 3.8.12/Python 3.9.7, Pytorch 1.8.2/Pytorch 1.10.0
 - Stanza version: 1.3.0/dev  https://github.com/stanfordnlp/stanza/commit/5f74e41c9279a96ed35b160a50db0b90ab85e15c

**Additional context**
The original text comes from wikipedia https://en.wikipedia.org/wiki/Pharmaceutical_industry

Thanks for brining constituency parse into stanza, and thanks for your help!
",
401,2021-11-11T03:52:11Z,https://github.com/stanfordnlp/stanza/pull/881,,"Return the start & end vectors when building a bert/phobert representation
",
402,2021-11-10T19:02:47Z,https://github.com/stanfordnlp/stanza/pull/880,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
403,2021-11-10T01:43:48Z,https://github.com/stanfordnlp/stanza/pull/879,,"Rearrange some tests into classes to make them free their resources sooner (the fixtures will be cleaned up once all tests in the class are finished).  Also, sort some tests into their own directories",
404,2021-11-09T17:26:28Z,https://github.com/stanfordnlp/stanza/pull/878,,Output brackets with labels as part of an attempt to make a language model,
405,2021-11-09T09:17:24Z,https://github.com/stanfordnlp/stanza/pull/877,,The partitioned attention just requires the appropriate bert outputs and attention masks. I will continue testing and will update you with any new updates I have. The 'magic number' 1024 here that I have not fixed in my tests is the d_model of the transformer,
406,2021-11-08T15:46:51Z,https://github.com/stanfordnlp/stanza/issues/876,,"Hi, I'm trying to use the deterministic coreference model in CoreNLP by Stanza Client. My client is created as follows.
`
client = CoreNLPClient(
    annotators=['dcoref'], 
    memory='4G', 
    endpoint='http://localhost:9001',
    outputFormat=""conll"",
    be_quiet=False)
client.start()

annotated_text = client.annotate(text, )
`
I can successfully get the outputs, but the output data structure is very complicated. I wonder if there is a way for me to get the CoNLL file directly. Thank you so much for your help.
",
407,2021-11-08T06:41:02Z,https://github.com/stanfordnlp/stanza/pull/875,,Make the usage of bert a bit more general.  This allows for IT and EN models to function with the parser,
408,2021-11-06T03:09:29Z,https://github.com/stanfordnlp/stanza/pull/874,,,
409,2021-11-06T00:58:43Z,https://github.com/stanfordnlp/stanza/pull/873,,Split chunks and keep newlines when doing so,
410,2021-11-04T20:09:50Z,https://github.com/stanfordnlp/stanza/pull/872,,"This current implementation has issues with CUDA, especially for the eval portion",
411,2021-11-04T18:11:38Z,https://github.com/stanfordnlp/stanza/pull/871,,Add a bert model (currently only tested on VI) to the parser.  Sneak in a change to the oracle.  Still need to fully integrate with the pipeline,
412,2021-11-04T03:32:02Z,https://github.com/stanfordnlp/stanza/issues/870,,Is there a built in stop words classification for simplified chinese? thank you!,
413,2021-11-03T23:30:37Z,https://github.com/stanfordnlp/stanza/pull/869,,This is not complete yet. I'll check later to ensure it's doing what it's meant to be doing. Thank you for your help! ,
414,2021-11-02T01:44:09Z,https://github.com/stanfordnlp/stanza/pull/868,,Actually none of these affect the oracle this time around,
415,2021-10-29T21:02:11Z,https://github.com/stanfordnlp/stanza/issues/867,,"Hi, I am using the spanish language model to annotate text:

import stanza
stanza.download('es')
nlp = stanza.Pipeline('es')
doc=nlp(texto)  #texto is a string with the text to be annotated

When I access the properties of the annotated document, it works fine (e.g. doc.text returns the original text and doc.entities a lis of entities). So far so good.

Then I convert the document to Python Object:

dicts = doc.to_dict()

Finally I convert the python object back to Stanza document:

from stanza.models.common.doc import Document
doc2 = Document(dicts)

When I try to access the properties of the document doc2, some of them are not available:

doc2.text: shows nothing
doc2.entities: shows empty list
doc2.sentences: shows list of sentences (OK)
doc2.num_tokens: shows int (OK)
doc2.num_words: shows int (OK)

My question is what am I doing wrong that I loose the ""text"" and ""entities"" properties of the document?

Thanks in advance!

Edit:

I used the example provided by Stanza documentation and have the same problem with the property ""text"" (the example has no NER):

from stanza.models.common.doc import Document

dicts = [[{'id': 1, 'text': 'Test', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=0|end_char=4'}, {'id': 2, 'text': 'sentence', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=5|end_char=13'}, {'id': 3, 'text': '.', 'upos': 'PUNCT', 'xpos': '.', 'misc': 'start_char=13|end_char=14'}]] # dicts is List[List[Dict]], representing each token / word in each sentence in the document

doc = Document(dicts) # doc is class Document

When I access the text (doc.text) I get nothing.
",
416,2021-10-29T05:34:33Z,https://github.com/stanfordnlp/stanza/pull/866,,Beginning of a dynamic oracle for the parser.  Also includes a separate change for an unknown tag embedding (models were retrained with both of these features).,
417,2021-10-28T14:07:38Z,https://github.com/stanfordnlp/stanza/issues/865,,"**Describe the bug**
Using the Stanza wrapper for the Corenlp client, the output is different to the [corenlp](corenlp.run) demo for OpenIE, i.e. the demo extract more/different triples.


**To Reproduce**
a.
1. go to [corenlp.run](corenlp.run)
2. enter `Pauls grandfather Simon went to the park with him.` in the textfield
3. select OpenIE from the annotators list
4. submit
![image](https://user-images.githubusercontent.com/68738999/139264072-b49a3470-d874-4d66-a0f4-a14203a0b799.png)


b.
1. run:
```
import stanza
stanza.download_corenlp_models(model='english', version='4.3.1')
from typing import Union
from stanza.server import CoreNLPClient

def get_annotation(text:Union[str,list],output_format:str='json'):
    with CoreNLPClient(annotators=['openie','ner'],
                       memory='4G',
                       endpoint='http://localhost:9001',
                       be_quiet=False,
                       server_id='OpenIE',) as client:
        if type(text)==str:
            document=client.annotate(text)
        else:
            document=[client.annotate(t) for t in text]
    return document

def extract_triples(document, return_type='dict'):
    keys_to_extract=['subject','relation','object']
    
    triples=[]
    for sent in document.sentence:
        for triple in sent.openieTriple:
            triples.append({key: getattr(triple,key) for key in keys_to_extract})
    if return_type=='tuple':
        return [(triple[key]for key in keys_to_extract) for triple in triples]
    return triples

text=['Pauls grandfather Simon went to the park with him.']
document=get_annotation(text)

for triple in extract_triples(document):
    print(triple)
```
2.this gives:
![image](https://user-images.githubusercontent.com/68738999/139265087-fdbb6fea-411a-4374-a166-2fe53c282264.png)


**Expected behavior**
The output should be consistent

**Environment (please complete the following information):**
 - OS: [e.g. Windows]
 - Python version: Python 3.8.5 from Anaconda
 - Stanza version: 1.2.3
 - corenlp version: 4.3.1

",
418,2021-10-27T02:32:42Z,https://github.com/stanfordnlp/stanza/issues/864,,"open class words being tagged by stanza as PART when should be open class word tag like noun or adj. Also feats appear only very 5-10 words or so. please advise, thank you!",
419,2021-10-26T01:08:56Z,https://github.com/stanfordnlp/stanza/issues/863,,"Hi,

I tried to follow [this tutorial](https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb#scrollTo=ezEjc9LeV2Xs) to run CoreNLP, but I got the error message saying that 

> TimeoutException: CoreNLP request timed out. Your document may be too long.

However, the length of my input string is 481, which is reasonable I think. The annotator I used is ""dcoref"". Could anyone help me to solve this issue? Many thanks.",
420,2021-10-25T15:19:52Z,https://github.com/stanfordnlp/stanza/issues/862,,"**Describe the bug**
When am I running `doc = nlp(original_text)` on a certain sentence, I get the above error.  >99% of the time, this error will not occur.

**To Reproduce**
1) `stanza.download('en')
2) `nlp = stanza.Pipeline('en', processors={'tokenize': 'spacy'}, tokenize_no_ssplit=True)`
3) create original_text as a normal Python string

**Expected behavior**
The sentence should parse like any other sentence.

**Environment (please complete the following information):**
 Python 3.6.8
RHEL 7.6
stanza 1.3.0

**Additional context**
Sorry, I cannot share the input with you.  I am trying to process a sentence.  The only remarkable thing about the sentence is there is an email address in it.  In trying with like sentence structures, I cannot find other cases where this GW is reported.  Hopefully we can figure out if GW makes sense as a tag for the constituency parser and if it does it is supported.",
421,2021-10-25T15:13:36Z,https://github.com/stanfordnlp/stanza/issues/861,,"**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment (please complete the following information):**
 - OS: [e.g. Windows, Ubuntu, CentOS, MacOS]
 - Python version: [e.g. Python 3.6.8 from Anaconda]
 - Stanza version: [e.g., 1.0.0]

**Additional context**
Add any other context about the problem here.
",
422,2021-10-25T09:57:46Z,https://github.com/stanfordnlp/stanza/issues/860,,"Hi, I am experiencing something which I don't know if it's a bug or not.

Basically, if I try to lemmatize the sentence in the snippet, if I use the pos dependency stanza does not output a lemma for the word ""caduta"", whereas if I don't it does.

```python
import stanza

nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', use_gpu=False)
sentence = ""Un'espressione non protetta è caduta dalle sue labbra.""
doc = nlp(sentence)
print(doc)
```

Is this normal / intended behaviour? Why is there no lemma output and how should I handle this case (if there's something better than just taking the text as the lemma)?

P.s.: this does not happen with the 'vit' package.

Output(s):
with pos
```json
{
  ""id"": 5,
  ""text"": ""è"",
  ""lemma"": ""essere"",
  ""upos"": ""AUX"",
  ""xpos"": ""VA"",
  ""feats"": ""Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin"",
  ""start_char"": 28,
  ""end_char"": 29
},
{
  ""id"": 6,
  ""text"": ""caduta"",
  // lemma is missing here
  ""upos"": ""VERB"",
  ""xpos"": ""V"",
  ""feats"": ""Gender=Fem|Number=Sing|Tense=Past|VerbForm=Part"",
  ""start_char"": 30,
  ""end_char"": 36
},
```

without pos
```json
{
  ""id"": 5,
  ""text"": ""è"",
  ""lemma"": ""essere"",
  ""start_char"": 28,
  ""end_char"": 29
},
{
  ""id"": 6,
  ""text"": ""caduta"",
  ""lemma"": ""caduta"",
  ""start_char"": 30,
  ""end_char"": 36
},
```",
423,2021-10-24T06:28:00Z,https://github.com/stanfordnlp/stanza/pull/859,,"right before where you get the word embeddings, the phobert embeddings are stored in phobert_embeddings and can be used for processing in the initial_word_queues",
424,2021-10-22T18:13:10Z,https://github.com/stanfordnlp/stanza/pull/858,,"Current demo has Farsi, Hebrew, and Urdu backwards
",
425,2021-10-22T09:34:14Z,https://github.com/stanfordnlp/stanza/pull/857,,"Use an external script (MIT license, from EmilStenstrom) to process the scrambled version of the SUC3 Swedish corpus.  Use this corpus to build an NER model for Swedish.",
426,2021-10-21T12:32:46Z,https://github.com/stanfordnlp/stanza/pull/856,,"## Description

Fix missing re import in models/classifier/data.py script.
Remove unused pretrain import.
Sort imports.
Improve typing.
Formatting with black.

## Fixes Issues

Fix missing re import in models/classifier/data.py script.

## Unit test coverage

Existing tests should cover
stanza/models/classifiers/data.py
stanza/models/classifiers/iterate_test.py

## Known breaking changes/behaviors
n/a",
427,2021-10-21T05:04:52Z,https://github.com/stanfordnlp/stanza/pull/855,,"Adds a Persian NER dataset, some tests of the NER dataset scripts, and a rearrangement of the NER tests in general",
428,2021-10-21T03:13:08Z,https://github.com/stanfordnlp/stanza/pull/854,,"Current demo has Farsi, Hebrew, and Urdu backwards",
429,2021-10-20T04:39:58Z,https://github.com/stanfordnlp/stanza/pull/852,,"Add a couple checks for the nonlinearity and use kaiming_normal_ to initialize Linear layers

Positive only initialization for Linear bias

Makes a noticeable improvement in an Italian dataset",
430,2021-10-19T11:48:43Z,https://github.com/stanfordnlp/stanza/issues/851,,"I am trying to find an equivalent functionality to the CoreferenceResolution() function that is part of pycorenlp in the stanza library. Is stanza capable of coreference resolution, beyond simply the coreference chain attribute 'corefChain'?",
431,2021-10-18T14:38:00Z,https://github.com/stanfordnlp/stanza/issues/850,,"Hi,
I used to train my model in local mechine. But unfortunately it is taking a while. So is it possible train my model using colab?If yes, could you help me with that?",
432,2021-10-17T06:29:46Z,https://github.com/stanfordnlp/stanza/pull/849,,"Initial word queues were built backwards in the conparser.  This change flips them to a more normal forward direction.

(This destroys the existing models, but that's okay)",
433,2021-10-17T00:53:32Z,https://github.com/stanfordnlp/stanza/issues/848,,"1. Is there anything that cannot go through stanza (e.g., semicolons, colons, two different languages in the same line, etc.)? Wasn't able to run thru a txt file that I had. 

2. If a lexical item was not parsed (e.g., new loanword, slang, etc.), which shows up at the end of the JSON, is there any way to add it to the word list so that next time I run the code it will recognize it. Or would I have to purchase/lease the code from Stanford? 
Thank you ",
434,2021-10-16T00:55:12Z,https://github.com/stanfordnlp/stanza/pull/847,,"Document 3 new NER models - two from an earlier release, one which is posted but no officially released yet

Add information about IT NER

Add Myanmar NER dataset

Post some information about the Afrikaans NER
",
435,2021-10-15T13:45:55Z,https://github.com/stanfordnlp/stanza/issues/846,,"Hello stanza team, recently I have been working with stanza and stanza CoreNLP client. I find that there are inconsistencies of property names with CoreNLP client results and stanza pipeline results. The results from CoreNLP was all singular such as token, sentence, etc 

```
with CoreNLPClient(
        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
        timeout=30000,
        memory='6G') as client:
    ann = client.annotate(text)

ann.corefChain[0].mention[0].sentenceIndex
ann.sentence[3].token[1].word
```

while results from stanza pipeline, according to [the doc](https://stanfordnlp.github.io/stanza/tokenize.html#start-with-pretokenized-text), they are all plural such as tokens, sentences

```
import stanza

nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)
doc = nlp('This is token.ization done my way!\nSentence split, too!')
for i, sentence in enumerate(doc.sentences):
    print(f'====== Sentence {i+1} tokens =======')
    print(*[f'id: {token.id}\ttext: {token.text}' for token in sentence.tokens], sep='\n')
```

Is it possible to make two results' property names consistent as there is not much documentation for CoreNLP client results.",
436,2021-10-14T23:26:36Z,https://github.com/stanfordnlp/stanza/pull/845,,"Scripts for organizing tokenizer and NER data for Myanmar, along with adding the resulting models to the resources directory",
437,2021-10-14T15:04:06Z,https://github.com/stanfordnlp/stanza/issues/844,,"I have a document where each line is a sentence already.  Therefore, the delimiter is `\n` and not `\n\n`.  I want stanza to respect this segmentation which is already present in the data.  It is not such that tokens are split by ` ` or any other nice delimiter, tokenization must be performed.  So then ...

1) Is there a way to accomplish this?
2) Is there a general way to set a delimiter or list of delimiters which serve as an end of sentence marker?  As crude as it is, for example, a list containing periods, end of lines and question marks.  This is just an example, as I said, the only recognized delimiter in my documents would be a single `\n`",
438,2021-10-14T05:31:18Z,https://github.com/stanfordnlp/stanza/pull/843,,Handle spaces in word vectors in the con parser.  Add a couple small tests to verify that spaces aren't messing up the VI operations,
439,2021-10-13T14:29:34Z,https://github.com/stanfordnlp/stanza/issues/842,,"Hi, I don't know if this is the right place to ask. It seems to me that stanza don't have coreference model yet! I did find a solution through CoreNLP client, but I am wondering if stanza have plans in the future to have coreference model.",
440,2021-10-13T14:25:50Z,https://github.com/stanfordnlp/stanza/issues/841,,"Hi, I want to try out coreference model with stanza, but I don't think there is pure python implementation. However, I did find useful information from [corenlp client](https://stanfordnlp.github.io/stanza/client_usage.html).

and yes, I have results there. However, I don't know how the results structured. For example, with this code:

```
from stanza.server import CoreNLPClient

text = ""The FM radio on my sailboat has a tuning button that advances too far when I hit it hard.""

with CoreNLPClient(
        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
        timeout=30000,
        memory='6G') as client:
    ann = client.annotate(text)

print(ann.corefChain)

for c in ann.corefChain:
    print(c)
```
and I have results like:
```
[chainID: 4
mention {
  mentionID: 4
  mentionType: ""PRONOMINAL""
  number: ""SINGULAR""
  gender: ""UNKNOWN""
  animacy: ""ANIMATE""
  beginIndex: 15
  endIndex: 16
  headIndex: 15
  sentenceIndex: 0
  position: 5
}
mention {
  mentionID: 1
  mentionType: ""PRONOMINAL""
  number: ""SINGULAR""
  gender: ""UNKNOWN""
  animacy: ""ANIMATE""
  beginIndex: 4
  endIndex: 5
  headIndex: 4
  sentenceIndex: 0
  position: 2
}
representative: 1
, chainID: 5
mention {
  mentionID: 5
  mentionType: ""PRONOMINAL""
  number: ""SINGULAR""
  gender: ""NEUTRAL""
  animacy: ""INANIMATE""
  beginIndex: 17
  endIndex: 18
  headIndex: 17
  sentenceIndex: 0
  position: 6
}
mention {
  mentionID: 0
  mentionType: ""NOMINAL""
  number: ""SINGULAR""
  gender: ""NEUTRAL""
  animacy: ""INANIMATE""
  beginIndex: 0
  endIndex: 6
  headIndex: 2
  sentenceIndex: 0
  position: 1
}
representative: 1
]
```

and even iterate each chain through:
```
for c in ann.corefChain:
    print(c)
```

However, how do I trace the exact phrase back (through python)?

For example, for this mention:
```
mention {
  mentionID: 1
  mentionType: ""PRONOMINAL""
  number: ""SINGULAR""
  gender: ""UNKNOWN""
  animacy: ""ANIMATE""
  beginIndex: 4
  endIndex: 5
  headIndex: 4
  sentenceIndex: 0
  position: 2
}
```
What is the phrase for it?",
441,2021-10-12T20:37:56Z,https://github.com/stanfordnlp/stanza/issues/840,,"Hey I'm using the following script for training process.
python -m stanza.models.ner_tagger --wordvec_pretrain_file saved_models\pos\combined.pt --train_file C:\Users\saseendr\stanza-train\data\nerbase\English-TEST\dataset.train.json --eval_file C:\Users\saseendr\stanza-train\data\nerbase\English-TEST\dataset.dev.json --lang en --shorthand en_combined --mode train --save_name dataset_nocharlm.pt. But unfortunetly I'm getting following error message:
<img width=""947"" alt=""stanza_is"" src=""https://user-images.githubusercontent.com/42188654/137025344-0cbaaa0b-7de1-4e72-babe-e11347f53a9b.png"">
I dont know if I have missed out something here. Could you please do me the needful",
442,2021-10-12T12:18:34Z,https://github.com/stanfordnlp/stanza/issues/839,,"**Describe the bug**
I'm simply trying to instantiate the tokenize and ner processors from Stanza as follows

`
import stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')
`

However, I get the following errors:

`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/conda/envs/genbert/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 143, in __init__
    use_gpu=self.use_gpu)
  File ""/opt/conda/envs/genbert/lib/python3.7/site-packages/stanza/pipeline/processor.py"", line 159, in __init__
    self._set_up_model(config, use_gpu)
  File ""/opt/conda/envs/genbert/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py"", line 40, in _set_up_model
    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
  File ""/opt/conda/envs/genbert/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py"", line 20, in __init__
    self.load(model_file)
  File ""/opt/conda/envs/genbert/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py"", line 98, in load
    self.model = Tokenizer(self.args, self.args['vocab_size'], self.args['emb_dim'], self.args['hidden_dim'], dropout=self.args['dropout'], feat_dropout=self.args['feat_dropout'])
KeyError: 'feat_dropout'`


# Here is my environment
stanza 1.3.0
python 3.7.10",
443,2021-10-11T23:39:46Z,https://github.com/stanfordnlp/stanza/issues/836,,"When I download the mimic package it often throws an md5 assertion error
```
import stanza; stanza.download('en', package='mimic') 
```

Error:
```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 140kB [00:00, 63.4MB/s]
2021-10-11 23:17:55 INFO: Downloading these customized packages for language: en (English)...
=======================
| Processor | Package |
-----------------------
| tokenize  | mimic   |
| pos       | mimic   |
| lemma     | mimic   |
| depparse  | mimic   |
| pretrain  | mimic   |
=======================

Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/tokenize/mimic.pt: 100%|██████████| 631k/631k [00:00<00:00, 8.43MB/s]
Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/pos/mimic.pt: 100%|██████████| 222/222 [00:00<00:00, 300kB/s]
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/site-packages/stanza/resources/common.py"", line 454, in download
    md5=resources[lang][key][value]['md5']
  File ""/usr/local/lib/python3.6/site-packages/stanza/resources/common.py"", line 143, in request_file
    assert_file_exists(path, md5)
  File ""/usr/local/lib/python3.6/site-packages/stanza/resources/common.py"", line 110, in assert_file_exists
    assert file_md5 == md5, ""md5 for %s is %s, expected %s"" % (path, file_md5, md5)
AssertionError: md5 for /root/stanza_resources/en/pos/mimic.pt is 1195812a37c01a4481a4748c85d0c6a9, expected ec1684778961db047434c2461e0d537d
```

When I rerun the command multiple times it eventually works

```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 140kB [00:00, 66.6MB/s]
2021-10-11 23:28:15 INFO: Downloading these customized packages for language: en (English)...
=======================
| Processor | Package |
-----------------------
| tokenize  | mimic   |
| pos       | mimic   |
| lemma     | mimic   |
| depparse  | mimic   |
| pretrain  | mimic   |
=======================

Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/tokenize/mimic.pt: 100%|██████████| 631k/631k [00:00<00:00, 6.27MB/s]
Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/pos/mimic.pt: 100%|██████████| 20.8M/20.8M [00:00<00:00, 37.6MB/s]
Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/lemma/mimic.pt: 100%|██████████| 4.19M/4.19M [00:00<00:00, 27.2MB/s]
Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/depparse/mimic.pt: 100%|██████████| 109M/109M [00:16<00:00, 6.64MB/s]
Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/pretrain/mimic.pt: 100%|██████████| 82.7M/82.7M [00:11<00:00, 7.18MB/s]
2021-10-11 23:28:45 INFO: Finished downloading models and saved to /root/stanza_resources.
```

**Environment (please complete the following information):**
Python 3.6
```
spacy-stanza==0.2.5
spacy==2.3.2
stanza==1.2.3
```

**Additional context**
I use stanza in a docker so I have to build it multiple times from scratch",
444,2021-10-11T15:50:54Z,https://github.com/stanfordnlp/stanza/issues/835,,"I was wondering if in Stanza is possible to **add some new tags**, to the existing NER model. 

I remember in **CORENLP** you can conditionate the output of the CRFs like expressed in this [link](https://stanfordnlp.github.io/CoreNLP/ner.html#customizing-the-fine-grained-ner).

```bash
Boston Red Sox       SPORTS_TEAM     ORGANIZATION,MISC       1
Denver Broncos       SPORTS_TEAM     ORGANIZATION,MISC       1
Detroit Red Wings    SPORTS_TEAM     ORGANIZATION,MISC       1
Los Angeles Lakers   SPORTS_TEAM     ORGANIZATION,MISC       1
```

Is there a similar way in Stanza?",
445,2021-10-11T09:38:26Z,https://github.com/stanfordnlp/stanza/issues/834,,"Hi, I have a csv file with 3 columns ""Text"",""Pos"",""Ner"". I want to use this csv as my training data. I need to convert this csv into expected json format. I have read prepare_ner_dataset.py .But it seems some how hard for me. Could you please explain how can I convert it and if possible please provide the sample of json file with the expected format. I'm using Windows.
Thank you!",
446,2021-10-10T14:59:20Z,https://github.com/stanfordnlp/stanza/pull/833,,"## Description

Start implementing static types and return values.  Fix ValueError incorrect reference in file.

Still some outstanding issues.

```shell
mypy --follow-imports=skip --disallow-untyped-defs --ignore-missing-imports stanza\utils\datasets\prepare_depparse_treebank.py
stanza\utils\datasets\prepare_depparse_treebank.py:52: error: Function is missing a type annotation for one or more arguments
stanza\utils\datasets\prepare_depparse_treebank.py:75: error: Function is missing a type annotation for one or more arguments
```

Is there a flake8 configuration preference, formatter preference for this project?

## Fixes Issues

Fixes incorrect valueerror parameter reference.
Formats file with black.
Fixes some mypy errors.

## Unit test coverage

I didn't see any existing unit test to modify here.

## Known breaking changes/behaviors

None
",
447,2021-10-10T03:17:55Z,https://github.com/stanfordnlp/stanza/issues/832,,"Hey there. I've started having this issue when trying to use stanza's biomedical pipelines a couple of days ago. The code below was copied from [stanza's documentation][stanza-doc].

[stanza-doc]: https://stanfordnlp.github.io/stanza/biomed_model_usage.html

```python
import stanza
# download and initialize the CRAFT pipeline
stanza.download('en', package='craft')
nlp = stanza.Pipeline('en', package='craft')
# annotate example text
doc = nlp('A single-cell transcriptomic atlas characterizes ageing tissues in the mouse.')
# print out dependency tree
doc.sentences[0].print_dependencies()
```
running these lines on my system or on a colab notebook gives the output below (minus the absolute paths):

```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json: 141kB [00:00, 13.8MB/s]
2021-10-09 23:46:50 INFO: Downloading these customized packages for language: en (English)...
=======================
| Processor | Package |
-----------------------
| pos       | craft   |
| lemma     | craft   |
| depparse  | craft   |
| pretrain  | craft   |
=======================

2021-10-09 23:46:50 INFO: File exists: MODELDIR/stanza_resources/en/pos/craft.pt.
2021-10-09 23:46:50 INFO: File exists: MODELDIR/stanza_resources/en/lemma/craft.pt.
2021-10-09 23:46:50 INFO: File exists: MODELDIR/stanza_resources/en/depparse/craft.pt.
2021-10-09 23:46:50 INFO: File exists: MODELDIR/stanza_resources/en/pretrain/craft.pt.
2021-10-09 23:46:50 INFO: Finished downloading models and saved to MODELDIR/stanza_resources.
2021-10-09 23:46:50 INFO: Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| pos       | craft   |
| lemma     | craft   |
| depparse  | craft   |
=======================

2021-10-09 23:46:50 INFO: Use device: cpu
2021-10-09 23:46:50 INFO: Loading: pos
2021-10-09 23:46:50 INFO: Loading: lemma
2021-10-09 23:46:50 INFO: Loading: depparse
2021-10-09 23:46:50 INFO:

Traceback (most recent call last):
  File ""ABSOLUTE/PATH/HERE/./stanza-bio.py"", line 6, in <module>
    nlp = stanza.Pipeline('en', package='craft')
  File ""ABSOLUTE/PATH/HERE/.venv/lib/python3.9/site-packages/stanza/pipeline/core.py"", line 177, in __init__
    raise PipelineRequirementsException(pipeline_reqs_exceptions)
stanza.pipeline.core.PipelineRequirementsException:

---
Pipeline Requirements Error!
        Processor: POSProcessor
        Pipeline processors list: pos,lemma,depparse
        Processor Requirements: {'tokenize'}
                - fulfilled: set()
                - missing: {'tokenize'}

The processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.


---
Pipeline Requirements Error!
        Processor: LemmaProcessor
        Pipeline processors list: pos,lemma,depparse
        Processor Requirements: {'tokenize'}
                - fulfilled: set()
                - missing: {'tokenize'}

The processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.


---
Pipeline Requirements Error!
        Processor: DepparseProcessor
        Pipeline processors list: pos,lemma,depparse
        Processor Requirements: {'tokenize', 'lemma', 'pos'}
                - fulfilled: {'lemma', 'pos'}
                - missing: {'tokenize'}

The processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.
```

# Info about the environment
* OS: Arch Linux
* Python version: 3.9.7

The installed modules:
```
certifi==2021.5.30
charset-normalizer==2.0.6
emoji==1.6.0
idna==3.2
numpy==1.21.2
protobuf==3.18.1
requests==2.26.0
six==1.16.0
stanza==1.3.0
torch==1.9.1
tqdm==4.62.3
typing-extensions==3.10.0.2
urllib3==1.26.7
```",
448,2021-10-09T02:21:17Z,https://github.com/stanfordnlp/stanza/pull/831,,"## Description
Noticed init may be incorrectly renamed while browsing the code base.

Rename init.",
449,2021-10-08T23:54:11Z,https://github.com/stanfordnlp/stanza/pull/830,,"Fix a specific test on Windows (AFTER corenlp is downloaded and the setup script is run)
",
450,2021-10-08T22:22:10Z,https://github.com/stanfordnlp/stanza/pull/829,,"Preliminary script to run the con parser, plus Hung's updates to the VI dataset",
451,2021-10-08T19:02:28Z,https://github.com/stanfordnlp/stanza/pull/828,,I added a small function that helps with unifying the labels for the train/dev/test set,
452,2021-10-08T18:27:27Z,https://github.com/stanfordnlp/stanza/pull/827,,"## Description 

Change master to main in the PR template",
453,2021-10-08T18:23:09Z,https://github.com/stanfordnlp/stanza/pull/826,,"## Description

This adds a pyproject toml to the project for configuration of various tools.  

- Configure pytest warning filters and markers. 
- exclude a src location file which is not a test

Add a fallback in case the test_server_request test fails to find its test file.  Copying this file and other test files to the out folder could be removed if not essential for testing.

Default branch in Github could be set to dev instead of main, to remove need to notify users on PR.

",
454,2021-10-08T05:51:02Z,https://github.com/stanfordnlp/stanza/pull/824,,Several changes to the preliminary processing script for the 2009 VLSP dataset.  Puts it in a condition where it processes VLSP into a form the parser can handle.  Some additional error checking is performed in the parser to help the debugging process as well,
455,2021-10-07T18:18:16Z,https://github.com/stanfordnlp/stanza/pull/823,,I added a function with a stack to check if the tree is properly closed before writing to the file,
456,2021-10-07T02:41:57Z,https://github.com/stanfordnlp/stanza/pull/822,,"If the script is reading and detects an error, it immediately refuses the tree that it is reading. I added a reading_tree boolean that keeps track of whether or not the current tree is valid and should be written or not. I also added a is_valid_line that checks for properly opened and closed constituents
",
457,2021-10-06T21:10:20Z,https://github.com/stanfordnlp/stanza/issues/821,,"**Describe the bug**
When connecting to an existing, local CoreNLP server, Stanza seems to hang—regardless of which annotators I specify.

**To Reproduce**
Steps to reproduce the behavior:
1. Start a local CoreNLP server.
2. Write some Python code such as this:

```
from stanza.server import CoreNLPClient, StartServer

with CoreNLPClient(start_server=StartServer.DONT_START, endpoint='http://localhost:9000', annotators='depparse') as client:
    text = 'Facebook is a toxic waste of time.'
    ann = client.annotate(text)
    print(ann)
```

**Expected behavior**
Annotations should be returned, but the call to `client.annotate(text)` never returns. 

**Environment (please complete the following information):**
 - OS: Linux Mint
 - Python version: 3.8.8
 - Stanza version: 1.3.0
 - CoreNLP version: 4.3.0

**Additional context**
I tested this with another Python library (stanfordcorenlp), and it worked just fine. Here's proof that my server is running on port 9000:

```
$ sudo systemctl status core-nlp
● core-nlp.service - CoreNLP
     Loaded: loaded (/etc/systemd/system/core-nlp.service; enabled; vendor preset: enabled)
     Active: active (running) since Wed 2021-10-06 11:16:38 PDT; 2h 32min ago
   Main PID: 280012 (java)
      Tasks: 140 (limit: 309046)
     Memory: 1.0G
     CGroup: /system.slice/core-nlp.service
             └─280012 /usr/bin/java -Xmx32g -cp /opt/CoreNLP/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeou>

```
",
458,2021-10-06T06:53:52Z,https://github.com/stanfordnlp/stanza/pull/820,,"Addressed #815 

@BramVanroy 
",
459,2021-10-06T00:04:40Z,https://github.com/stanfordnlp/stanza/pull/819,,"Need to build parsers out of this, of course",
460,2021-10-05T19:24:03Z,https://github.com/stanfordnlp/stanza/pull/818,,"Add leaky relu as an option, although it didn't help much",
461,2021-10-04T01:41:14Z,https://github.com/stanfordnlp/stanza/pull/817,,Updated to using CoreNLP 4.3.0 in tests.,
462,2021-10-03T20:01:33Z,https://github.com/stanfordnlp/stanza/pull/816,,,
463,2021-09-30T09:39:43Z,https://github.com/stanfordnlp/stanza/issues/815,,"I am trying to run the stanza tests but I am running into issues with the environment variables that are expected.

```
Please set STANZA_TEST_HOME environment variable for test working dir, base name must be: stanza_test
```

I tried setting such envvar in a new directory with the path ending in a `stanza_test` directory but that did not seem to work. So I am not sure how I should go about this. It would be useful if this information is added to CONTRIBUTING as well.

Additionally, I found that setup.py includes extras, but `test` does not include pytest, which would be a useful addition.

https://github.com/stanfordnlp/stanza/blob/c457a9309ad15c522e94230f919c25d1e7aebf64/setup.py#L88-L91",
464,2021-09-29T01:23:33Z,https://github.com/stanfordnlp/stanza/pull/814,,"## Description

Add a script that goes through a directory and convert all the trees in the directory's files into the correct tree format
",
465,2021-09-29T00:47:31Z,https://github.com/stanfordnlp/stanza/pull/813,,"## Description
Added in utils/datasets/constituency a script called vtb_script.py that goes through all the files in a directory and convert the tree to the correct tree format

",
466,2021-09-21T20:52:03Z,https://github.com/stanfordnlp/stanza/issues/812,,"I want to extract the medical NER from some sentences. However, I found I can not repeat the results a week ago, and the new results may generate more false positive results.

**code** 
after download necessary package,  I tested with this code:
a = stanza.Pipeline('en', processors={'ner': 'i2b2'})
text = str('his father did pass away from his cancer')
doc = a(text)
doc.entities

**Results**
[{
   ""text"": ""his cancer"",
   ""type"": ""PROBLEM"",
   ""start_char"": 30,
   ""end_char"": 40
 }]

**Problem:**
The code return his cancer as NER. But actually it is his father's cancer not his own. The current version can not distinguish the family disease history and real diseases.   In previous result, the code didn't return this NER. 

I guess stanza update its i2b2  training model? Anybody know this?

Thanks
",
467,2021-09-18T12:53:40Z,https://github.com/stanfordnlp/stanza/issues/810,,"I keep getting this error, but the thing is, I can see the server is already running by visiting http://localhost:9000/",
468,2021-09-16T22:35:45Z,https://github.com/stanfordnlp/stanza/issues/809,,"**Describe the bug**
Certain multiple punctuation marks ending farsi text will fail pos processor.

**To Reproduce**

```python
import requests
import stanza

def main():
    pipeline = stanza.Pipeline(
        processors='tokenize,pos,lemma,depparse',
        lang=""fa"",
        tokenize_pretokenized=False)
    farsi_text = requests.get(""https://gist.githubusercontent.com/qiuhaoling/1b65d0eb95f9622f3b4515bf9ed51134/raw/3179b1f65fe39e6b001e4f8aa0f81ad539437cea/doc-14207.txt"").text
    stanza_doc = pipeline(farsi_text)
    stanza_sentence_6 = None
    for idx,sentence in enumerate(stanza_doc.sentences):
        if idx == 6:
            stanza_sentence_6 = sentence
    print(stanza_sentence_6.text)
    second_call_on_sent_6 = pipeline(stanza_sentence_6.text)

if __name__ == ""__main__"":
    main()
```

**Expected behavior**
Both `stanza_doc = pipeline(farsi_text)` and `second_call_of_sent_6 = pipeline(stanza_sentence_6.text)` would run through without exception.

**Environment (please complete the following information):**
 - OS: ArchLinux and/or CentOS7.4
 - Python version:Python 3.7.10 from conda-forge, pytorch==1.5
 - Stanza version: 1.2.3
 - Locate: en_US.UTF-8
 
**Additional context**
None


Thanks a lot for your attention!",
469,2021-09-16T12:41:13Z,https://github.com/stanfordnlp/stanza/issues/808,,"Down below testcase.
Stanza is fast if `parallel == 1`, but becomes sluggish when distributed among processes.

````
import os, multiprocessing, time
import stanza

parallel = os.cpu_count()
language = 'cs'
sentence = 'ponuže dobře al ja nemam i zpětlou vas dbu od policiei teto ty teto věci najitam spravným orbganutyperypřecuji nas praco zdněspotaměcham'

stanza.download( language )

def nlp_stanza( ignore ):
    nlp_pipeline = stanza.Pipeline( language, logging_level='WARN', use_gpu=False )
    for i in range(50):
        s = int(time.process_time()*1000)
        nlp_pipeline( sentence )
        e = int(time.process_time()*1000)
        print( os.getpid(), str(e-s)+'ms:', sentence )

pool = multiprocessing.Pool( processes=parallel )
pool.map( nlp_stanza, range(parallel) )
pool.join()
````",
470,2021-09-15T12:17:48Z,https://github.com/stanfordnlp/stanza/issues/807,,"Can Stanza extract entity relations in this format: (e1,relation,e2) ? If yes, please advise on the input data format and if any tool is available for data annotation.

Thank you
Marur",
471,2021-09-13T14:18:07Z,https://github.com/stanfordnlp/stanza/issues/806,,"I have an application that analyses text looking for keywords using natural language processing. I created an executable and it works fine on my computer. I've sent it to a friend but in his computer he gets an error:

```
Traceback (most recent call last):
  File ""Main.py"", line 15, in <module>
  File ""Menu.py"", line 349, in main_menu
  File ""Menu.py"", line 262, in analyse_text_menu
  File ""Menu.py"", line 178, in analyse_text_function
  File ""AnalyseText\ProcessText.py"", line 232, in process_text
  File ""AnalyseText\ProcessText.py"", line 166, in generate_keyword_complete_list
  File ""AnalyseText\ProcessText.py"", line 135, in lemmatize_text
  File ""stanza\pipeline\core.py"", line 88, in _init_
stanza.pipeline.core.ResourcesFileNotFoundError: Resources file not found at: C:\Users\jpovoas\stanza_resources\resources.json  Try to download the model again.
[26408] Failed to execute script 'Main' due to unhandled exception!
```

It's looking for resources.json inside a folder in his computer. Even though I've added stanza as a hidden import with pyinstaller.

I'm using a model in another language, as opposed to the default one in english.

I tried using pyinstaller --hidden-import=stanza Main.py, but it's to no avail.
I also included the model directory by using `--add-data ""C:\Users\Laila\stanza_resources\pt;Stanza` when creating the executable.

How do I tell stanza to look for the model inside the executable folder generated instead?

Can I just add stanza.download(""language"") in my script? If so how do I change stanza's model download folder? I want it to be downloaded into a folder inside the same directory as the executable. How do I do that?

Thank you",
472,2021-09-13T09:16:44Z,https://github.com/stanfordnlp/stanza/issues/805,,"**Describe the bug**
Stanza sentiment processor automatically remove some char causing error

from `def update_text(sentence, wordvec_type):` in `stanza/models/classifiers/data.py`
```
# stanford sentiment dataset has a lot of random - and /
sentence = sentence.replace(""-"", "" "")
sentence = sentence.replace(""/"", "" "")
sentence = sentence.split()
```

**To Reproduce**
Steps to reproduce the behavior:
```
nlp = stanza.Pipeline('en',
                          tokenize_no_ssplit=True,
                          tokenize_pretokenized=True)
text = '--'
tagged_sent = nlp(sent)
```


See error
```
    tagged_sent = nlp(sent)
File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 229, in __call__
    doc = self.process(doc)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 223, in process
    doc = process(doc)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/pipeline/sentiment_processor.py"", line 53, in process
    labels = cnn_classifier.label_text(self._model, text, batch_size=self._batch_size)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/models/classifiers/cnn_classifier.py"", line 462, in label_text
    labels = unsort(labels, orig_idx)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/models/common/utils.py"", line 204, in unsort
    assert len(sorted_list) == len(oidx), ""Number of list elements must match with original indices.""
AssertionError: Number of list elements must match with original indices.
```



**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: [e.g. Python 3.8 from Anaconda]
 - Stanza version: [e.g., 1.2.3]


",
473,2021-09-13T09:15:59Z,https://github.com/stanfordnlp/stanza/issues/804,,"**Describe the bug**
Stanza sentiment processor automatically remove some char causing error

from `def update_text(sentence, wordvec_type):` in `stanza/models/classifiers/data.py`
```
# stanford sentiment dataset has a lot of random - and /
sentence = sentence.replace(""-"", "" "")
sentence = sentence.replace(""/"", "" "")
sentence = sentence.split()
```

**To Reproduce**
Steps to reproduce the behavior:
```
nlp = stanza.Pipeline('en',
                          tokenize_no_ssplit=True,
                          tokenize_pretokenized=True)
text = '--'
tagged_sent = nlp(sent)
```


See error
```
    tagged_sent = nlp(sent)
File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 229, in __call__
    doc = self.process(doc)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 223, in process
    doc = process(doc)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/pipeline/sentiment_processor.py"", line 53, in process
    labels = cnn_classifier.label_text(self._model, text, batch_size=self._batch_size)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/models/classifiers/cnn_classifier.py"", line 462, in label_text
    labels = unsort(labels, orig_idx)
  File ""/miniconda3/envs/py38/lib/python3.8/site-packages/stanza/models/common/utils.py"", line 204, in unsort
    assert len(sorted_list) == len(oidx), ""Number of list elements must match with original indices.""
AssertionError: Number of list elements must match with original indices.
```



**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: [e.g. Python 3.8 from Anaconda]
 - Stanza version: [e.g., 1.2.3]


",
474,2021-09-10T10:08:24Z,https://github.com/stanfordnlp/stanza/pull/803,,"This pull request:

1.) enables CI (via the new workflow file that defines how tests should be run)
2.) updates to setup.py to download language id resources
3.) small changes to language id system to allow for specifying models dir for multilingual pipelines.",
475,2021-09-08T21:24:01Z,https://github.com/stanfordnlp/stanza/pull/802,,Add an option tqdm progress bar to the pos annotator,
476,2021-09-08T00:06:40Z,https://github.com/stanfordnlp/stanza/issues/801,,"Hi,
I found some strange difference between stanza demo(http://stanza.run/) and output of my script while comparing both.
 1)**_Splitting a single word as Two_** 
one example : ""37 East, Exit 1A will be closed for bridge work, Tues. night, 9:30 p.m.-5:30 a.m.""
Here 1A is splitting as ""1"" and ""A"" .Then output is like:
{
  ""id"": 5,
  ""text"": ""1"",
  ""lemma"": ""1"",
  ""upos"": ""NUM"",
  ""xpos"": ""CD"",
  ""feats"": ""NumForm=Digit|NumType=Card"",
  ""head"": 4,
  ""deprel"": ""dep"",
  ""start_char"": 14,
  ""end_char"": 15
}
{
  ""id"": 1,
  ""text"": ""A"",
  ""lemma"": ""a"",
  ""upos"": ""DET"",
  ""xpos"": ""DT"",
  ""feats"": ""Definite=Ind|PronType=Art"",
  ""head"": 4,
  ""deprel"": ""nsubj:pass"",
  ""start_char"": 15,
  ""end_char"": 16
}
But we cannot find this splitting in Stanza demo.
2) **_Relationship between words_**
37 For example ""East, Exit 1A will be closed for bridge work, Tues. night, 9:30 p.m.-5:30 a.m"" when running this sentence we can see that ""Exit"" and ""closed"" has no relation. But if we are running the same sentence in stanza demo both have ""nsubj:pass"" relation.
Do you guys have any idea about these differences? 
_print(*[f'id: {word.id}\tword: {word.text}\thead id: {word.head}\thead: {sent.words[word.head-1].text if word.head > 0 else ""root""}\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\n')_  I'm using this function in my script.",
477,2021-09-06T10:25:15Z,https://github.com/stanfordnlp/stanza/issues/800,,"When I use the Chinese coreference resolution function, I found that there is no result and the value of parseTree is garbled.
code
```
from stanza.server import CoreNLPClient 

text = ""奥巴马是美国的总统，他是个黑人""

with CoreNLPClient(
        properties=""chinese"",
        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
        timeout=30000,
        memory='6G') as client:
    ann = client.annotate(text)

sentence = ann.sentence[0]
constituency_parse = sentence.parseTree
print(constituency_parse)

token = sentence.token[0]
print(token.value, token.pos, token.ner)

print(ann.corefChain)
```

result
```            
 value: ""M""
            }
            value: ""CLP""
          }
          value: ""QP""
        }
        child {
          child {
            child {
              value: ""\351\273\221\344\272\272""
            }
            value: ""NN""
          }
          value: ""NP""
        }
        value: ""NP""
      }
      value: ""VP""
    }
    value: ""IP""
  }
  value: ""IP""
}
value: ""ROOT""
score: 1609.3066711425781

奥巴马 NR PERSON
[]
```",
478,2021-09-05T15:09:04Z,https://github.com/stanfordnlp/stanza/pull/799,,,
479,2021-09-01T15:49:12Z,https://github.com/stanfordnlp/stanza/pull/798,,"Copy files using shutil.copy2 to preserve metadata.  Don't include the directories so they don't have any metadata at all.  This makes the zip files have the same md5sum at the end

Fixes a minor annoyance - prepare_resources.py would have different md5sums for the same exact zip files",
480,2021-08-31T14:48:12Z,https://github.com/stanfordnlp/stanza/issues/797,,"**Describe the bug**
I notice some tiny bug in [stanza demo](http://stanza.run/) while working with Persian model and here they are:
1. after submitting the sentence the reports was left-to-right and it raised a confusion in my mind (although the alphabets in a word hadn't been changed but the whole sentence was reversed).
2. NER section was not supported. (NER is not available for this language at this time) and I wanted to know if it's OK I can work on it to add NER support for this language too.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to [Stanza Run](http://stanza.run/)
2. Write `سلام خدمت دوستان عزیز امیدوارم حال همگی خوب باشد` on `— Text to annotate —` section
3. Select `persian` as `— Language —`.
3. Click on `Submit`

**Actual behavior**
![Actual behavior](https://user-images.githubusercontent.com/43045767/131523714-b1b91f3f-2397-41d2-87fa-3f863ac9753c.png)

**Expected behavior**
+ As described above I expected to see my input string as imputed (and not in a reversed way).
+ Also `NER` section could be available.

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: -
 - Stanza version: 1.2.3

<hr>

### Tasks To Be Done:

- [x] Check if this problem exists for other RTL languages like `Arabic`
- [x] Solve RTL languages representation problem
- [x] Propose a NER model for Persian (and probable updates)",
481,2021-08-30T17:26:16Z,https://github.com/stanfordnlp/stanza/pull/796,,"Efficiency improvements, some more documentation and other word to respond to the original code review, save & load the optimizer as part of the model, and an attempt to add in-order parsing",
482,2021-08-27T16:40:26Z,https://github.com/stanfordnlp/stanza/issues/795,,"When I'm trying to start the CoreNLPClient using Google Colab and Stanza with the code below I get an ""AnnotationException"" error.
```
text = ""Some Text""  
with CoreNLPClient(properties='/content/corenlp/StanfordCoreNLP-german.properties',
                   annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'depparse', 'coref','parse'],
                   timeout=300000,
                   be_quiet=True, 
                   endpoint='http://localhost:9001',
                   memory='8G') as client:
        ann = client.annotate(text)
2021-08-26 12:07:48 INFO: Starting server with command: java -Xmx8G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 300000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties /content/corenlp/StanfordCoreNLP-german.properties -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,parse -preload -outputFormat serialized
```

The whole error message is:
```
HTTPError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/stanza/server/client.py in _request(self, buf, properties, reset_default, **kwargs)
    454                               timeout=(self.timeout*2)/1000, **kwargs)
--> 455             r.raise_for_status()
    456             return r

3 frames
HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9001/?properties=%7B%27annotators%27%3A+%27tokenize%2Cssplit%2Cpos%2Clemma%2Cner%2Cdepparse%2Ccoref%2Cparse%27%2C+%27outputFormat%27%3A+%27serialized%27%7D&resetDefault=false

During handling of the above exception, another exception occurred:

AnnotationException                       Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/stanza/server/client.py in _request(self, buf, properties, reset_default, **kwargs)
    459                 raise TimeoutException(r.text)
    460             else:
--> 461                 raise AnnotationException(r.text)
    462 
    463     def annotate(self, text, annotators=None, output_format=None, properties=None, reset_default=None, **kwargs):

AnnotationException: edu.stanford.nlp.io.RuntimeIOException: java.io.InvalidClassException: edu.stanford.nlp.parser.shiftreduce.Weight; local class incompatible: stream classdesc serialVersionUID = 1, local class serialVersionUID = 3
```

When I start the client without the 'parse' annotator the error doesn't occour.
Has somebody an idea what's causing this issue?",
483,2021-08-25T08:29:24Z,https://github.com/stanfordnlp/stanza/issues/794,,"Hi,
I'm using stanza nlp for finding the dependency parsing between words. For example
doc=nlp(""His home was in violation of local and state zoning and environmental regulations, and there was no access to a road"").
keyword={""""changes"""": [""""no access""""], """"features"""": [""""road""""]}.
The values of variable keyword ,""no access"" and ""road"" are present in doc. If both of this words has any relation then I need to print ""True"". If no relation then print False. How can I write this code ?

@AngledLuffa ",
484,2021-08-23T23:12:50Z,https://github.com/stanfordnlp/stanza/issues/793,,"I'm using the `craft` and `genia` packages for processing biomedical text. There are multiple NER models with different lists of supported entities. For example, `bc5cdr` for the entities: `CHEMICAL, DISEASE` and `jnlpba` for entities: `PROTEIN, DNA, RNA, CELL_LINE, CELL_TYPE`. By default, I can use the stanza pipeline in the following way and use these NER models independently:

```
stanza.download('en', package='craft', processors={'ner': 'bc5cdr'})
nlp = stanza.Pipeline('en', package='craft', processors={'ner': 'bc5cdr'})
doc = nlp(MY_INPUT_TEXT)
# print out all entities
print(doc.entities)
```

But I wonder if there's any way to include a list of multiple models for the `ner` processor? Or, should we just every time create a new stanza pipeline with a different `ner` processor?",
485,2021-08-22T23:53:12Z,https://github.com/stanfordnlp/stanza/pull/792,,"Download corenlp models from HuggingFace instead of stanford by default
Also, download all of corenlp from HuggingFace
",
486,2021-08-22T07:10:52Z,https://github.com/stanfordnlp/stanza/pull/791,,"Fix two minor issues a user encountered

https://github.com/stanfordnlp/stanza/issues/788#issuecomment-903163170
https://github.com/stanfordnlp/stanza/issues/788#issuecomment-903216543
",
487,2021-08-19T13:39:43Z,https://github.com/stanfordnlp/stanza/issues/790,,"for some reason, i want old version of corenlp to get same result

for example, i want my stanza output the same result as corenlp 2016-10-31

is there anyway to do?

thx",
488,2021-08-17T10:03:27Z,https://github.com/stanfordnlp/stanza/issues/789,,"How to change default model download dir?I don‘t have root permission,but default dir is root/stanza-resources",
489,2021-08-17T09:12:35Z,https://github.com/stanfordnlp/stanza/issues/788,,"I have found documentation to be able to train NER models from scratch, but is there an API that'd allow one to update an existing model locally, adding both fresh text and annotations or fresh labels, onto say i2b2 or radiology?

",
490,2021-08-14T18:14:45Z,https://github.com/stanfordnlp/stanza/pull/787,,"## Description
The prior version of code that created train/dev/test split used a completely random approach with weights by sets.
languk community whose data set is used provides files that list recommended split for train and test sets.
The reason to use it is that split was verified not to contain documents from the same sources: several documents can be from the same book and to avoid bias in train data, such occurrences were manually removed. So train set does not contain parts from the book that exists in the test set. 
The dev set is randomly created from train as before.

Overall this allows getting more realistic results of the model. But no performance changes in the trained model were observed after introducing this change.

## Fixes Issues
none, just new features

## Unit test coverage
no additional tests were added as modified code just reads from files and I felt it's ok to keep it as is.

## Known breaking changes/behaviors
none
",
491,2021-08-13T15:17:32Z,https://github.com/stanfordnlp/stanza/pull/786,,"A significantly faster mechanism for doing inference, which also has a large impact on training time",
492,2021-08-10T07:17:38Z,https://github.com/stanfordnlp/stanza/issues/785,,"**Describe the bug**
Everything worked yesterday, but today I got the following error:

**To Reproduce**

```
Python 3.8.10 (default, Jun  2 2021, 10:49:15) 
[GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import stanza
>>> stanza.download(""es"")
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 19.6MB/s]
2021-08-10 09:08:19 INFO: Downloading default packages for language: es (Spanish)...
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 159, in _new_conn
    conn = connection.create_connection(
  File ""/usr/lib/python3/dist-packages/urllib3/util/connection.py"", line 84, in create_connection
    raise err
  File ""/usr/lib/python3/dist-packages/urllib3/util/connection.py"", line 74, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 665, in urlopen
    httplib_response = self._make_request(
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 387, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib/python3.8/http/client.py"", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/usr/lib/python3.8/http/client.py"", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.8/http/client.py"", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.8/http/client.py"", line 1007, in _send_output
    self.send(msg)
  File ""/usr/lib/python3.8/http/client.py"", line 947, in send
    self.connect()
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 187, in connect
    conn = self._new_conn()
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 171, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8ed1938160>: Failed to establish a new connection: [Errno 111] Connection refused
```

It keeps retrying until max retries are exceeded.

**Expected behavior**

The model should download correctly. 

**Environment (please complete the following information):**
 - OS: Ubuntu 20.04.2
 - Python version: Python 3.8.10
 - Stanza version: I tried with 1.1.1 and 1.2.0 but looks like it happens with every version
 
",
493,2021-08-08T13:18:37Z,https://github.com/stanfordnlp/stanza/issues/784,,"I am trying to get F1 scores for the pre-trained english model on my specific text domain without doing any training.

The docs mention the following command:
`
python -m stanza.utils.training.run_ete ${corpus} --score_${split}
`
However as I dont want to do any training, how can I evaluate the model as is?
I've got an annotated dataset for my domain in BIO format.
",
494,2021-08-05T18:33:00Z,https://github.com/stanfordnlp/stanza/issues/783,,"**Describe the bug**
TypeError when attempting to download english model with ""stanza.download('en')""

**To Reproduce**
Steps to reproduce the behavior:
1. Go to https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_Beginners_Guide.ipynb
2. Run the first 2 code fields
4. See error: TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'

**Expected behavior**
English model should download into the default directory

**Environment (please complete the following information):**
 - OS: Windows 10 & on Colab environment
 - Python version: Python 3.7.9 & Python 3.7.11
 - Stanza version: stanza-1.2.2

**Additional context**
Tried downloading the model on my own Windows machine, and tried running it on one of the official tutorial notebooks from the documentation, but the same error occurs.
![error](https://user-images.githubusercontent.com/58341413/128402772-da3b4044-ffee-4329-b011-08f74cf20b65.png)
",
495,2021-08-05T15:22:07Z,https://github.com/stanfordnlp/stanza/issues/782,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
496,2021-08-03T19:23:31Z,https://github.com/stanfordnlp/stanza/pull/781,,"Two minor bugfixes and a few small improvements to the NER training process.  Also, two new NER models",
497,2021-08-02T03:56:17Z,https://github.com/stanfordnlp/stanza/issues/780,,"How to add a prop for tokenregex using python ?
[https://nlp.stanford.edu/software/tokensregex.html](url) mentions how to use a basic_ner.rules for rule based annotations, i have tried adding to prop but it didnt work. Something similar to [https://stackoverflow.com/questions/61235575/stanford-corenlp-tokensregex-error-while-parsing-the-rules-file-in-python](url) . Am i missing something ?
Or can you provide an example of how to pass on a rule file for tokenregex on the corenlp client ",
498,2021-07-31T17:45:48Z,https://github.com/stanfordnlp/stanza/issues/779,,"Hi,
This code,
nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos,depparse,sentiment') 
doc = nlp(""No worries!"")

gives output sentiment as negative as follows:
[
  {
    ""id"": 1,
    ""text"": ""No"",
    ""upos"": ""DET"",
    ""xpos"": ""DT"",
    ""start_char"": 0,
    ""end_char"": 2
  },
  {
    ""id"": 2,
    ""text"": ""worries"",
    ""upos"": ""NOUN"",
    ""xpos"": ""NNS"",
    ""feats"": ""Number=Plur"",
    ""start_char"": 3,
    ""end_char"": 10
  },
  {
    ""id"": 3,
    ""text"": ""!"",
    ""upos"": ""PUNCT"",
    ""xpos"": ""."",
    ""start_char"": 10,
    ""end_char"": 11
  }
] 0",
499,2021-07-27T12:52:08Z,https://github.com/stanfordnlp/stanza/issues/778,,"

### **Code Below**
```
import stanza 
nlp = stanza.Pipeline('ur', processors='tokenize',tokenize_no_ssplit=True)

def tokenizee(text):
    text=nlp(text)
    return text
```

```
import urduhack
from urduhack.preprocessing import replace_urls
from urduhack.preprocessing import remove_english_alphabets
from urduhack.preprocessing import normalize_whitespace
from urduhack.normalization import remove_diacritics
from urduhack import normalize
from urduhack.tokenization import word_tokenizer
from urduhack.normalization import normalize_characters
from urduhack.normalization import normalize_combine_characters

punctuations = '''`%÷×؛<>_()*&^%][ـ،/:""؟—.,'{}~¦+|!”…“–ـ''' + string.punctuation


#normalize text
def normalize_text(text):
    text =normalize(text)
    return text

#normalize_characters
def normalize_chars(text):
    text =normalize_characters(text)
    return text

#normalize_combine_characters
def normalize_combine_chars(text):
    text =normalize_combine_characters(text)
    return text

#remove urls
def remove_urls(text):
    text =replace_urls(text)
    return text

#remove diacritics
def remove_diacriticss(text):
    text =remove_diacritics(text)
    return text

#normalize whitespace
def normalize_white_space(text):
    text = normalize_whitespace(text)
    return text

#remove english letters
def remove_english_letters(text):
    text = remove_english_alphabets(text)
    return text

#remove punctuations
def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations)
    text = text.translate(translator)
    return text


# remove numbers
def remove_numbers(text):
    result = re.sub(r'\d+', '', text)
    return result

# tokenize
def tokenize(text):
    text = word_tokenize(text)
    return text

# remove stopwords
stop_words = set(stopwords.words('urdu.txt'))
def remove_stopwords(text):
    text = [i for i in text if not i in stop_words]
    return text


#Remove Emoji
def remove_emoji(text):
    emoji_pattern = re.compile(""[""
                               u""\U0001F600-\U0001F64F""  # emoticons
                               u""\U0001F300-\U0001F5FF""  # symbols & pictographs
                               u""\U0001F680-\U0001F6FF""  # transport & map symbols
                               u""\U0001F1E0-\U0001F1FF""  # flags (iOS)
                               u""\U00002500-\U00002BEF""  # chinese char
                               u""\U00002702-\U000027B0""
                               u""\U00002702-\U000027B0""
                               u""\U000024C2-\U0001F251""
                               u""\U0001f926-\U0001f937""
                               u""\U00010000-\U0010ffff""
                               u""\u2640-\u2642""
                               u""\u2600-\u2B55""
                               u""\u200d""
                               u""\u23cf""
                               u""\u23e9""
                               u""\u231a""
                               u""\ufe0f""  # dingbats
                               u""\u3030""
                               ""]+"", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)






def preprocess(text):
    text=normalize_chars(text)
    text=normalize_combine_chars(text)
    text=remove_urls(text)
    text=remove_emoji(text)
    text=remove_diacriticss(text)
    text=normalize_white_space(text)
    text=remove_english_letters(text)
    text=remove_punctuations(text)
    text=remove_numbers(text)
    text= nlp(text)
    return text
```
`DAS['Text'][0:5].apply(preprocess)`

### **Output**

**0    [\n  [\n    {\n      ""id"": 1,\n      ""text"": ""...
1    [\n  [\n    {\n      ""id"": 1,\n      ""text"": ""...
2    [\n  [\n    {\n      ""id"": 1,\n      ""text"": ""...
3    [\n  [\n    {\n      ""id"": 1,\n      ""text"": ""...
4    [\n  [\n    {\n      ""id"": 1,\n      ""text"": ""...
Name: Text, dtype: object**



",
500,2021-07-26T22:19:07Z,https://github.com/stanfordnlp/stanza/pull/777,,"## Description
This PR fixes an issue where if `tokenize` processor is passed without `mwt` and there is a corresponding `tokenize` and `mwt` pair in the resources file. This fixes the scenario where the user downloads and uses only the `tokenize` processor.

## Fixes Issues
#774

## Unit test coverage
Unit test to that download and installation for package that has tokenize and mwt pair works. 

## Known breaking changes/behaviors
N / A
",
501,2021-07-26T21:57:07Z,https://github.com/stanfordnlp/stanza/pull/776,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description

This new version of the tokenizer model incorporates the dictionary feature, especially useful for languages that
have multi-syllable words such as Vietnamese, Chinese or Thai. In summary, a lexicon contains all unique words found in 
training dataset, and an external lexicon (if any) is created during training and saved alongside the model after training.
Using this lexicon, a dictionary is created which includes ""words"", ""prefixes"" and ""suffixes"" sets. During data preparation,
dictionary features are extracted at each character position, to ""look ahead"" and ""look backward"" to see if any words formed found in the dictionary. The window size (or the dictionary feature-length) is defined at the 95-percentile among all the existing words in the lexicon, this is to eliminate the less frequent but long words (avoid having a high-dimension feat vector). Prefixes and suffixes are used to stop early during the window-dictionary checking process.  

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

Changes are made in:
- stanza/models/tokenization/data.py
- stanza/models/tokenization/utils.py
- stanza/models/tokenization/model.py
- stanza/models/tokenization/trainer.py
- stanza/models/tokenizer.py

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?

- Removed a redundant feature in the feature list (capitalized and app_caps). This might break down current models. It can be addressed by re-training all the tokenizer again. 
",
502,2021-07-26T19:21:45Z,https://github.com/stanfordnlp/stanza/issues/775,,"import stanza 
nlp = stanza.Pipeline('ur', processors='tokenize')

def tokenizee(text):
    text=nlp(text)
    return text

def preprocess(text):   
    text=tokenizee(text)
    return text

DAS['Text'][0:5].apply(preprocess)",
503,2021-07-26T09:37:35Z,https://github.com/stanfordnlp/stanza/issues/774,,"**Describe the bug**
When installing only the ""tokenize"" processor for a specific language it does not install the `mwt` processor when required and the Pipeline instantiation fails. This was introduced in version 1.2.1.

**To Reproduce**
Steps to reproduce the behavior:
```python
In [1]: import stanza

In [2]: stanza.download(""en"", processors=""tokenize"", package=""ewt"")
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 139kB [00:00, 138MB/s]
2021-07-26 10:32:30 INFO: Downloading these customized packages for language: en (English)...
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
=======================

Downloading http://nlp.stanford.edu/software/stanza/1.2.2/en/tokenize/ewt.pt: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 632k/632k [00:01<00:00, 427kB/s]
2021-07-26 10:32:34 INFO: Finished downloading models and saved to ...\stanza_resources.

In [3]: pipeline = stanza.Pipeline(lang=""en"", processors=""tokenize"", package=""ewt"")
2021-07-26 10:32:36 WARNING: Language en package ewt expects mwt, which has been added
2021-07-26 10:32:36 INFO: Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
| mwt       | ewt     |
=======================

2021-07-26 10:32:36 INFO: Use device: cpu
2021-07-26 10:32:36 INFO: Loading: tokenize
2021-07-26 10:32:36 INFO: Loading: mwt
2021-07-26 10:32:36 ERROR: Cannot load model from ...\stanza_resources\en\mwt\ewt.pt
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
...\site-packages\stanza\pipeline\core.py in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)
    135                 # try to build processor, throw an exception if there is a requirements issue
--> 136                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
    137                                                                                           pipeline=self,

...\site-packages\stanza\pipeline\processor.py in __init__(self, config, pipeline, use_gpu)
    158         if not hasattr(self, '_variant'):
--> 159             self._set_up_model(config, use_gpu)
    160

...\site-packages\stanza\pipeline\mwt_processor.py in _set_up_model(self, config, use_gpu)
     20     def _set_up_model(self, config, use_gpu):
---> 21         self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
     22

...\site-packages\stanza\models\mwt\trainer.py in __init__(self, args, vocab, emb_matrix, model_file, use_cuda)
     35             # load from file
---> 36             self.load(model_file, use_cuda)
     37         else:

...\site-packages\stanza\models\mwt\trainer.py in load(self, filename, use_cuda)
    140         try:
--> 141             checkpoint = torch.load(filename, lambda storage, loc: storage)
    142         except BaseException:

...\site-packages\torch\serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    593
--> 594     with _open_file_like(f, 'rb') as opened_file:
    595         if _is_zipfile(opened_file):

...\site-packages\torch\serialization.py in _open_file_like(name_or_buffer, mode)
    229     if _is_path(name_or_buffer):
--> 230         return _open_file(name_or_buffer, mode)
    231     else:

...\site-packages\torch\serialization.py in __init__(self, name, mode)
    210     def __init__(self, name, mode):
--> 211         super(_open_file, self).__init__(open(name, mode))
    212

FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\dariffde\\stanza_resources\\en\\mwt\\ewt.pt'

The above exception was the direct cause of the following exception:

FileNotFoundError                         Traceback (most recent call last)
<ipython-input-3-d608c527ebd4> in <module>
----> 1 pipeline = stanza.Pipeline(lang=""en"", processors=""tokenize"", package=""ewt"")

...\site-packages\stanza\pipeline\core.py in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)
    161                         # TODO: before recommending this, check that such a thing exists in resources.json.
    162                         # currently that case is handled by ignoring the model, anyway
--> 163                         raise FileNotFoundError('Could not find model file %s, although there are other models downloaded for language %s.  Perhaps you need to download a specific model.  Try: stanza.download(lang=""%s"",package=None,processors={""%s"":""%s""})' % (model_path, lang, lang, processor_name, model_name)) from e
    164
    165                 # if we couldn't find a more suitable description of the

FileNotFoundError: Could not find model file ...\stanza_resources\en\mwt\ewt.pt, although there are other models downloaded for language en.  Perhaps you need to download a specific model.  Try: stanza.download(lang=""en"",package=None,processors={""mwt"":""ewt""})
````



**Expected behavior**
In Stanza 1.2 this works fine:

```python
In [1]: import stanza

In [2]: stanza.download(""en"", processors=""tokenize"", package=""ewt"")
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 64.0MB/s]
2021-07-26 10:35:09 WARNING: Language en package ewt expects mwt, which has been added
2021-07-26 10:35:09 INFO: Downloading these customized packages for language: en (English)...
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
| mwt       | ewt     |
=======================

Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/tokenize/ewt.pt: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 631k/631k [00:01<00:00, 426kB/s]
Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/mwt/ewt.pt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 586k/586k [00:01<00:00, 394kB/s]
2021-07-26 10:35:15 INFO: Finished downloading models and saved to C:\Users\dariffde\stanza_resources.

In [3]: pipeline = stanza.Pipeline(lang=""en"", processors=""tokenize"", package=""ewt"")
2021-07-26 10:35:19 WARNING: Language en package ewt expects mwt, which has been added
2021-07-26 10:35:19 INFO: Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
| mwt       | ewt     |
=======================

2021-07-26 10:35:19 INFO: Use device: cpu
2021-07-26 10:35:19 INFO: Loading: tokenize
2021-07-26 10:35:19 INFO: Loading: mwt
2021-07-26 10:35:19 INFO: Done loading processors!
```

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: 3.8.10
 - Stanza version: >1.2

**Additional context**
If this is really a bug and not an expected behavior I would love to give it a go and contribute to the project. Thanks!
",
504,2021-07-26T07:42:21Z,https://github.com/stanfordnlp/stanza/pull/773,,"WIP: add a constituency parser.  Needs to be MUCH faster, hopefully achievable by batching.  Also needs an interface to the pipeline.  Current accuracy is not quite known because it takes too long to run...",
505,2021-07-26T07:15:03Z,https://github.com/stanfordnlp/stanza/pull/772,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
506,2021-07-23T02:04:54Z,https://github.com/stanfordnlp/stanza/pull/771,,"Include code for few NER improvements:
- confusion matrix over results
- process non-BIO tags to BIOES
- IT ner model
- AF ner model",
507,2021-07-20T15:30:06Z,https://github.com/stanfordnlp/stanza/issues/770,,"**Describe the bug**
Connection reset by peer when downloading models.

```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: 139kB [00:00, 9.01MB/s]
2021-07-20 10:27:07 INFO: Downloading default packages for language: en (English)...
Traceback (most recent call last):
...
ConnectionResetError: [Errno 104] Connection reset by peer
```

**To Reproduce**
```sh
python -c ""import stanza; stanza.download('en')""
```

**Expected behavior**
The model is downloaded.

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: Python 3.8.10 from Anaconda
 - Stanza version: 1.2.2

**Additional context**
It seems that the CoreNLP website https://nlp.stanford.edu/ is also not responding.
",
508,2021-07-20T12:28:31Z,https://github.com/stanfordnlp/stanza/issues/769,,"**Describe the bug**
Sentiments analysis on an empty input raises the following exception: `IndexError: tuple index out of range`.
I didn't see this error mentionned in current issues.

The pipeline used to obtain this result:
```python
stanza.Pipeline(lang='en', processors='tokenize,sentiment')
```

The actual result:
```
2021-07-20 14:21:11 INFO: Loading these models for language: en (English):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| sentiment | sstplus  |
========================

2021-07-20 14:21:11 INFO: Use device: gpu
2021-07-20 14:21:11 INFO: Loading: tokenize
2021-07-20 14:21:18 INFO: Loading: sentiment
2021-07-20 14:21:18 INFO: Done loading processors!
Traceback (most recent call last):
  File ""test.py"", line 4, in <module>
    doc = nlp('')
  File ""/home/amillo/.local/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 253, in __call__
    doc = self.process(doc)
  File ""/home/amillo/.local/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 247, in process
    doc = process(doc)
  File ""/home/amillo/.local/lib/python3.8/site-packages/stanza/pipeline/sentiment_processor.py"", line 53, in process
    labels = cnn_classifier.label_text(self._model, text, batch_size=self._batch_size)
  File ""/home/amillo/.local/lib/python3.8/site-packages/stanza/models/classifiers/cnn_classifier.py"", line 449, in label_text
    text, orig_idx = sort_with_indices(text, key=len, reverse=True)
  File ""/home/amillo/.local/lib/python3.8/site-packages/stanza/models/common/utils.py"", line 223, in sort_with_indices
    return result[1], result[0]
IndexError: tuple index out of range
```

**To Reproduce**
To get the error, you can try this basic snippet of code, which is a slightly modified example from the official documentation.
However, this example assumes that you've already downloaded the corresponding model.

```python
import stanza

nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')
doc = nlp('')
for i, sentence in enumerate(doc.sentences):
    print(i, sentence.sentiment)
```

**Expected behavior**
The expected result would be to get back a working document, without any exception.
The expected output (from my limited experience) should be:

```
2021-07-20 14:06:44 INFO: Loading these models for language: en (English):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| sentiment | sstplus  |
========================

2021-07-20 14:06:44 INFO: Use device: gpu
2021-07-20 14:06:44 INFO: Loading: tokenize
2021-07-20 14:06:52 INFO: Loading: sentiment
2021-07-20 14:06:52 INFO: Done loading processors!
```

**Environment (please complete the following information):**
 - OS: Ubuntu (WSL 2 - 21H2 - Build 22000.71)
 - Python version: Python 3.8.10
 - Stanza version: 1.2.2

**Additional context**
I don't know if this error is happening with older versions of Stanza as well, since I didn't try to feed any pipeline with an empty input before. Nevertheless, I've never met this error with Stanza 1.2.0.
",
509,2021-07-20T11:30:36Z,https://github.com/stanfordnlp/stanza/issues/768,,"## Aim
To finetune an NER model and save it in another location. 

## Why I want to do this 
Finetuning the model does not always make it do better on the test set.  So I would like to  be able to finetune a model with different hyperparams and check it against the original model performance

## Issue
Fine tuning seems to overwrite the original model rather than allow a different path at which to save the model

## Additional info
Branch used:1.2.2  Commit: de44be871282e05f79f23f5f5e284aceb672726b

## What I've tried
Command:
```
MAX_STEPS=500 # param to debug shell scripts
# Now train the NER using finetuning (also finetunes the embedding)
python -m stanza.models.ner_tagger \
    --wordvec_pretrain_file /home/app/stanza_resources/en/pretrain/mimic.pt \
    --train_file /home/app/data/ner/i2b2/trainshort.json \
    --eval_file /home/app/data/ner/i2b2/devshort.json \
    --mode train\
    --lang en \
    --finetune \
    --word_emb_dim 200 \
    --scheme bio \
    --max_steps $MAX_STEPS \
    --charlm_forward_file /home/app/stanza_resources/en/forward_charlm/mimic.pt \
    --charlm_backward_file /home/app/stanza_resources/en/backward_charlm/mimic.pt \
    --save_dir /home/app/stanza_resources/en/ner/i2b2.pt \
    --save_name /home/app/stanza_resources/en/ner/finetune_bio.pt
```
However this yields the following message
> 2021-07-20 10:38:36 INFO: Running NER tagger in train mode
2021-07-20 10:38:36 WARNING: **Finetune is set to true but model file is not found. Continuing with training from scratch.**

even though the model file path was specified in `--save_dir`.

To allow the program to find and use the existing trained file required setting the parameter `--save_name` to the path of the original i2b2.pt file (`/home/app/stanza_resources/en/ner/i2b2.pt `).  **This runs counter to the help instructions in the ner_tagger.py file** that states that 
> setting the finetune parameter would  ""Load existing model during `train` mode from `save_dir` path""

In addition - in this case I am not sure how to set the path to the folder/file to which the finetuned model should be saved.  Unsure if this is a bug in the doc or if I misunderstood it.  Insights appreciated. ",
510,2021-07-20T04:10:05Z,https://github.com/stanfordnlp/stanza/pull/767,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
511,2021-07-20T02:38:10Z,https://github.com/stanfordnlp/stanza/pull/766,,"Finish integrating lst20 into the prepare_tokenization_dataset script, along with an option to use or not use pythainlp segmentation to redo the sentence splits",
512,2021-07-19T16:13:01Z,https://github.com/stanfordnlp/stanza/pull/765,,"Integrate a conversion script for lst20 into the tokenizer dataset script.  Sentence splitting is not working great, though.",
513,2021-07-17T01:30:15Z,https://github.com/stanfordnlp/stanza/pull/764,,Move orchid (and best) thai processing script into stanza/utils/datasets/tokenization and integrate with prepare_tokenization,
514,2021-07-16T20:48:30Z,https://github.com/stanfordnlp/stanza/pull/763,,Add a script for converting the tokenization portion of the VLSP data,
515,2021-07-16T19:59:50Z,https://github.com/stanfordnlp/stanza/pull/762,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
516,2021-07-16T18:44:13Z,https://github.com/stanfordnlp/stanza/pull/761,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
517,2021-07-16T13:15:26Z,https://github.com/stanfordnlp/stanza/pull/760,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
518,2021-07-16T03:36:57Z,https://github.com/stanfordnlp/stanza/pull/759,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
519,2021-07-15T23:52:16Z,https://github.com/stanfordnlp/stanza/pull/758,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
520,2021-07-15T18:03:02Z,https://github.com/stanfordnlp/stanza/issues/757,,"## tl;dr
Running the latest pull of `run_ner` with a pretrained model yields an error about treebank and using the stanza.models.ner_tagger completes an evaluation run but returns zero accuracy/f1 scores.

### Background information 
Downloaded the pre-trained models for clinical NER and tested that they work fine when used for inference (via stanza pipelines). However as my aim is to train/fine tune other NER models - I started by trying to evaluate a trained NER model on clinical data. 

## First try using the new `run_ner` script
```
python -m stanza.utils.training.run_ner --score_test \
    --wordvec_pretrain_file /home/app/stanza_resources/en/pretrain/mimic.pt \
    --eval_file /home/app/data/ner/i2b2/testnew.json \
    --lang en \
    --mode predict \
    --save_dir /home/app/stanza_resources/en/ner \
    --save_name i2b2.pt 	
```
Unfortunately led to the following stack trace. 

>   Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/app/stanza/stanza/utils/training/run_ner.py"", line 166, in <module>
    main()
  File ""/home/app/stanza/stanza/utils/training/run_ner.py"", line 163, in main
    common.main(run_treebank, ""ner"", ""nertagger"", add_ner_args)
  File ""/home/app/stanza/stanza/utils/training/common.py"", line 89, in main
    short_name = treebank_to_short_name(treebank)
  File ""/home/app/stanza/stanza/models/common/constant.py"", line 163, in treebank_to_short_name
    assert len(splits) == 2, ""Unable to process %s"" % treebank
AssertionError: Unable to process /home/app/stanza_resources/en/pretrain/mimic.pt
	
I was unable to dig into the root cause of the above so I just went ahead with using the  `ner_tagger` module as follows
```
 python -m stanza.models.ner_tagger \
     --wordvec_pretrain_file /home/app/stanza_resources/en/pretrain/mimic.pt \
     --word_emb_dim 200 \
     --eval_file /home/app/data/ner/i2b2/testnew.json \
     --mode predict \
     --cpu \
     --lang en \
     --save_dir /home/app/stanza_resources/en/ner \
     --save_name i2b2.pt \
     --scheme bio \
     --charlm_forward_file /home/app/stanza_resources/en/forward_charlm/mimic.pt \
     --charlm_backward_file /home/app/stanza_resources/en/backward_charlm/mimic.pt 
```
This runs  but yields the following output
> 2021-07-15 17:53:31 INFO: Running NER tagger in predict mode
2021-07-15 17:53:31 INFO: Loading data with batch size 32...
2021-07-15 17:53:32 DEBUG: 6 batches created.
2021-07-15 17:53:32 INFO: Start evaluation...
2021-07-15 17:53:39 INFO: Prec. Rec.    F1d
2021-07-15 17:53:39 INFO: 0.00  0.00    0.00
2021-07-15 17:53:39 INFO: NER tagger score:
2021-07-15 17:53:39 INFO: None 0.00

Which is pretty unexpected given that the same model works using the stanza pipeline. 
NOTE: I only took a smaller number of phrases/sentences 6 batches of size 32 however this still would not explain these results. My suspicion is that I'm missing an step/layer. From my understanding the NER requires 
- char embedding for contextual information (and i have linked both forward/backward layers) and 
-  word embedding (for which I provide the pretrained file path)
As the input is already in BIO form (converted to json) there was no need to provide a tokenizer.  

Any insight would be welcome. Thank you.

",
521,2021-07-15T14:31:51Z,https://github.com/stanfordnlp/stanza/issues/756,,"I am trying to run two (bc4chemd and bc5cdr) NER models on pretokenized data sets in parallel (if this isn't the best way to go about this please advise). Upon researching I found a similar previously closed issue: #79. I tried the code that from the issue (with some minor print statements to follow allow) and it is pasted below. Code hangs on second Pool call at the bottom. 

`from concurrent import futures
import stanza
if __name__ == '__main__':
    nlp = stanza.Pipeline(model_dir='./stanza_resources/')

    def process(text):
        return len(text)

    def process_nlp(text):
        d = nlp(text)
        return len(d.text)

    texts = [""This is a text."", ""This is another text.""]

    # Process with NLP serially
    print([process_nlp(t) for t in texts])
    
    # Process without NLP in parallel
    print(""Process not in parallel"")
    result = []
    with futures.ProcessPoolExecutor(max_workers=1) as executor:
         for res in executor.map(process, texts, chunksize=1):
            result.append(res)
    print(result)

    # Process with NLP in parallel
    print(""Process in parallel"")
    result = []
    with futures.ProcessPoolExecutor(max_workers=1) as executor:
         for res in executor.map(process_nlp, texts, chunksize=1):
            result.append(res)
    print(result)`


**Environment (please complete the following information):**
 - OS: Ubuntu 20.04 LTS on Windows 10
 - Python version: 3.7.10
 - Stanza version: tried Stanza 1.2.2 and 1.3.0rc1 (seperate conda envs)
 - Torch version : 1.9.0

If this isn't the right way to run multiple NER models, please advise (just started to dive into parallel processing). Thanks!",
522,2021-07-14T17:03:01Z,https://github.com/stanfordnlp/stanza/issues/755,,"I was reading through the documentation on [training](https://stanfordnlp.github.io/stanza/training.html) where I got the impression that the models are exclusively trained on the publicly available UD treebanks. However, there are two exceptions:

- [handparsed treebanks](https://github.com/stanfordnlp/handparsed-treebank): for ""combined models"" (not sure what that implies though)
- data augmentation in code:

https://github.com/stanfordnlp/stanza/blob/de44be871282e05f79f23f5f5e284aceb672726b/stanza/models/tagger.py#L140-L144

So my question is: are there any other instances of data augmentation or other datasets that are used for training the models, or do you only rely on the public UD treebanks? For the initial word vectors, you seem to use pretrained ones from CoNLL and fastText as described [here](https://stanfordnlp.github.io/stanza/word_vectors.html).",
523,2021-07-14T04:27:11Z,https://github.com/stanfordnlp/stanza/issues/754,,"this is error info:
stanza.server.client.AnnotationException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space

this is my code:
client = CoreNLPClient(annotators=['tokenize', 'pos', 'lemma', 'ner', 'parse', 'depparse'],
                memory='8G',
                endpoint='http://localhost:9001',
                be_quiet=True,
                threads=4,
                timeout=600000,
                max_char_length=1000000,
                properties={
                    ""ssplit.isOneSentence"":True
                }
                )",
524,2021-07-13T17:54:27Z,https://github.com/stanfordnlp/stanza/issues/753,,"I read in [the documentation](https://stanfordnlp.github.io/stanza/performance.html) that the reported performance metrics are based on the evaluation script of the [CoNLL 2018 shared task](https://universaldependencies.org/conll18/evaluation.html). But I cannot find the gold labels that are required to run that script. In the side bar there is a link to [""Test data made public""](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2837) but I cannot find a download button or any useful information on that end.

Can you direct me to the gold dataset, or share it if it is not available publicly anymore?

Thanks",
525,2021-07-13T16:36:42Z,https://github.com/stanfordnlp/stanza/issues/752,,"Hey, I want to train a new model and follow the instructions in the documentation. I prepared UD_English-Tweet UD data in a folder and wanted to prepare tokenizer data for tokenizer training, so I used `python stanza/utils/datasets/prepare_tokenizer_treebank.py UD_English-Tweet` to convert the data. However, I run into the following error. 

```
user$ python stanza/utils/datasets/prepare_tokenizer_treebank.py UD_English-Tweet
2021-07-13 00:33:15 INFO: Datasets program called with:
stanza/utils/datasets/prepare_tokenizer_treebank.py UD_English-Tweet
Preparing data for UD_English-Tweet: en_tweet, en
Augmented 13 quotes: Counter({'""""': 3, '„“': 3, '““': 3, '《》': 1, '””': 1, '„”': 1, '「」': 1})
Swapped 'w1, w2' for 'w1 ,w2' 6 times
Added 4 new sentences with asdf, zzzz -> asdf,zzzz
Traceback (most recent call last):
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1062, in <module>
    main()
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1059, in main
    common.main(process_treebank, add_specific_args)
  File ""/u/hjian42/twitternlp/stanza/stanza/utils/datasets/common.py"", line 140, in main
    process_treebank(treebank, paths, args)
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 1050, in process_treebank
    process_ud_treebank(treebank, udbase_dir, tokenizer_dir, short_name, short_language, args.augment)
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 969, in process_ud_treebank
    prepare_ud_dataset(treebank, udbase_dir, tokenizer_dir, short_name, short_language, ""test"", augment)
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 959, in prepare_ud_dataset
    shutil.copyfile(input_conllu, output_conllu)
  File ""/usr/lib/python3.8/shutil.py"", line 238, in copyfile
    if _samefile(src, dst):
  File ""/usr/lib/python3.8/shutil.py"", line 217, in _samefile
    return os.path.samefile(src, dst)
  File ""/usr/lib/python3.8/genericpath.py"", line 100, in samefile
    s1 = os.stat(f1)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType
```

My understanding is that I need to add some `--save_dir` argument. So I tried `python stanza/utils/datasets/prepare_tokenizer_treebank.py UD_English-Tweet --save_dir xxx`  and it reports another error `prepare_tokenizer_treebank.py: error: unrecognized arguments: --save_dir ./`

Can you provide some guidance about what I need to specify in `${other options}` for UD tokenizer data preparation? Thanks. 

It would be really helpful for users if some examples on how to use `${other options}` for other UD data preparation/model training are provided. Thanks! ",
526,2021-07-13T13:58:27Z,https://github.com/stanfordnlp/stanza/issues/751,,"Hello

As a project of mine I am training Universal Dependencies models for spaCy. Performance up to now is quite good but the lemmatizer is rule-based and slightly lacking. I know that stanza allows for different ways to lemmatize, some lookup-based and others learned (e.g. edits). The look-ups are available in the torch-pickled lemmatizer models.

```python
import torch
model = torch.load(""~/stanza_resources/en/lemma/combined.pt""), map_location='cpu')
word_dict, composite_dict = model['dicts']
```

... and from looking at them, they seem to be quite extensive. I would therefore like to use these in my spaCy lemmatizer. Is such usage allowed? More information about the project:

- models will be made available to the public (particularly on the HF [model hub](https://huggingface.co/models))
- I will refer to stanza/your papers as you request in the acknowledgments
- if requested I can add a LICENSE file for your lemmatizer files

If this usage is not allowed I will try to find other options. 


Thanks

Bram",
527,2021-07-13T04:53:23Z,https://github.com/stanfordnlp/stanza/pull/750,,,
528,2021-07-10T02:54:37Z,https://github.com/stanfordnlp/stanza/pull/749,,"These are the bugfix pushes, not Jason's langid model, so that we can make 1.2.x a functional endpoint",
529,2021-07-09T16:49:29Z,https://github.com/stanfordnlp/stanza/issues/748,,"Hello All

## Problem
Getting NER finetuning/retraining for MIMIC models/biomed models  working

## What I've done so far:
1. Checked the instructions at the [model training documentation](https://stanfordnlp.github.io/stanza/training.html)  as the  stanza_train instructions seemed to be outdated.
2. Attempted to follow the instructions for 1.2.1 main branch with a toy example 
3. Tried to repeat the above out of the dev branch as the stanza.models.ner_tagger.py file does not seem to be in the main branch 
4. Searched here for previous questions regarding this particular issue. None found. 

Thought it might be time to seek some help here. Any insights would be very appreciated. 

## Details
Converted a train and dev CONLL file to json using the command. Note these were in the stanza_train data folder. Yes these are not mimic/bioner files however for the purpose of debugging this I suspect it would be fine. 
```
python stanza/utils/datasets/ner/prepare_ner_file.py  <snip>/English-TEST/train.bio <snip>/data/nerbase/English-TEST/train.json
```
these worked fine
Then tried to run the training command
``` 
python -m stanza.models.ner_tagger --wordvec_pretrain_file /home/stanza_resources/en/pretrain/mimic.pt \
 --train_file /home/stanza-train/data/nerbase/English-TEST/train.json \
 --eval_file /home/stanza-train/data/nerbase/English-TEST/dev.json --shorthand mimic \
  --mode train  --lang mimic
```
I was not too sure about the `lang` for the above.  

Trace: 
 >  /data/nerbase/English-TEST/dev.json --shorthand mimic --mode train  --lang mimic
  2021-07-09 18:54:46 INFO: Running NER tagger in train mode
  2021-07-09 18:54:46 INFO: Loading data with batch size 32...
  2021-07-09 18:54:46 DEBUG: BIO tagging scheme found in input; converting into BIOES scheme...
  2021-07-09 18:54:46 DEBUG: Loaded pretrain from /home/stanza_resources/en/pretrain/mimic.pt
  2021-07-09 18:54:46 DEBUG: 1 batches created.
  2021-07-09 18:54:46 DEBUG: BIO tagging scheme found in input; converting into BIOES scheme...
  2021-07-09 18:54:46 DEBUG: 1 batches created.
  2021-07-09 18:54:46 INFO: Training tagger...
  Traceback (most recent call last):
    File ""/usr/local/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/local/lib/python3.8/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/home/stanza/stanza/models/ner_tagger.py"", line 272, in <module>
      main()
    File ""/home/stanza/stanza/models/ner_tagger.py"", line 105, in main
      train(args)
    File ""/home/stanza/stanza/models/ner_tagger.py"", line 159, in train
      trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'],
    File ""/home/stanza/stanza/models/ner/trainer.py"", line 58, in __init__
      self.model = NERTagger(args, vocab, emb_matrix=pretrain.emb)
    File ""/home/stanza/stanza/models/ner/model.py"", line 31, in __init__
      self.init_emb(emb_matrix)
    File ""/home/stanza/stanza/models/ner/model.py"", line 75, in init_emb
      assert emb_matrix.size() == (vocab_size, dim), \
  AssertionError: Input embedding matrix must match size: 100000 x 100
  
  
  Could this be because the default values in the ner_tagger file for the word embedding dimension is 100 where as the Mimic one is 200 (in the word embedding).? Also I am not sure what the char embedding dimension should be set to here. 

Any/all ideas welcome. ",
530,2021-07-08T23:16:47Z,https://github.com/stanfordnlp/stanza/pull/747,,Merge readme change back from main into dev,
531,2021-07-08T17:38:53Z,https://github.com/stanfordnlp/stanza/issues/746,,"When specified `resources_version=1.3.0`, download fails. 

``` bash 
$ python -c ""import stanza; stanza.download(\""en\"", resources_version='1.3.0')""
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.3.0.json: 100%|███████████| 14.0/14.0 [00:00<00:00, 6.05kB/s]
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/anaconda3/envs/r2v/lib/python3.8/site-packages/stanza/resources/common.py"", line 394, in download
    resources = json.load(fin)
  File ""/usr/local/anaconda3/envs/r2v/lib/python3.8/json/__init__.py"", line 293, in load
    return loads(fp.read(),
  File ""/usr/local/anaconda3/envs/r2v/lib/python3.8/json/__init__.py"", line 357, in loads
    return _default_decoder.decode(s)
  File ""/usr/local/anaconda3/envs/r2v/lib/python3.8/json/decoder.py"", line 340, in decode
    raise JSONDecodeError(""Extra data"", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 4 (char 3)
```
",
532,2021-07-08T10:25:40Z,https://github.com/stanfordnlp/stanza/issues/745,,"Hello, dear friends, 
first of all thank you for a great programm, which helps a lot.
But can you give me a hint how can I manage with these deals:
1. I have  multiple pieces of document - 'сквозь которого видно', 'не касаясь земли' and so on. I need to analyze them and get output for each piece in new string, like this, for instance: 
ADP case PRON Acc obl ADV Degree=Pos advmod 
VERB Aspect=Imp|Tense=Pres|VerbForm=Conv|Voice=Mid rootNOUN Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing obl

I wrote code: 

import stanza
nlp = stanza.Pipeline('ru')
doc = nlp('это, увы, было' 'сквозь которого видно' 'не касаясь земли')
for sentence in doc.sentences:
        for word in sentence.words:
                print(word.text,word.pos,word.feats,word.deprel, end = ' ')
That's the best I could do. it gives me only one string. Like this:
ADP case PRON Acc obl ADV Degree=Pos advmodVERBAspect=Imp|Tense=Pres|VerbForm=Conv|Voice=Mid rootNOUN Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing obl

Everything in one string.
How can I deal with this? 

And the second is, how can I call only one morphological feature? I don't need any from them except 'case'. Is it possible to call only this one?

Sorry for may be simple questions (I suppose, they realy are), but I'm new in this. 

Best regards.",
533,2021-07-08T10:18:04Z,https://github.com/stanfordnlp/stanza/issues/744,,"Is it possible to extract the probabilities of NER predictions of the available models? 

Example:
```
Washington is a state
```

could be something like:
```
(""Washington"" , {""GPE"": 0.7, ""LOC"": 0.2, ""O"": 0.1}),
(""is"", {""O"": 1.0}),
(""a"", {""O"": 1.0}),
(""state"", {""O"": 1.0})
```",
534,2021-07-05T08:34:41Z,https://github.com/stanfordnlp/stanza/issues/743,,"Hello everyone! I have a document that contains a list of quotation such as:

Manzoni, Pr. Sp., 3 (48):
Se volete ch’io v’aiuti, bisogna dirmi tutto, dall’a fino alla zeta.

How can I keep out the first line when I run the tokenizer? I would like to get something like:

====== Sentence 1 =======
text= Manzoni, Pr. Sp., 3 (48):
Se volete ch’io v’aiuti, bisogna dirmi tutto, dall’a fino alla zeta.
1	Se	se	SCONJ	CS	_	_	_	_	_
2	volete	volere	AUX	VM	Mood=Ind|Number=Plur|Person=2|Tense=Pres|VerbForm=Fin	_	_	_	_
etc,

Is that possible?


",
535,2021-07-05T08:14:09Z,https://github.com/stanfordnlp/stanza/pull/742,,"## Description
Because the CNNClassifier contains two lambda functions to lower case a string in init, pickling fails on Windows.

## Fixes Issues
Fixes an issue I originally encountered when using `nlp.pipe()` of `spacy_stanza`, which uses multiprocessing under the hood. So it tries to pickle the model for new processes, but fails due to the lambdas. So this relates to https://github.com/explosion/spacy-stanza/issues/34

## Unit test coverage
Not necessary: the new added method has the same name as the original lambda property. Usage is identical as before. 
",
536,2021-07-04T07:16:03Z,https://github.com/stanfordnlp/stanza/issues/741,,"Describe the bug

Unable to download the Croatian pipeline using stanza.download(""hr"").
AssertionError is thrown, it appears the ""default_md5"" within ""https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json"" for ""hr"" differs from the md5 of ""http://nlp.stanford.edu/software/stanza/1.2.1/hr/default.zip""

To Reproduce

import stanza
stanza.download(""hr"")

Expected behavior

The pipeline binaries are downloaded.

Environment (please complete the following information)
 - OS: Windows
 - Python version: Python 3.7.4
 - Stanza version: 1.2.1


",
537,2021-07-02T09:39:01Z,https://github.com/stanfordnlp/stanza/pull/740,,"## Description
Because the CNNClassifier contains two lambda functions to lower case a string in init, pickling fails on Windows.

## Fixes Issues
Fixes an issue I originally encountered when using `nlp.pipe()` of `spacy_stanza`, which uses multiprocessing under the hood. So it tries to pickle the model for new processes, but fails due to the lambdas. So this relates to https://github.com/explosion/spacy-stanza/issues/34

## Unit test coverage
Not necessary: the new added method has the same name as the original lambda property. Usage is identical as before. 
",
538,2021-07-01T16:54:35Z,https://github.com/stanfordnlp/stanza/issues/739,,"After upgrading stanza from version 1.0.1 to 1.2.1 we've noticed some strange behavior with German NER, namely the performance drops significantly, a lot of entities are missed completely or in part, others are misclassified. For a concrete example things like this happen all around:
Version 1.0.1, working well:
```
>>>import stanza
>>>nlp = stanza.Pipeline('de', processors='tokenize,mwt,ner', dir='/usr/lib/stanza', use_gpu=True)
2021-07-01 18:13:16 INFO: Loading these models for language: de (German):`
=======================
| Processor | Package |
-----------------------
| tokenize  | gsd     |
| mwt       | gsd     |
| ner       | conll03 |
=======================

2021-07-01 18:13:18 INFO: Use device: gpu
2021-07-01 18:13:18 INFO: Loading: tokenize
2021-07-01 18:13:20 INFO: Loading: mwt
2021-07-01 18:13:20 INFO: Loading: ner
2021-07-01 18:13:22 INFO: Done loading processors!
>>>doc = nlp(""Wir haben uns bei Herrn Giscard d'Estaing beschwert und um Akteneinsicht in die Arbeit des Präsidiums gebeten."")
>>>doc.entities
[{
  ""text"": ""Giscard d'Estaing"",
  ""type"": ""PER"",
  ""start_char"": 24,
  ""end_char"": 41
}]`
```
Exact same code/input, only with version 1.2.1:
```
>>>doc.entities
[]
```
Environment is the same in both cases (including the models), only the stanza module is updated.
English does not seem to show this behavior, other languages we haven't (yet) tested. 
It's on a Centos 7.9 with default Python 3.6.8 but I'm not sure it plays any role here.
Any advice or help on how to fix it or how to find the possible reason is greatly appreciated.
",
539,2021-06-30T16:43:37Z,https://github.com/stanfordnlp/stanza/pull/738,,"Replaces the run_ner.sh script with a run_ner.py script - a bit more portable with a bit more functionality.

Also included are some upgrades to the charlm parameters for ner_tagger.py",
540,2021-06-30T09:24:09Z,https://github.com/stanfordnlp/stanza/issues/737,,"I trained my model with stanza, and I wanted to know how to evaluate it.
I now have test datasets and models.

I found a command:          python stanza/utils/training/run_ete.py ${corpus} --score_${split}
but I don't know how to use it

",
541,2021-06-29T20:47:29Z,https://github.com/stanfordnlp/stanza/issues/736,,"**Describe the bug**
Unable to access the demo site at http://stanza.run

**To Reproduce**
Steps to reproduce the behavior:
1. Go to 'http://stanza.run'
2. See error

**Expected behavior**
Expected to see the demo.

**Environment (please complete the following information):**
 - OS: Windows, MacOS, and iOS.

**Additional context**
We have tried to access the demo site from multiple machines and from different networks. It appears as there might be an issue with DNS.

The site was available when we accessed it last week.
",
542,2021-06-29T00:28:51Z,https://github.com/stanfordnlp/stanza/pull/735,,"Sort and then unsort the text by length in the sentiment processor.  Use this to set a limit on processing length by text length.  Set the default batch limit to 5000.  Makes it so that a huge document doesn't default to using up the entire GPU
",
543,2021-06-28T06:10:44Z,https://github.com/stanfordnlp/stanza/issues/734,,"System Specifications: - Device:- NVIDIA Jetson AGX Xavier [16 GB] - Jetpack 4.5.1
                                                      8 core CPU 
                                                      RAM:- 32 GB
The Pipeline looks like 
nlp = stanza.Pipeline('en', use_gpu=True, batch_size=100, tokenize_batch_size = 32, pos_batch_size = 32, , depparse_batch_size = 32)

doc = nlp(corpus)

I am trying to build a Stanza Document with processors:- tokenizer, pos, depparse, error, sentiment, ner;
While using a dataset of around 300MB of txt to build the Stanza Document i am running out of memory (RAM) and then the jupyter notebook stops and kernel dies, even with 100MB of data the kernel dies.
(I have tried using higher batch sizes and even lower as well but the problem persists) 
",
544,2021-06-26T08:31:24Z,https://github.com/stanfordnlp/stanza/issues/733,,"**Describe the bug**
For these two sentences: 
```
The first challenge that we have before we can do any kind of analysis of these interstellar dust particles is to find them.
```
```
But it is that a word can have just any vocal sound .
```
The word `is` in both sentences should be tagged as `AUX`, since they are both verbal copulas. According to Universal Dependencies Standard for [VERB](https://universaldependencies.org/u/pos/VERB.html) and [AUX](https://universaldependencies.org/u/pos/AUX.html), `VERB` does not cover AUX. 
>  Note that the VERB tag covers main verbs (content verbs) but it does not cover auxiliary verbs and verbal copulas (in the narrow sense), for which there is the AUX tag.

However, Stanza tags `is` as `VERB`.

**To Reproduce**
```
$ python3 test1.py 
2021-06-26 15:11:56 INFO: Loading these models for language: en (English):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| pos       | combined |
========================

2021-06-26 15:11:56 INFO: Use device: cpu
2021-06-26 15:11:56 INFO: Loading: tokenize
2021-06-26 15:11:56 INFO: Loading: pos
2021-06-26 15:11:56 INFO: Done loading processors!
The None DET
first None ADJ
challenge None NOUN
that None PRON
we None PRON
have None VERB
before None SCONJ
we None PRON
can None AUX
do None VERB
any None DET
kind None NOUN
of None ADP
analysis None NOUN
of None ADP
these None DET
interstellar None ADJ
dust None NOUN
particles None NOUN
is None VERB
to None PART
find None VERB
them None PRON
. None PUNCT

$ cat test1.py 
import stanza

#stanza.download('en')
nlp = stanza.Pipeline('en',processors='tokenize,pos',tokenize_pretokenized=True)
doc = nlp(""The first challenge that we have before we can do any kind of analysis of these interstellar dust particles is to find them ."")

for sentence in doc.sentences:
    for word in sentence.words:
        print(word.text, word.lemma, word.pos)
```
```
$ python3 test2.py 
2021-06-26 15:13:09 INFO: Loading these models for language: en (English):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| pos       | combined |
========================

2021-06-26 15:13:09 INFO: Use device: cpu
2021-06-26 15:13:09 INFO: Loading: tokenize
2021-06-26 15:13:09 INFO: Loading: pos
2021-06-26 15:13:09 INFO: Done loading processors!
But None CCONJ
it None PRON
is None VERB
that None SCONJ
a None DET
word None NOUN
can None AUX
have None VERB
just None ADV
any None DET
vocal None ADJ
sound None NOUN
. None PUNCT

$ cat test2.py 
import stanza

#stanza.download('en')
nlp = stanza.Pipeline('en',processors='tokenize,pos',tokenize_pretokenized=True)
doc = nlp(""But it is that a word can have just any vocal sound ."")

for sentence in doc.sentences:
    for word in sentence.words:
        print(word.text, word.lemma, word.pos)
```

**Expected behavior**
`is` should be tagged as `AUX`.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.9.4
 - Stanza version: 68aa42653d656f6131ec14837d5f99927ab17d02/1.2.1

**Additional context**
I notice that there are quite some incorrect labels (nearly 300 items) in the corpus [UD_English-GUM](https://github.com/UniversalDependencies/UD_English-GUM) that tag verbal copulas `be` as `VERB`, which might be the root cause. Do you think so? 
 
",
545,2021-06-25T21:04:08Z,https://github.com/stanfordnlp/stanza/pull/732,,,
546,2021-06-25T10:42:56Z,https://github.com/stanfordnlp/stanza/pull/731,,"This is the documentation for the language id module. This should be merged with gh-pages upon the next release.
",
547,2021-06-25T03:56:36Z,https://github.com/stanfordnlp/stanza/issues/730,,"**Describe the bug**

For this sentence, 
```
He also designed furniture and houses for the wealthy.
```
`wealthy` is a adjective that exceptionally head a nominal phrase, which should be tagged as `ADJ` according to [Universal Dependencies](https://universaldependencies.org/u/pos/ADJ.html)
>  On the other hand, adjectives that exceptionally head a nominal phrase (as in the sick, the healthy) are still tagged ADJ.

**To Reproduce**
```
$ python3 test.py 
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json: 139kB [00:00, 688kB/s]                                                                            
2021-06-25 11:32:45 INFO: Downloading default packages for language: en (English)...
2021-06-25 11:32:54 INFO: Finished downloading models and saved to /Users/stanza_resources.
2021-06-25 11:32:54 INFO: Loading these models for language: en (English):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| pos       | combined |
========================

2021-06-25 11:32:54 INFO: Use device: cpu
2021-06-25 11:32:54 INFO: Loading: tokenize
2021-06-25 11:32:54 INFO: Loading: pos
2021-06-25 11:32:56 INFO: Done loading processors!
He None PRON
also None ADV
designed None VERB
furniture None NOUN
and None CCONJ
houses None NOUN
for None ADP
the None DET
wealthy None NOUN
. None PUNCT

$ cat test.py
import stanza

stanza.download('en')
nlp = stanza.Pipeline('en',processors='tokenize,pos')
doc = nlp('He also designed furniture and houses for the wealthy.')

for sentence in doc.sentences:
    for word in sentence.words:
        print(word.text, word.lemma, word.pos)
```

**Expected behavior**

`wealthy` should be tagged as `ADJ`.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.9.4
 - Stanza version: 68aa42653d656f6131ec14837d5f99927ab17d02/1.2.1
",
548,2021-06-24T13:07:20Z,https://github.com/stanfordnlp/stanza/issues/729,,"How do I convert the constituency parseTree generated from the code below to [`nltk.Tree`](https://www.nltk.org/_modules/nltk/tree.html)?

```python
from stanza.server import CoreNLPClient


with CoreNLPClient(annotators=[""parse""], timeout=30000, memory=""16G"") as client:
    ann = client.annotate(""Is there such a thing as x ray glasses"")
    sentence = ann.sentence[0]
    constituency_parse = sentence.parseTree
```",
549,2021-06-24T09:09:33Z,https://github.com/stanfordnlp/stanza/issues/728,,"Updating my environment and I found that whenever I run stanza with torch 1.9.0, I get the following UserWarning:

> lib\site-packages\torch\nn\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\c10/core/TensorImpl.h:1156.)
>   return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)

It is ""just"" a warning, but I thought it could be useful if to report this.

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: 3.8.8
 - Stanza version: 1.2.1
 - torch version: 1.9.0

",
550,2021-06-23T11:49:38Z,https://github.com/stanfordnlp/stanza/issues/727,,"**Describe the bug**
* Unable to download the Swedish pipeline using stanza.download(""sv"").
* AssertionError is thrown, it appears the ""default_md5"" within ""https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json"" for ""sv"" differs from the md5 of ""http://nlp.stanford.edu/software/stanza/1.2.1/sv/default.zip""

```python
File ""app.py"", line 291, in main
    stanza.download(""sv"")
  File ""/home/svcaiaas/aiaasenv_37/lib/python3.7/site-packages/stanza/resources/common.py"", line 410, in download
    md5=resources[lang]['default_md5'],
  File ""/home/svcaiaas/aiaasenv_37/lib/python3.7/site-packages/stanza/resources/common.py"", line 141, in request_file
    assert(not md5 or file_exists(path, md5))
AssertionError
```

**To Reproduce**
```python
import stanza
stanza.download(""sv"")
```

**Expected behavior**
The pipeline binaries are downloaded.

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: 3.7.10
 - Stanza version:1.2.1

**Additional context**
sv[""default_md5""] is 639e47b5462cf968a0127e9312a37026
get_md5(path/to/default.zip) is 3765de067b38b1cdfb3da3ffb9cae554
",
551,2021-06-22T14:01:30Z,https://github.com/stanfordnlp/stanza/issues/726,,"**Description**
I ran stanza on my machine and checked the same string in the online demo and I found a mismatch between the results.

**To Reproduce**
Steps to reproduce the behavior:

*Online Demo*
1. Go to http://stanza.run/bio
2. Select Biology: BioNLP13CG
3. Paste : `Neuroblastoma cells the strain/cell line of the sample is BE2C.`
4. Click on Submit

You will observe in NER section, there are three outputs for the type *CELL*
- Neuroblastoma cells 
- cell line
- BE2C 

*On Machine*
```
import stanza
nlp = stanza.Pipeline('en', package=None, processors={'tokenize': ""mimic"", 'ner': 'bionlp13cg'})
ent = nlp(""Neuroblastoma cells the cell line of the sample is BE2C."")
print(ent.entities)
```
The output will be 
```
[{
   ""text"": ""Neuroblastoma cells"",
   ""type"": ""CELL"",
   ""start_char"": 0,
   ""end_char"": 19
 }]
```

**Expected behavior**
In the output there should be three entities
```
[{
   ""text"": ""Neuroblastoma cells"",
   ""type"": ""CELL"",
   ""start_char"": 0,
   ""end_char"": 19
},
{
   ""text"": ""cell line"",
   ""type"": ""CELL"",
   ""start_char"": 24,
   ""end_char"": 33
},
{
   ""text"": ""BE2C"",
   ""type"": ""CELL"",
   ""start_char"": 51,
   ""end_char"": 55
}]
```

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: 3.6.9
 - Stanza version: 1.2.1 & 1.2.0 (checked on both)
",
552,2021-06-17T15:19:34Z,https://github.com/stanfordnlp/stanza/issues/725,,"We are trying to leverage Stanza's dep parser with custom data. We have text in CoNLL-U format that is accompanied by other features that have been generated by a CNN, both of which are encoded into a single concatenated embedding representation which we would like to use as input directly into Stanza's dep parser.

According to the [documentation](https://stanfordnlp.github.io/stanza/depparse.html#start-with-pretagged-document), the dep parser can bypass other modules if given preprocessed input. But will it be feasible to modify the source code to accommodate this input instead of CoNLL-U files/command line text?",
553,2021-06-15T23:07:40Z,https://github.com/stanfordnlp/stanza/issues/723,,"I've tried to download Kazakh model via Stanza 1.2.1, but failed:

```py
>>> import stanza
>>> stanza.download(""kk"")
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json: 139kB [00:00, 6.28MB/s]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python3.7/site-packages/stanza/resources/common.py"", line 390, in download
    raise ValueError(f'Unsupported language: {lang}.')
ValueError: Unsupported language: kk.
```

Though `kk` is included in [Available UD Models](https://stanfordnlp.github.io/stanza/available_models.html). How do I do about it?",
554,2021-06-14T14:07:39Z,https://github.com/stanfordnlp/stanza/pull/722,,addresses https://github.com/stanfordnlp/stanza/issues/721,
555,2021-06-14T05:49:19Z,https://github.com/stanfordnlp/stanza/issues/721,,"**Describe the bug**
When annotate multiple documents together, entities of each output doc is empty (token level annotation results is available).

**To Reproduce**
Steps to reproduce the behavior:
```
nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')
documents = [""This is a test document."", ""Chris Manning teaches at Stanford University. He lives in the Bay Area.""]
in_docs = [stanza.Document([], text=d) for d in documents]
out_docs = nlp(in_docs)
print(out_docs[1].ents)
```

**Expected behavior**
Display extracted entities similar to the results of single document processing. However the result is []. When print out `out_docs[1]`, token level annotation is available.

**Environment:**
 - OS: Ubuntu
 - Python version: Python 3.9.1
 - Stanza version: 1.2.1",
556,2021-06-08T19:47:46Z,https://github.com/stanfordnlp/stanza/pull/719,,Record all the recent changes in one big pull request,
557,2021-06-07T18:25:32Z,https://github.com/stanfordnlp/stanza/pull/718,,Various code changes for UD 2.8 and some random stuff for the stanza 1.2.1 release,
558,2021-06-03T18:45:29Z,https://github.com/stanfordnlp/stanza/issues/717,,"Hi, I have to do a training with my dataset of a **new NER model**.

I'm following the instructions in [https://github.com/stanfordnlp/stanza-train](https://github.com/stanfordnlp/stanza-train) to train a NER model, so to train **2 charlm**:
```
bash scripts/run_charlm.sh English-TEST forward --epochs 2 --cutoff 0 --batch_size 2
bash scripts/run_charlm.sh English-TEST backward --epochs 2 --cutoff 0 --batch_size 2
```

and then train the **NER model**:
```
bash scripts/run_ner.sh English-TEST --max_steps 500 --word_emb_dim 5 --charlm --charlm_shorthand en_test --char_hidden_dim 1024
```

My question is: do you have experimented **good parameters** to pass to these scripts? The parameters are described in [ner_tagger.py](https://github.com/stanfordnlp/stanza/blob/main/stanza/models/ner_tagger.py) and on the [charlm.py](https://github.com/stanfordnlp/stanza/blob/main/stanza/models/charlm.py). For example a good learning rate or parameters like `--charlm`, `char_lowercase`, etc.

```python
# in ner_tagger.py
parser = argparse.ArgumentParser()
parser.add_argument('--data_dir', type=str, default='data/ner', help='Root dir for saving models.')
parser.add_argument('--wordvec_dir', type=str, default='extern_data/word2vec', help='Directory of word vectors')
parser.add_argument('--wordvec_file', type=str, default='', help='File that contains word vectors')
parser.add_argument('--wordvec_pretrain_file', type=str, default=None, help='Exact name of the pretrain file to read')
parser.add_argument('--train_file', type=str, default=None, help='Input file for data loader.')
parser.add_argument('--eval_file', type=str, default=None, help='Input file for data loader.')

parser.add_argument('--mode', default='train', choices=['train', 'predict'])
parser.add_argument('--finetune', action='store_true', help='Load existing model during `train` mode from `save_dir` path')
parser.add_argument('--train_classifier_only', action='store_true',
                    help='In case of applying Transfer-learning approach and training only the classifier layer this will freeze gradient propagation for all other layers.')
parser.add_argument('--lang', type=str, help='Language')
parser.add_argument('--shorthand', type=str, help=""Treebank shorthand"")

parser.add_argument('--hidden_dim', type=int, default=256)
parser.add_argument('--char_hidden_dim', type=int, default=100)
parser.add_argument('--word_emb_dim', type=int, default=100)
parser.add_argument('--char_emb_dim', type=int, default=100)
parser.add_argument('--num_layers', type=int, default=1)
parser.add_argument('--char_num_layers', type=int, default=1)
parser.add_argument('--pretrain_max_vocab', type=int, default=100000)
parser.add_argument('--word_dropout', type=float, default=0)
parser.add_argument('--locked_dropout', type=float, default=0.0)
parser.add_argument('--dropout', type=float, default=0.5)
parser.add_argument('--rec_dropout', type=float, default=0, help=""Word recurrent dropout"")
parser.add_argument('--char_rec_dropout', type=float, default=0, help=""Character recurrent dropout"")
parser.add_argument('--char_dropout', type=float, default=0, help=""Character-level language model dropout"")
parser.add_argument('--no_char', dest='char', action='store_false', help=""Turn off training a character model."")
parser.add_argument('--charlm', action='store_true', help=""Turn on contextualized char embedding using pretrained character-level language model."")
parser.add_argument('--charlm_save_dir', type=str, default='saved_models/charlm', help=""Root dir for pretrained character-level language model."")
parser.add_argument('--charlm_shorthand', type=str, default=None, help=""Shorthand for character-level language model training corpus."")
parser.add_argument('--char_lowercase', dest='char_lowercase', action='store_true', help=""Use lowercased characters in character model."")
parser.add_argument('--no_lowercase', dest='lowercase', action='store_false', help=""Use cased word vectors."")
parser.add_argument('--no_emb_finetune', dest='emb_finetune', action='store_false', help=""Turn off finetuning of the embedding matrix."")
parser.add_argument('--no_input_transform', dest='input_transform', action='store_false', help=""Do not use input transformation layer before tagger lstm."")
parser.add_argument('--scheme', type=str, default='bioes', help=""The tagging scheme to use: bio or bioes."")

parser.add_argument('--sample_train', type=float, default=1.0, help='Subsample training data.')
parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')
parser.add_argument('--lr', type=float, default=0.1, help='Learning rate.')
parser.add_argument('--min_lr', type=float, default=1e-4, help='Minimum learning rate to stop training.')
parser.add_argument('--momentum', type=float, default=0, help='Momentum for SGD.')
parser.add_argument('--lr_decay', type=float, default=0.5, help=""LR decay rate."")
parser.add_argument('--patience', type=int, default=3, help=""Patience for LR decay."")

parser.add_argument('--max_steps', type=int, default=200000)
parser.add_argument('--eval_interval', type=int, default=500)
parser.add_argument('--batch_size', type=int, default=32)
parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')
parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')
parser.add_argument('--save_dir', type=str, default='saved_models/ner', help='Root dir for saving models.')
parser.add_argument('--save_name', type=str, default=None, help=""File name to save the model"")

parser.add_argument('--seed', type=int, default=1234)
parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())
parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')
```
One very important question is: it is recommended to use an **uncased text** or a **cased one** for charlm and ner tagger? My intuition is uppercase is good to recognize entities, so cased. And I imagine that if the NER dataset is cased, also the plain text for charlm has to be cased and viceversa. Am I right?

If there is a **standard** set of training parameters you have used to train all the Stanza models, it should be good to follow your best practices. Thank you.

",
559,2021-06-03T07:49:01Z,https://github.com/stanfordnlp/stanza/issues/716,,"Hi, Thank you for making such a good NLP processing framework.
I try to train a new NER model on stanza, but I face some problem.
I use the stanza_train for this trainning process.

when I run this command: bash scripts/run_ner.sh English-TEST --max_steps 500 --word_emb_dim 5
I get this return 
![image](https://user-images.githubusercontent.com/60081518/120607524-d5ff4100-c482-11eb-99a4-bc631d8e4429.png)

What changes should I make to the file? thank you.
",
560,2021-06-03T00:05:49Z,https://github.com/stanfordnlp/stanza/pull/715,,"Addresses the issue of stanza installing tests in the site-packages directory

https://github.com/stanfordnlp/stanza/issues/711",
561,2021-06-02T21:33:48Z,https://github.com/stanfordnlp/stanza/issues/714,,"I'm working on extracting data from free text clinical notes and lab reports. I read about Stanza and was attracted to how it has models specifically trained for NER on clinical documents. However, a large part of my project is already written using spaCy, and I need some of the features spaCy provides like the rule-based matcher.

I found the spacy-stanza package, and it appears that I could use this to get the benefits of Stanza's clinical models, while still using the features I need from spaCy. My question is, am I losing any benefits by not using Stanza directly? ",
562,2021-06-02T21:02:14Z,https://github.com/stanfordnlp/stanza/pull/713,,"Don't normalize None or int.  Don't unnecessarily map the same text twice in mwt_expander
",
563,2021-06-02T06:09:35Z,https://github.com/stanfordnlp/stanza/issues/712,,"When I tried to download the language models using stanza.download(lang='en', processors='tokenize,pos',model_dir='C:/Users/the_User/folder/folder'), I always receive an error message like this:

Traceback (most recent call last):
  File ""C:\Users\the_User\folder\folder\Stanza.py"", line 26, in <module>
    stanza.download('en')
  File ""C:\Python37\lib\site-packages\stanza\resources\common.py"", line 411, in download
    md5=resources[lang]['default_md5']
  File ""C:\Python37\lib\site-packages\stanza\resources\common.py"", line 134, in request_file
    assert(not md5 or file_exists(path, md5))
AssertionError

Following the ideas of this post: https://github.com/stanfordnlp/stanza/issues/213

I implemented a code with mutex like this.

```
import stanza
from threading import Thread,Lock
mutex = Lock()


def processData():
    mutex.acquire()
    try:
        stanza.download(lang='zh', processors='tokenize,pos',model_dir='C:/Users/the_User/folder/folder')
    finally:
        mutex.release()

for _ in range(1):
    t = Thread(target = processData)
    t.start()
    t.join()
```

This didn't work. Now the error became:

Exception in thread Thread-1:
Traceback (most recent call last):
  File ""C:\Python37\lib\threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""C:\Python37\lib\threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\the_User\folder\folder\downloader_file.py"", line 11, in processData
    stanza.download(lang='zh', processors='tokenize,pos',model_dir='C:\Users\the_User\folder\folder\')
  File ""C:\Python37\lib\site-packages\stanza\resources\common.py"", line 433, in download
    md5=resources[lang][key][value]['md5']
  File ""C:\Python37\lib\site-packages\stanza\resources\common.py"", line 134, in request_file
    assert(not md5 or file_exists(path, md5))
AssertionError


Even in one rare ocasion when ends the download of the model fine, when I try to read the model with stanza.Pipeline
appear a pickle error maybe indicating that the file is corrupt.

Can you suggest me an idea to solve this problem?

Have you another place where I can download the models? I saw that the browsers are faster, and I even found some of your
models in https://stanfordnlp.github.io/stanza/model_history.html

But when I unzip them, the folder structure looks different from what python is appearing to download.

How can I solve this? 

Very thanks for your response!!",
564,2021-06-02T00:08:31Z,https://github.com/stanfordnlp/stanza/issues/711,,"**Describe the bug**
After running `pip install stanza`, the test cases will be installed into the site-packages of the user's environment. This actually creates issues since this package name can conflict with the user's own `test` folder, and causing Python to look for different test folders during import.  For example, I saw the following errors multiple times:

```
E               pydoc.ErrorDuringImport: problem in tests - AssertionError: Please set STANZA_TEST_HOME environment variable for test working dir, base name must be: stanza_test
```

This is because PYTHON is trying to load `tests/__init__.py` but there is an assert statement here: https://github.com/stanfordnlp/stanza/blob/main/tests/__init__.py#L17

The following is what would reside in user's `site-packages` after `pip install`:

```
ls xxxx/lib/python3.7/site-packages/tests
__init__.py               test_decorators.py        test_lemmatizer.py        test_requirements.py      test_tokenize_data.py
__pycache__               test_depparse.py          test_mwt_expander.py      test_semgrex.py           test_tokenize_utils.py
test_client.py            test_depparse_data.py     test_ner_tagger.py        test_server_misc.py       test_tokenizer.py
test_common_data.py       test_doc.py               test_prepare_resources.py test_server_request.py    test_utils.py
test_data_conversion.py   test_english_pipeline.py  test_pretrain.py          test_server_start.py
test_data_objects.py      test_installation.py      test_protobuf.py          test_tagger.py
```

The fix should also be simple, just exclude `tests*` during `find_packages` for next PYPI release:
https://github.com/stanfordnlp/stanza/blob/main/setup.py#L73

For people who want to run the test, he can probably use the source code, but I doubt that would be a common use pattern. 

**To Reproduce**
It is a bit involved to come up with the minimum reproduction that raises the above-mentioned error. But it should be clear that the current setup adds unexpected `asserts` to the user's environments.

**Expected behavior**
The tests files, or at least the `__init__` file that contains the assert statement should not be in a directory named `tests` after installation.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: python 3.7.4
 - Stanza version: 1.2

**Additional context**
Add any other context about the problem here.
",
565,2021-05-30T17:20:10Z,https://github.com/stanfordnlp/stanza/pull/710,,Misc field would not print out correctly if there was an annotation in the misc column of the conll doc.  Fix this bug and add some more tests of the data conversion code,
566,2021-05-30T06:01:15Z,https://github.com/stanfordnlp/stanza/pull/709,,Updates to add another Hungarian NER dataset and a combined model which uses all of the Hungarian data at once,
567,2021-05-28T12:48:47Z,https://github.com/stanfordnlp/stanza/pull/708,,Read an embedding vocab with spaces as nbsp instead of ascii space.  Includes a toy test of this behavior,
568,2021-05-27T22:26:50Z,https://github.com/stanfordnlp/stanza/issues/707,,"There are several open questions on how to train a new model for the recognition of named entities or other tasks in a new language, for example, Brazilian Portuguese.

I tried to configure the environment variables in Windows 10 and Linux, but without success, in the tutorial on the official page everything is extremely summarized.

I am also trying to understand where I should change the github source code and the order of changes, but without success.

Please, you could create a more detailed tutorial on how to create new models, it could even be a video shared on Youtube.

I would love to use Stanza, it is perfect for my university article, but I don't have much experience in the field of data science and programming, I can't use the tool to train a new NER model.

For example, I don't know how to train the new vector of words for my search context and what commands and folders should I run each thing.

I apologize for the inconvenience and thank you for your understanding.

By: Priscilla
",
569,2021-05-25T18:23:06Z,https://github.com/stanfordnlp/stanza/pull/706,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
A brief and concise description of what your pull request is trying to accomplish.
For some compound Vietnamese words (multi-word vocab), the word vector embeddings used non-breaking space while in training data (treebank), normal spaces are used. This discrepancy might be one of the reasons why the Vietnamese model performs poorly. 

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)
We fixed it by  replacing the "" "" (normal space character) with non-breaking space ""\xa0"" in normalize_unit() function of vocab.py (#50).  We also applied normalize_unit in __contain__ for count_coverage accordingly (#83).

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
570,2021-05-22T23:03:53Z,https://github.com/stanfordnlp/stanza/pull/705,,Add a mechanism for processing a pair of Hungarian NER datasets,
571,2021-05-22T15:02:11Z,https://github.com/stanfordnlp/stanza/pull/704,,"A couple changes to preparing & running pos/depparse models based on user experiences.  Can specify the location for getting the word vectors for depparse, and the pos & depparse models look in a couple different places for the pt vectors now",
572,2021-05-22T03:12:32Z,https://github.com/stanfordnlp/stanza/pull/703,,setup.py now includes the .pl script needed for processing conllu datasets,
573,2021-05-20T18:50:35Z,https://github.com/stanfordnlp/stanza/issues/702,,"Hi,

I run code on a dataframe of text documents as follows:

    df_ner = pd.DataFrame()
    for j in range(0, len(df_temp) - 1):
        df_ner_temp = pd.DataFrame()
        out_docs = nlp(df_temp['Text'].str.strip('\.').loc[j])
        df_ner_temp['nerstartchar'] = [f'{ent.start_char}' for ent in out_docs.ents]
        df_ner_temp['nerendchar'] = [f'{ent.end_char}' for ent in out_docs.ents]
        df_ner_temp['nertext'] = [f'{ent.text}' for ent in out_docs.ents]
        df_ner_temp['nertype'] = [f'{ent.type}' for ent in out_docs.ents]

        df_ner = df_ner.append(df_ner_temp)
    
The dataframe has many rows (about 400,000). After some time (and it does not seem to be constant when that time is, after about 40,000 iterations), I get the following error:

RuntimeError: CUDA error: unspecified launch failure

And then, if I run any particular line in the same session:

out_docs = nlp(df_temp['Text'].str.strip('\.').loc[1000])

I hit the same error (until I restart Python). Is there a way to avoid this?",
574,2021-05-18T16:11:34Z,https://github.com/stanfordnlp/stanza/issues/701,,"I was doing some basic parsing tests and found that a very mundane word was lemmatised incorrectly. The Dutch word eten (""to eat"") is incorrectly lemmatised as ""emmen"" when given in its singular form. Below is an example with variations of ""I eat like-ADV cookies"" (I like to eat cookies).

```python
import stanza

nlp = stanza.Pipeline(""nl"")

print(""Singular forms"")
print([w.lemma for w in nlp(""Ik eet graag koekjes"").sentences[0].words])
print([w.lemma for w in nlp(""Jij eet graag koekjes"").sentences[0].words])
print([w.lemma for w in nlp(""Hij eet graag koekjes"").sentences[0].words])
print([w.lemma for w in nlp(""Zij eet graag koekjes"").sentences[0].words])
print(""Plural forms"")
print([w.lemma for w in nlp(""Wij eten graag koekjes"").sentences[0].words])
print([w.lemma for w in nlp(""Jullie eten graag koekjes"").sentences[0].words])
print([w.lemma for w in nlp(""Zij eten graag koekjes"").sentences[0].words])
print(""Infinitive"")
print([w.lemma for w in nlp(""Stop met eten"").sentences[0].words])
```

Output:
```
Singular forms (incorrect lemma)
['ik', 'emmen', 'graag', 'koek']
['jij', 'emmen', 'graag', 'koek']
['hij', 'emmen', 'graag', 'koek']
['zij', 'emmen', 'graag', 'koek']
Plural forms (correct)
['wij', 'eten', 'graag', 'koek']
['jullie', 'eten', 'graag', 'koek']
['zij', 'eten', 'graag', 'koek']
Infinitive
['stoppen', 'met', 'eten']
```

As you can see, the singular forms are lemmatised incorrectly. To check, I did a quick grep (`grep ""\seet"" nl_alpino-ud-train.conllu.txt`) on the [Alpino train file](https://github.com/UniversalDependencies/UD_Dutch-Alpino/blob/master/nl_alpino-ud-train.conllu) and it does contain the right form.

```
2       eet     eten    VERB    WW|pv|tgw|ev    Number=Sing|Tense=Pres|VerbForm=Fin     0       root    0:root  _
2       eet     eten    VERB    WW|pv|tgw|ev    Number=Sing|Tense=Pres|VerbForm=Fin     0       root    0:root  _
2       eet     eten    VERB    WW|pv|tgw|ev    Number=Sing|Tense=Pres|VerbForm=Fin     0       root    0:root  _
4       eet     eten    VERB    WW|pv|tgw|ev    Number=Sing|Tense=Pres|VerbForm=Fin     1       parataxis       1:parataxis     _
2       eet     eten    VERB    WW|pv|tgw|ev    Number=Sing|Tense=Pres|VerbForm=Fin     0       root    0:root  _
```

So I am not sure where this incorrect lemmatisation is coming from.

**Environment:**
 - OS: Windows
 - Python version: 3.92.
 - Stanza version: 1.2
",
575,2021-05-17T19:02:21Z,https://github.com/stanfordnlp/stanza/issues/700,,"I have question: how can analayze the output of a dependency parser? I want to use the output tree to create an expression tree. In particular, I have already extracted the keyword in the text that represent the operations, after that i want to use a dependency tree to establish the flows of operation in the expression tree that i want to construct. Any suggestion on the methods to analyze the parse tree to do that? 
thank you",
576,2021-05-17T16:51:34Z,https://github.com/stanfordnlp/stanza/pull/699,,"Address this issue by augmenting the training data with a few sentences with the leading `¿` removed

https://github.com/stanfordnlp/stanza/issues/698
",
577,2021-05-15T07:58:49Z,https://github.com/stanfordnlp/stanza/issues/698,,"**Describe the bug**
In some cases, when a sentence lacks the opening question (¿) or exclamation (¡) mark in Spanish, the tagger will tags the first word of the sentence as PUNCT.

**To Reproduce**
Steps to reproduce the behavior:

`import stanza

processor_dict = {'tokenize':  'ancora', 'mwt': 'ancora', 'pos' : 'ancora'}

config = {'lang': 'es', 'processors':  processor_dict}

nlp = stanza.Pipeline(**config)

nlp('Mas qué me va a mí?')`

The result is:
`[
  [
    {
      ""id"": 1,
      ""text"": ""Mas"",
      ""upos"": ""PUNCT"",
      ""xpos"": ""PUNCT"",
      ""feats"": ""PunctSide=Ini|PunctType=Qest"",
      ""misc"": ""start_char=0|end_char=3""
    },
    {
      ""id"": 2,
      ""text"": ""qué"",
      ""upos"": ""PRON"",
      ""xpos"": ""PRON"",
      ""feats"": ""Number=Sing|PronType=Int,Rel"",
      ""misc"": ""start_char=4|end_char=7""
    },
    {
      ""id"": 3,
      ""text"": ""me"",
      ""upos"": ""PRON"",
      ""xpos"": ""PRON"",
      ""feats"": ""Case=Dat|Number=Sing|Person=1|PrepCase=Npr|PronType=Prs"",
      ""misc"": ""start_char=8|end_char=10""
    },
    {
      ""id"": 4,
      ""text"": ""va"",
      ""upos"": ""VERB"",
      ""xpos"": ""VERB"",
      ""feats"": ""Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin"",
      ""misc"": ""start_char=11|end_char=13""
    },
    {
      ""id"": 5,
      ""text"": ""a"",
      ""upos"": ""ADP"",
      ""xpos"": ""ADP"",
      ""feats"": ""AdpType=Prep"",
      ""misc"": ""start_char=14|end_char=15""
    },
    {
      ""id"": 6,
      ""text"": ""mí"",
      ""upos"": ""PRON"",
      ""xpos"": ""PRON"",
      ""feats"": ""Case=Acc|Number=Sing|Person=1|PrepCase=Pre|PronType=Prs"",
      ""misc"": ""start_char=16|end_char=18""
    },
    {
      ""id"": 7,
      ""text"": ""?"",
      ""upos"": ""PUNCT"",
      ""xpos"": ""PUNCT"",
      ""feats"": ""PunctSide=Fin|PunctType=Qest"",
      ""misc"": ""start_char=18|end_char=19""
    }
  ]
]`
**Expected behavior**
""Id"":1 should be as in ""¿Mas qué me va a mí?""
`    {
      ""id"": 1,
      ""text"": ""Mas"",
      ""upos"": ""ADP"",
      ""xpos"": ""ADP"",
      ""feats"": ""AdpType=Prep"",
      ""misc"": ""start_char=0|end_char=3""
    }`

**Environment (please complete the following information):**
 - OS: Debian GNU/Linux bullseye/sid x86_64 
 - Python version: 3.9.2
 - Stanza version: 1.2


**Additional context**
It does not happen always. In this case, there may be a issue with the model as ""Mas"" is wrongly tagged as ADP when it should be CCONJ. For instance, ""Pero qué me va a mí?"" is tagged correctly despite ""Pero"" and ""Mas"" being syntactically equivalent.",
578,2021-05-12T18:14:23Z,https://github.com/stanfordnlp/stanza/issues/697,,"Hello! I am training a model on a new version of Seraji treebank for Persian. When I run ```python stanza/utils/training/run_pos.py UD_Persian-newseraji```, it throws this error```NotImplementedError: Language shorthand ""fa_newseraji"" not found!```! The reason is that there is not any ```fa_newseraji``` in ```stanza/stanza/models/pos/xpos_vocab_factory.py``` . I could have changed the name of the treebank to ```fa_seraji``` to get around this error. However, I am curious to know how this can be solved without changing the name of the treebank to either ```fa_seraji``` or ```fa_perdt``` as I am planning to train on other treebanks with different names. 

On the ```https://github.com/stanfordnlp/stanza-train``` page, it says:

 ```The xpos_vocab_factory.py script is used to build XPOS vocabulary file for our provided UD_English-TEST toy treebank. Compared with the original file in the downloaded Stanza repo, we only add the shorthand name of the toy treebank (en_test) to the script, so that it can be recognized during training. If you want to use another dataset other than UD_English-TEST after running this tutorial, you can add the shorthand of your treebank in the same way. ``` 

So I added ```fa_newseraji``` to ```xpos_vocab_factory.py``` script. I am not sure if this is fine as on the top of this script it mentions that we should not edit this file.  Anyway, even after adding this, it still throws the same ```NotImplementedError``` error. I appreciate if you can help me with this. 
",
579,2021-05-12T13:38:02Z,https://github.com/stanfordnlp/stanza/issues/696,,"Hello!
I'm using Stanza for Italian and I'm trying to generate a pred file starting with a gold file. Unfortunately, if I start with pretokenized text the new pipeline doesn't read mwt tokens, so I can't have file aligned. I saw a similar question (#95), but I don't think the problem has been solved... Can anyone help me?",
580,2021-05-12T11:50:28Z,https://github.com/stanfordnlp/stanza/pull/695,,"add support of writing files in Semitic languages
",
581,2021-05-12T05:46:40Z,https://github.com/stanfordnlp/stanza/issues/694,,"Hi all,
I trained a few modules for ukrainian language using word embeddings with 300 dimensions. Modules I trained - pos, lemma, tokenize, mwt, depparse. 
But when I try to use this models in a pipeline, I get this error: 

> RuntimeError: Error(s) in loading state_dict for Parser:
> 	size mismatch for trans_pretrained.weight: copying a param with shape torch.Size([125, 300]) from checkpoint, the shape in current model is torch.Size([125, 100]).
I select models with config like this: 
`config = {
	'processors': 'pos, lemma, tokenize, depparse', # Comma-separated list of processors to use
	'lang': 'uk', # Language code for the language to build the Pipeline in
	'depparse_model_path': '/Users/oleksandry/stanza/stanza-train/stanza/saved_models/depparse/uk_iu_parser.pt',
    'tokenize_model_path': '/Users/oleksandry/stanza/stanza-train/stanza/saved_models/tokenize/uk_iu_tokenizer.pt',
    'tokenize_pretokenized': True, # Use pretokenized text as input and disable tokenization,
    'pos_model_path': '/Users/oleksandry/stanza/stanza-train/stanza/saved_models/pos/uk_iu_tagger.pt',
    'lemma_model_path': '/Users/oleksandry/stanza/stanza-train/stanza/saved_models/lemma/uk_iu_lemmatizer.pt',
    'mwt_model_path': '/Users/oleksandry/stanza/stanza-train/stanza/saved_models/mwt/uk_iu_mwt_expander.pt'
}
nlp = stanza.Pipeline(**config)`
Maybe I am missing some steps? Or did I have to train models with some additional parameter?",
582,2021-05-07T20:59:09Z,https://github.com/stanfordnlp/stanza/pull/693,,"## Description
Adds language id functionality. Implements a multilingual pipeline that takes in text, identifies language, and routes text to appropriate language specific pipeline. Also adds lang field to Document.

## Unit test coverage
TO DO

## Known breaking changes/behaviors
This should just add additional functionality.
",
583,2021-05-06T19:20:42Z,https://github.com/stanfordnlp/stanza/pull/692,,,
584,2021-05-05T23:51:54Z,https://github.com/stanfordnlp/stanza/pull/691,,Update Spanish AnCora processing in anticipation of UD 2.8,
585,2021-05-05T20:59:21Z,https://github.com/stanfordnlp/stanza/issues/690,,"Hello! Thanks so much for your amazing library. I have trained all the processors on my Persian data based on the Model Training and Evaluation documentation. I would like to run the trained model on some test data and evaluate them. I also need to save the prediction files to manually go through them to check some specific cases. On this documentation, it says that we can use ```bash scripts/run_ete.sh ${corpus} ${split}``` to evaluate the full parsing pipeline. However, it seems that there is no such file as ```run_ete.sh``` in this repository. I was wondering if there is any update available on this part of the documentation. I very much appreciate your help. ",
586,2021-05-02T11:09:46Z,https://github.com/stanfordnlp/stanza/pull/688,,"## Description
Changed from 'dobj' to 'obj' in the demo/corenlp.py script

## Fixes Issues
#687
",
587,2021-05-02T11:01:08Z,https://github.com/stanfordnlp/stanza/issues/687,,"**Describe the bug**

The following error appears when running the `demo/corenlp.py` script: 

```
Traceback (most recent call last):
  File ""C:\Users\marti\AppData\Local\Programs\Python\Python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\marti\AppData\Local\Programs\Python\Python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""E:\Documents\NLP\stanza\demo\corenlp.py"", line 95, in <module>
    assert matches[""sentences""][1][""length""] == 1
AssertionError
```

After some digging, the bug seems to be because no match is found for the Semgrex `{word:wrote} >nsubj {}=subject >dobj {}=object`. 

When testing the Semgrex from the demo on the same sentence at https://corenlp.run/, I found that the reason the Semgrex in the demo is not getting any matches is because the dependency parse returns `obj` and not `dobj` for the object of the sentence. 

**To Reproduce**
Steps to reproduce the behavior:
1. In the stanza repo, run `python -m demo.corenlp` 

**Expected behavior**
No error should appear.

**Environment (please complete the following information):**
 - OS: Windows 10
 - Python version: Python 3.8.8
 - Stanza version: 1.2
",
588,2021-04-30T15:14:01Z,https://github.com/stanfordnlp/stanza/pull/686,,Add a script to convert the FIRE 2013 dataset for Indian languages,
589,2021-04-30T11:49:04Z,https://github.com/stanfordnlp/stanza/issues/685,,"Is there any way to run Stanza on pretokenized text and feed it with tokens containing white spaces? The only way to analyze pretokenized text I can find in the documentation is by separating sentences with newlines and tokens with spaces, so if a token should contain a white space it would automatically be separated. An example for a token containing a space are large numbers which are written like this in many languages: ""1 000"".",
590,2021-04-30T07:55:26Z,https://github.com/stanfordnlp/stanza/issues/684,,"**Describe the bug**
When tokenising a Spanish MW with infinitive root starting with an uppercase letter, the uppercase letter changes to another letter.  E.g., Juntarse -> quntar,se
**To Reproduce**
Steps to reproduce the behavior:
1. Procedure
```
import stanza
processor_dict = {'tokenize': 'ancora', 'mwt':'ancora', 'pos' : 'ancora'}
config = {""lang"":""es"",""processors"": processor_dict,""package"":""None""}
nlp = stanza.Pipeline(**config)
nlp('Juntarse dos.')
```

It also happens with gsd models

```
processor_dict = {'tokenize': 'gsd', 'mwt':'gsd', 'pos' : 'gsd'}
```
2.  Result:

**q**untar, se, dos
> [
>   [
>     {
>       ""id"": 1,
>       ""text"": ""Juntarse"",
>       ""upos"": ""PROPN"",
>       ""xpos"": ""PROPN"",
>       ""misc"": ""start_char=0|end_char=8""
>     }
>   ]
> ]
> >>> nlp('Juntarse dos')
> [
>   [
>     {
>       ""id"": [
>         1,
>         2
>       ],
>       ""text"": ""Juntarse"",
>       ""misc"": ""start_char=0|end_char=8""
>     },
>     {
>       ""id"": 1,
>       ""text"": ""quntar"",
>       ""upos"": ""VERB"",
>       ""xpos"": ""VERB"",
>       ""feats"": ""VerbForm=Inf""
>     },
>     {
>       ""id"": 2,
>       ""text"": ""se"",
>       ""upos"": ""PRON"",
>       ""xpos"": ""PRON"",
>       ""feats"": ""Case=Acc|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes""
>     },
>     {
>       ""id"": 3,
>       ""text"": ""dos"",
>       ""upos"": ""NUM"",
>       ""xpos"": ""NUM"",
>       ""feats"": ""NumType=Card|Number=Plur"",
>       ""misc"": ""start_char=9|end_char=12""
>     }
>   ]
> ]

**Expected behavior**
**j**untar, se ,dos
>       ""id"": 1,
>       ""text"": ""juntar"",

**Environment (please complete the following information):**
 - OS: Debian GNU/Linux bullseye/sid x86_64 
 - Python version: Python 3.9.2 
 - Stanza version: 1.2
",
591,2021-04-29T05:36:42Z,https://github.com/stanfordnlp/stanza/pull/683,,Add @gawy's lang-uk conversion script to stanza.  Thank you again!,
592,2021-04-29T02:11:46Z,https://github.com/stanfordnlp/stanza/pull/682,,addresses issue https://github.com/stanfordnlp/stanza/issues/680,
593,2021-04-28T07:12:50Z,https://github.com/stanfordnlp/stanza/pull/681,,"Add a script to process 2019 bsnlp

includes a couple minor tokenizer changes needed to get there",
594,2021-04-27T13:57:59Z,https://github.com/stanfordnlp/stanza/issues/680,,"I've found some strange NER tags using the **Spanish model**.

After a preprocessing I've some pieces of text written like this:
```python
text = ""Roberto: \n\nPablo\n\nCarlos""
```

If I run the analysis using the Spanish model:
```python
nlp = stanza.Pipeline(lang='es', processors='tokenize,ner')
doc = nlp(text)
```

I've this strange result:
```bash
[
  [
    {
      ""id"": 1,
      ""text"": ""Roberto"",
      ""start_char"": 0,
      ""end_char"": 7,
      ""ner"": ""S-PER""
    },
    {
      ""id"": 2,
      ""text"": "":"",
      ""start_char"": 7,
      ""end_char"": 8,
      ""ner"": ""O""
    }
  ],
  [
    {
      ""id"": 1,
      ""text"": ""Pablo"",
      ""start_char"": 11,
      ""end_char"": 16,
      ""ner"": ""B-PER""
    }
  ],
  [
    {
      ""id"": 1,
      ""text"": ""Carlos"",
      ""start_char"": 18,
      ""end_char"": 24,
      ""ner"": ""B-PER""
    }
  ]
]
```

I usually use `doc.ents` property to get the entities, but I need to do a token by token analysis, so I expected to build the same tokens of `doc.ents`, putting together the `B-`, `I-`, `E-` and `S-` tokens respecting the BIOES format. Why do I find the token `Pablo` as `B-PER` and the next token isn't a `I-PER` or `E-PER` ?
",
595,2021-04-27T11:49:19Z,https://github.com/stanfordnlp/stanza/issues/679,,"1.
Why the result we get after the training is different from the result we get when executing scripts/run_ete.sh? 

2.
Where can I find the detail architecture/more technical details of Stanza? I looked at the paper which is given for us to cite and the official website. Do we have any other references?

Thank you",
596,2021-04-27T05:04:02Z,https://github.com/stanfordnlp/stanza/pull/678,,"    Initial version of a script to process the IJC Hindi NER dataset.  Problem with this is the results are rather low when trained
",
597,2021-04-26T14:54:26Z,https://github.com/stanfordnlp/stanza/issues/677,,"I have a user-case where I need to know for all tokens whether or not they have space before/after them. I cannot find such information in the documentation and from glancing over the source code, there does not seem to be such an attribute.

Am I missing something? If it is not present, I would request it to be added as a feature.
",
598,2021-04-15T08:54:13Z,https://github.com/stanfordnlp/stanza/pull/676,,Store comments when loading a conll file.  Write them back out later.,
599,2021-04-14T15:07:45Z,https://github.com/stanfordnlp/stanza/pull/675,,Add functionality to use the Java UD enhancer from python,
600,2021-04-14T14:54:49Z,https://github.com/stanfordnlp/stanza/issues/674,,"Hi everyone,
As far as I understood from https://universaldependencies.org/docs/en/overview/migration-guidelines.html there is a considerable difference between Stanford dependency annotations (used by older stanford models) and the Universal Dependency annotation ones (used by the models of stanza). 
For my purposes, I need the output of stanza in a stanford dependency annotation format, since it clearly outputs the prepositional phrases. For instance, when I run an old version of the stanford parser (Sentence:""Put the scissors on the table"") it outputs: nmod:on(Put-1, table-6), however stanza's models use the ""case"" relationship like case(on, table) - in that case the second word (scissors) is missing from the relation. Is there a way to make stanza work with the old annotation format? Or can someone give me an idea of how to transform the ""case"" relationships to nmod:prep - type ?
The only solution I came up with is to train a new stanza model using the old annotation, however I am not sure 1) where to find the training data, 2) if my current computational ressources can cope with the training without taking ages.

I would be really grateful if someone has a work-around.
Thank you! ",
601,2021-04-11T16:59:35Z,https://github.com/stanfordnlp/stanza/issues/673,,"I noticed in https://stanfordnlp.github.io/stanza/depparse.html that you could run the parser on either raw data (string sentences), or ""your own tokenization, multi-word token expansion, POS tagging and lemmatization"".
I would like to work on data that was tokenized, and multi-word expanded, but **does not** have POS tagging and lemmatization.
",
602,2021-04-08T01:07:57Z,https://github.com/stanfordnlp/stanza/pull/672,,"Add an option to the lemma processor to reuse existing tokenization & tags, similar to depparse_pretagged

https://github.com/stanfordnlp/stanza/issues/666",
603,2021-04-07T10:04:13Z,https://github.com/stanfordnlp/stanza/issues/671,,"Hello! 👋
First of all, thank you for creating and supporting this library! And thank you for making it multilingual - I've been using dependency parsing for Ukrainian quite a lot!

My issue concerns [Converting UD data](https://stanfordnlp.github.io/stanza/training.html#converting-ud-data) from your tutorial. I would like to retrain dependency parsing for Ukrainian. I followed your instructions, put all necessary data in the folders, but when I run `python3 stanza/utils/datasets/prepare_depparse_treebank.py UD_Ukrainian-IU --gold`, I get the following error:

```
2021-04-07 12:18:50 INFO: Datasets program called with:
stanza/utils/datasets/prepare_depparse_treebank.py UD_Ukrainian-IU
Traceback (most recent call last):
  File ""stanza/utils/datasets/prepare_depparse_treebank.py"", line 65, in <module>
    main()
  File ""stanza/utils/datasets/prepare_depparse_treebank.py"", line 62, in main
    common.main(process_treebank, add_specific_args)
  File ""/usr/local/lib/python3.7/site-packages/stanza/utils/datasets/common.py"", line 134, in main
    process_treebank(treebank, paths, args)
  File ""stanza/utils/datasets/prepare_depparse_treebank.py"", line 56, in process_treebank
    prepare_tokenizer_treebank.copy_conllu_treebank(treebank, paths, paths[""DEPPARSE_DATA_DIR""], retag_dataset)
  File ""/usr/local/lib/python3.7/site-packages/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 66, in copy_conllu_treebank
    process_treebank(treebank, paths, args)
  File ""/usr/local/lib/python3.7/site-packages/stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 639, in process_treebank
    raise ValueError(""Cannot find train file for treebank %s"" % treebank)
ValueError: Cannot find train file for treebank UD_Ukrainian-IU
```
When I run it for English `python3 stanza/utils/datasets/prepare_depparse_treebank.py UD_English-EWT --gold`, the error is the same.

Even if I run `python3 stanza/utils/datasets/prepare_tokenizer_treebank.py UD_English-EWT`, the error occurs at the same place:

```
2021-04-07 12:59:36 INFO: Datasets program called with:
stanza/utils/datasets/prepare_tokenizer_treebank.py UD_English-EWT
Traceback (most recent call last):
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 653, in <module>
    main()
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 650, in main
    common.main(process_treebank, add_specific_args)
  File ""/usr/local/lib/python3.7/site-packages/stanza/utils/datasets/common.py"", line 134, in main
    process_treebank(treebank, paths, args)
  File ""stanza/utils/datasets/prepare_tokenizer_treebank.py"", line 639, in process_treebank
    raise ValueError(""Cannot find train file for treebank %s"" % treebank)
ValueError: Cannot find train file for treebank UD_English-EWT
```

My `config.sh` file looks like this:
```
export UDBASE=../data/udbase
export DATA_ROOT=../data
export TOKENIZE_DATA_DIR=$DATA_ROOT/tokenize
export MWT_DATA_DIR=$DATA_ROOT/mwt
export LEMMA_DATA_DIR=$DATA_ROOT/lemma
export POS_DATA_DIR=$DATA_ROOT/pos
export DEPPARSE_DATA_DIR=$DATA_ROOT/depparse
export ETE_DATA_DIR=$DATA_ROOT/ete
export NER_DATA_DIR=$DATA_ROOT/ner
export CHARLM_DATA_DIR=$DATA_ROOT/charlm
export SENTIMENT_DATA_DIR=$DATA_ROOT/sentiment
export WORDVEC_DIR=../data/wordvec
```

 The directory is ordered in the following way:
```
data
├── charlm
├── depparse
├── ete
├── lemma
├── mwt
├── ner
├── pos
├── sentiment
├── tokenize
├── udbase
│   ├── UD_English-EWT
│   │   ├── CONTRIBUTING.md
│   │   ├── LICENSE.txt
│   │   ├── README.md
│   │   ├── en_ewt-ud-dev.conllu
│   │   ├── en_ewt-ud-dev.txt
│   │   ├── en_ewt-ud-test.conllu
│   │   ├── en_ewt-ud-test.txt
│   │   ├── en_ewt-ud-train.conllu
│   │   ├── en_ewt-ud-train.txt
│   │   ├── eval.log
│   │   └── stats.xml
│   └── UD_Ukrainian-IU
│       ├── CONTRIBUTING.md
│       ├── LICENSE.txt
│       ├── README.md
│       ├── eval.log
│       ├── stats.xml
│       ├── uk_iu-ud-dev.conllu
│       ├── uk_iu-ud-dev.txt
│       ├── uk_iu-ud-test.conllu
│       ├── uk_iu-ud-test.txt
│       ├── uk_iu-ud-train.conllu
│       └── uk_iu-ud-train.txt
└── wordvec
    ├── English
    │   └── en.vectors.xz
    └── Ukrainian
        └── uk.vectors.xz
```

I got `UD_Ukrainian-IU` from the official github repo, as well as `UD_English-EWT`. And transformed train, dev, and test data from `conllu` to `txt` with [the official script](https://github.com/UniversalDependencies/tools#conllu_to_textpl).

I've run out of ideas of what can be wrong. Could you help? 
(I'm using MacOS.)



",
604,2021-04-07T05:55:08Z,https://github.com/stanfordnlp/stanza/issues/670,,"Is there any documentation on the different ```MENTION_TYPES``` supported in ```Coreference Resolution```.

Commonly see 3 - ```NOMINAL, PRONOMINAL, PROPER```.  Any documentation on other mention types supported will be helpful.",
605,2021-04-07T02:53:02Z,https://github.com/stanfordnlp/stanza/issues/669,,"I am trying to use the ```Coref``` functionality. 
Please let me know if there is any ```inbuilt method``` to ```substitute the original text with detected coreferences```.

For example:
```
from stanza.server import CoreNLPClient

text = 'XYZ was born in ABC. He is very good at dancing'

with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
               properties={'annotators': 'coref', 'coref.algorithm' : 'neural'},timeout=30000, memory='16G') as client:

    ann = client.annotate(text)
```

Expected OP after detecting the coreference ```He```:

IP: ```XYZ was born in ABC. He is very good at dancing```
OP: ```XYZ was born in ABC. XYZ is very good at dancing```

",
606,2021-04-07T01:02:38Z,https://github.com/stanfordnlp/stanza/issues/668,,"step 1: pip install stanza
step 2: import stanza
step 3: stanza.download('en')

Got below connection error

stanza.download('en')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 3.02MB/s]                    
2021-04-06 20:55:28 INFO: Downloading default packages for language: en (English)...
Traceback (most recent call last):

  File ""<ipython-input-2-c2f724e525cb>"", line 1, in <module>
    stanza.download('en')

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\stanza\resources\common.py"", line 411, in download
    md5=resources[lang]['default_md5']

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\stanza\resources\common.py"", line 133, in request_file
    download_file(url, path)

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\stanza\resources\common.py"", line 111, in download_file
    r = requests.get(url, stream=True)

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\requests\api.py"", line 75, in get
    return request('get', url, params=params, **kwargs)

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\requests\api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\requests\sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\requests\sessions.py"", line 646, in send
    r = adapter.send(request, **kwargs)

  File ""C:\Users\arjun\Anaconda3\lib\site-packages\requests\adapters.py"", line 498, in send
    raise ConnectionError(err, request=request)

ConnectionError: ('Connection aborted.', TimeoutError(10060, 'A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond', None, 10060, None))",
607,2021-04-07T00:22:10Z,https://github.com/stanfordnlp/stanza/issues/667,,"[Argos Translate](https://github.com/argosopentech/argos-translate) uses Stanza 1.1.1 and until the next package format update can't upgrade stanza versions. It appears in the last day the ability to auto download models in 1.1.1 is broken and the [stanza-resources](https://github.com/stanfordnlp/stanfordnlp/stanza-resources) repo is down. What's the best way to access these models?

```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 68.7MB/s]                                                        
2021-04-07 00:17:26 INFO: Downloading these customized packages for language: en (English)...
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
=======================

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 665, in urlopen
    httplib_response = self._make_request(
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 421, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 416, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/lib/python3.8/http/client.py"", line 1347, in getresponse
    response.begin()
  File ""/usr/lib/python3.8/http/client.py"", line 307, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python3.8/http/client.py"", line 268, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""/usr/lib/python3.8/socket.py"", line 669, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/requests/adapters.py"", line 439, in send
    resp = conn.urlopen(
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 719, in urlopen
    retries = retries.increment(
  File ""/usr/lib/python3/dist-packages/urllib3/util/retry.py"", line 400, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/usr/lib/python3/dist-packages/six.py"", line 702, in reraise
    raise value.with_traceback(tb)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 665, in urlopen
    httplib_response = self._make_request(
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 421, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 416, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/lib/python3.8/http/client.py"", line 1347, in getresponse
    response.begin()
  File ""/usr/lib/python3.8/http/client.py"", line 307, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python3.8/http/client.py"", line 268, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""/usr/lib/python3.8/socket.py"", line 669, in readinto
    return self._sock.recv_into(b)
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/stanza/resources/common.py"", line 383, in download
    request_file(
  File ""/usr/local/lib/python3.8/dist-packages/stanza/resources/common.py"", line 130, in request_file
    download_file(url, path)
  File ""/usr/local/lib/python3.8/dist-packages/stanza/resources/common.py"", line 108, in download_file
    r = requests.get(url, stream=True)
  File ""/usr/local/lib/python3.8/dist-packages/requests/api.py"", line 76, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/requests/api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/requests/adapters.py"", line 498, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
```

Thanks for any help!
",
608,2021-04-06T19:00:18Z,https://github.com/stanfordnlp/stanza/issues/666,,"I am parsing a pretagged document using the `depparse_pretagged` option. However, my input data does not contain lemmatization, and I would like to predict lemmas based on the existing tags. The problem I am running into is that stanza requires the tokenization preprocessor for lemmatization to work, but tokenization has been disabled due to the pretagged option. I'm getting this error:

```
Pipeline Requirements Error!
	Processor: LemmaProcessor
	Pipeline processors list: lemma,depparse
	Processor Requirements: {'tokenize'}
		- fulfilled: set()
		- missing: {'tokenize'}
```

Is there a way to lemmatize pre-tagged text? If not, can this be added as a feature request?",
609,2021-04-03T18:04:51Z,https://github.com/stanfordnlp/stanza/issues/663,,"**Describe the bug**

Stanza stumbles on some particular text in Russian. Noting so special about it at the first sight, but it leads to the attempt of processing an empty Tensor: RuntimeError: stack expects a non-empty TensorList.
It happens on both CPU and GPU.

**To Reproduce**
```python

!pip install stanza
import stanza

stanza.download('ru')
nlp = stanza.Pipeline(lang='russian',use_gpu=False)

nlp('ФГБУ Гематологический научный центр Минздрава России АО Гeнepиум МБЦ Гeнepиум ФИЦ Биотехнологии Москва Владимирская область Техник на опытно-промышленном производстве глубокой переработки плазмы Младший научный сотрудник лаборатории фракционирования белков крови и стандартизации методов контроля препаратов плазмы Младший научный сотрудник отдела масштабирования и внедрения технологий Младший научный сотрудник лаборатории хроматографических методов Научный сотрудник лаборатории хроматографических методов Научный сотрудник Фармацевтическая продукция (производство) Фармацевтическая продукция (производство),Научно-исследовательская, научная, академическая деятельность Лаборатория, исследовательский центр,Фармацевтическая продукция (производство),Научно-исследовательская, научная, академическая деятельность Лаборатория, исследовательский центр,Фармацевтическая продукция (производство) Научно-исследовательская, научная, академическая деятельность Выполнение различных технологических операций, таких как, криофракционирование, хроматография, ультра/диафильтрация, вирусная инактивация и др. Подготовка и обслуживание оборудования. Разработка методов получения препаратов крови. \rФракционирование и хроматографическая очистка белков, анализ получаемых образцов (определение специфической активности факторов свертывания крови методом коагулометрии, спектрофотометрия, электрофорез и т.д). \rЗа период работы в ГНЦ мной были разработаны и успешно внедрены в производство технологии получения препаратов активированного и полного протромбиновых комплексов. Опубликовано два патента и несколько статей в научных журналах. Разработка методов выделения и очистки рекомбинантных белков из культуральной жидкости, масштабирование и оптимизация downstream процессов, трансфер технологий на производство. Наработка серий белков для доклинических исследований от 10 мг до 2,5 кг. Написание отчетов, регламентов, СОПов. Разработка протоколов очистки рекомбинантных белков в качестве ответственного исполнителя (включая подготовку соответствующих разделов лабораторных регламентов, масштабирование и трансфер технологий на производство) по ключевым проектам компании. Участвовал в исследовании 5-7 лекарственных препаратов (биоаналоги, биобеттеры, инновационные/оригинальные) в год. В мои обязанности также входило руководство младшим научным и техническим персоналом лаборатории. Являлся научным руководителем студентов во время прохождения ими практики на территории R&D центра Генериум. Руководство небольшой группой, состоящей из младших научных сотрудников и лаборантов. В задачи группы входило: выделение и очистка рекомбинантных белков из бактериальных клеток (E.coli), полученных как в растворимой форме, так и в виде телец включения. Проведение различны технологических операций (гомогенизация клеток, солюбилизация белков из телец включения, ренатурация/рефолдинг белков, очистка целевых молекул хроматографическими методами и т.д.), постановка различных методов анализа (ВЭЖХ, электрофорез и др.).')
```


**Expected behavior**
Returning parsed text

**Actual behavior**
```python
~/.local/lib/python3.7/site-packages/stanza/pipeline/core.py in __call__(self, doc)
    208         assert any([isinstance(doc, str), isinstance(doc, list),
    209                     isinstance(doc, Document)]), 'input should be either str, list or Document'
--> 210         doc = self.process(doc)
    211         return doc
    212 

~/.local/lib/python3.7/site-packages/stanza/pipeline/core.py in process(self, doc)
    202             if self.processors.get(processor_name):
    203                 process = self.processors[processor_name].bulk_process if bulk else self.processors[processor_name].process
--> 204                 doc = process(doc)
    205         return doc
    206 

~/.local/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py in process(self, document)
     90                                    self.config.get('max_seqlen', TokenizeProcessor.MAX_SEQ_LENGTH_DEFAULT),
     91                                    orig_text=raw_text,
---> 92                                    no_ssplit=self.config.get('no_ssplit', False))
     93         return doc.Document(document, raw_text)

~/.local/lib/python3.7/site-packages/stanza/models/tokenization/utils.py in output_predictions(output_file, trainer, data_generator, vocab, mwt_dict, max_seqlen, orig_text, no_ssplit, use_regex_tokens)
    151                 en = max(ens)
    152                 batch1 = batch[0][:, :en], batch[1][:, :en], batch[2][:, :en], [x[:en] for x in batch[3]]
--> 153                 pred1 = np.argmax(trainer.predict(batch1), axis=2)
    154 
    155                 for j in range(len(batchparas)):

~/.local/lib/python3.7/site-packages/stanza/models/tokenization/trainer.py in predict(self, inputs)
     65             features = features.cuda()
     66 
---> 67         pred = self.model(units, features)
     68 
     69         return pred.data.cpu().numpy()

~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--> 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/.local/lib/python3.7/site-packages/stanza/models/tokenization/model.py in forward(self, x, feats)
     47         emb = torch.cat([emb, feats], 2)
     48 
---> 49         inp, _ = self.rnn(emb)
     50 
     51         if self.args['conv_res'] is not None:

~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--> 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py in forward(self, input, hx)
    580         if batch_sizes is None:
    581             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
--> 582                               self.dropout, self.training, self.bidirectional, self.batch_first)
    583         else:
    584             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,

RuntimeError: stack expects a non-empty TensorList
```

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: 3.7
 - Stanza version: 1.2

**Additional context**

It may be not a big deal, just could be good perhaps if you could trace its roots so that it won't happen under other circumstances.
Error goes away if, for example, I replace last dot (.) with exclamation sign (!)
",
610,2021-04-02T01:57:20Z,https://github.com/stanfordnlp/stanza/pull/662,,Splitting out the start_char and end_char from misc actually saves about 8% of the time spent in the tokenizer simply by not creating and then splitting strings,
611,2021-03-31T23:14:39Z,https://github.com/stanfordnlp/stanza/pull/661,,Add a python version of the script for processing NER datasets.  Include processing of the Finnish Turku dataset.,
612,2021-03-31T18:12:28Z,https://github.com/stanfordnlp/stanza/issues/660,,"I'm seeing that if I have two Named Entities on different lines, like:
```
Entity1
Entity2
```

the output of NER return a single entity `Entity1\nEntity2` containing a `\n`.

I would like to have two different entities, since they are in different rows. For now I have put a `\n\n` to divide the sentences and force the double entities. Is there any parameter to control this problem?",
613,2021-03-30T22:23:45Z,https://github.com/stanfordnlp/stanza/pull/659,,Add an initial version of an interface directly to tokensregex using stanza docs,
614,2021-03-30T12:43:50Z,https://github.com/stanfordnlp/stanza/issues/658,,"Hi,

I tried to reproduce French NER training on **WikiNER** but I didn’t manage to get as good result as expected.

I used WikiNER [1] dataset (wp2 version, 9001 documents). I split randomly the dataset into 60% for training, 20% for development, 20% for testing.

I followed stanza training tutorial [2]. I used French word2vec word embeddings  [3]. I also used backward and forward charlm, trained on 1000 .txt documents (~200k) from WikiNER corpus.

The trained model was loaded this way to tag my test corpus:

```
self.model = stanza.Pipeline(lang= ""fr"", processors='tokenize, mwt, ner', ner_model_path=""PATH_TO/fr_french_nertagger.pt"", ner_forward_charlm_path=""PATH_TO/fr_french_forward_charlm.pt"", ner_backward_charlm_path= ""PATH_TO/fr_french_backward_charlm.pt"")
```

F1 score on test set is **0.767** (on LOC, PER and ORG) which is far from the F1 score I get using your stanza French NER Tagger (>0.90) on the same test set.

I assume charlm is not trained on a sufficient amout of data? **On how much data did you trained French charlm ?**

**Otherwise, how can I find and load a pretrained forward and backward French charlm?**

Are there other parameters that I should modify to reproduce exactly your French NER Training process/parameters?

Thank you in advance

[1] https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500?file=9446317
[2] https://github.com/stanfordnlp/stanza-train
[3] https://stanfordnlp.github.io/stanza/training.html
",
615,2021-03-30T08:21:25Z,https://github.com/stanfordnlp/stanza/issues/657,,"As title, I want to release the GPU memory after finish the dependency parse but cannot find the solution to solve it.
Basically, I write the test code for this problem

```
import stanza
import torch

dep_config = {
    'processors': 'tokenize,pos,lemma,depparse',
    'tokenize_pretokenized': True,
    'lang': 'en',
    'use_gpu': True}
dep_parser = stanza.Pipeline(**dep_config)

del dep_config
del dep_parser

torch.cuda.empty_cache()

simulate_running_program = input()
```
In the code, I try to release GPU memory using `del`, even using `torch.cuda.empty_cache()`
When code running, I use `watch -n 0.1 nvidia-smi` to monitor the usage of the GPU memory,
and find that it still not release memory when code run to wait for IO

Thank you!",
616,2021-03-29T16:44:15Z,https://github.com/stanfordnlp/stanza/issues/656,,"I can only load certain processors in the Pipeline with `processors=""tokenize, pos""`, however if I want to load a custom tokenizer that I implemented, `processors={""tokenize"":""my_tokenizer""}` would also load all the other components as well. I only need the pos-tagger and the custom tokenizer, the rest of the functionality unnecessarily slows down my program, is there any way to do that?
Thank you!",
617,2021-03-28T09:13:36Z,https://github.com/stanfordnlp/stanza/issues/655,,"Hi. If Stanza has a similar function like `MultiPatternMatcher` in Java, and if has, how can I solve the question of overlapped patterns? 
The description of `MultiPatternMatcher` is below:

   List<CoreLabel> tokens = ...;
   List<TokenSequencePattern> tokenSequencePatterns = ...;
   MultiPatternMatcher multiMatcher = TokenSequencePattern.getMultiPatternMatcher(tokenSequencePatterns);
   
   // Finds all non-overlapping sequences using specified list of patterns
   // When multiple patterns overlap, matches selected based on priority, length, etc.
   List<SequenceMatchResult<CoreMap>> multiMatcher.findNonOverlapping(tokens);

Thank you very much.",
618,2021-03-27T15:40:27Z,https://github.com/stanfordnlp/stanza/pull/654,,"Remove the vi-specific stuff, as it makes a very small difference in accuracy (less than 1%) and is a hassle to maintain.  At some point soon we'll implement a new tokenization algorithm for vi which is hopefully much more accurate.",
619,2021-03-27T05:10:04Z,https://github.com/stanfordnlp/stanza/issues/653,,"Hi! I have a few questions on the different languages represented in the tokenization and POS pipelines. I have text files of raw data that come from transcribed Mandarin-English codeswitched speech (for example, something like ""there was this really great 包子 stand and they have the best 包子s I have ever had"" but a longer piece of text). I'm trying to see if there is any way to ""combine"" the 'zh' and 'en' pipelines for tokenization and POS/morpheme tagging so that the pipeline could read the codeswitched data. 

I found that the 'en' pipeline performed fine where there was English words, but not great with 'zh' text. The 'zh' pipeline performed better at tokenizing the Mandarin parts of the text, and seemed to do okay on the English text as well. However, when I did the POS tagging with the 'zh' pipeline on the full codeswitched text, only the Chinese words got POS tags (as one would expect). I was thinking that I could run both pipelines on the text and kind of piece together the good parts of each output so that I have the physically, text that has all the labels on the words, but how would I go about putting that into a data type to be able to be read by other Stanza models? As in, is there a way I could combine the pipeline POS and tokenization outputs to begin with for the two languages, or is there a way I could turn my manually fixed output/data into something that I will be able to use to read/feed into models?

I can post the code and outputs too if that helps! Just let me know.

Thank you! 

Edit: are there any models in Stanza that *predict* POS tags?",
620,2021-03-26T10:48:08Z,https://github.com/stanfordnlp/stanza/issues/652,,"Do stanza has hindi wordnet features too..like synonym, antonym etc?",
621,2021-03-25T18:44:36Z,https://github.com/stanfordnlp/stanza/issues/651,,"It seems that systematically the lemma for an English comparative for adjectives is the comparative itself.

I tried this sentence in the online-demo
The best thermal conductor conducts heat better from hotter to cooler.
Here is a screen shot of the result:
![stanzaComparative](https://user-images.githubusercontent.com/6200743/112526111-f8d0f300-8d77-11eb-8f64-c79fc10787ce.jpg)
I would have expected that the lemma for ""best"" and ""better"" would be ""good"", for ""hotter""=> ""hot"", and ""cooler""=>""cool""

thanks
Guy Lapalme",
622,2021-03-22T22:49:11Z,https://github.com/stanfordnlp/stanza/pull/650,,,
623,2021-03-22T22:46:56Z,https://github.com/stanfordnlp/stanza/pull/649,,"…ow consecutive newlines are handled
",
624,2021-03-18T00:39:10Z,https://github.com/stanfordnlp/stanza/issues/648,,"For some reason I am getting a winerror 2 file cannot be found error. It crashes when starting the server. It looks like it has happening within the stanza files. This is the code:
```
import stanza
from stanza.server import CoreNLPClient

if __name__ == ""__main__"":
    corenlp_dir = '../corenlp'
    try:
        assert os.path.exists(corenlp_dir)
    except:
        stanza.install_corenlp(dir=corenlp_dir)
    os.environ[""CORENLP_HOME""] = corenlp_dir
    with CoreNLPClient(annotators=[""tokenize""], timeout=30000, memory='16G') as client:
        print(""test"")
```

And here is the stack trace:
```
2021-03-17 20:34:37 INFO: Writing properties to tmp file: corenlp_server-23f17a3f30854db4.props
2021-03-17 20:34:37 INFO: Starting server with command: java -Xmx16G -cp ../corenlp\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-23f17a3f30854db4.props -annotators tokenize -preload -outputFormat serialized
Traceback (most recent call last):
  File ""C:\Users\jdisa\Documents\11411 Project\NLPenguinsProject\ask\yesno.py"", line 37, in <module>
    with CoreNLPClient(annotators=[""tokenize""], timeout=30000, memory='16G') as client:
  File ""C:\Users\jdisa\AppData\Local\Programs\Python\Python39\lib\site-packages\stanza\server\client.py"", line 182, in __enter__
    self.start()
  File ""C:\Users\jdisa\AppData\Local\Programs\Python\Python39\lib\site-packages\stanza\server\client.py"", line 152, in start
    self.server = subprocess.Popen(self.start_cmd,
  File ""C:\Users\jdisa\AppData\Local\Programs\Python\Python39\lib\subprocess.py"", line 947, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""C:\Users\jdisa\AppData\Local\Programs\Python\Python39\lib\subprocess.py"", line 1416, in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
FileNotFoundError: [WinError 2] The system cannot find the file specified
```",
625,2021-03-17T17:09:45Z,https://github.com/stanfordnlp/stanza/pull/647,,,
626,2021-03-17T16:05:57Z,https://github.com/stanfordnlp/stanza/issues/646,,"Hi,

I wonder whether stanza supports SUTime, or there is any official python wrapper for SUTime from stanfordNLP? 

Many thanks in advance! 

",
627,2021-03-17T05:58:11Z,https://github.com/stanfordnlp/stanza/pull/645,,Use .xz when processing data for charlm training,
628,2021-03-16T01:56:52Z,https://github.com/stanfordnlp/stanza/pull/644,,"Add a test, plus this should be a little faster",
629,2021-03-15T21:01:16Z,https://github.com/stanfordnlp/stanza/pull/643,,,
630,2021-03-15T16:18:21Z,https://github.com/stanfordnlp/stanza/pull/642,,,
631,2021-03-12T12:33:39Z,https://github.com/stanfordnlp/stanza/issues/641,,"Hi there,

I am starting a project which will use NLP. The most important step for this project is tokenization, with accurate part of speech tagging and named entity recognition useful but not as important.

Is it possible to say which of Stanza or Corenlp would be more suitable for this task? Is one dramatically better than the other? The domain under consideration is simplified modern Mandarin Chinese. I had been planning to use corenlp, but just read about stanza today.

Thanks for any advice,

Xavier",
632,2021-03-12T03:12:16Z,https://github.com/stanfordnlp/stanza/issues/640,,"Basically what the title says, I'm using the CoreNLPClient to do OpenIE triplet extraction but it is very slow. Is there a way to take advantage of a GPU? 

Thank you",
633,2021-03-09T02:26:36Z,https://github.com/stanfordnlp/stanza/issues/639,,"The stanza seems hard to segment the Chinese sentences, may I pass a list of Chinese characters  for dependency parsing? Just like below, 
`doc = nlp([""白蛋白"", ""33.5"", ""g/l"", ""↓""])`
rather than
`doc = nlp([""白蛋白33.5g/l↓])`",
634,2021-03-08T13:05:55Z,https://github.com/stanfordnlp/stanza/pull/638,,"Add proxy parameters when downloading the model to make it more convenient for Chinese users to use, the proxy parameter is optional. #638 

```python
>>> import stanza
>>> proxies = {'http': 'http://127.0.0.1:10809', 'https': 'http://127.0.0.1:10809'}
>>> stanza.download('en', proxies)
```",
635,2021-03-08T13:05:10Z,https://github.com/stanfordnlp/stanza/issues/637,,"When we download the model, the code is as follows:

```python
>>> import stanza
>>> stanza.download('en')
```

In China, network problems may arise `requests.exceptions.ConnectionError`, Although we can use a VPN to solve this problem, I suggest adding the proxies parameter, which makes it more convenient to use a proxy to download the model, this proxy parameter is optional, the improved code is as follows:

For Chinese users:

```python
>>> import stanza
>>> proxies = {'http': 'http://127.0.0.1:10809', 'https': 'http://127.0.0.1:10809'}
>>> stanza.download('en', proxies)
```

For other users, the usage method remains the same.

I have created a pull requests: [https://github.com/stanfordnlp/stanza/pull/638](url)",
636,2021-03-08T00:43:17Z,https://github.com/stanfordnlp/stanza/issues/636,,"I am trying to train a depparse, with predicted mode.

I thought ""saved_models/depparse/ta_thamizhi_parser.pt"" and ""../data/processed/depparse/ta_thamizhi.dev.pred.conllu"" are created during the process. However, it seems we need to have those two files already in respective folders before the training. 

Can you please guide me?

This is what I did:

bash scripts/run_depparse.sh UD_Tamil-Thamizhi predicted
Using batch size 5000
Running parser with ...
Running parser in train mode
Loading data with batch size 5000...
2021-03-08 06:02:59 DEBUG: Loaded pretrain from saved_models/depparse/ta_thamizhi.pretrain.pt
2021-03-08 06:02:59 DEBUG: 2 batches created.
2021-03-08 06:02:59 DEBUG: 1 batches created.
Training parser...
Traceback (most recent call last):
  File ""/userdirs/sarves/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/userdirs/sarves/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/parser.py"", line 259, in <module>
    main()
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/parser.py"", line 101, in main
    train(args)
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/parser.py"", line 154, in train
    loss = trainer.update(batch, eval=False) # update step
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/depparse/trainer.py"", line 63, in update
    loss.backward()
  File ""/userdirs/sarves/anaconda3/lib/python3.7/site-packages/torch/tensor.py"", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/userdirs/sarves/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 770.00 MiB (GPU 0; 23.65 GiB total capacity; 2.54 GiB already allocated; 172.12 MiB free; 3.13 GiB reserved in total by PyTorch)
Running parser in predict mode
Loading model from: saved_models/depparse/ta_thamizhi_parser.pt
2021-03-08 06:03:04 ERROR: Cannot load model from saved_models/depparse/ta_thamizhi_parser.pt
Traceback (most recent call last):
  File ""/userdirs/sarves/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/userdirs/sarves/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/parser.py"", line 259, in <module>
    main()
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/parser.py"", line 103, in main
    evaluate(args)
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/parser.py"", line 225, in evaluate
    trainer = Trainer(pretrain=pretrain, model_file=model_file, use_cuda=use_cuda)
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/depparse/trainer.py"", line 36, in __init__
    self.load(model_file, pretrain)
  File ""/userdirs/sarves/Documents/stanza/stanza/stanza/models/depparse/trainer.py"", line 107, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""/userdirs/sarves/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 581, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""/userdirs/sarves/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""/userdirs/sarves/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/depparse/ta_thamizhi_parser.pt'
Traceback (most recent call last):
  File ""stanza/utils/conll18_ud_eval.py"", line 532, in <module>
    main()
  File ""stanza/utils/conll18_ud_eval.py"", line 500, in main
    evaluation = evaluate_wrapper(args)
  File ""stanza/utils/conll18_ud_eval.py"", line 483, in evaluate_wrapper
    system_ud = load_conllu_file(args.system_file)
  File ""stanza/utils/conll18_ud_eval.py"", line 477, in load_conllu_file
    _file = open(path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {}))
FileNotFoundError: [Errno 2] No such file or directory: '../data/processed/depparse/ta_thamizhi.dev.pred.conllu'
ta_thamizhi
",
637,2021-03-03T21:17:35Z,https://github.com/stanfordnlp/stanza/issues/635,,"Stanza has default batch size values for tokenizer, POS, NER, ... but it is ""None"" for sentiment processor.
> batch_size’ | int | None | If None, run everything at once. If set to an integer, break processing into chunks of this size
according to [https://stanfordnlp.github.io/stanza/sentiment.html](url)

Since people usually do not explicitly set the batch size, would it be better to set a numerical value as default for Sentiment  batch_size? Use ""None"" may cause OOM in Sentiment if the input document has lots of sentences, though each sentence is short.

Is the default ""None"" set with any other concern? 

I tried to set it to a small size, but the OOM still happens.  Did I set the right parameter (""batch_size"") for Sentiment?
```
#sentence_list is a list of short sentences (max char length is ~150).
split_sentences_string = '\n\n ### \n\n'.join( [x.lower() for x in sentence_list]) + ""\n\n ### \n\n""  # in a batch manner
nlp = stanza.Pipeline(lang='en',use_gpu=True, batch_size = 10, processors='tokenize,sentiment', tokenize_no_ssplit=True)
doc = nlp(split_sentences_string)


Exception has occurred: RuntimeError
CUDA out of memory. Tried to allocate 6.31 GiB (GPU 0; 47.46 GiB total capacity; 36.05 GiB already allocated; 5.68 GiB free; 36.07 GiB reserved in total by PyTorch)

```
",
638,2021-03-03T16:12:30Z,https://github.com/stanfordnlp/stanza/issues/634,,"**Describe the bug**
this quite short file gi
[5853.txt](https://github.com/stanfordnlp/stanza/files/6077347/5853.txt)
ves the following error: 
RuntimeError: stack expects a non-empty TensorList
**To Reproduce**
Steps to reproduce the behavior:

nlp = stanza.Pipeline(lang='fr', processors='tokenize')
doc = nlp(<text_of_the_txt_file>)

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: 3.7
 - Stanza version:  1.2.0
[5853.txt](https://github.com/stanfordnlp/stanza/files/6077368/5853.txt)


**Additional context**
Add any other context about the problem here.
",
639,2021-03-02T14:11:34Z,https://github.com/stanfordnlp/stanza/issues/633,,"I'm using `download` with `model_dir`.

```python
stanza.download(lang=lang, model_dir=model_path)
```

```
2021-03-02 13:53:46 INFO: Downloading default packages for language: en (English)...
I:[common.py:406]:Downloading default packages for language: en (English)...
Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|█| 411M/411M [08
2021-03-02 14:02:54 INFO: Finished downloading models and saved to /root/stanza.
I:[common.py:440]:Finished downloading models and saved to /root/stanza.
Traceback (most recent call last):
  File ""examples/nlp.py"", line 22, in <module>
    (_, model) = modelHub.getModel(""ner-stanza-classifier"", model_class='nlp', model_type=""stanza"")
  File ""/app/examples/../modelhub.py"", line 493, in getModel
    (_, model) = self.loadNLPModel(model_config)
  File ""/app/examples/../modelhub.py"", line 353, in loadNLPModel
    (_, model) = self.loadNLPStanza(model_config['lang'], model_config['model_path'])
  File ""/app/examples/../modelhub.py"", line 280, in loadNLPStanza
    model = stanza.Pipeline(lang)
  File ""/usr/local/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 86, in __init__
    raise ResourcesFileNotFoundError(resources_filepath)
stanza.pipeline.core.ResourcesFileNotFoundError: Resources file not found at: /root/stanza_resources/resources.json  Try to download the model again.
```

If I go on the disk I get
```
/root/stanza/
├── en
│   ├── backward_charlm
│   ├── default.zip
│   ├── depparse
│   ├── forward_charlm
│   ├── lemma
│   ├── ner
│   ├── pos
│   ├── pretrain
│   ├── sentiment
│   └── tokenize
└── resources.json
```",
640,2021-03-02T00:42:56Z,https://github.com/stanfordnlp/stanza/pull/632,,"## Description
Fixes an issue where the tokenizer's `skip_newline` functionality fails the pipeline when a newline appears in a token.

## Fixes Issues
Issue reported for Simplified Chinese models in #531 .

## Unit test coverage
New test added to `tests/test_tokenizer.py`.

## Known breaking changes/behaviors
N/A
",
641,2021-03-01T07:57:04Z,https://github.com/stanfordnlp/stanza/issues/631,,"I've been trying to process Persian text, and most of the time, everything is fine, but sometimes I get this strange error. In this particular document removing the last character solves the problem.

My pipeline is:
`nlp = stanza.Pipeline('fa', processors='tokenize, pos, lemma', use_gpu=True, pos_batch_size=8000, logging_level='DEBUG', verbose=True)`

For this document
`t='با دیجیتال مارکتینگ گوگل رو منفجر کنیدسلام دوستان عزیز ، مطلبی که میخوام در موردش صحبت کنم یک مطلب جالب در مورد دیجیتال مارکتینگ هست و اینکه چطوری بتونیم تو گوگل خیلی دیده بشیم . خوب همه میدونیم یکی از رمز های موفقیت تو دیجیتال مارکتینگ و سئو بحث تولید محتوا هست . اینکه ما چه محتوایی رو چطوری با چه فرمتی تولید کنیم و کجا انتشار بدیم . تو بحث تولید محتوا به غیر از اصول و قوانینی که وجود داره و باید رعایت بشه برای بهتر دیده شدن و نتیجه خوب گرفتن ، داشتن خلاقیت بالاست که شما چطوری یک محتوا رو خلق کنی و ایده اصلیتون چی باشه که مخاطبو جذب کنه تا با بقیه فرق کنه . چون چیزی که همه جا دیده میشه محتواست ولی اون محتوایی خیلی خیلی دیده میشه و تو ذهن همه ماندگار میشه محتوای خلاق و جذابه .حالا ما اومدیم و محتوای جذاب و اصولی خلق کردیم چه کنیم که بیشتر دیده بشیم ؛ اینجا باید بگم که باید گوگل رو منفجر کنید ؛ یعنی با محتواتون با مدل های مختلف ، گوگل رو منفجر .خوب اینجا باید بگیم چه جوری ؟ چیزی که برای گوگل خیلی مهمه اینه که شما به مخاطباتون اهمیت بدید ، و مهمتر از اون اینکه که به همه مخاطباتون اهمیت بدید . تا اونجایی که میتونید باید هوای همه سلیقه های مخاطباتون یا مشتریاتون رو داشته باشید . خوب شاید سوال باشه چجوری ؟ جوابش اینه که باید محتواتون رو با فرمت های مختلف انتشار بدید .باید کلیپ درست کنید (چون خیلی ها هستن که ویدیو دوست دارن و با ویدیو ارتباط بیشتری برقرار میکنن مثل کسانی که فیلم دیدن رو به کتاب خوندن ترجیح میدن ، و از همه مهمتر آمار ها نشون میدن از بین مدل های مختلف محتواهایی که تو گوگل و شبکه های اجتماعی وجود دارن محتوای ویدئویی بیشترین بازدید و اثر گذاری رو داره)مثل همیشه مقاله ( نباید از این بگذریم که خیلی ها هنوز خوندن مقاله و محتوای متنی براشون جذابیت زیادی داره و این مدل محتوا رو به بقیه ترجیح میدن مثل کتابخوان ها ، البته محتوای متنی هم انواع خودشو داره که میشه به کتاب الکترونیکی هم اشاره کرد)تولید محتوای متنیتصاویر ؛ در حد استاندارد از تصاویر مرتبط با مقاله یا محصول استفاده کنید چون سئوی تصاویر خیلی خیلی مهمه و در صورت امکان با لوگوی خودتون انتشار بدید .اینفوگرافیک ؛ یعنی یه عکس نوشته جذاب ( به نظرم با اینفوگرافیک میتونید با یه تیر دوتا نشون بزنید ، هم برای اونایی که دوست دارن متن بخونن جذابیت دارید ، هم برای اونایی که دنبال عکس و جذابیت های بصری هستن و با خوش ذوقی و خلاقیت یه اینفوگرافی جذاب درست کنید چون معمولا اینفوگرافیک ها خیلی شسته و رفته و خلاصه هستن و اصل مطلب رو به صورت جذابی انتقال میدن و نکته اینکه باز هم سئو تصاویر رو از دست ندید )محتوای صوتیو در نهایت قطعا فراگیری پادکست و کتاب صوتی رو بین افراد مختلف دیدید ( خیلی الان مد شده که پادکست یا کتاب صوتی گوش میدن چون خیلی راحت تره میتونن تو ماشین یا موبایل و یا هرجا به راحتی گوش بدن ، پس احتمال خیلی زیاد مخاطباتون علاقه نشون میدن ، فراموش نکنید که بخشی از سرچ آینده گوگل به سرچ صوتی تعلق داره و پیدا کردن محتوا های صوتی برای مخاطبین مخصوصا افراد نابینا تخصیص پیدا میکنه )با این مدل شما میتونید محتواتون رو به طیف گسترده ای از مخاطباتون برسونید ، وقتی هم گوگل میبینه شما برای مخاطباتون ارزش قائل شدید و برای همه سلیقه ها محتوا تولید کردید ، گوگل هم برای شما ارزش قائل میشه و اگه محتواتون رو اصولی ، خلاق و جذاب تولید کرده باشید ، وقتی مخاطب تو گوگل سرچ میکنه شانس دیدن شما چند برابرمیشه چون مکنه هم با مقاله شما ، هم با تصاویر و اینفوگرافیک شما و هم با ویدئو شما رو به رو بشه ، شما با این روش به غیر از گوگل شبکه های اجتماعی رو هم میتونید منفجر کنید ، مثل اینستاگرام که خوراک کلیپ و اینفوگرافیکه ، آپارت و یوتیوب که بازدیدکننده های بسیار زیادی دارن و ابزار بسیار خوبی هستن برای دیده شدن محتوای شما و آمار بازدید ویدیو هاتون رو هم میتونید ببینید ، از همه مهمتر این دو سایت چون سئو و ارزش بالایی دارن و در برندیگ شما هم خیلی تاثیر گذارن ، پادکست هم که میشه توی کانال تلگرام یا ، با اپلیکیشن های مختلف که وجود داره توی اینستاگرام هم انتشار بدید .زمانی که مخاطبان ، محتوای شما در در فضای مجازی به صورت های گوناگون و مکرر دیدن این موقع هست که برند شما در ذهن مشتری جای خودشو پیدا میکنه و شما با یک تیر چند نشان زدید ، و بعد از این فرایند نحوه ادامه فعالیت شما در فضای مجازی و حقیقی مهمه که باعث تثبیت برند شما تو ذهن مخاطب میشهخلاصه برای انتشار حداکثری و تاثیر گذاری حداکثری محتوا از هیچ کار اصولی و خلاقانه ای دریغ نکنیداین مطلب از آموخته ها و تجربه های کوچیک من از کارکردن تو این حوزه هست ، خوشحال میشم با نظراتون این مطلب رو تکمیل تر کنید#محمدرضا_پورقدیری #دیجیتال_مارکتینگ #سئو #تولید_محتوا'`

calling `d = nlp(t[:4091])`raises:

RuntimeError                              Traceback (most recent call last)

<ipython-input-23-d2bd886ece57> in <module>()
      1 #%%time
      2 
----> 3 d = nlp(t[:4091])
      4 # for i in range(len(in_docs)):
      5 

8 frames

/usr/local/lib/python3.7/dist-packages/stanza/pipeline/core.py in __call__(self, doc)
    208         assert any([isinstance(doc, str), isinstance(doc, list),
    209                     isinstance(doc, Document)]), 'input should be either str, list or Document'
--> 210         doc = self.process(doc)
    211         return doc
    212 

/usr/local/lib/python3.7/dist-packages/stanza/pipeline/core.py in process(self, doc)
    202             if self.processors.get(processor_name):
    203                 process = self.processors[processor_name].bulk_process if bulk else self.processors[processor_name].process
--> 204                 doc = process(doc)
    205         return doc
    206 

/usr/local/lib/python3.7/dist-packages/stanza/pipeline/tokenize_processor.py in process(self, document)
     90                                    self.config.get('max_seqlen', TokenizeProcessor.MAX_SEQ_LENGTH_DEFAULT),
     91                                    orig_text=raw_text,
---> 92                                    no_ssplit=self.config.get('no_ssplit', False))
     93         return doc.Document(document, raw_text)

/usr/local/lib/python3.7/dist-packages/stanza/models/tokenization/utils.py in output_predictions(output_file, trainer, data_generator, vocab, mwt_dict, max_seqlen, orig_text, no_ssplit, use_regex_tokens)
    151                 en = max(ens)
    152                 batch1 = batch[0][:, :en], batch[1][:, :en], batch[2][:, :en], [x[:en] for x in batch[3]]
--> 153                 pred1 = np.argmax(trainer.predict(batch1), axis=2)
    154 
    155                 for j in range(len(batchparas)):

/usr/local/lib/python3.7/dist-packages/stanza/models/tokenization/trainer.py in predict(self, inputs)
     65             features = features.cuda()
     66 
---> 67         pred = self.model(units, features)
     68 
     69         return pred.data.cpu().numpy()

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--> 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

/usr/local/lib/python3.7/dist-packages/stanza/models/tokenization/model.py in forward(self, x, feats)
     47         emb = torch.cat([emb, feats], 2)
     48 
---> 49         inp, _ = self.rnn(emb)
     50 
     51         if self.args['conv_res'] is not None:

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--> 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py in forward(self, input, hx)
    580         if batch_sizes is None:
    581             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
--> 582                               self.dropout, self.training, self.bidirectional, self.batch_first)
    583         else:
    584             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,

RuntimeError: stack expects a non-empty TensorList

but `d = nlp(t[:4090])` works fine, and last character itself works fine too `d = nlp(t[4090])`.


 - I'm running the code on google colab
 - Python version: 3.7.10
 - Stanza version: 1.2

Is this an issue or I'm doing something wrong?
",
642,2021-02-26T13:44:41Z,https://github.com/stanfordnlp/stanza/issues/630,,"```
2021-02-26 21:05:04 INFO: Loading these models for language: en (English):
=========================
| Processor | Package   |
-------------------------
| tokenize  | combined  |
| pos       | combined  |
| lemma     | combined  |
| depparse  | combined  |
| sentiment | sstplus   |
| ner       | ontonotes |
=========================

2021-02-26 21:05:04 INFO: Use device: gpu
2021-02-26 21:05:04 INFO: Loading: tokenize
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-75-543633ac586b> in <module>
----> 1 nlp = stanza.Pipeline('en')

~/anaconda3/lib/python3.8/site-packages/stanza/pipeline/core.py in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, **kwargs)
    126             try:
    127                 # try to build processor, throw an exception if there is a requirements issue
--> 128                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
    129                                                                                           pipeline=self,
    130                                                                                           use_gpu=self.use_gpu)

~/anaconda3/lib/python3.8/site-packages/stanza/pipeline/processor.py in __init__(self, config, pipeline, use_gpu)
    153         self._vocab = None
    154         if not hasattr(self, '_variant'):
--> 155             self._set_up_model(config, use_gpu)
    156 
    157         # build the final config for the processor

~/anaconda3/lib/python3.8/site-packages/stanza/pipeline/tokenize_processor.py in _set_up_model(self, config, use_gpu)
     37             self._trainer = None
     38         else:
---> 39             self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
     40 
     41     def process_pre_tokenized_text(self, input_src):

~/anaconda3/lib/python3.8/site-packages/stanza/models/tokenization/trainer.py in __init__(self, args, vocab, model_file, use_cuda)
     25         self.criterion = nn.CrossEntropyLoss(ignore_index=-1)
     26         if use_cuda:
---> 27             self.model.cuda()
     28             self.criterion.cuda()
     29         else:

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in cuda(self, device)
    461         r""""""Registers a forward pre-hook on the module.
    462 
--> 463         The hook will be called every time before :func:`forward` is invoked.
    464         It should have the following signature::
    465 

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    357 
    358         .. function:: to(tensor, non_blocking=False)
--> 359 
    360         Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
    361         floating point desired :attr:`dtype` s. In addition, this method will

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py in _apply(self, fn)
    160 
    161     def get_expected_hidden_size(self, input, batch_sizes):
--> 162         # type: (Tensor, Optional[Tensor]) -> Tuple[int, int, int]
    163         if batch_sizes is not None:
    164             mini_batch = batch_sizes[0]

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py in flatten_parameters(self)
    149     def check_input(self, input, batch_sizes):
    150         # type: (Tensor, Optional[Tensor]) -> None
--> 151         expected_input_dim = 2 if batch_sizes is not None else 3
    152         if input.dim() != expected_input_dim:
    153             raise RuntimeError(

~/anaconda3/lib/python3.8/site-packages/torch/backends/cudnn/rnn.py in get_cudnn_mode(mode)
      9         return cudnn.CUDNN_RNN_TANH
     10     elif mode == 'LSTM':
---> 11         return cudnn.CUDNN_LSTM
     12     elif mode == 'GRU':
     13         return cudnn.CUDNN_GRU

~/anaconda3/lib/python3.8/site-packages/torch/backends/__init__.py in __getattr__(self, attr)
     45 
     46     def __getattr__(self, attr):
---> 47         return self.m.__getattribute__(attr)

AttributeError: module 'torch.backends.cudnn' has no attribute 'CUDNN_LSTM'
```

i don't know how to fix it 
thanks a lot!",
643,2021-02-26T09:56:42Z,https://github.com/stanfordnlp/stanza/issues/629,,"**Describe the bug**
Comma included in the lemma

**To Reproduce**
Input: `bra,` in `no` works as expected:
```json
[
  [
    {
      ""id"": 1,
      ""text"": ""bra"",
      ""lemma"": ""bra"",
      ""upos"": ""ADJ"",
      ""feats"": ""Definite=Ind|Degree=Pos|Gender=Neut|Number=Sing"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=0|end_char=3""
    }
  ],
  [
    {
      ""id"": 1,
      ""text"": "","",
      ""lemma"": ""$,"",
      ""upos"": ""PUNCT"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=3|end_char=4""
    }
  ]
]
```
Input: `bra` in the sentence`Seng og rom var bra, men badet trenger oppgradering. Badet` in `no` doesn't work:
```json
[
  [
    {
      ""id"": 1,
      ""text"": ""Seng"",
      ""lemma"": ""seng"",
      ""upos"": ""NOUN"",
      ""feats"": ""Definite=Ind|Gender=Fem|Number=Sing"",
      ""head"": 5,
      ""deprel"": ""nsubj"",
      ""misc"": ""start_char=0|end_char=4""
    },
    {
      ""id"": 2,
      ""text"": ""og"",
      ""lemma"": ""og"",
      ""upos"": ""CCONJ"",
      ""head"": 3,
      ""deprel"": ""cc"",
      ""misc"": ""start_char=5|end_char=7""
    },
    {
      ""id"": 3,
      ""text"": ""rom"",
      ""lemma"": ""rom"",
      ""upos"": ""NOUN"",
      ""feats"": ""Definite=Ind|Gender=Neut|Number=Sing"",
      ""head"": 1,
      ""deprel"": ""conj"",
      ""misc"": ""start_char=8|end_char=11""
    },
    {
      ""id"": 4,
      ""text"": ""var"",
      ""lemma"": ""være"",
      ""upos"": ""AUX"",
      ""feats"": ""Mood=Ind|Tense=Past|VerbForm=Fin"",
      ""head"": 5,
      ""deprel"": ""cop"",
      ""misc"": ""start_char=12|end_char=15""
    },
    {
      ""id"": 5,
      ""text"": ""bra,"",
      ""lemma"": ""bra,"",
      ""upos"": ""ADJ"",
      ""feats"": ""Definite=Ind|Degree=Pos|Gender=Neut|Number=Sing"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=16|end_char=20""
    },
    {
      ""id"": 6,
      ""text"": ""men"",
      ""lemma"": ""men"",
      ""upos"": ""CCONJ"",
      ""head"": 8,
      ""deprel"": ""cc"",
      ""misc"": ""start_char=21|end_char=24""
    },
    {
      ""id"": 7,
      ""text"": ""badet"",
      ""lemma"": ""bad"",
      ""upos"": ""NOUN"",
      ""feats"": ""Definite=Def|Gender=Neut|Number=Sing"",
      ""head"": 8,
      ""deprel"": ""nsubj"",
      ""misc"": ""start_char=25|end_char=30""
    },
    {
      ""id"": 8,
      ""text"": ""trenger"",
      ""lemma"": ""trenge"",
      ""upos"": ""VERB"",
      ""feats"": ""Mood=Ind|Tense=Pres|VerbForm=Fin"",
      ""head"": 5,
      ""deprel"": ""conj"",
      ""misc"": ""start_char=31|end_char=38""
    },
    {
      ""id"": 9,
      ""text"": ""oppgradering."",
      ""lemma"": ""oppgradering."",
      ""upos"": ""NOUN"",
      ""feats"": ""Definite=Ind|Gender=Fem|Number=Sing"",
      ""head"": 8,
      ""deprel"": ""obj"",
      ""misc"": ""start_char=39|end_char=52""
    },
    {
      ""id"": 10,
      ""text"": ""Badet"",
      ""lemma"": ""bad"",
      ""upos"": ""NOUN"",
      ""feats"": ""Definite=Def|Gender=Neut|Number=Sing"",
      ""head"": 8,
      ""deprel"": ""obj"",
      ""misc"": ""start_char=53|end_char=58""
    }
  ]
]
```

**Expected behavior**
Split the comma from the word

**Environment (please complete the following information):**
 - OS: Linux
 - Python version: 3.9.0
 - Stanza version: 1.1.1
",
644,2021-02-24T19:46:23Z,https://github.com/stanfordnlp/stanza/issues/628,,"I want to process a csv file that contains a column called ""tweets"" instead of just one sentence like below:
doc = nlp(""Barack Obama was born in Hawaii.  He was elected president in 2008."")

I want to read the csv file instead, is that possible and how it should be?

thank you.",
645,2021-02-18T20:36:02Z,https://github.com/stanfordnlp/stanza/pull/627,,"Augment tokenizer models in a couple useful ways:

- shuffle sentences & add random paragraph splits during iterations
- handle a couple different common unicode punctuations",
646,2021-02-18T19:31:12Z,https://github.com/stanfordnlp/stanza/pull/626,,"## Description
A pytorch-based dataloader and transformation for faster tokenization inferencing implementing the bulk_process mechanism. Makes use of batch-wise model inferencing, multiprocessed dataloader and tranformations to prepare the data. Currently, only the docs forward pass through the model is implemented. I have no glue how your code to convert the neural network results back to a tokenization result works (the code in tokenization/utils/output_prediction line 144 on).
I had to implemented the data preparation in an ugly way. Currently, it is realized as a transformation (which is automatically multiprocessed by pytorch which is nice) called transform(). This transformation calls your DataLoader and its next() function (which is ugly). However, I suggest to reimplement the functionality of next() directly into the transformation. I wasn't able to do it.
I tested this on one of our server using 8 workers and a batch size of 256 on100k documents. Inferencing is more than 20x faster (21s vs. 443s) than the standard bulk_process mechanism (as mentioned, the conversion back to tokens is not implemented yet).
All other parts of stanza using pytorch models can use almost the same logic to inference/train their models. 

## Fixes Issues
#615 

## Unit test coverage
Could not be tested so far.

## Known breaking changes/behaviors
No.
",
647,2021-02-17T08:48:45Z,https://github.com/stanfordnlp/stanza/issues/625,,"Using 
```
import stanza
stanza.download('en')
snlp = stanza.Pipeline(lang=""en"",processors='tokenize')
doc = snlp(text)
doc_sents = [sentence.text for sentence in doc.sentences]
```
Output:
```
[""Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century."",
 'Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.',
 'In May 1846 it was merged into ""Godey\'s Lady\'s Book"".',
 ""First for Women is a woman's magazine published by Bauer Media Group in the USA."",
 'The magazine was started in 1989.',
 'It is based in Englewood Cliffs, New Jersey.',
 'In 2011 the circulation of the magazine was 1,310,696 copies.']
```
But, Then I'm losing the trailing whitespace, is there a way to ""keep"" them similar to the behavior in spacy using 
`[sent.**text_with_ws** for sent in doc.sents]`

```
[""Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century. "",
 'Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. ',
 'In May 1846 it was merged into ""Godey\'s Lady\'s Book"". ',
 ""First for Women is a woman's magazine published by Bauer Media Group in the USA. "",
 'The magazine was started in 1989. ',
 'It is based in Englewood Cliffs, New Jersey. ',
 'In 2011 the circulation of the magazine was 1,310,696 copies.']
```
",
648,2021-02-16T09:35:41Z,https://github.com/stanfordnlp/stanza/issues/624,,"**Describe the bug**
The pipeline throws error with bulgarian language loaded, for specific inputs.

**To Reproduce**
Steps to reproduce the behavior:
1. `snlp = stanza.Pipeline(lang=""bg"")`
2. `snlp(""Думи и срички: Горско училище ......................9 Буквен етап • "")`
3. Result:
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-7-baedfa75650b> in <module>
----> 1 snlp(""Думи и срички: Горско училище ......................9 Буквен етап • "")

~/miniconda3/envs/gudgyo/lib/python3.8/site-packages/stanza/pipeline/core.py in __call__(self, doc)
    164         assert any([isinstance(doc, str), isinstance(doc, list),
    165                     isinstance(doc, Document)]), 'input should be either str, list or Document'
--> 166         doc = self.process(doc)
    167         return doc
    168 

~/miniconda3/envs/gudgyo/lib/python3.8/site-packages/stanza/pipeline/core.py in process(self, doc)
    158         for processor_name in PIPELINE_NAMES:
    159             if self.processors.get(processor_name):
--> 160                 doc = self.processors[processor_name].process(doc)
    161         return doc
    162 

~/miniconda3/envs/gudgyo/lib/python3.8/site-packages/stanza/pipeline/depparse_processor.py in process(self, document)
     46         # build dependencies based on predictions
     47         for sentence in batch.doc.sentences:
---> 48             sentence.build_dependencies()
     49         return batch.doc

~/miniconda3/envs/gudgyo/lib/python3.8/site-packages/stanza/models/common/doc.py in build_dependencies(self)
    479                 # id is index in words list + 1
    480                 head = self.words[word.head - 1]
--> 481                 assert(word.head == head.id)
    482             self.dependencies.append((head, word.deprel, word))
    483 

AssertionError: 
```

**Expected behavior**
Pipeline normal output.

**Environment (please complete the following information):**
 - OS: JupyterLab
 - Python version: 3.8.5 from Anaconda
 - Stanza version: 1.1.1

**Additional context**
Tried replicating the bug with other languages with different inputs ineffectively.
With slight modification on the input the bug can remain, or go away (based on the amount of modification)",
649,2021-02-15T15:14:42Z,https://github.com/stanfordnlp/stanza/issues/623,,"Hi, I noticed that your model server is extremely slow. E.g.

```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 30.1MB/s]                    
2021-02-15 15:49:49 INFO: Downloading default packages for language: ca (Catalan)...
Downloading http://nlp.stanford.edu/software/stanza/1.2.0/ca/default.zip: 100%|██████████| 208M/208M [15:55<00:00, 218kB/s]  
2021-02-15 16:05:49 INFO: Finished downloading models and saved to ...
``` 

As you can see it took 15 min to download 200M, with your server providing only about 200kB/s. It's not my connection, since I can easily download 10s of GB of other files in the same time from other sources. 

I will probably end up mirroring the models I need on another server, but I thought you may be interested to know... Any reason you couldn't just host them on github for example, like spaCy does? ",
650,2021-02-11T02:41:15Z,https://github.com/stanfordnlp/stanza/issues/622,,"I tested  stanza with OntoNotes test set (CONLL-12 test set), with OntoNotes' tokenization, and using spacy-stanza. The average micro F1 (entity level evaluation) I got was 85.32 and the Stanza paper reports 88.XX%. Any idea what other factors should I take into account to replicate the result? Just to clarify again: I did not train any new model - I just used Stanza's English NER with OntoNotes test set. ",
651,2021-02-09T23:46:53Z,https://github.com/stanfordnlp/stanza/issues/621,,"On https://stanfordnlp.github.io/CoreNLP/truecase.html, `truecase.bias` is described as: 

> Biases to choose certain behaviors. You can use this to adjust the proclivities of the truecaser. The truecaser classes are: UPPER, LOWER, INIT_UPPER, and O (for mixed case words like McVey).

It has the default values of `INIT_UPPER:-0.7,UPPER:-0.7,O:0`, and I'd like to change these, but it looks like the only way to do that (using `stanza`) is by modifying `self.client.start_cmd` directly in the constructor. This is what I'm doing: 

```
from stanza.server import CoreNLPClient

class TrueCaseAnnotator(object):
    def __init__(self, classpath=CLASSPATH, bias=""INIT_UPPER:-1,UPPER:-1,O:0""):  # note updated values 
        self.client = CoreNLPClient(
            annotators=[""tokenize,ssplit,truecase""],
            classpath=classpath,
            output_format='json',
        )
        self.client.start_cmd.append(""-bias"")
        self.client.start_cmd.append(bias)
```

And this correctly logs the command with the adjusted bias:

> 2021-02-04 21:12:51 INFO: Starting server with command: java -Xmx5G -cp /app/artifacts/stanford-corenlp-4.2.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-462b8ca2f7024c78.props -annotators tokenize,ssplit,truecase -preload -outputFormat json -bias INIT_UPPER:-1,UPPER:-1,O:0

Assuming the range for these variables is -1 to 1 and an `INIT_UPPER` value of -1 means never capitalize the initial word of a sentence, I expect the first word to come out lowercase at all times. **However, this is not what's happening.** 

First, can you confirm -1 to 1 is the correct range? (I've also tried other numbers like -100, 0.2 and 1, and nothing changes).

I did notice that `classBias` (with emphasis on *class*) is always set to the default values (`INIT_UPPER:-0.7,UPPER:-0.7,O:0`) no matter what I pass for the `bias` parameter (see logs below). Is it possible `classBias` is overwriting the value of `bias`? Where is that getting set and what's the difference between the two anyway?

```
2021-02-04 21:29:55 INFO: Starting server with command: java -Xmx5G -cp /app/artifacts/stanford-corenlp-4.2.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-ceededd66e0947d7.props -annotators tokenize,ssplit,truecase -preload -outputFormat json -bias INIT_UPPER:-1,UPPER:-1,O:0
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - Server default properties:
			(Note: unspecified annotator properties are English defaults)
			annotators = tokenize,ssplit,truecase
			bias = INIT_UPPER:1,UPPER:-100,O:0
			inputFormat = text
			outputFormat = json
			prettyPrint = false
			threads = 5
[main] INFO CoreNLP - Threads: 5
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator truecase
[main] INFO edu.stanford.nlp.sequences.SeqClassifierFlags - classBias=INIT_UPPER:-0.7,UPPER:-0.7,O:0
[main] INFO edu.stanford.nlp.sequences.SeqClassifierFlags - loadClassifier=edu/stanford/nlp/models/truecase/truecasing.fast.caseless.qn.ser.gz
[main] INFO edu.stanford.nlp.sequences.SeqClassifierFlags - mixedCaseMapFile=edu/stanford/nlp/models/truecase/MixDisambiguation.list
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/truecase/truecasing.fast.caseless.qn.ser.gz ... done [5.9 sec].
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0.0.0.0:9000
```
",
652,2021-02-08T17:15:12Z,https://github.com/stanfordnlp/stanza/issues/620,,"I download my models with the default `stanza.download(""en"")` with stanza 1.2. When I then load the English model, I see

```
2021-02-08 18:13:22 INFO: Loading these models for language: en (English):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| pos       | combined |
| lemma     | combined |
| depparse  | combined |
========================
```

This is different from before, where I would get ""ewt"" or ""default"". What has changed, and why does it say ""combined"" this time in the new version?",
653,2021-02-08T17:08:16Z,https://github.com/stanfordnlp/stanza/issues/619,,"Since 1.2, a missing processor can throw an error rather than silently be ignored (as was the case in previous versions). This does not happen for all languages. For English, I get the previous bejaviour. But for Dutch, I get an `UnsupportedProcessorError`.

```
stanza.Pipeline(processors=""tokenize,mwt,pos,lemma,depparse"", lang=""nl"")

2021-02-08 18:05:22 ERROR: Cannot load model from C:\Users\bramv\stanza_resources\nl\mwt\default.pt
Traceback (most recent call last):
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\stanza\pipeline\core.py"", line 128, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\stanza\pipeline\processor.py"", line 155, in __init__
    self._set_up_model(config, use_gpu)
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\stanza\pipeline\mwt_processor.py"", line 21, in _set_up_model
    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\stanza\models\mwt\trainer.py"", line 36, in __init__
    self.load(model_file, use_cuda)
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\stanza\models\mwt\trainer.py"", line 141, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\torch\serialization.py"", line 584, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\torch\serialization.py"", line 234, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\torch\serialization.py"", line 215, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\bramv\\stanza_resources\\nl\\mwt\\default.pt'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\IPython\core\interactiveshell.py"", line 3418, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-17-ae17c6e66ce4>"", line 1, in <module>
    stanza.Pipeline(
  File ""C:\Users\bramv\.virtualenvs\astred-Jt9eBbch\lib\site-packages\stanza\pipeline\core.py"", line 150, in __init__
    raise UnsupportedProcessorError(processor_name, lang)
stanza.pipeline.core.UnsupportedProcessorError: Processor mwt is not known for language nl.  If you have created your own model, please specify the mwt_model_path parameter when creating the pipeline.
```

This is interesting, because the verbose output shows me this:

```
2021-02-08 18:05:22 INFO: Loading these models for language: nl (Dutch):
=======================
| Processor | Package |
-----------------------
| tokenize   | alpino  |
| mwt          | default |
| pos           | alpino  |
| lemma      | alpino  |
| depparse  | alpino  |
=======================
```

**Expected behavior**
Either an error for all but preferable the silent ignore (as in previous versions).

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: 3.8.2
 - Stanza version: 1.2
",
654,2021-02-08T16:53:31Z,https://github.com/stanfordnlp/stanza/issues/618,,"Hi

I upgraded today to 1.2 to use the new UD 2.7 models. However, after upgrading the library, it seems as if you cannot have different model versions of the same language. As a result, I had to delete the language folder in `stanza_resources` before I could download the new models. Is this intended behavior? It would be cool if the version of models could be satisfied and downloaded as you require.

Thanks",
655,2021-02-08T00:38:19Z,https://github.com/stanfordnlp/stanza/issues/616,,"**Describe the bug**
`stanza.download(dir="""")` used to work and now doesn't. The [documentation](https://stanfordnlp.github.io/stanza/download_models.html) shows the parameter name as ""dir"", while Python's help function shows ""model_dir"":
```
# Stanza docs

>>> import stanza
>>> help(stanza.download)
Help on function download in module stanza.resources.common:

download(lang='en', model_dir='/home/argosopentech/stanza_resources', package='default', processors={}, logging_level=None, verbose=None, resources_url='https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master', resources_branch=None, resources_version='1.2.0', model_url='default')
    # main download function
```
Changing the parameter name to `model_dir` fixed the issue for me.

[Code](https://github.com/argosopentech/onmt-models/blob/a0c8c2b0a136743ef5ed554c32252a49782a5e53/download_stanza_model.sh):
```
# Old
python3 -c ""import stanza; stanza.download('$stanza_lang_code', dir='stanza', processors='tokenize')""

# New
python3 -c ""import stanza; stanza.download('$stanza_lang_code', model_dir='stanza', processors='tokenize')""
```
",
656,2021-02-03T15:43:55Z,https://github.com/stanfordnlp/stanza/issues/614,,"**Describe the bug**
In some cases, and since the 1.2.0 update,  the French GSD model considers the last word of the phrase and the final punctuation as the same token.

**To Reproduce**

```
nlp_fr = stanza.Pipeline('fr', package='gsd', processors='tokenize')
phrase = ""C'est mon ami.""
doc = nlp_fr(phrase)

for s in doc.sentences:
    for t in s.tokens:
        print(t.text)
```
Observed Output:

```
C'
est
mon
ami.
```


**Expected behavior**
I expected the final output to consider the last word and the punctuation as two separate tokens, similarly to what happens with some other phrases (see below):

```
nlp_fr = stanza.Pipeline('fr', package='gsd', processors='tokenize')
phrase = ""Je n'apprends pas le français.""
doc = nlp_fr(phrase)

for s in doc.sentences:
    for t in s.tokens:
        print(t.text)
```

Observed Output:

```
Je
n'
apprends
pas
le
français
.
```

**Environment:**
 - OS: MacOS
 - Python version: 3.7.3, also observed in Python 3.6.9 on Google Colab
 - Stanza version: 1.2.0",
657,2021-02-03T06:41:28Z,https://github.com/stanfordnlp/stanza/issues/613,,"Since some languages are similar and based on a root language, I have some words with POS and want to add on top a pretrained model. Can I train a new language based on a pretrained model?",
658,2021-02-03T03:23:13Z,https://github.com/stanfordnlp/stanza/pull/612,,"update `tqdm` import to cooperate with the `jupyter`
test with the conda-forge version with py 3.9.1

**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Description
Update `tqdm` import to cooperate with the `jupyter`

## Fixes Issues
- Fix download progress bar in jupyter

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
- No

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
- No
",
659,2021-02-03T02:02:02Z,https://github.com/stanfordnlp/stanza/pull/611,,"## Description
This PR aims at providing a transparent `bulk_process` interface for processors that operate on the sentence-level to batch across documents for optimized speed, and implements cross-document batching for the tokenizer, which doesn't operate on the sentence level.

## Fixes Issues
Could help significantly with #550, especially in cases where the user wishes to process a large collection of short documents.

## Unit test coverage
Covered by existing tests, also added new tests in `tests/test_english_pipeline.py` for character offsets in multi-document mode (`test_conllu_multidoc`) and processor variants in multi-document mode (`test_dependency_parse_multidoc_variant`).

## Known breaking changes/behaviors
N/A
",
660,2021-02-01T05:37:46Z,https://github.com/stanfordnlp/stanza/issues/610,,"I used to install stanza in a conda env by PyPI.
But I found it appeared in the conda-forge channel and found it worked fine with light usage.
If the conda-forge version works fine, you may suggest the users install that one, since `stanfordnlp` channel has fewer versions supported.",
661,2021-01-30T08:19:21Z,https://github.com/stanfordnlp/stanza/pull/609,,,
662,2021-01-29T10:57:34Z,https://github.com/stanfordnlp/stanza/issues/608,,"Running that [exact code](https://stanfordnlp.github.io/stanza/data_conversion.html#python-object-to-document) throws this:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-55-99f49c80a14a> in <module>
      2 
      3 dicts = [[{'id': '1', 'text': 'Test', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=0|end_char=4'}, {'id': '2', 'text': 'sentence', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=5|end_char=13'}, {'id': '3', 'text': '.', 'upos': 'PUNCT', 'xpos': '.', 'misc': 'start_char=13|end_char=14'}]] # dicts is List[List[Dict]], representing each token / word in each sentence in the document
----> 4 doc = Document(dicts) # doc is class Document

~/.conda/envs/ialearn/lib/python3.7/site-packages/stanza/models/common/doc.py in __init__(self, sentences, text)
     77 
     78         self.text = text
---> 79         self._process_sentences(sentences)
     80         self._ents = []
     81 

~/.conda/envs/ialearn/lib/python3.7/site-packages/stanza/models/common/doc.py in _process_sentences(self, sentences)
    143         self.sentences = []
    144         for tokens in sentences:
--> 145             self.sentences.append(Sentence(tokens, doc=self))
    146             begin_idx, end_idx = self.sentences[-1].tokens[0].start_char, self.sentences[-1].tokens[-1].end_char
    147             if all([self.text is not None, begin_idx is not None, end_idx is not None]): self.sentences[-1].text = self.text[begin_idx: end_idx]

~/.conda/envs/ialearn/lib/python3.7/site-packages/stanza/models/common/doc.py in __init__(self, tokens, doc)
    342         self._doc = doc
    343 
--> 344         self._process_tokens(tokens)
    345 
    346     def _process_tokens(self, tokens):

~/.conda/envs/ialearn/lib/python3.7/site-packages/stanza/models/common/doc.py in _process_tokens(self, tokens)
    361                 self.words.append(new_word)
    362                 idx = entry.get(ID)[0]
--> 363                 if idx <= en:
    364                     self.tokens[-1].words.append(new_word)
    365                 else:

TypeError: '<=' not supported between instances of 'str' and 'int'

```
It tries to compare the `'id'` with an `int`, but `'id'` is a `str`.

I changed `'id'` fields and worked for me:

```python
from stanza.models.common.doc import Document

dicts = [
    [
            {
                ""id"": 1,
                ""text"": ""Test"",
                ""upos"": ""NOUN"",
                ""xpos"": ""NN"",
                ""feats"": ""Number=Sing"",
                ""misc"": ""start_char=0|end_char=4"",
            }
        ,
            {
                ""id"": 2,
                ""text"": ""sentence"",
                ""upos"": ""NOUN"",
                ""xpos"": ""NN"",
                ""feats"": ""Number=Sing"",
                ""misc"": ""start_char=5|end_char=13"",
            }
        ,
            {
                ""id"": 3,
                ""text"": ""."",
                ""upos"": ""PUNCT"",
                ""xpos"": ""."",
                ""misc"": ""start_char=13|end_char=14"",
            }
        ,
    ]
]  # dicts is List[List[Dict]], representing each token / word in each sentence in the document
doc = Document(dicts)  # doc is class Document
```

So either the example is wrong or, if you want to be all strings in the dicts, a casting might be necessary. If it's the latter, I can try to send a PR.",
663,2021-01-27T16:35:38Z,https://github.com/stanfordnlp/stanza/pull/607,,Merge everything from dev to master for version 1.2,
664,2021-01-25T23:49:45Z,https://github.com/stanfordnlp/stanza/issues/605,,I tried to use Stanza as a wrapper around StanfordCoreNLP but I couldn't be able to get the dependency parsing or head deprel value. Is this possible? Thank you!,
665,2021-01-25T23:46:11Z,https://github.com/stanfordnlp/stanza/pull/604,,"Save models to be usable with older versions of torch.  Also, shrink pretrains a bit",
666,2021-01-25T21:53:26Z,https://github.com/stanfordnlp/stanza/issues/603,,"Traceback (most recent call last):
  File ""tree.py"", line 145, in <module>
    segment = lemmatize(text)
  File ""tree.py"", line 35, in lemmatize
    doc = nlp(sentence)
  File ""/home/alberto/.local/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 166, in __call__
    doc = self.process(doc)
  File ""/home/alberto/.local/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 160, in process
    doc = self.processors[processor_name].process(doc)
  File ""/home/alberto/.local/lib/python3.7/site-packages/stanza/pipeline/pos_processor.py"", line 30, in process
    sort_during_eval=True)
  File ""/home/alberto/.local/lib/python3.7/site-packages/stanza/models/pos/data.py"", line 48, in __init__
    self.data = self.chunk_batches(data)
  File ""/home/alberto/.local/lib/python3.7/site-packages/stanza/models/pos/data.py"", line 150, in chunk_batches
    (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data])
  File ""/home/alberto/.local/lib/python3.7/site-packages/stanza/models/common/data.py"", line 39, in sort_all
    return sorted_all[2:], sorted_all[1]
IndexError: list index out of range",
667,2021-01-21T10:44:06Z,https://github.com/stanfordnlp/stanza/issues/602,,"Hello,

Instantiating `stanza.Pipeline` without having downloaded a language resource raises a generic exception such as 
e.g . `stanza_model = StanzaNER(dir_model=STANZA_DIR_MODEL, processors=STANZA_PROCESSORS)` may result in 

`Exception(f""Resources file not found at: {resources_filepath}. Try to download the model again."")`

It could be helpful to instead raise a `FileNotFoundError` for better downstream handling (ie other libraries calling stanza). 

",
668,2021-01-19T21:38:52Z,https://github.com/stanfordnlp/stanza/pull/601,,"…ng useful

**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Desciption
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
669,2021-01-19T14:00:46Z,https://github.com/stanfordnlp/stanza/issues/600,,"I'm trying to use dependency tree in my neural model, however, I found that many words do not match the dependency analysis and word segmentation. For example, most tokenizers will treat flip-flops as one word, and in the dependency analysis of stanza, they will always be treated as three words:'flip','-','flops',even if after i used spacy tokenize, is there any way to solve this problem? 

like：
```
nlp = stanza.Pipeline(lang='en', processors={'tokenize': 'spacy'}) # spaCy tokenizer is currently only allowed in English pipeline.
doc = nlp('a girl sitting on the steps of a building wearing a green short-sleeve shirt,blue jeans ,flip-flops holding a cup and writing in a journal with her purse sitting at her feet.')
for i, sentence in enumerate(doc.sentences):
    print(f'====== Sentence {i+1} tokens =======')
    print(*[f'id: {token.id}\ttext: {token.text}' for token in sentence.tokens], sep='\n')
```


will get:
```
id: (1,)	text: a
id: (2,)	text: man
id: (3,)	text: -
id: (4,)	text: like
id: (5,)	text: girl
id: (6,)	text: sitting
id: (7,)	text: on
id: (8,)	text: the
id: (9,)	text: steps
id: (10,)	text: of
id: (11,)	text: a
id: (12,)	text: building
id: (13,)	text: wearing
id: (14,)	text: a
id: (15,)	text: green
id: (16,)	text: short
id: (17,)	text: -
id: (18,)	text: sleeve
id: (19,)	text: shirt
id: (20,)	text: ,
id: (21,)	text: blue
id: (22,)	text: jeans
id: (23,)	text: ,
id: (24,)	text: flip
id: (25,)	text: -
id: (26,)	text: flops
id: (27,)	text: holding
id: (28,)	text: a
id: (29,)	text: cup
id: (30,)	text: and
id: (31,)	text: writing
id: (32,)	text: in
id: (33,)	text: a
id: (34,)	text: journal
id: (35,)	text: with
id: (36,)	text: her
```
",
670,2021-01-18T18:48:19Z,https://github.com/stanfordnlp/stanza/issues/599,,"**Is your feature request related to a problem? Please describe.**
Request Amharic support which does not exist in stanza

**Describe the solution you'd like**
Add Amharic support 

**Describe alternatives you've considered**
SpaCY, still work in progress for handling multi-word tokens.
https://github.com/explosion/spaCy/discussions/6648#discussioncomment-266720

**Additional context**
Would like to contribute more for this language
",
671,2021-01-18T16:33:15Z,https://github.com/stanfordnlp/stanza/issues/598,,"I have Stanza installed via pip, but when I try to download a default processor, I get the error message: ""module 'stanza' has no attribute 'download'."" In the FAQ, it says this problem may be happening because of my Python version, but I have Python 3.8.5. I am running this on a Windows computer in JupyterLab. Any help would be appreciated. 
",
672,2021-01-18T15:38:49Z,https://github.com/stanfordnlp/stanza/issues/597,,"Hello,

I'm not enough skilled at programming, so maybe my question is easy to answer. I've started from a single text file, something like this

> Wolf, currently a journalist in Argentina, played with Del Bosque in the final years of the seventies in Real Madrid. [...]

Next, I use `stanza.Pipeline('en', processors = 'tokenize, pos, ner', ...)` obtaining a Stanza document type and the NER problem solved using the BIOES NER tags. Now, how can I process the `stanza. Pipeline` result to a txt file like this

> [PER Wolf ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid ] .

Thank you very much!",
673,2021-01-18T01:50:59Z,https://github.com/stanfordnlp/stanza/issues/596,,"Hello,
I'm analysing a dataset of tweets and I get some tweets broken into several sentences. I found that some word sequences are causing this and just wondering if there's any workaround. One example is _appreciated thanks_, where _thanks_ would break the sentence if it appears right after _appreciated_ in the text passed as below.

thanks in advance.
 
<img width=""1215"" alt=""Screenshot 2021-01-18 at 1 48 05 am"" src=""https://user-images.githubusercontent.com/44207018/104863944-79feaa80-592f-11eb-99e6-86ee55c12aaf.png"">
",
674,2021-01-17T09:32:51Z,https://github.com/stanfordnlp/stanza/issues/595,,"I intend to annotate multiple sentences for <POS, NER, LEM> at the same time. However, the speed is too slow for my work with CoreNLP Client on CPU. To speed up, I try to use stanza. However, the result from stanza is different from CoreNLP Client. I show an example of NER task following.

for the sentence:   I feel our people should learn more from the DPRK.
CoreNLP:     [DPRK] is tagged as [ORGANIZATION]
stanza:  [DPRK] is tagged as [ORG]

So, is there some mapping rules to indicate [ORG] is [ORGANIZATION] or some methods to produce the same result with CoreNLP Client for stanza? THX a lot!!",
675,2021-01-16T23:18:22Z,https://github.com/stanfordnlp/stanza/issues/594,,"I couldn't find a similar issue,
Essentially as  the title says, every time I rum CoreNLP server, it  creates a new .props file ex. ""corenlp_server-0a59d4cf7af74ed4.props""
They are all the same and document which annotators I initialized the server with. 
ex. 
```
annotators = tokenize,ssplit,pos,lemma,ner

outputFormat = serialized
```

I start the server as follows, and it closes properly each time
```
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner'], timeout=30000, 
                   threads=6,memory='6G',be_quiet=True) as client:
```
Is there any way to prevent it from creating the props file in the directory that the script runs?
",
676,2021-01-16T18:50:55Z,https://github.com/stanfordnlp/stanza/pull/593,,A few minor changes to the NER training,
677,2021-01-16T18:14:00Z,https://github.com/stanfordnlp/stanza/pull/592,,Adds a couple scripts which make it easier to process charlm data,
678,2021-01-14T11:19:30Z,https://github.com/stanfordnlp/stanza/issues/591,,"Hi Stanza Team,

Is there any JavaScript wrapper or JavaScript based implementation available for Stanza's Neural pipeline for English language? 

Thanks,
",
679,2021-01-14T00:05:49Z,https://github.com/stanfordnlp/stanza/pull/590,,Add a potentially helpful message when sentences or batches which are too big cause an OOM error,
680,2021-01-12T05:10:29Z,https://github.com/stanfordnlp/stanza/pull/589,,,
681,2021-01-12T04:15:43Z,https://github.com/stanfordnlp/stanza/pull/588,,"Keep tokenization of URLs and emails as one item by post-processing the predictions and altering anything matched by the regex

Should answer at least one issue with French, although the problem seems to be present in many languages

https://github.com/stanfordnlp/stanza/issues/539",
682,2021-01-07T17:57:45Z,https://github.com/stanfordnlp/stanza/pull/587,,"Include sentiment and a few NER-specific rules to the resource packaging.  Also, improve a few edge cases and add more logging.",
683,2021-01-06T14:00:06Z,https://github.com/stanfordnlp/stanza/issues/586,,"Hey there, 

Hope you'll are doing well

This library is great help to me for NER in my project work. Special thanks to team for same.
May I know, whether there is any way to custom train model using existing pre-trained stanza model and my own corpus. So that I can add value to default stanza model 

Many thanks in advance

Best regards
",
684,2021-01-04T08:05:13Z,https://github.com/stanfordnlp/stanza/issues/585,,"**Describe the bug**
Hello, I use stanza with Pyspark for preprocessing issue on Cloudera Data Platform. To access all dependecies from all nodes,
i zip dependencies including stanza within libs.zip. When i execute my spark job giving this zip file as a parameter, i encounter error like below:

```
     _from .stanford_nlp_preprocess import StanfordNlp
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 656, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 626, in _load_backward_compatible
  File ""./libs.zip/dependencies/preprocess/stanford_nlp_preprocess.py"", line 7, in <module>
    import stanza
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 656, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 626, in _load_backward_compatible
  File ""./libs.zip/stanza/__init__.py"", line 1, in <module>
    from stanza.pipeline.core import Pipeline
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 656, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 626, in _load_backward_compatible
  File ""./libs.zip/stanza/pipeline/core.py"", line 8, in <module>
    import torch
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 656, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 626, in _load_backward_compatible
  File ""./libs.zip/torch/__init__.py"", line 189, in <module>
    _load_global_deps()
  File ""./libs.zip/torch/__init__.py"", line 142, in _load_global_deps
    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
  File ""/usr/lib64/python3.6/ctypes/__init__.py"", line 343, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: /yarn/nm/usercache/centos/appcache/application_1607800799654_0219/container_1607800799654_0219_01_000013/libs.zip/torch/lib/libtorch_global_deps.so: cannot open shared object file: Not a directory_

```
I try to import torch via Python terminal and it doesn't give any error but when i try to execute within zip, it gives this error. I control the path which written in error log (/xxxx/libs.zip/torch/lib/libtorch_global_deps.so) and i can see that libtorch_global_deps.so is there but yes it is not directory. So why give this error? 


**To Reproduce**
Steps to reproduce the behavior:
1. Create a Pyspark job with stanza 
2. Write neccessary packages to Pipfile and create PipFile.lock
3. Write a script to install and zip all packages
4. Give this zip file as a argument to spark job
5. `import stanza`

**Expected behavior**
Execute without an error

**Environment:**
 - OS: CentOS
 - Python version: 3.6
 - PySpark version: 3.0.0
 - Stanza version: 1.1.1
 - Torch version: 1.7.1
 - There are 5 hosts in cluster

",
685,2021-01-03T22:06:59Z,https://github.com/stanfordnlp/stanza/issues/584,,"
When working on GPU (although pool (3)) 'anaconda3 / envs / venv / lib / python3.6 / site-packages / torch / multiprocessing / reductions.py "", line 117, in rebuild_cuda_tensor
    event_sync_required)
RuntimeError: CUDA error: out of memory
'I get the error. So I want to try to work on cpu.",
686,2021-01-02T22:17:49Z,https://github.com/stanfordnlp/stanza/issues/583,,"```python
import stanza
nlp_stanza = stanza.Pipeline(lang='en',processors='tokenize,mwt,pos')
doc_test = nlp_stanza(""Bill Gates is a good person."")
for token in doc_test.sentences[0].tokens:
    print(token.text,end="", "")
```
gives
`Bill, Gates, is, a, good, person, ., `

Is there some way to have a list of tokens where `Bill Gates` is a single token ?

Tks in advance!",
687,2020-12-31T23:12:09Z,https://github.com/stanfordnlp/stanza/issues/582,,"Hi

I trained a mwt. However, instead of training with sentences, I just used words and their expansions, as shown below.
Is this OK? does mwt consider context before does any expansions? Because, it works strangely; at time it breaks certain words and some other times not. Therefore, wondering this multi word tokeniser takes the context into the consideration when tokening.
How it works?

Where can I find the detail documentation about Stanza architecture? I looked at the paper, but, what it has is very high level information.

1-2	சேர்த்துக்கொள்வோம்	_	_	_	_	_	_	_	_
1	சேர்த்து	சேர்	_	_	_	0	root	0:root	_
2	கொள்வோம்	கொள்	_	_	_	1	dep	1:dep	_

1-3	கொண்டிருக்கவேண்டும்	_	_	_	_	_	_	_	_
1	கொண்டு	கொள்	_	_	_	0	root	0:root	_
2	இருக்க	இரு	_	_	_	1	dep	1:dep	_
3	வேண்டும்	வேண்டு	_	_	_	2	dep	2:dep	_

1-2	மக்களும்	_	_	_	_	_	_	_	_
1	மக்கள்	மக்கள்	_	_	_	0	root	0:root	_
2	உம்	உம்	_	_	_	1	dep	1:dep	_",
688,2020-12-31T14:06:52Z,https://github.com/stanfordnlp/stanza/pull/581,,,
689,2020-12-30T13:46:23Z,https://github.com/stanfordnlp/stanza/issues/580,,"Greeting,

I am new to CoreNLP enviroment and trying run the example code given on documentation. However, I got two errors as follows;

First code: 
`from stanza.server import CoreNLPClient
with CoreNLPClient(
        annotators=['tokenize','ssplit','pos',""ner""],
        timeout=30000,
        memory='2G',be_quiet=True) as client:
        anno = client.annotate(text)`

> 2020-12-30 16:40:53 INFO: Writing properties to tmp file: corenlp_server-a15136448b834f79.props
2020-12-30 16:40:53 INFO: Starting server with command: java -Xmx2G -cp C:\Users\fatih\stanza_corenlp\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a15136448b834f79.props -annotators tokenize,ssplit,pos,ner -preload -outputFormat serialized

```
`Traceback (most recent call last):

  File ""C:\Users\fatih\anaconda3\lib\site-packages\stanza\server\client.py"", line 446, in _request
    r.raise_for_status()
  File ""C:\Users\fatih\anaconda3\lib\site-packages\requests\models.py"", line 941, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%27annotators%27%3A+%27tokenize%2Cssplit%2Cpos%2Cner%27%2C+%27outputFormat%27%3A+%27serialized%27%7D&resetDefault=false
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<ipython-input-6-2fbdcdb77b41>"", line 6, in <module>
    anno = client.annotate(text)
  File ""C:\Users\fatih\anaconda3\lib\site-packages\stanza\server\client.py"", line 514, in annotate
    r = self._request(text.encode('utf-8'), request_properties, reset_default, **kwargs)
  File ""C:\Users\fatih\anaconda3\lib\site-packages\stanza\server\client.py"", line 452, in _request
    raise AnnotationException(r.text)
AnnotationException: Could not handle incoming annotation`
```

What am I doing wrong? It's on windows, Anaconda, Spyder. 



",
690,2020-12-29T01:31:21Z,https://github.com/stanfordnlp/stanza/issues/579,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
691,2020-12-28T20:35:42Z,https://github.com/stanfordnlp/stanza/pull/578,,Make some errors which are friendlier and easier to interpret than the existing strategy of KeyError or FileNotFoundError,
692,2020-12-28T18:30:16Z,https://github.com/stanfordnlp/stanza/pull/577,,"## Desciption
This PR adds a basic support for passing a list of stanza `Document` objects into the neural pipeline and getting a list of `Document`s on the other end (although it could potentially be sped up in a later iteration to improve CPU/GPU utility via cross-document batching).

## Fixes Issues
Feature request from email.

## Unit test coverage
Covered by added tests in [test_english_pipeline.py](https://github.com/stanfordnlp/stanza/blob/pipeline-multidoc/tests/test_english_pipeline.py#L141-L159).

## Known breaking changes/behaviors
N/A
",
693,2020-12-28T08:22:32Z,https://github.com/stanfordnlp/stanza/issues/576,,"I run this code:
nlp = stanza.Pipeline(lang='zh', processors='tokenize,pos,lemma,parse',tokenize_pretokenized=True)   
It raise: 
assert(key in PIPELINE_NAMES)
AssertionError

However, when I use Client. I don't know how to skip over the tokenization step. 

",
694,2020-12-27T22:29:12Z,https://github.com/stanfordnlp/stanza/issues/575,,"**Describe the bug**
Cannot instantiate Japanese pipeline with NER model.

Throws the following KeyError exception:
```
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
  File ""/root/miniconda3/envs/stanza-test/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 111, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""/root/miniconda3/envs/stanza-test/lib/python3.8/site-packages/stanza/pipeline/processor.py"", line 146, in __init__
    self._set_up_model(config, use_gpu)
  File ""/root/miniconda3/envs/stanza-test/lib/python3.8/site-packages/stanza/pipeline/ner_processor.py"", line 25, in _set_up_model
    args = {'charlm_forward_file': config['forward_charlm_path'], 'charlm_backward_file': config['backward_charlm_path']}
KeyError: 'forward_charlm_path'
```

**To Reproduce**
```python
import stanza
stanza.Pipeline('ja', processors='tokenize,ner', use_gpu=False)
```

**Environment (please complete the following information):**
 - OS: Ubuntu 20.04
 - Python version: 3.8.5
 - Stanza version: 1.1.1
",
695,2020-12-23T14:16:18Z,https://github.com/stanfordnlp/stanza/issues/573,,"**Is your feature request related to a problem? Please describe.**

It's not what it takes to add a new language.

**Describe the solution you'd like**

Expand on the documentation what is required to add support for a language.
",
696,2020-12-21T05:09:08Z,https://github.com/stanfordnlp/stanza/pull/572,,"Build one dataset out of several Italian datasets

Also includes an English model, currently from just 2 datasets",
697,2020-12-19T21:07:58Z,https://github.com/stanfordnlp/stanza/issues/571,,"Hi

1.

Why tokeniser script looks for ud_ttb-ud-dev-mwt.json when try to train a tokeniser? This has to be created during the training, isnt it?

2.
Where can I find detail documentation about model training scripts and their parameters (I looked at https://stanfordnlp.github.io/stanza/training.html#training-with-scripts)

Thank you
bash scripts/run_tokenize.sh UD_Tamil-TTB --batch_size 100 --dropout 0.33
Running tokenizer with --batch_size 100 --dropout 0.33...
Running tokenizer in train mode
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenizer.py"", line 183, in <module>
    main()
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenizer.py"", line 93, in main
    train(args)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenizer.py"", line 98, in train
    mwt_dict = load_mwt_dict(args['mwt_json_file'])
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenize/utils.py"", line 16, in load_mwt_dict
    with open(filename, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/processed/tokenize/ta_ttb-ud-dev-mwt.json'
Running tokenizer in predict mode
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenizer.py"", line 183, in <module>
    main()
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenizer.py"", line 95, in main
    evaluate(args)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenizer.py"", line 161, in evaluate
    mwt_dict = load_mwt_dict(args['mwt_json_file'])
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenize/utils.py"", line 16, in load_mwt_dict
    with open(filename, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/processed/tokenize/ta_ttb-ud-dev-mwt.json'
Traceback (most recent call last):
  File ""stanza/utils/conll18_ud_eval.py"", line 532, in <module>
    main()
  File ""stanza/utils/conll18_ud_eval.py"", line 500, in main
    evaluation = evaluate_wrapper(args)
  File ""stanza/utils/conll18_ud_eval.py"", line 482, in evaluate_wrapper
    gold_ud = load_conllu_file(args.gold_file)
  File ""stanza/utils/conll18_ud_eval.py"", line 478, in load_conllu_file
    return load_conllu(_file)
  File ""stanza/utils/conll18_ud_eval.py"", line 209, in load_conllu
    process_word(word)
  File ""stanza/utils/conll18_ud_eval.py"", line 205, in process_word
    process_word(parent)
  File ""stanza/utils/conll18_ud_eval.py"", line 205, in process_word
    process_word(parent)
  File ""stanza/utils/conll18_ud_eval.py"", line 205, in process_word
    process_word(parent)
  File ""stanza/utils/conll18_ud_eval.py"", line 197, in process_word
    raise UDError(""There is a cycle in a sentence"")
__main__.UDError: There is a cycle in a sentence
ta_ttb --batch_size 100 --dropout 0.33
",
698,2020-12-17T18:50:03Z,https://github.com/stanfordnlp/stanza/issues/570,,"Hello, 
We have been developing a FastAPI application where we use some external libraries to perform some NLP tasks, such as tokenization. On top of this, we are launching the service with Gunicorn so that we can parallelize the requests.
However, we are having difficulties using [Stanza](https://stanfordnlp.github.io/stanza/) with Gunicorn’s preload flag active.
It is a requirement to use this flag because since Stanza models can be large, we want the models to be loaded only once, in the Gunicorn master process. This way, Gunicorn workers can access the models that were previously loaded in the master process.

The difficulties that we are facing resumes on the fact that Gunicorn workers hang when trying to make an inference over a given model (that was loaded initially by the master process).

We’ve done some research and debugging but we weren’t able to find a solution. However, we noticed that the worker hangs when the code reaches the prediction step on PyTorch.
Although we are talking about Stanza, this problem also occurred with [Sentence Transformers library](https://github.com/UKPLab/sentence-transformers). And both of them are using the PyTorch library. 

Following, I’ll present more details:

**Environment:**
```
FastApi version: 0.54.2
Gunicorn version: 20.0.4
Uvicorn version: 0.12.3
Python version: 3.7
Stanza version: 1.1.1
OS: macOS Catalina 10.15.6
```
**Steps executed:**

- Gunicorn command:
```
gunicorn --workers 1 --worker-class uvicorn.workers.UvicornWorker --max-requests=0 --max-requests-jitter=0 --timeout=120 --keep-alive=2 \
     --log-level=info --access-logfile - --preload -b 0.0.0.0:8010 my_app:app
```

- The code that will run before launching the workers
```
def initialize_application() -> None:
     ...
     model = stanza.Pipeline(
                lang=cls._TOKENIZER_MODEL_LANGUAGES[language],
                package=cls._MODEL_TYPE[language],
                processors=cls._TOKENIZER_MODEL,
                tokenize_no_ssplit=True,
            )

```
This way, the model can be loaded only once, in the master process. 
Once the required workers are launched, they should have access to the previous model, without having to load it by themselves (saving computational resources).

The problem happens when we receive a request that will make use of the model that was initially loaded. The worker that will be responsible for handling the request, won’t be able to use the model for inference. As so, the worker will be hanged until the timeout occurs.

After analyzing the code and debugging it, we reached the following step until the code stopped working:

1. Our code has a call to the `process()` method, `class Pipeline`, on the core.py file of Stanza.
2. That line calls the specific `process()` method, in this case from the tokenize_process.py, `class TokenizeProcessor`
3. Which calls the PyTorch code, `output_predictions()` method, from the utils.py
4. After some steps, it reaches the model.py file still in PyTorch, `class Tokenizer(nn.Module)`, `forward(self, x, feats) method`, in the following line: `nontok = F.logsigmoid(-tok0)`. It seems that this line is calling some C++ code where we didn’t investigate any further.

Of course, if we remove the --preload flag, everything will run smoothly. Removing it is something that we want to avoid because of the added computational resources that will be necessary (the models will be duplicated in every worker).

We looked through several other issues that could be related to this one, such as:
https://github.com/benoitc/gunicorn/issues/2157
https://github.com/tiangolo/fastapi/issues/2425
https://github.com/tiangolo/fastapi/issues/596
https://github.com/benoitc/gunicorn/issues/2124
and others...

After trying multiple solutions, it wasn’t possible to solve the issue. Do you have any suggestions to handle this? Or other tests that I can perform to give you more information?

Thanks in advance.

P.S.: I also opened issues on the Gunicorn and PyTorch github pages:
- https://github.com/benoitc/gunicorn/issues/2478
- https://github.com/pytorch/pytorch/issues/49555",
699,2020-12-16T02:31:00Z,https://github.com/stanfordnlp/stanza/pull/569,,"Add functionality for creating 'segmenter' datasets out of GSD, Kaist, or both at the same time

",
700,2020-12-16T02:28:24Z,https://github.com/stanfordnlp/stanza/pull/568,,"Add python versions of the scripts to build depparse data, run the depparse trainer, and run the end to end evaluation.",
701,2020-12-15T23:57:49Z,https://github.com/stanfordnlp/stanza/pull/567,,"## Description
This PR integrats PyThaiNLP as an external tokenizer for the Stanza Thai pipeline. This integration can be used as:
```python
text = ""ข้าราชการได้รับการหมุนเวียนเป็นระยะ และเขาได้รับมอบหมายให้ประจำในระดับภูมิภาค""
nlp = stanza.Pipeline(lang='th', processors={'tokenize': 'pythainlp'}, package=None)
doc = nlp(text)
```

Note:
- I only integrated the default sentence and word segmentation models in PyThaiNLP. For sentence segmentation it is a CRF-based model; for word segmentation it is a dictionary-based model (newmm). 
- To make it consistent with other tokenizers, I am currently throwing out all whitespace tokens output by PyThaiNLP. Not sure if this is the best decision for Thai.

## Unit test coverage
Unit test covers basic sentence segmentation and word segmentation.

## Known breaking changes/behaviors
None.
",
702,2020-12-14T04:25:19Z,https://github.com/stanfordnlp/stanza/pull/566,,Should fix https://github.com/stanfordnlp/stanza/issues/561,
703,2020-12-14T04:16:22Z,https://github.com/stanfordnlp/stanza/pull/565,,,
704,2020-12-14T04:01:15Z,https://github.com/stanfordnlp/stanza/issues/564,,"I am using the stanza pipeline function combined with flask, but the core file is generated every time I start it. Is it because of my lack of memory? How can I specify memory parameters?",
705,2020-12-13T13:25:35Z,https://github.com/stanfordnlp/stanza/issues/563,,"I have initialized the pipeline as follows. I prefer to use 4 entities provided by `conll03` model.

`nlp= stanza.Pipeline('en', use_gpu= True, processors={""tokenize"": ""ewt"", 'ner': 'conll03'}, verbose= True) 
`
```
2020-12-13 13:22:06 INFO: Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
| pos       | ewt     |
| lemma     | ewt     |
| depparse  | ewt     |
| sentiment | sstplus |
| ner       | conll03 |
=======================

2020-12-13 13:22:06,877 :: INFO :: Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
| pos       | ewt     |
| lemma     | ewt     |
| depparse  | ewt     |
| sentiment | sstplus |
| ner       | conll03 |
=======================

2020-12-13 13:22:06 INFO: Use device: gpu
2020-12-13 13:22:06,883 :: INFO :: Use device: gpu
2020-12-13 13:22:06 INFO: Loading: tokenize
2020-12-13 13:22:06,884 :: INFO :: Loading: tokenize
2020-12-13 13:22:06 INFO: Loading: pos
2020-12-13 13:22:06,894 :: INFO :: Loading: pos
2020-12-13 13:22:07 INFO: Loading: lemma
2020-12-13 13:22:07,998 :: INFO :: Loading: lemma
2020-12-13 13:22:08 INFO: Loading: depparse
2020-12-13 13:22:08,146 :: INFO :: Loading: depparse
2020-12-13 13:22:09 INFO: Loading: sentiment
2020-12-13 13:22:09,365 :: INFO :: Loading: sentiment
2020-12-13 13:22:10 INFO: Loading: ner
2020-12-13 13:22:10,442 :: INFO :: Loading: ner
2020-12-13 13:22:11 INFO: Done loading processors!
2020-12-13 13:22:11,055 :: INFO :: Done loading processors!

```

Despite using this specification, the output is `ontonotes` entities.  Not sure if I am doing anything wrong.",
706,2020-12-11T18:18:58Z,https://github.com/stanfordnlp/stanza/issues/562,,"Hi, 
I have a question about the language models one can download by calling the stanza.download() method. More specifically, I have downloaded the Lithuanian and North Sami models for a project I am currently working on trying to improve tagging results. I have run every part of the pipeline on the appropriate test sets to produce baseline performance results. I acquired these results by running the stanza.utils.conll18_ud_eval script on the gold and predicted test files. The score tables I ended up with are however different from the ones presented on your website https://stanfordnlp.github.io/stanza/performance.html . 

This is my score table for Lithuanian  (alksnis package): 

```
Metric     | Precision |    Recall |  F1 Score | AligndAcc
-----------+-----------+-----------+-----------+-----------
Tokens     |     99.84 |     99.90 |     99.87 |
Sentences  |     90.15 |     88.30 |     89.22 |
Words      |     99.84 |     99.90 |     99.87 |
UPOS       |     93.31 |     93.36 |     93.34 |     93.46
XPOS       |     85.65 |     85.70 |     85.68 |     85.79
UFeats     |     86.64 |     86.69 |     86.66 |     86.77
AllTags    |     83.61 |     83.65 |     83.63 |     83.74
Lemmas     |     92.48 |     92.53 |     92.51 |     92.63
UAS        |     78.53 |     78.57 |     78.55 |     78.65
LAS        |     69.86 |     69.90 |     69.88 |     69.97
CLAS       |     66.19 |     65.48 |     65.83 |     65.51
MLAS       |     55.83 |     55.23 |     55.53 |     55.26
BLEX       |     61.49 |     60.83 |     61.16 |     60.85
```

The tokenization, tagging, and lemma scores here are either the same as the ones presented on your website, or smaller by one percent, as is the case for UFeats, which I could understand as random fluctuation. But the difference is much greater when it comes to dependency scores. Here are the results posted on your website:

```
UAS 78.54%
LAS 73.11%
CLAS 70.66%
MLAS 60.81%
BLEX 65.53%
```

So, while UAS score is the same in both, my LAS score is more than 2% worse, and the difference reaches 5% when it comes to CLAS and MLAS. 

I have also tried training the tagger myself and achieved better results than the downloaded model produced. The training process was not complete due to time limitations using a Google Collab GPU, but after 12 hours of training with default parameters I obtained the following scores:

```
Metric     | Precision |    Recall |  F1 Score | AligndAcc
-----------+-----------+-----------+-----------+-----------
UPOS       |     93.29 |     93.34 |     93.32 |     93.44
XPOS       |     85.90 |     85.95 |     85.92 |     86.04
UFeats     |     87.30 |     87.35 |     87.33 |     87.44
AllTags    |     84.75 |     84.80 |     84.77 |     84.88
```
With a higher UFeats score I then also got better parsing results, but they still did not reach the ones on your website. 

Furthermore, I have the same score discrepancy issue with the North Sami model:

```
Metric     | Precision |    Recall |  F1 Score | AligndAcc
-----------+-----------+-----------+-----------+-----------
Tokens     |     99.78 |     99.58 |     99.68 |
Sentences  |     99.31 |     99.31 |     99.31 |
Words      |     99.78 |     99.58 |     99.68 |
UPOS       |     86.37 |     86.19 |     86.28 |     86.56
XPOS       |     92.94 |     92.76 |     92.85 |     93.15
UFeats     |     87.81 |     87.63 |     87.72 |     88.00
AllTags    |     79.19 |     79.03 |     79.11 |     79.36
Lemmas     |     88.88 |     88.70 |     88.79 |     89.08
UAS        |     73.13 |     72.99 |     73.06 |     73.29
LAS        |     67.30 |     67.16 |     67.23 |     67.45
CLAS       |     65.57 |     65.40 |     65.48 |     65.56
MLAS       |     54.64 |     54.49 |     54.57 |     54.63
BLEX       |     58.12 |     57.96 |     58.04 |     58.11
```
The model's UPOS score is 5 points smaller than the 91.11% on the website, while XPOS and UFeats results are identical. The UAS and LAS scores difference is only around 1%, but the MLAS is nearly 3% worse. 

So my question is, is there any chance that at least some models available for download through the stanza.download() method are not the most recent ones? If so, would it be possible to obtain the most recent models?  Otherwise, what could be the reason for these discrepancies?",
707,2020-12-10T13:58:32Z,https://github.com/stanfordnlp/stanza/issues/561,,"**Describe the bug**
If we run Stanza in parallel on 8-14 different texts, the Stanza CRFLoss function returns an Error, that size of Tensors in the `crf_binary_score` function do not match. See the exception below.
```
  File ""/usr/local/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 166, in __call__
    doc = self.process(doc)
  File ""/usr/local/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 160, in process
    doc = self.processors[processor_name].process(doc)
  File ""/usr/local/lib/python3.8/site-packages/stanza/pipeline/ner_processor.py"", line 34, in process
    preds += self.trainer.predict(b)
  File ""/usr/local/lib/python3.8/site-packages/stanza/models/ner/trainer.py"", line 78, in predict
    _, logits, trans = self.model(word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)
  File ""/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/stanza/models/ner/model.py"", line 129, in forward
    loss, trans = self.crit(logits, word_mask, tags)
  File ""/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/stanza/models/common/crf.py"", line 34, in forward
    unary_scores = self.crf_unary_score(inputs, masks, tag_indices)
  File ""/usr/local/lib/python3.8/site-packages/stanza/models/common/crf.py"", line 52, in crf_unary_score
    flat_tag_indices = tag_indices + \
RuntimeError: The size of tensor a (18) must match the size of tensor b (16) at non-singleton dimension 1
```

**To Reproduce**
Steps to reproduce the behavior:
1. Run the script at the end of this description and make breakpoints at model.py, line 129.
2. The script will run the stanza in 3 threads initialized with 1 stanza instance.
3. When all 3 threads are at self.crit() function, run first thread to the line in crf.py, line 51
4. Run second thread to line 54. It will fail with the error:
RuntimeError: The size of tensor a (31) must match the size of tensor b (22) at non-singleton dimension 1
 

**Expected behavior**
If one instance of Stanza is initialized, but multiple threads are accessing it, the model should be thread aware and do not share variables between the thread instances

**Environment (please complete the following information):**
 - OS:  Windows (but same happens on CentOS)
 - Python version: Python 3.6.8 from Anaconda
 - Stanza version:1.1.1

**Additional context**
The problem is this url stanza/models/common/crf.py at line 34.



Script to reproduce error:
```
from threading import Thread
import stanza

stanza = stanza.Pipeline(
    lang='en',
    processors='tokenize, ner, pos',
    dir='D:\cogniware\CogniwareInsights\DEV\\fake-news-detection\datasets\models\stanza\en'
)

def run(data):
    res = stanza(data)
    print('sentences: ', len(res.sentences), '. Thread finished.')

data = 'Any sentence can be placed here, because this is more about the parallel processing than about the content itself.' \
       'However in order to make the Stanza compute the text for a while, I tried to write some text, that will be able to simulate what is wrong. However in order to make  I tried to write some text, that will be late what is wrong.' \
        'World placed here, because this is more about the parallel bulk head motor.'
data2 = 'T @samaasport: #Juventus striker #CristianoRonaldo reflects on the side\'s performance against #Barcelona in their group stage fixture in U…'
data3 = 'In some cases, you might have already tokenized your text, and just want to use Stanza for downstream processing. In these cases, you can feed in pretokenized (and sentence split) text to the pipeline, as newline (\n) separated sentences, where each sentence is space separated tokens. Just set tokenize_pretokenized as True to bypass the neural tokenizer.'
thread1 = Thread(target=run, args=[data])
thread2 = Thread(target=run, args=[data2])
thread3 = Thread(target=run, args=[data3])
thread1.start()
thread2.start()
thread3.start()
thread1.join()
thread2.join()
thread3.join()
print('finished')
```
",
708,2020-12-09T22:40:31Z,https://github.com/stanfordnlp/stanza/issues/560,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
709,2020-12-09T16:15:28Z,https://github.com/stanfordnlp/stanza/issues/559,,"Hello,

I would be very happy if you could advise me with the following issues:

I tried to download the German NER Model GermEval14 as stated here: https://stanfordnlp.github.io/stanza/installation_usage.html 
`import stanza
stanza.download('de', processors={'ner': 'GermEval14`})
It says a warning germeval14 could not be found in the official model list. Am I doing it wrong or is this model not available anymore? I noticed there is a download of an tsv-file on the website https://sites.google.com/site/germeval2014ner/data - But as I am no expert I do not know if this is of use for stanza.

When I specify the processors in a dictionary as suggested on the same manual site as above, even processors I did not name are evoked (in my case: depparse). Can I prevent this?

Thank you, with best regards,
Nora",
710,2020-12-09T14:11:47Z,https://github.com/stanfordnlp/stanza/issues/558,,"I'm curious if there is a recommended pattern for handling GPU OOM conditions.  I am running tokenize-mwt-ner over Arabic, and if the segment is long enough, there will be a CUDA out-of-memory exception from PyTorch.  The GPU I'm testing on is a Titan X (12GB).

I was thinking of holding a `use_gpu=False` pipeline to run over long segments in case the exception happens, or just stashing those segments aside for later processing, but I'm not sure what I need to do to get the GPU pipeline to recover, if anything.",
711,2020-12-08T03:31:52Z,https://github.com/stanfordnlp/stanza/pull/556,,"Fixes this issue:

https://github.com/stanfordnlp/stanza/issues/523",
712,2020-12-07T17:58:53Z,https://github.com/stanfordnlp/stanza/pull/555,,Python versions of the scripts to copy POS data & train the model,
713,2020-12-07T05:20:15Z,https://github.com/stanfordnlp/stanza/pull/554,,"## Desciption
This PR adds `sent` backpointers from the `Token`, `Word`, and `Span` data objects to the sentence they are from.

## Fixes Issues
N/A

## Unit test coverage
Added new tests in `test_data_objects.py`.

## Known breaking changes/behaviors
N/A
",
714,2020-12-06T12:56:56Z,https://github.com/stanfordnlp/stanza/issues/552,,"OK, I am not certain this really is a bug, but it is very weird. 

I am trying to change an existing NLP pipeline from using SpaCy to using Stanza instead.
Since I want to apply this pipeline on a large number of (relatively short) texts, I am already using multiprocessing, where I run k processes in a pool. Each process reads the same input file but only processes every k+j th line if there are k processes and this is process number j of k.

If I do this with Spacy, I get a speed-up when I increase the number of processes. My server has 16 CPUs, so I get a speedup up to about 14 or so processes (also depends on the relative overhead of loading k copies of the pipeline).

With Stanza, there is a small speedup with 2 processes and beyond that, overall processing time is getting slower than just having a single process. How is this even possible? ",
715,2020-12-05T22:40:25Z,https://github.com/stanfordnlp/stanza/pull/551,,"Fixes up some bugs in the Arabic and Telugu segmenter.  Can also add more, but wanted to show this as an example.

The tokenizer data loader is pretty complicated, hence the desire to not put it there as we did with the pos and depparse.",
716,2020-12-05T17:20:21Z,https://github.com/stanfordnlp/stanza/issues/550,,"There is currently a recommendation to concatenate documents to improve speed but there is almost not information about how to determine the optimal size of the input depending on the config settings for the processor, e.g. max sequence length and batchsize. 

Ideally the way of how to combine individual documents to optimally create a batch given the config parameters should be determined by the pipeline. 

This could be done by adding a streaming mode, where the method for processing consumes a stream of texts and yields a stream of stanza documents. So, instead of actively passing a document and getting back a document, one would pass something that yields the documents to process and would consume the results yielded, e.g. 

```
def docsource(infile):
    with open(infile,""rt"") as infp:
        for line in infp:
            doc = json.loads(line)[""text""]
            yield doc

for doc in stanzapipeline.pipe(docsource, otherargs=couldbeset):
    # do something with the stanza result doc
```

the `pipe` method could internally accumulate or split input texts as need to run them through the network so that the batch is ideally used, then split or merge the result to create individual documents and yield those. 

This way the ad hoc-method of manually doing this before invoking the stanza pipeline would be replaced by something that should know much better how to do this properly and would avoid the problems of using some weird pattern to separate documents which may actually occur in the document. 

BTW, a more detailed explanation of the batch size and other parameters for the various processors and how they influence performance, relate to document size to process and how they should be adapted to each other among the different processors  would be great to have in the documentation! 
",
717,2020-12-05T03:32:47Z,https://github.com/stanfordnlp/stanza/issues/549,,"I want to get the result of Part-of-Speech and Dependency parse using my own tokenization in Chinese.
How to get it like below?

![image](https://user-images.githubusercontent.com/24559732/101232573-5cfa7900-36ed-11eb-9488-5ce2517e46a3.png)
",
718,2020-12-04T16:20:32Z,https://github.com/stanfordnlp/stanza/pull/548,,"## Description

Extend `SpacyTokenizer` so that it supports both spaCy v2 and spaCy v3 when adding the sentencizer to the pipeline.

## Unit test coverage

In the existing test `test_spacy`.

## Known breaking changes/behaviors

None.",
719,2020-12-04T11:06:45Z,https://github.com/stanfordnlp/stanza/issues/547,,"I'm training various models (pos, lemmatization, dependency parsing) separately (not as a pipeline) and wonder what the model files which end in ""pretrain.pt"" actually are. Are they just Stanza-style representations of word embeddings that I pass as input? Are they always the same regardless of the task (i.e. it is the same file for pos and parsing)?
Thanks!",
720,2020-12-04T07:52:41Z,https://github.com/stanfordnlp/stanza/issues/546,,"I'd like to have a try, can dependency of a sentence improve the performance of the word vector. However, when I use nltk as the tokenizer for my model, I found that the index is totally different with which the Stanza give, so is there any way to change this?
",
721,2020-12-02T19:25:17Z,https://github.com/stanfordnlp/stanza/issues/545,,"
**Describe the bug**
I am trying to sentence-segment Arabic text from CommonCrawl News.  I get tokens just fine, but no sentences.

**To Reproduce**
Steps to reproduce the behavior:
1. Here is the article I am testing on, attached.
[article.txt](https://github.com/stanfordnlp/stanza/files/5631562/article.txt)
2. I am using the following stupid simple script:
```
#!/usr/bin/env python3

import stanza
import json
import argparse

ap = argparse.ArgumentParser(description='Split arabic sentenes')
ap.add_argument('doc')
args = ap.parse_args()

doc = open(args.doc, 'r').read()
doc = doc.replace('\\n', '\n')
print(doc)

nlp = stanza.Pipeline(lang='ar', processors='tokenize')
doc = nlp(doc)
for i, sentence in enumerate(doc.sentences):
    print(i, ':', sentence)
```
The output is a sequence of tokens in a single sentence.

**Expected behavior**
Sentences.  I was hoping for sentences.

**Environment (please complete the following information):**
 - OS: macOS
 - Python version: Python 3.8.2, MacPorts
 - Stanza version: 1.1.1

**Additional context**
My source document has newlines wrong (literal backslash-n instead of newlines), but the script fixes those.
",
722,2020-12-02T04:50:38Z,https://github.com/stanfordnlp/stanza/pull/544,,Add a bunch of new language codes,
723,2020-11-30T15:31:10Z,https://github.com/stanfordnlp/stanza/pull/543,,Python versions of the scripts to prep & run the lemmatizer,
724,2020-11-28T18:20:58Z,https://github.com/stanfordnlp/stanza/pull/541,,Replace the shell scripts for mwt with python scripts that prep & run mwt,
725,2020-11-28T18:20:25Z,https://github.com/stanfordnlp/stanza/pull/540,,Replace the shell scripts for mwt with python scripts that prep & run mwt,
726,2020-11-25T14:40:37Z,https://github.com/stanfordnlp/stanza/issues/539,,"**Describe the bug**
The French GSD model splits URLs into multiple tokens, instead of preserving the URL as a single token.

**To Reproduce**
```
nlp_fr = stanza.Pipeline('fr', package='gsd', processors='tokenize')
url = ""https://stanfordnlp.github.io/stanza/""
doc = nlp_fr(url)

for s in doc.sentences:
    for t in s.tokens:
        print(t.text)
```

The observed output is:
```
https:/
/
stanfordnlp.github.io/stanza
/
```

**Expected behavior**
I expected the URL to be preserved as a single token, similar to what happens with other Stanza language models, e.g. the Spanish GSD model (see below)

```
nlp_es = stanza.Pipeline('es', package='gsd', processors='tokenize')
url = ""https://stanfordnlp.github.io/stanza/""
doc = nlp_es(url)

for s in doc.sentences:
    for t in s.tokens:
        print(t.text)
```

The output is: `https://stanfordnlp.github.io/stanza/`

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.7,  also observed with Python 3.8 from Anaconda
 - Stanza version: 1.1.1
",
727,2020-11-24T18:36:07Z,https://github.com/stanfordnlp/stanza/pull/538,,So far just some minor changes,
728,2020-11-24T09:50:57Z,https://github.com/stanfordnlp/stanza/issues/537,,"Hello ， when I use stanza to split Arabic, I have some problems and need your help
When I use the stanza to segment the arabic sentences , and try to get the lemmo and word , I find **the lemmo always has the phonetic symbol** , even it's a simple word .  Actually, I want to get the lemmo word but without the phonetic symbal  and in daily life, Arabs also don't specify symbols when searching.
**Could I get the lemmo infomation without phonetic symbol ?** 
Thank you :-)

Examples:
1)if the sentence is ""كوب"" , output:
in one word for one sentence, the text: كوب, lemma: كَوب

2)if the sentence is ""اليوم هو""  output:

in one word for one sentence, the text: اليوم, lemma: يَوم
in one word for one sentence, the text: هو, lemma: هُوَ 

We can find every lemma has the phonetic symbol .


```
import stanza
nlp = stanza.Pipeline('ar', processors='tokenize, mwt, pos, lemma')
doc = nlp(origin_doc)
for one_sentence in doc.sentences:
    for word in one_sentence.words:
          print (""in one word for one sentence, the text: %s, lemma: %s"" % (word.text, word.lemma))
```

Best Regards",
729,2020-11-23T07:34:08Z,https://github.com/stanfordnlp/stanza/pull/536,,Rewrite run_tokenize.sh in python,
730,2020-11-22T22:57:10Z,https://github.com/stanfordnlp/stanza/pull/535,,This will fix issue https://github.com/stanfordnlp/stanza/issues/531,
731,2020-11-22T22:54:18Z,https://github.com/stanfordnlp/stanza/pull/534,,This will fix issue https://github.com/stanfordnlp/stanza/issues/531,
732,2020-11-22T16:53:15Z,https://github.com/stanfordnlp/stanza/issues/533,,"Hello,

is there an option to access the lemma from ents as ents is specified in https://stanfordnlp.github.io/stanza/ner.html#accessing-named-entities-for-sentence-and-document ?

At the moment I only see two other opportunities:
- to iterate over words, which contain the lemma attribute, and make a comparison to the ents
- to use words instead of ents and filter them by the ner attribute (which gives worse results)

I am kinda new here, so sorry if this is a dumb question.

Best Regards,
Nora",
733,2020-11-22T16:09:21Z,https://github.com/stanfordnlp/stanza/issues/532,,"Hi folks,

first of all thanks for this amazing package and the work you have done!

I am trying to use OpenIE for german, so I tried it with stanza as well as with the CoreNLPClient.

```python
        text=""Christian Klose wurde in Deutschland geboren. Er lebt in Nürnberg.""
        with CoreNLPClient(
          annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse', 'depparse', 'coref','natlog', 'openie'],
          timeout=30000,
          memory='4G',
          properties=""german"",
          output_format=""json"",
          endpoint='http://localhost:9001') as client:
          ann = client.annotate(text=text)
 ````
I get the following error:

```java
Caused by: class java.lang.IllegalArgumentException: No head rule defined for MPN using class edu.stanford.nlp.trees.SemanticHeadFinder in (MPN (PROPN Christian) (PROPN Klose))
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineNonTrivialHead(AbstractCollinsHeadFinder.java:245)
  edu.stanford.nlp.trees.SemanticHeadFinder.determineNonTrivialHead(SemanticHeadFinder.java:445)
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:193)
  edu.stanford.nlp.trees.Tree.headTerminal(Tree.java:1106)
  edu.stanford.nlp.trees.Tree.headTerminal(Tree.java:1121)
  edu.stanford.nlp.coref.data.DocumentPreprocessor.fillMentionInfo(DocumentPreprocessor.java:330)
  edu.stanford.nlp.coref.data.DocumentPreprocessor.initializeMentions(DocumentPreprocessor.java:169)
  edu.stanford.nlp.coref.data.DocumentPreprocessor.preprocess(DocumentPreprocessor.java:62)
  edu.stanford.nlp.coref.data.DocumentMaker.makeDocument(DocumentMaker.java:100)
  edu.stanford.nlp.coref.data.DocumentMaker.makeDocument(DocumentMaker.java:72)
  edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:59)
  ...8 more
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.
```

Could it be that openIE for German does not work?  If so, is there a way how I can integrate it myself? 
```python
gr_nlp = stanza.Pipeline('de', processors='tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,natlog,openie', verbose=False, use_gpu=False)
gr_doc = gr_nlp(""Christian Klose wurde in Deutschland geboren. Er lebt in Nürnberg."")
```
Is OpenIE already supported in Stanza? 
I could not see it in the overview diagram either.




",
734,2020-11-22T02:03:25Z,https://github.com/stanfordnlp/stanza/issues/531,,"**Describe the bug**
when use the Vietnamese's POS, there have this problem
**To Reproduce**
Steps to reproduce the behavior:
1. read the sentences s;
2. call nlp(s);
3.'ValueError: substring not found' come out then.

**Environment (please complete the following information):**
 - OS:  CentOS
 - Python version: Python 3.6.8
 - Stanza version: 1.1.1

**Additional context**",
735,2020-11-21T17:31:05Z,https://github.com/stanfordnlp/stanza/issues/530,,"**Describe the bug**
RuntimeError: CUDA error: no kernel image is available for execution on the device

**To Reproduce**
```
import stanza
nlp= stanza.Pipeline('en', use_gpu= True)
```

**Expected behavior**
Use GPU

**Environment (please complete the following information):**
 - OS: Ubuntu 20.10
 - Python version: Python 3.8.5 [GCC 7.3.0] :: Anaconda, Inc. on linux
 - Stanza version: 1.1.1

**Additional context**
PyTorch installed successfully with `conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch`.
```
```
>>> import torch
>>> torch.cuda.current_device()
>>> torch.cuda.device(0)
<torch.cuda.device object at 0x7fb9ee6a1cd0>
>>> torch.cuda.get_device_name(0)
'GeForce GT 710'
>>> torch.cuda.is_available()
True


```",
736,2020-11-20T18:22:26Z,https://github.com/stanfordnlp/stanza/issues/529,,"**Describe the bug**
Currently if a language isn't supported a generic error with the class `Exception` and message `No processor to load. Please check if your language or package is correctly set.` is raised.
If an user is catching exceptions it will need to do something like:
```
   except Exception as e:                                                                                                                                     
        if str(e) == 'No processor to load. Please check if your language or package is correctly set.':   
           <handle error>
```

An exception class like `UnsupportedLanguage` should be used for easier exceptions catch.",
737,2020-11-20T13:26:26Z,https://github.com/stanfordnlp/stanza/issues/528,,"A sentence like this: `Si subís al pico tendréis unas vistas espectaculares,llevarse una chaqueta,arriba hace frío por el viento,reservar un coche con un poco de potencia` where the sentences are separated by comma, but lack a space for it are not split by stanza as in other languages and bring to a big mistake in pos processing.
 
```JSON
{                                                                                                                                                                                                                           
  ""id"": [                                                                                                                                                                                                                       
    8,                                                                                                                                                                                                                          
    10                                                                                                                                                                                                                          
  ],                                                                                                                                                                                                                            
  ""text"": ""espectaculares,llevarse"",                                                                                                                                                                                            
  ""misc"": ""start_char=38|end_char=61""    
},                             
{                                                                                                                                                                                                                               
  ""id"": 8,          
  ""text"": ""espectutueslar"",                                                                                                                                                                                                     
  ""lemma"": ""espectutueslar"",                                                                                                                                                                                                    
  ""upos"": ""ADJ"",                                                                                                                                                                                                                
  ""xpos"": ""ADJ"",                                                                                                                                                                                                                
  ""feats"": ""Number=Plur""                                                                                                                                                                                                        
},                                                                                                                                                                                                                              
{                                                                                                                                                                                                                               
  ""id"": 9,                                                                                                                                                                                                                      
  ""text"": ""ler"",                                                                                                                                                                                                                
  ""lemma"": ""ler"",                                                                                                                                                                                                               
  ""upos"": ""VERB"",                                                                                                                                                                                                               
  ""xpos"": ""VERB"",                                                                                                                                                                                                               
  ""feats"": ""VerbForm=Inf""                                                                                                                                                                                                       
},                                                                                                                                                                                                                              
{                                                                                                                                                                                                                    
  ""id"": 10,                                                                                                                                                                                                                     
  ""text"": ""se"",                                                                                                                                                                                                                 
  ""lemma"": ""él"",                                                                                                                                                                                                                
  ""upos"": ""PRON"",    
  ""xpos"": ""PRON"",                                                                                                                                                                                                               
  ""feats"": ""Case=Acc,Dat|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes""                                                                                                                                                         
}
```

**To Reproduce**
Steps to reproduce the behavior:
run the following:
```Python3
 import stanza
 nlp = stanza.Pipeline('es', processors='tokenize, mwt, lemma, pos')
 nlp('Si subís al pico tendréis unas vistas espectaculares,llevarse una chaqueta,arriba hace frío por el viento,reservar un coche con un poco de potencia')
```

**Expected behavior**
It should have split the sentences into `espectaculares` being an ADJ, `,` being a PUNCT and `llevarse` being MWT of llevar (VERB) and se (PRON).

As you can expect, this result comes from the default model. I have already tried the `gsd` alternative but it's equally wrong.

```JSON
{
  ""id"": [
    9,  
    10  
  ],  
  ""text"": ""espectaculares,llevarse"",
  ""misc"": ""start_char=38|end_char=61""
},
{
  ""id"": 9,
  ""text"": ""espectactacucevar"",
  ""lemma"": ""espectactacucevar"",
  ""upos"": ""VERB"",
  ""feats"": ""VerbForm=Inf""
},
{
  ""id"": 10, 
  ""text"": ""se"",
  ""lemma"": ""él"",
  ""upos"": ""PRON"",
  ""feats"": ""Case=Acc,Dat|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes""
}
```",
738,2020-11-20T01:21:54Z,https://github.com/stanfordnlp/stanza/issues/526,,"Hi guys!

I'm working on a project using stanza NER for Spanish , I already create some services with both of the pretrained models you have [here](https://stanfordnlp.github.io/stanza/available_models.html) and both services are working really well.

Although i trained my own model using your documentation to train a model with Stanza, but i couldn't use it because i get the following error `Resources file not found at: {resources_filepath}. Try to download the model again`.
I have not a resources file for my trained model, but i have it for your models, should i create this file for my own model?

**Relevant information**

- Your models are in the dir: `stanza_resources/es/ner` with the resources file in `stanza` and my own model is on `lib/models/standford_trained`
- I'm trying to initializate my own model with `stanza.Pipeline(lang='es', processors='tokenize,ner', verbose=False, ner_model_path='lib/models/standford_trained/es_ancora_nertagger.pt', ner_forward_charlm_path="""", ner_backward_charlm_path="""")`

Can you help me with that?

Thanks!
Guille.


",
739,2020-11-18T17:41:00Z,https://github.com/stanfordnlp/stanza/pull/525,,Grab a few non-controversial tokenizer changes,
740,2020-11-18T09:42:10Z,https://github.com/stanfordnlp/stanza/issues/524,,"**Describe the bug**
(1) Telugu tokenizer failed to remove punctuations
(2) The lemma is often None for Telugu.

**To Reproduce**
Steps to reproduce the behavior:
```
te_pos_tagger = stanza.Pipeline('te', processors='tokenize,pos,lemma')
doc = te_pos_tagger('సింక్ ముందు ఉన్నారు.')
for sentence in doc.sentences:
    for word in sentence.words:
        print(""%s\t%s\t%s"" % (word.text, word.lemma, word.pos))
```

and this gives:
```
సింక్	None	PRON
ముందు	ముందు	ADV
ఉన్నారు.	None	VERB
```

Notice the lemma on line 1 and line 3 is `None`, and the Telugu token on line 3 still has a full stop mark.

**Expected behavior**
(1) for Telugu tokenizer: remove punctuations
(2) for Telugu lemmatizer: generate the correct form

**Environment (please complete the following information):**
 - OS: Ubuntu 18.04
 - Python version: Python 3.6.12 from Anaconda
 - Stanza version: 1.1.1
",
741,2020-11-18T08:54:00Z,https://github.com/stanfordnlp/stanza/issues/523,,"Hi, after careful comparison with jieba tokenizer with your Chinese tokenizer, I am using jieba tokenizer for my downstream NER task, as it shows better performance on Chinese names and locations identification. 

I do NER in batchces in order to speed up, so I add ""\n\n"" between all the segmented sentences as mentioned in the [tutorial](https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-without-sentence-segmentation) and enable ""tokenize_no_ssplit=True"" in the pipeline.

When using your tokenizer in the pipeline, ""tokenize_no_ssplit=True"" works perfectly well.
```
ZH_DOC = ""北京是中国的首都。\n\n北京有2100万人口。是一个直辖市。""
nlp_zh = stanza.Pipeline(lang='zh-hans', dir=r'C:\Users\WT.YX\stanza_resources', processors='tokenize,ner',
                                     use_gpu=False, tokenize_no_ssplit=True)
doc = nlp_zh(ZH_DOC)
for item in doc.sentences:
    print(item._text)

北京是中国的首都。
北京有2100万人口。是一个直辖市。
```
However, when introducing jieba tokenizer into the pipeline, it no longer works.
```
ZH_DOC = ""北京是中国的首都。\n\n北京有2100万人口。是一个直辖市。""
nlp_zh = stanza.Pipeline(lang='zh-hans',
                                  processors={'tokenize': 'jieba', 'ner': 'ontonotes'}, package=None,
                                use_gpu=False, tokenize_no_ssplit=True)
doc = nlp_zh(ZH_DOC)
for item in doc.sentences:
    print(item._text)

北京是中国的首都。
北京有2100万人口。
是一个直辖市。

```
```
ZH_DOC = ZH_DOC = ""北京是中国的首都 \n\n北京有2100万人口。是一 \n\n 个直辖市，""
nlp_zh = stanza.Pipeline(lang='zh-hans',
                                  processors={'tokenize': 'jieba', 'ner': 'ontonotes'}, package=None,
                                use_gpu=False, tokenize_no_ssplit=True)
doc = nlp_zh(ZH_DOC)
for item in doc.sentences:
    print(item._text)

北京是中国的首都 

北京有2100万人口。
是一 
 个直辖市，

len(doc.sentences) gives 2

doc.sentences
[[
  {
    ""id"": 1,
    ""text"": ""北京"",
    ""misc"": ""start_char=0|end_char=2"",
    ""ner"": ""S-GPE""
  },
  {
    ""id"": 2,
    ""text"": ""是"",
    ""misc"": ""start_char=2|end_char=3"",
    ""ner"": ""O""
  },
  {
    ""id"": 3,
    ""text"": ""中国"",
    ""misc"": ""start_char=3|end_char=5"",
    ""ner"": ""S-GPE""
  },
  {
    ""id"": 4,
    ""text"": ""的"",
    ""misc"": ""start_char=5|end_char=6"",
    ""ner"": ""O""
  },
  {
    ""id"": 5,
    ""text"": ""首都"",
    ""misc"": ""start_char=6|end_char=8"",
    ""ner"": ""O""
  },
  {
    ""id"": 6,
    ""text"": ""北京"",
    ""misc"": ""start_char=11|end_char=13"",
    ""ner"": ""S-GPE""
  },
  {
    ""id"": 7,
    ""text"": ""有"",
    ""misc"": ""start_char=13|end_char=14"",
    ""ner"": ""O""
  },
  {
    ""id"": 8,
    ""text"": ""2100"",
    ""misc"": ""start_char=14|end_char=18"",
    ""ner"": ""B-CARDINAL""
  },
  {
    ""id"": 9,
    ""text"": ""万"",
    ""misc"": ""start_char=18|end_char=19"",
    ""ner"": ""E-CARDINAL""
  },
  {
    ""id"": 10,
    ""text"": ""人口"",
    ""misc"": ""start_char=19|end_char=21"",
    ""ner"": ""O""
  },
  {
    ""id"": 11,
    ""text"": ""。"",
    ""misc"": ""start_char=21|end_char=22"",
    ""ner"": ""O""
  }
], [
  {
    ""id"": 1,
    ""text"": ""是"",
    ""misc"": ""start_char=22|end_char=23"",
    ""ner"": ""O""
  },
  {
    ""id"": 2,
    ""text"": ""一"",
    ""misc"": ""start_char=23|end_char=24"",
    ""ner"": ""O""
  },
  {
    ""id"": 3,
    ""text"": ""个"",
    ""misc"": ""start_char=28|end_char=29"",
    ""ner"": ""O""
  },
  {
    ""id"": 4,
    ""text"": ""直辖市"",
    ""misc"": ""start_char=29|end_char=32"",
    ""ner"": ""O""
  },
  {
    ""id"": 5,
    ""text"": ""，"",
    ""misc"": ""start_char=32|end_char=33"",
    ""ner"": ""O""
  }
]]


```



Is this a bug or am I implementing the pipeline correctly? Thanks for your insights!

Besides, regarding NER performance, for a neural network, Chinese word segmentation might not be needed. Both research and Bert NER model performance support this. Moreover currently there is no good Chinese tokenizer, and all the segment errors affect downstream NER performance. Are you considering bypassing Chinese word segmentation and focusing on the variety of language model embeddings (n-gram, n-char, Bert-embeddings, etc.) with user-defined vocabs &dictionaries incorporated?",
742,2020-11-18T00:28:27Z,https://github.com/stanfordnlp/stanza/pull/522,,"## Desciption
This PR refactors the tokenizer data pipeline especially to speed up evaluation and improve GPU utilization (when available).

## Fixes Issues
N/A

## Unit test coverage
Should be covered by current tokenizer tests.

## Known breaking changes/behaviors
N/A, unless someone's using the tokenizer data loader (should be very rare), where the `sentences` property now has a different structure.
",
743,2020-11-17T12:33:28Z,https://github.com/stanfordnlp/stanza/issues/521,,"**code:**
import stanza
en_nlp = stanza.Pipeline('de', dir= r'E:\working\test1\venv\stanza_resources', processors= 'tokenize,lemma,pos,depparse,ner', verbose= False)
sentence = 'Seine Botschaft zeige die feste Entschlossenheit der chinesischen Regierung, die multilaterale Zusammenarbeit zu fördern und sich aktiv an globalen Aktionen im Zusammenhang mit Schuldenfragen zu beteiligen, kommentierten Experten anschließend.'
doc = en_nlp(sentence)
**error:**
Traceback (most recent call last):
  File ""E:/working/repack/test.py"", line 10, in <module>
    doc = en_nlp(sentence)
  File ""E:\working\test1\venv\lib\site-packages\stanza\pipeline\core.py"", line 166, in __call__
    doc = self.process(doc)
  File ""E:\working\test1\venv\lib\site-packages\stanza\pipeline\core.py"", line 160, in process
    doc = self.processors[processor_name].process(doc)
  File ""E:\working\test1\venv\lib\site-packages\stanza\pipeline\depparse_processor.py"", line 48, in process
    sentence.build_dependencies()
  File ""E:\working\test1\venv\lib\site-packages\stanza\models\common\doc.py"", line 481, in build_dependencies
    assert(word.head == head.id)
AssertionError
",
744,2020-11-17T07:16:11Z,https://github.com/stanfordnlp/stanza/issues/520,,"I installed stanza from source. After that for testing I opened python interpreter but at the time of importing stanza I am getting ModuleNotFoundError. I am giving my code snippet below-
Python 3.7.9 (default, Nov 11 2020, 15:19:12) 
[GCC 7.5.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import stanza
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/giuser/stanza/stanza/__init__.py"", line 1, in <module>
    from stanza.pipeline.core import Pipeline
  File ""/home/giuser/stanza/stanza/pipeline/core.py"", line 20, in <module>
    from stanza.pipeline.pos_processor import POSProcessor
  File ""/home/giuser/stanza/stanza/pipeline/pos_processor.py"", line 6, in <module>
    from stanza.models.common.pretrain import Pretrain
  File ""/home/giuser/stanza/stanza/models/common/pretrain.py"", line 7, in <module>
    import lzma
  File ""/usr/local/lib/python3.7/lzma.py"", line 27, in <module>
    from _lzma import *
ModuleNotFoundError: No module named '_lzma'",
745,2020-11-16T16:49:09Z,https://github.com/stanfordnlp/stanza/pull/519,,"Instead of the prep_tokenize.sh script, have a script which preps all of train/dev/test for either a single treebank or ALL the UD tokenize treebanks.

Also sneaks in early termination when training tokenizers",
746,2020-11-16T14:56:49Z,https://github.com/stanfordnlp/stanza/issues/518,,"Hi, I have followed instructions to install stanza and by default it downloads Processor/Package for NER as ontonotes. How can i get CoNLL instead ? Please help!!",
747,2020-11-14T16:57:44Z,https://github.com/stanfordnlp/stanza/issues/517,,"Hi 

Is there a way in which I can run Stanza as a service?
Basically, I need to accept an input from a web interface (backend is in PHP), and then process it using Stanza.
Let me know please.

Sarves",
748,2020-11-13T05:28:05Z,https://github.com/stanfordnlp/stanza/pull/516,,does what it says on the can,
749,2020-11-12T16:27:31Z,https://github.com/stanfordnlp/stanza/issues/515,,"MWT isn't available for NL, for instance.
This makes it hard to handle processors in a generic manner without knowing the availability of them on a per language level

```
>>> import stanza; nlpru = stanza.download('nl', processors='tokenize,mwt,lemma,pos')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 27.1MB/s]             
2020-11-12 13:25:33 INFO: Downloading these customized packages for language: nl (Dutch)...
=======================
| Processor | Package |
-----------------------
| tokenize  | alpino  |
| mwt       | default |
| pos       | alpino  |
| lemma     | alpino  |
| pretrain  | alpino  |
=======================

2020-11-12 13:25:33 INFO: File exists: /home/braulio/stanza_resources/nl/tokenize/alpino.pt.
Traceback (most recent call last):
  File ""/home/braulio/.local/lib/python3.8/site-packages/stanza/resources/common.py"", line 386, in download
    md5=resources[lang][key][value]['md5']
KeyError: 'mwt'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/braulio/.local/lib/python3.8/site-packages/stanza/resources/common.py"", line 389, in download
    raise Exception(
Exception: Cannot find the following processor and model name combination: mwt, default. Please check if you have provided the correct model name.
```

This issue is derived from https://github.com/stanfordnlp/stanza/issues/514",
750,2020-11-12T11:48:47Z,https://github.com/stanfordnlp/stanza/issues/514,,"Steps to reproduce:
1) Create an `it` pipeline with processors `tokenize,lemma,pos`
2) Use "" Nulla"" as input

Without the space in the beginning it works

```
uWSGI exceptions catcher for ""GET /?lang=it&input=%20Nulla"" (request plugin: ""python"", modifier1: 0)

Exception: builtins.AssertionError: Number of list elements must match with original indices.

Exception class: builtins.AssertionError

Exception message: Number of list elements must match with original indices.

Backtrace:
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
```

It isn't a concurrency error as reported initially at https://github.com/stanfordnlp/stanza/issues/510",
751,2020-11-12T11:47:28Z,https://github.com/stanfordnlp/stanza/issues/513,,"Steps to reproduce:
1) Create an `it` pipeline with processors `tokenize,lemma,pos`
2) Use ""farci? Nulla"" as input

```
uWSGI exceptions catcher for ""GET /?lang=it&input=farci?%20Nulla"" (request plugin: ""python"", modifier1: 0)

Exception: builtins.RuntimeError: The expanded size of the tensor (4) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [0, 4].  Tensor sizes: [0]

Exception class: builtins.RuntimeError

Exception message: The expanded size of the tensor (4) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [0, 4].  Tensor sizes: [0]

Backtrace:
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
filename: """" line: -1 function: """" 
```

It isn't a concurrency error as reported initially at https://github.com/stanfordnlp/stanza/issues/510",
752,2020-11-10T02:38:23Z,https://github.com/stanfordnlp/stanza/pull/512,,List available languages in the resources file,
753,2020-11-09T23:08:09Z,https://github.com/stanfordnlp/stanza/issues/511,,"How to programatically list all available languages in the stanza resources json file?
I need to download all languages supported (with `stanza.download()`) so that they later are ready for use.",
754,2020-11-09T21:41:04Z,https://github.com/stanfordnlp/stanza/issues/510,,"**Describe the bug**
Under UWSGI with 32 threads, when receiving many concurrent requests, the following error occur frequently:
```
Exception: builtins.RuntimeError: Expected hidden[0] size (2, 4, 100), got (2, 3, 100)
```

**To Reproduce**
1. Configure it under UWSGI with 32 threads
2. Run several requests in parallel
3. See error above

**Expected behavior**
No errors on multithreading

**Environment (please complete the following information):**
 - OS: Archlinux
 - Python version: Python 3.8.6 from Archlinux
 - Stanza version: 1.1.1
 - Environment variable OMP_NUM_THREADS set to 32 doesn't help
 - Processors: `tokenize,lemma,pos`
 - Languages processed concurrently: `en,es,it,nl,de,fr,ru` ",
755,2020-11-05T07:40:09Z,https://github.com/stanfordnlp/stanza/issues/508,,"**Describe the bug**

After downloading the Polish (`pl`) model, I try to import it and get the following warning:

```>>> nlp = stanza.Pipeline(lang='pl',
...                           processors='tokenize,lemma,mwt,pos,depparse')
2020-11-05 09:35:08 WARNING: Can not find mwt: default from official model list. Ignoring it.```

When I then proceed to parse the input, I get the assertion error:

```Traceback (most recent call last):955
  File ""parse_bible.py"", line 10, in <module>
    parses = libbible.get_parses(path, lang, verbose)
  File ""/home/dniko/bible-corpus/src/libbible.py"", line 78, in get_parses
    nlp(verse).to_dict())
  File ""/home/dniko/.local/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 166, in __call__
    doc = self.process(doc)
  File ""/home/dniko/.local/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 160, in process
    doc = self.processors[processor_name].process(doc)
  File ""/home/dniko/.local/lib/python3.7/site-packages/stanza/pipeline/depparse_processor.py"", line 48, in process
    sentence.build_dependencies()
  File ""/home/dniko/.local/lib/python3.7/site-packages/stanza/models/common/doc.py"", line 481, in build_dependencies
    assert(word.head == head.id)
AssertionError```

This error usually corresponds to incorrect handling of mwt.",
756,2020-11-04T06:47:41Z,https://github.com/stanfordnlp/stanza/issues/507,,"Everything was working well, but, suddenly, now cannot download any models. Getting the following error.

~/Stanza$ python3.8
Python 3.8.5 (default, Jul 28 2020, 12:59:40) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import stanza
>>> stanza.download('en')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json:   0%| | 0.00/24.1K [00:00<?, ?B/s]Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/stanza/utils/resources.py"", line 223, in download
    request_file(f'{DEFAULT_RESOURCES_URL}/resources_{__resources_version__}.json', os.path.join(dir, 'resources.json'))
  File ""/usr/local/lib/python3.8/dist-packages/stanza/utils/resources.py"", line 83, in request_file
    download_file(url, path)
  File ""/usr/local/lib/python3.8/dist-packages/stanza/utils/resources.py"", line 71, in download_file
    with tqdm(total=file_size, unit='B', unit_scale=True, disable=not verbose, desc=desc) as pbar:
AttributeError: __enter__

Also, now cannot do the parsing with existing models, may be because of the above issue. Getting the following error:

Traceback (most recent call last):
  File ""/home/sarves/Documents/PhDDoc-GDrive/My-POS-Tagger/thamizhiPOSt-ud.py"", line 11, in <module>
    nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict
  File ""/usr/local/lib/python3.8/dist-packages/stanza/pipeline/core.py"", line 82, in __init__
    resources = json.load(infile)
  File ""/usr/lib/python3.8/json/__init__.py"", line 293, in load
    return loads(fp.read(),
  File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
757,2020-11-03T17:34:26Z,https://github.com/stanfordnlp/stanza/issues/506,,"`I was amazed that the reception new my name` output:
```
[
[
{
""id"": 1,
""text"": ""I"",
""lemma"": ""I"",
""upos"": ""PRON"",
""xpos"": ""PRP"",
""feats"": ""Case=Nom|Number=Sing|Person=1|PronType=Prs"",
""misc"": ""start_char=0|end_char=1""
},
{
""id"": 2,
""text"": ""was"",
""lemma"": ""be"",
""upos"": ""AUX"",
""xpos"": ""VBD"",
""feats"": ""Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin"",
""misc"": ""start_char=2|end_char=5""
},
{
""id"": 3,
""text"": ""amazed"",
""lemma"": ""amazed"",
""upos"": ""ADJ"",
""xpos"": ""JJ"",
""feats"": ""Degree=Pos"",
""misc"": ""start_char=6|end_char=12""
},
{
""id"": 4,
""text"": ""that"",
""lemma"": ""that"",
""upos"": ""SCONJ"",
""xpos"": ""IN"",
""misc"": ""start_char=13|end_char=17""
},
{
""id"": 5,
""text"": ""the"",
""lemma"": ""the"",
""upos"": ""DET"",
""xpos"": ""DT"",
""feats"": ""Definite=Def|PronType=Art"",
""misc"": ""start_char=18|end_char=21""
},
{
""id"": 6,
""text"": ""reception"",
""lemma"": ""reception"",
""upos"": ""NOUN"",
""xpos"": ""NN"",
""feats"": ""Number=Sing"",
""misc"": ""start_char=22|end_char=31""
},
{
""id"": 7,
""text"": ""new"",
""lemma"": ""know"",
""upos"": ""VERB"",
""xpos"": ""VBD"",
""feats"": ""Mood=Ind|Tense=Past|VerbForm=Fin"",
""misc"": ""start_char=32|end_char=35""
},
{
""id"": 8,
""text"": ""my"",
""lemma"": ""my"",
""upos"": ""PRON"",
""xpos"": ""PRP$"",
""feats"": ""Number=Sing|Person=1|Poss=Yes|PronType=Prs"",
""misc"": ""start_char=36|end_char=38""
},
{
""id"": 9,
""text"": ""name"",
""lemma"": ""name"",
""upos"": ""NOUN"",
""xpos"": ""NN"",
""feats"": ""Number=Sing"",
""misc"": ""start_char=39|end_char=43""
}
]
]
```

`she amazed doctors by fighting back` output:
```
[
[
{
""id"": 1,
""text"": ""she"",
""lemma"": ""she"",
""upos"": ""PRON"",
""xpos"": ""PRP"",
""feats"": ""Case=Nom|Gender=Fem|Number=Sing|Person=3|PronType=Prs"",
""misc"": ""start_char=0|end_char=3""
},
{
""id"": 2,
""text"": ""amazed"",
""lemma"": ""amaze"",
""upos"": ""VERB"",
""xpos"": ""VBD"",
""feats"": ""Mood=Ind|Tense=Past|VerbForm=Fin"",
""misc"": ""start_char=4|end_char=10""
},
{
""id"": 3,
""text"": ""doctors"",
""lemma"": ""doctor"",
""upos"": ""NOUN"",
""xpos"": ""NNS"",
""feats"": ""Number=Plur"",
""misc"": ""start_char=11|end_char=18""
},
{
""id"": 4,
""text"": ""by"",
""lemma"": ""by"",
""upos"": ""SCONJ"",
""xpos"": ""IN"",
""misc"": ""start_char=19|end_char=21""
},
{
""id"": 5,
""text"": ""fighting"",
""lemma"": ""fight"",
""upos"": ""VERB"",
""xpos"": ""VBG"",
""feats"": ""VerbForm=Ger"",
""misc"": ""start_char=22|end_char=30""
},
{
""id"": 6,
""text"": ""back"",
""lemma"": ""back"",
""upos"": ""ADV"",
""xpos"": ""RB"",
""misc"": ""start_char=31|end_char=35""
}
]
]
```

Processors used: `tokenize,lemma,pos`

The lemma `amaze` is only being set when pos is verb, and not when is adjetive
",
758,2020-11-03T15:23:02Z,https://github.com/stanfordnlp/stanza/issues/505,,"Hi!

I would like to have my own NER. But I have few questions

- The first step is to have all the embeddings. Downloading the the entire WordVec representation

- Once I have this embeddings how can I add my new entities?  I can't find it.

Or the option to have my NER is to have my vectors, but how can I calculate them?

Thanks a lot!
",
759,2020-10-29T16:30:56Z,https://github.com/stanfordnlp/stanza/issues/504,,"**Describe the bug**
When creating a pipeline in `pl` and with `tokenize` as the only processor, the following sentence results in the first token having no words:
`Otrzymaliśmy różne propozycje, ale pańska jest wyjątkowa.`

The same sentence, with the first token's stripped of its accents, produces words.

**To Reproduce**
```python
import stanza

nlp = stanza.Pipeline('pl', processors='tokenize')
doc = nlp(""Otrzymaliśmy różne propozycje, ale pańska jest wyjątkowa."")
print(doc.sentences[0].tokens[0].words)  # prints empty list

doc = nlp(""Otrzymalismy różne propozycje, ale pańska jest wyjątkowa."")
print(doc.sentences[0].tokens[0].words)  # contains one word
```

**Environment:**
 - OS: Ubuntu
 - Python version: Python 3.8.5
 - Stanza version: 1.1.1 (resources_version 1.1.0)
",
760,2020-10-29T07:21:19Z,https://github.com/stanfordnlp/stanza/pull/503,,,
761,2020-10-29T03:57:10Z,https://github.com/stanfordnlp/stanza/issues/502,,"**Describe the bug**
In `stanza/resources/common.py` in lines 74 and 342 files are opened but never closed which leads to Python throwing a ResourceWarning. This was displayed to me when testing my code with Unittest.

**To Reproduce**
Steps to reproduce the behavior:
1. Create Unit-Test
2. Download a Model with `stanza.download()`
3. Receive several ResourceWarnings about unclosed files

**Expected behavior**
Files were correctly closed and no ResourceWarnings were emitted.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.8.6 from Homebrew in virtualenv
 - Stanza version: 1.1.1

**Additional context**
I guess this is a no-brainer but I wanted to follow your Contributing Guidelines.
Using context managers for opening the files closes the files correctly and prevents ResourceWarnings for me.",
762,2020-10-28T03:01:26Z,https://github.com/stanfordnlp/stanza/pull/501,,"Train the tokenizer to either 3 or 5 output layers based on whether or not the training data has MWT.  For one thing, this fixes a bug in the Dutch tokenizer:

https://github.com/stanfordnlp/stanza/issues/332
",
763,2020-10-26T22:12:13Z,https://github.com/stanfordnlp/stanza/issues/500,,"I presume that the Stanza POS-tagger uses some kind of estimation when POS-tagging a sentence. It does not really **predict** the part-of-speech of a word, it just estimates probabilities. Then, in a second step, it generates the output from these probabilities. For example, in a neural network, there could be one output neuron for each part-of-speech, and a softmax over all of them.

From these probabilities, the model could calculate a confidence level for the parts-of-speech in the output.

**Can Stanza output such a confidence level, along with the generated part-of-speech tags?**

Examples:

Basic, standard sentences like

> He goes there.

could achieve a confidence level of 99% or more, because all estimated probabilities are very high. The model ""is very sure"" of its output.

Slightly ambiguous sentences like

> You go there.

could result in lower confidence levels (""you"" can be singular or plural). But maybe this is a detail that does not reflect in the probabilities.

Difficult sentences like

> The old man the boat.

result in low confidence level. The model does not really know how to classify this sentence, so the probabilities estimated by the model are not high.

Same goes for non-sentences like

> You you winter above.

**Can Stanza output such a confidence level, along with the generated part-of-speech tags?**
",
764,2020-10-24T03:44:20Z,https://github.com/stanfordnlp/stanza/issues/499,,The converted data type Conll is a list. How to change this list to a dataframe?,
765,2020-10-23T18:46:41Z,https://github.com/stanfordnlp/stanza/issues/498,,"I want to make some dependancy parsing for English texts with stanza. It tells me that processor=depparse needs mwt. But mwt is not available for English. How to handle that? Thanks!

When I try to run 

```
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')
doc = nlp(""I like to use my cell phone in the car media player."")
```

it tells me: 
WARNING: Can not find mwt: default from official model list. Ignoring it.

and as well:
AttributeError: module 'torch' has no attribute 'bool'

But when i leave processors='depparse' and initialize the nlp pipeline like that 

`nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')`

it is working. 
",
766,2020-10-23T07:12:02Z,https://github.com/stanfordnlp/stanza/issues/496,,"Currently, I am using stanza client to [extract triple relations using ""OpenIE"" ](https://stanfordnlp.github.io/CoreNLP/openie.html)annotator. Considering the following example 
`""John works at Microsoft and he lives in Washington.""`
I would like to apply OpenIE with a flag `""openie.resolve_coref"" ` set to True so that instead of returning `('he', 'lives in', 'Washington')` it should return `('John', 'lives in', 'Washington')`

How can I do that using Stanza? is there any workaround. 
Also, I want to apply [coref](https://stanfordnlp.github.io/CoreNLP/coref.html) on relations extracted using [KBP](https://stanfordnlp.github.io/CoreNLP/kbp.html) as well as on [NER](https://stanfordnlp.github.io/CoreNLP/ner.html).",
767,2020-10-22T01:26:37Z,https://github.com/stanfordnlp/stanza/issues/495,,"Hi, I have trained my own Chinese NER model using a combination of People's Daily1998 and msra datasets (3 categories: PER, LOC, ORG), the pretrained [Chinese word2vec](https://github.com/Embedding/Chinese-Word-Vectors) and your charlm, and got F1 scores of  90.03 and 85.46 on msra test set and people's daily test set respectively.

In order to further improve NER performance on Chinese PERSON, I have also made a Chinese celebrity name datasets and would like to do continued training on the abovementioned own NER model. As a start, this NER model gives F1 scores of 89.29 and 89.01 on Chinese name test and dev sets, respectively.

After the continued training on my Chinese name train dataset (2800 names), the F1 score on dev set keeps dropping to nearly zero.
```
2020-10-21 11:35:51 INFO: 2020-10-21 11:35:51: step 20/3000, loss = 16.708048 (0.875 sec/batch), lr: 0.001000
2020-10-21 11:36:08 INFO: 2020-10-21 11:36:08: step 40/3000, loss = 14.488121 (0.738 sec/batch), lr: 0.001000
2020-10-21 11:36:16 INFO: Evaluating on dev set...
2020-10-21 11:36:16 INFO: Prec.	Rec.	F1
2020-10-21 11:36:16 INFO: 74.46	57.33	64.78
2020-10-21 11:36:16 INFO: step 50: train_loss = 16.133973, dev_score = 0.6478
2020-10-21 11:36:17 INFO: Model saved to saved_models/ner/combined_msra_people_merge_char300_batch400_lishimingren_newtrain_batch1000.pt
2020-10-21 11:36:17 INFO: New best model saved.
2020-10-21 11:36:17 INFO: 
2020-10-21 11:36:25 INFO: 2020-10-21 11:36:25: step 60/3000, loss = 12.643028 (0.608 sec/batch), lr: 0.001000
2020-10-21 11:36:42 INFO: 2020-10-21 11:36:42: step 80/3000, loss = 10.787024 (0.883 sec/batch), lr: 0.001000
2020-10-21 11:36:59 INFO: 2020-10-21 11:36:59: step 100/3000, loss = 9.663561 (0.881 sec/batch), lr: 0.001000
2020-10-21 11:36:59 INFO: Evaluating on dev set...
2020-10-21 11:36:59 INFO: Prec.	Rec.	F1
2020-10-21 11:36:59 INFO: 11.21	8.33	9.56
2020-10-21 11:36:59 INFO: step 100: train_loss = 11.301924, dev_score = 0.0956
2020-10-21 11:36:59 INFO: 
2020-10-21 11:37:16 INFO: 2020-10-21 11:37:16: step 120/3000, loss = 8.841722 (0.494 sec/batch), lr: 0.001000
2020-10-21 11:37:33 INFO: 2020-10-21 11:37:33: step 140/3000, loss = 8.238783 (0.807 sec/batch), lr: 0.001000
2020-10-21 11:37:41 INFO: Evaluating on dev set...
2020-10-21 11:37:41 INFO: Prec.	Rec.	F1
2020-10-21 11:37:41 INFO: 0.37	0.33	0.35
2020-10-21 11:37:41 INFO: step 150: train_loss = 8.727804, dev_score = 0.0035
2020-10-21 11:37:41 INFO: 
2020-10-21 11:37:50 INFO: 2020-10-21 11:37:50: step 160/3000, loss = 7.647988 (0.807 sec/batch), lr: 0.001000
2020-10-21 11:38:07 INFO: 2020-10-21 11:38:07: step 180/3000, loss = 7.316283 (0.504 sec/batch), lr: 0.001000
2020-10-21 11:38:24 INFO: 2020-10-21 11:38:24: step 200/3000, loss = 6.977455 (0.888 sec/batch), lr: 0.001000
2020-10-21 11:38:24 INFO: Evaluating on dev set...
2020-10-21 11:38:24 INFO: Prec.	Rec.	F1
2020-10-21 11:38:24 INFO: 0.36	0.33	0.34
2020-10-21 11:38:24 INFO: step 200: train_loss = 7.437676, dev_score = 0.0034
```
Lowering the learning rate below 1e-4 and adding gradient clipping only makes the F1 drop more slowly.

My Chinese name dataset bio is formatted like this below: 
```
米	B-PER
歇	I-PER
尔	I-PER
是	O
人	O
。	O
	
蒋	B-PER
介	I-PER
石	I-PER
是	O
人	O
。	O
	
亚	B-PER
历	I-PER
山	I-PER
大	I-PER
是	O
人	O
。	O
	
鲁	B-PER
迅	I-PER
是	O
人	O
。	O
	
张	B-PER
扬	I-PER
是	O
人	O
。	O
```
In order to make TagVocab index consistent with the pretrained model, I also added several short sentences with LOC and ORG entities, and revised the script in stanza/models/ner/vocab.py, as below:
```
class TagVocab(BaseVocab):
    """""" A vocab for the output tag sequence. """"""
    def build_vocab(self):
        # counter = Counter([w[self.idx] for sent in self.data for w in sent])
        # self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))
        # self._unit2id = {w:i for i, w in enumerate(self._id2unit)}
        self._id2unit = ['<PAD>', '<UNK>', '<EMPTY>', '<ROOT>', 'O', 'I-ORG', 'I-LOC', 'B-LOC', 'I-PER', 'B-ORG', 'B-PER']
        self._unit2id = {'<PAD>': 0, '<UNK>': 1, '<EMPTY>': 2, '<ROOT>': 3, 'O': 4, 'I-ORG': 5, 'I-LOC': 6, 'B-LOC': 7, 'I-PER': 8,
         'B-ORG': 9, 'B-PER': 10}
```
My continued training shell script and logging info are as below:
`python3 -m stanza-master.stanza.models.ner_tagger_resumed_training --train_file /root/wt/datasets/lishimingren/train_split_PER.json --eval_file /root/wt/datasets/lishimingren/dev_split_PER.json --mode train --lang zh-hans --shorthand zh_ --word_emb_dim 300 --char_emb_dim 100 --charlm_forward_file /root/wt/stanza-master/saved_models/charlm/zh_gigaword_forward_charlm.pt --charlm_backward_file /root/wt/stanza-master/saved_models/charlm/zh_gigaword_backward_charlm.pt --char_hidden_dim 1024 --max_steps 3000 --eval_interval 50 --lr=0.0001 --save_name combined_msra_people_merge_char300_batch400_lishimingren_newtrain_batch1000.pt --scheme bio --cuda CUDA --wordvec_file /root/wt/stanza-master/saved_models/fasttext/Simplified_Chinese/merge_sgns_bigram_char300.txt.xz --base_model /root/wt/saved_models/ner/combined_msra_people_merge_char300_batch400.pt --batch_size 1000 --min_lr 1e-6 --lr_decay 0.1`
```
2020-10-22 09:48:33 INFO: Running tagger in train mode
2020-10-22 09:48:33 INFO: Loading data with batch size 1000...
2020-10-22 09:48:33 INFO: Reading pretrained vectors from /root/wt/stanza-master/saved_models/fasttext/Simplified_Chinese/merge_sgns_bigram_char300.txt.xz...
2020-10-22 09:53:21 DEBUG: 3 batches created.
2020-10-22 09:53:21 DEBUG: 1 batches created.
2020-10-22 09:53:21 INFO: Training tagger...
2020-10-22 09:53:24 INFO: NERTagger(
  (word_emb): Embedding(100000, 300, padding_idx=0)
  (charmodel): CharacterModel(
    (char_emb): Embedding(4802, 100, padding_idx=0)
    (charlstm): PackedLSTM(
      (lstm): LSTM(100, 1024, batch_first=True, bidirectional=True)
    )
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (input_transform): Linear(in_features=2348, out_features=2348, bias=True)
  (taggerlstm): PackedLSTM(
    (lstm): LSTM(2348, 256, batch_first=True, bidirectional=True)
  )
  (tag_clf): Linear(in_features=512, out_features=11, bias=True)
  (crit): CRFLoss()
  (drop): Dropout(p=0.5, inplace=False)
  (worddrop): WordDropout(p=0)
  (lockeddrop): LockedDropout(p=0.0)
```


Are there any tricks I am missing here? Any advice on improving F1 score would be appreciated. Thanks!",
768,2020-10-22T00:11:26Z,https://github.com/stanfordnlp/stanza/issues/494,,"I have changed the NER_DATA_DIR in the config.sh file to point to data/ner, i have converted the ncbi datasets into json files and stored in the data/ner folder from the root stanza directory, but it is saying the file cannot be found. 

For training on the NCBI disease dataset what are all the preparations that need to be done?(the train,test,val file of NCBI were tsv files.)
",
769,2020-10-21T12:54:09Z,https://github.com/stanfordnlp/stanza/issues/493,,"I am trying to train pos on RuCor corpus (http://rucoref.maimbava.net/), 

`bash scripts/run_pos.sh UD_Russian-RUCOREF`

but I get an error about XPOS:

`File ""/home/user/stanza/stanza/models/pos/xpos_vocab_factory.py"", line 22, in xpos_vocab_factory`
`raise NotImplementedError('Language shorthand ""{}"" not found!'.format(shorthand))`
`NotImplementedError: Language shorthand ""ru_rucoref"" not found!`

Do I understand correctly that pos and depparse can be trained only on the list of corpora from xpos_vocab_factory.py?
",
770,2020-10-20T21:45:37Z,https://github.com/stanfordnlp/stanza/pull/491,,"This subsumes the tagger pull request, if you want to check it all at once.  The same code changes apply to both models for the most part.  The only difference is that for depparse I checked that the word at the end of a sentence doesn't have any other words depending on it before removing the word.",
771,2020-10-20T15:01:34Z,https://github.com/stanfordnlp/stanza/issues/490,,"I test multiple Korean examples using the 1.1.0 model. The performance seems really bad. 

**To Reproduce**
import stanza
nlp = stanza.Pipeline(lang='ko')
doc = nlp(""종지부를나눈다."")

**Expected behavior**
종지부[space]를[space]나눈다

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: 3.6
 - Stanza version: 1.1.0

",
772,2020-10-20T01:32:34Z,https://github.com/stanfordnlp/stanza/pull/489,,Add some non-punct as needed to a punct filled training set,
773,2020-10-16T15:21:58Z,https://github.com/stanfordnlp/stanza/issues/488,,"```
>>> import stanza
>>> nlp=stanza.Pipeline(""fa"")
>>> doc=nlp(""دروغگو کم حافظه است"")
>>> print(doc)
[
  [
    {
      ""id"": 1,
      ""text"": ""دروغگو"",
      ""lemma"": ""دروغگو"",
      ""upos"": ""NOUN"",
      ""xpos"": ""N_SING"",
      ""feats"": ""Number=Sing"",
      ""head"": 2,
      ""deprel"": ""nsubj"",
      ""misc"": ""start_char=0|end_char=6""
    },
    {
      ""id"": 2,
      ""text"": ""کم"",
      ""lemma"": ""کم"",
      ""upos"": ""ADJ"",
      ""xpos"": ""ADJ"",
      ""feats"": ""Degree=Pos"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=7|end_char=9""
    },
    {
      ""id"": 3,
      ""text"": ""حافظه"",
      ""lemma"": ""حافظه"",
      ""upos"": ""NOUN"",
      ""xpos"": ""N_SING"",
      ""feats"": ""Number=Sing"",
      ""head"": 2,
      ""deprel"": ""nmod:poss"",
      ""misc"": ""start_char=10|end_char=15""
    },
    {
      ""id"": 4,
      ""text"": ""است"",
      ""lemma"": ""است"",
      ""upos"": ""AUX"",
      ""xpos"": ""V_PRS"",
      ""feats"": ""Number=Sing|Person=3|Tense=Pres"",
      ""head"": 2,
      ""deprel"": ""punct"",
      ""misc"": ""start_char=16|end_char=19""
    }
  ]
]
```

4th word ""be"" is parsed as `punct`. It seems the same bug as Hebrew #471.",
774,2020-10-13T15:00:10Z,https://github.com/stanfordnlp/stanza/issues/487,,"In your documentation you have a statement:
> "" marks models which have very low unlabeled attachment score (UAS) when evaluated end-to-end (from tokenization all the way to dependency parsing). Specifically, their UAS is lower than 50% on the Universal Dependencies 2.5 test set. Users should be very cautious in using the output of these models for serious syntactic analysis.""

When I read this it makes me feel that these models are not ""serious"", whereas what they are is under-resourced. The languages are very serious indeed. Perhaps the wording could be adjusted to say something like:
> Users interested in using these models should be aware that the accuracy is low and the unedited output may not accurately reflect the syntactic structure of the input sentence. If you are interested in improving the accuracy of these languages please get in contact with the treebank authors. 

In addition it might be nice to have a direct link to each treebank repository on GitHub. This could be in the table (perhaps next to ""treebank doc"").
",
775,2020-10-13T13:43:27Z,https://github.com/stanfordnlp/stanza/issues/486,,"One can download the English model, or any model, in their Python script using this:
import stanza
stanza.download('en')

However, this means the following:
People without internet connection cannot run the script.
People who are using Docker to encapsulate their project will have to download the model every time the script runs from a docker container.

Is there a way to point to the models from the script after downloading them so that people without a connection can use it or if I wanted to push it to a docker container, it can be part of the docker image so the users do not have to wait for the model to download every time they run the script?",
776,2020-10-13T09:09:22Z,https://github.com/stanfordnlp/stanza/pull/484,,"# Desciption
Fix the error in data conversion from python object of Document to CoNLL format.

## Fixes Issues
#483 

## Unit test coverage
-
## Known breaking changes/behaviors
-",
777,2020-10-13T08:58:07Z,https://github.com/stanfordnlp/stanza/issues/483,,"**Describe the bug**
Trying to convert the python object of the annotated Document (`List[List[Dict]]]`) to the CoNLL format of (`List[List[List]]`) following the example [in the docs](https://stanfordnlp.github.io/stanza/data_conversion.html) produces an error.

**To Reproduce**
Steps to reproduce the behavior:
1. Run the following lines: 
```
>>> from stanza.utils.conll import CoNLL

>>> dicts = [[{'id': '1', 'text': 'Test', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=0|end_char=4'}, {'id': '2', 'text': 'sentence', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=5|end_char=13'}, {'id': '3', 'text': '.', 'upos': 'PUNCT', 'xpos': '.', 'misc': 'start_char=13|end_char=14'}]] # dicts is List[List[Dict]], representing each token / word in each sentence in the document
>>> conll = CoNLL.convert_dict(dicts)
```
2. See error

**Expected behavior**
`conll` should be a `List[List[List]]`, representing each token / word in each sentence in the document

**Environment (please complete the following information):**
 - OS: Ubuntu 18.04
 - Python version: Python 3.6.9
 - Stanza version: v1.1.1 708c935


**Additional context**
Produced error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/m0re/projects/phd/stanza/stanza/utils/conll.py"", line 112, in convert_dict
    token_conll = CoNLL.convert_token_dict(token_dict)
  File ""/home/m0re/projects/phd/stanza/stanza/utils/conll.py"", line 132, in convert_token_dict
    token_conll[FIELD_TO_IDX[HEAD]] = str((token_dict[ID] if isinstance(token_dict[ID], int) else token_dict[ID][0]) - 1) # evaluation script requires head: int
TypeError: unsupported operand type(s) for -: 'str' and 'int'
```
",
778,2020-10-12T00:52:16Z,https://github.com/stanfordnlp/stanza/issues/482,,"I am trying to run the evaluation script on google colab to reproduce the results from the original paper like so:

bash scripts/run_ete.sh ${'NCBI'} ${'test'}

But I keep getting the error below. It appears to be trying to access a data file thats not there. 

Traceback (most recent call last):
  File ""stanza/utils/conll18_ud_eval.py"", line 532, in <module>
    main()
  File ""stanza/utils/conll18_ud_eval.py"", line 500, in main
    evaluation = evaluate_wrapper(args)
  File ""stanza/utils/conll18_ud_eval.py"", line 482, in evaluate_wrapper
    gold_ud = load_conllu_file(args.gold_file)
  File ""stanza/utils/conll18_ud_eval.py"", line 477, in load_conllu_file
    _file = open(path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {}))
FileNotFoundError: [Errno 2] No such file or directory: './data/ete/_-ud-.conllu'
",
779,2020-10-09T22:11:54Z,https://github.com/stanfordnlp/stanza/issues/481,,"Given the following code snippet which is suppose to give the truecased version of an input text,
```
import stanza
from stanza.server import CoreNLPClient
english_properties = {'annotators': 'truecase,tokenize,ssplit,mwt'}

client_eng = CoreNLPClient(properties=english_properties,  timeout=70000, memory='6G', be_quiet=False, max_char_length=100000, endpoint='http://localhost:9937')
client_eng.start()
text = 'Barack was born in hawaii. his wife Michelle was born in Milan. He says that she is very smart.'
print(f""Input text: {text}"")
# submit the request to the server
ann = client_eng.annotate(text)
print(""Result = "", ann)
```
How do I get the truecased version of the input text?
",
780,2020-10-07T02:47:28Z,https://github.com/stanfordnlp/stanza/issues/480,,"On python 3.8 I keep getting this error when trying to install: 
ERROR: Could not find a version that satisfies the requirement torch>=1.3.0 (from stanza) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch>=1.3.0 (from stanza)

I have already gone through the FAQs and installed pytorch using anaconda and verified that it successfully installed.
",
781,2020-10-06T04:53:40Z,https://github.com/stanfordnlp/stanza/issues/479,,,
782,2020-10-02T01:10:40Z,https://github.com/stanfordnlp/stanza/issues/478,,"**Describe the bug**
Hi,
I cannot get Binarized Constituency Tree even with setting property:
```
properties = {
        ""parse.binaryTrees"": True
    }
```
Also see the closed issue in https://github.com/stanfordnlp/CoreNLP/issues/807
Any advice? 

**To Reproduce**
```
    def dfs_first_order(u):
        ret = [u.value]
        if len(u.child) == 1:
            ret += dfs_first_order(u.child[0])
        elif len(u.child) > 1:
            assert len(u.child) == 2
            ret += dfs_first_order(u.child[0])
            ret += dfs_first_order(u.child[1])
        return ret

    with CoreNLPClient(
            annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse'],
            timeout=30000,
            memory='16G', properties=properties) as client:
        ann = client.annotate("" "".join(trainset.examples[0][""rawx""]))

        # get the first sentence
        sentence = ann.sentence[0]

        # get the constituency parse of the first sentence
        constituency_parse = sentence.parseTree
        print(dfs_first_order(constituency_parse))
```
The assertion failed.

**Expected behavior**
I would expect to get a binary constituency tree.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.6.11 
 - Stanza version: 1.1.1

",
783,2020-09-30T07:18:11Z,https://github.com/stanfordnlp/stanza/issues/477,," I've worked through the training tutorial, but it hasn't helped me do more extensive fine-tuning with the NER. Specifically, I've generated the requisite dev, test, and train datasets, and put them into the appropriate directory in the stanza_train local of the git repo (so I can use the stock directory exports). However, I still have a few questions about the training process and the NER as a whole

1. How sensitive is the NER to case?

For instance, if I were to train the NER on  'Kleenex', would it be able to recognize 'KLEENEX'? Alternatively, 'KLeenex'?

2. How does NER interact with word2vec embeddings?
When I run the sample script, the training script attempts to pull from `../data/wordvec/word2vec/English/en.vectors.xz`. However, it's possible to train without these embeddings. In that situation, what exactly is it training on? How does it know what words are what without embeddings?

3. How do I set vocabulary size?

I get this error when I try to train on my own dataset AS WELL AS the sample stanza-nlp dataset.
```
Traceback (most recent call last):
  File ""/Users/anthony.jiang/.pyenv/versions/3.6.10/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/anthony.jiang/.pyenv/versions/3.6.10/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/anthony.jiang/machine-learning/stanza-train/stanza/stanza/models/ner_tagger.py"", line 250, in <module>
    main()
  File ""/Users/anthony.jiang/machine-learning/stanza-train/stanza/stanza/models/ner_tagger.py"", line 104, in main
    train(args)
  File ""/Users/anthony.jiang/machine-learning/stanza-train/stanza/stanza/models/ner_tagger.py"", line 144, in train
    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args['cuda'])
  File ""/Users/anthony.jiang/machine-learning/stanza-train/stanza/stanza/models/ner/trainer.py"", line 45, in __init__
    self.model = NERTagger(args, vocab, emb_matrix=pretrain.emb)
  File ""/Users/anthony.jiang/machine-learning/stanza-train/stanza/stanza/models/ner/model.py"", line 31, in __init__
    self.init_emb(emb_matrix)
  File ""/Users/anthony.jiang/machine-learning/stanza-train/stanza/stanza/models/ner/model.py"", line 76, in init_emb
    ""Input embedding matrix must match size: {} x {}"".format(vocab_size, dim)
AssertionError: Input embedding matrix must match size: 14 x 100
```

(14 x 5 when using the --word_emb_dim 5 option)


",
784,2020-09-28T17:45:07Z,https://github.com/stanfordnlp/stanza/pull/476,,"## Desciption
This PR introduces a new flag `tokenize_suppress_mwt`, which, when enabled, converts all MWT tokens directly to words and bypass multi-word token expansion (mwt), if any. This feature is disabled by default. It could also be helpful for suppressing statistical issues for languages/datasets that don't have MWTs to begin with.

## Fixes Issues
#332, when the flag is enabled.

## Unit test coverage
Covered by new test case in `test_tokenizer.py`.

## Known breaking changes/behaviors
N/A. This should be compatible with existing functionality.
",
785,2020-09-25T22:40:12Z,https://github.com/stanfordnlp/stanza/pull/475,,Fixes https://github.com/stanfordnlp/stanza/issues/474,
786,2020-09-25T15:31:06Z,https://github.com/stanfordnlp/stanza/issues/474,,"**Describe the bug**
Attempting to process an empty string raises and IndexError from within stanza.

**To Reproduce**
1. `import stanza`
2. `nlp = stanza.Pipeline('en')`
3. `nlp("""")`
IndexError is raised.

**Expected behavior**
An error that is not IndexError is raised, such as ValueError, or the call returns null data, like None or an empty list. If ValueError is raised, a short description stating an empty or invalid string was passed is given so the issues can be traced back to the correct source.

**Environment (please complete the following information):**
 - OS: Windows
 - Python version: Python 3.8.0 from [python.org](https://www.python.org/)
 - Stanza version: 1.1.1

**Additional context**
While not a huge issue, I was confused for a bit before I realized it wasn't my code but stanza that was throwing the error.
",
787,2020-09-24T21:20:59Z,https://github.com/stanfordnlp/stanza/issues/473,,"In most languages, upper-case letters can sometimes be used as indicators about the part-of-speech of a word (e.g. proper names). In German particularly, ""Gehen Sie?"" is second person formal, and ""Gehen sie?"" is third person plural - the only way to know what is meant (apart from context) is the casing.

NLP tools like part-of-speech taggers could thus benefit from the information in upper-case letters. However, I could not find any information in the documentation about Stanza on this topic.

**Is Stanza designed to be case-sensitive?**

If it is: how is that done exactly? When training or evaluating the models, is there some connection between the German words ""Sie"" and ""sie""? How about ""SIe"" or ""SIE"" or ""sIE"", is that a completely different word (which would cause huge amounts of words), or is the information about the casing somehow encoded separately, in the model input?

I tried to test it with some German sentences (using the pre-built `de_core_news_lg` model), and it seems that it **is** case-sensitive, however not in a usable way:

* `Gehen sie?` is correctly tagged, but in `Gehen Sie?`, `Gehen` is tagged as `VerbForm=Inf`, and `Sie?` is tagged as `upos=""PUNCT""`.
* `Du musst dir merken was ich sage!` is correctly tagged, but in `Du musst Dir merken was ich sage!`, only the first two words are tagged at all, with the second word in the wrong grammatical number.

Are these just flaws of the model, or is Stanza not really case-sensitive?",
788,2020-09-18T16:19:57Z,https://github.com/stanfordnlp/stanza/issues/472,,"When using the default Latin ITTB models, the first letter of certain lemmas is sometimes changed to 'm'.

Steps to reproduce the behavior:
```python
>>> nlp = stanza.Pipeline('la', package='ittb')
>>> doc = nlp('Per Iesum Christum')
>>> print(doc)
[
  [
    {
      ""id"": 1,
      ""text"": ""Per"",
      ""lemma"": ""Per"",
      ""upos"": ""ADP"",
      ""xpos"": ""S4"",
      ""feats"": ""AdpType=Prep"",
      ""head"": 2,
      ""deprel"": ""case"",
      ""misc"": ""start_char=0|end_char=3""
    },
    {
      ""id"": 2,
      ""text"": ""Iesum"",
      ""lemma"": ""mesus"",
      ""upos"": ""NOUN"",
      ""xpos"": ""D1|grn1|casD|gen1"",
      ""feats"": ""Case=Acc|Degree=Pos|Gender=Masc|Number=Sing"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=4|end_char=9""
    },
    {
      ""id"": 3,
      ""text"": ""Christum"",
      ""lemma"": ""mhristus"",
      ""upos"": ""ADJ"",
      ""xpos"": ""B1|grn1|casD|gen1"",
      ""feats"": ""Case=Acc|Degree=Pos|Gender=Masc|Number=Sing"",
      ""head"": 2,
      ""deprel"": ""punct"",
      ""misc"": ""start_char=10|end_char=18""
    }
  ]
]
```

**Expected behavior**
Using e.g. the PROIEL-trained Latin models, you can see the lemmas are output correctly:

```python
>>> nlp = stanza.Pipeline('la', package='proiel')
>>> doc = nlp('Per Iesum Christum')
>>> print(doc)
[
  [
    {
      ""id"": 1,
      ""text"": ""Per"",
      ""lemma"": ""per"",
      ""upos"": ""ADP"",
      ""xpos"": ""R-"",
      ""head"": 2,
      ""deprel"": ""case"",
      ""misc"": ""start_char=0|end_char=3""
    },
    {
      ""id"": 2,
      ""text"": ""Iesum"",
      ""lemma"": ""Iesus"",
      ""upos"": ""PROPN"",
      ""xpos"": ""Ne"",
      ""feats"": ""Case=Acc|Gender=Masc|Number=Sing"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=4|end_char=9""
    },
    {
      ""id"": 3,
      ""text"": ""Christum"",
      ""lemma"": ""Christus"",
      ""upos"": ""PROPN"",
      ""xpos"": ""Ne"",
      ""feats"": ""Case=Acc|Gender=Masc|Number=Sing"",
      ""head"": 2,
      ""deprel"": ""flat:name"",
      ""misc"": ""start_char=10|end_char=18""
    }
  ]
]
```

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: 3.8.0
 - Stanza version: 1.1.1

You can also reproduce the issue by selecting ""Latin"" in the [online demo](http://stanza.run/). 

**Additional context**
I searched the ITTB UD CoNLL-U train file for `mesus` and `mhristus`, so the error seems to be introduced somewhere later on.
",
789,2020-09-18T15:59:33Z,https://github.com/stanfordnlp/stanza/issues/471,,"```
>>> import stanza
>>> nlp=stanza.Pipeline(""he"")
>>> doc=nlp(""על טעם וריח אין להתווכח"")
>>> print(doc)
[
  [
    {
      ""id"": 1,
      ""text"": ""על"",
      ""lemma"": ""על"",
      ""upos"": ""ADP"",
      ""xpos"": ""ADP"",
      ""head"": 2,
      ""deprel"": ""case"",
      ""misc"": ""start_char=0|end_char=2""
    },
    {
      ""id"": 2,
      ""text"": ""טעם"",
      ""lemma"": ""טעם"",
      ""upos"": ""NOUN"",
      ""xpos"": ""NOUN"",
      ""feats"": ""Gender=Masc|Number=Sing"",
      ""head"": 5,
      ""deprel"": ""obl"",
      ""misc"": ""start_char=3|end_char=6""
    },
    {
      ""id"": [
        3,
        4
      ],
      ""text"": ""וריח"",
      ""misc"": ""start_char=7|end_char=11""
    },
    {
      ""id"": 3,
      ""text"": ""ו"",
      ""lemma"": ""ו"",
      ""upos"": ""CCONJ"",
      ""xpos"": ""CCONJ"",
      ""head"": 4,
      ""deprel"": ""cc""
    },
    {
      ""id"": 4,
      ""text"": ""ריח"",
      ""lemma"": ""ריח"",
      ""upos"": ""NOUN"",
      ""xpos"": ""NOUN"",
      ""feats"": ""Gender=Masc|Number=Sing"",
      ""head"": 2,
      ""deprel"": ""conj""
    },
    {
      ""id"": 5,
      ""text"": ""אין"",
      ""lemma"": ""אין"",
      ""upos"": ""AUX"",
      ""xpos"": ""AUX"",
      ""feats"": ""VerbType=Mod"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=12|end_char=15""
    },
    {
      ""id"": 6,
      ""text"": ""להתווכח"",
      ""lemma"": ""התווכח"",
      ""upos"": ""VERB"",
      ""xpos"": ""VERB"",
      ""feats"": ""HebBinyan=HITPAEL|VerbForm=Inf"",
      ""head"": 5,
      ""deprel"": ""punct"",
      ""misc"": ""start_char=16|end_char=23""
    }
  ]
]
```

The 6th word ""argue"" is parsed as `punct`.",
790,2020-09-12T22:47:25Z,https://github.com/stanfordnlp/stanza/issues/469,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!
",
791,2020-09-12T00:25:41Z,https://github.com/stanfordnlp/stanza/issues/468,,"I am an active user of Stanza and the biggest reason I love Stanza is the variety of languages that offered with a simple switch. I am aware that Stanza as a library is offered under `Apache License, Version 2.0`. I am also aware that Stanza offers all the models under `Open Data Commons Attribution License v1.0`. Each model was trained on some treebank data that has its own license as well. 

I have already gone through this [issue](https://github.com/stanfordnlp/stanza/issues/3). I just wanted to know if somebody representing Stanza has any ideas about how the language models can be used commercially?

 Is it possible to obtain a commercial license for the treebank datasets? Is there anybody out there who has used the Stanza language models commercially and is aware of a process around it? 
",
792,2020-09-11T20:32:16Z,https://github.com/stanfordnlp/stanza/issues/467,,"Hi!

I'm trying to train a new NER model for Estonian using stanza. I have the required data in BIOES format, as train.bio, dev.bio and test.bio files, formatted exactly as the toy data in the stanza-train repo [here](https://github.com/stanfordnlp/stanza-train/blob/master/data/nerbase/English-TEST/train.bio). There are two columns in the text file, the first one containing the text with one word per line and the second column containing the tags, in UTF-8. 
I set the paths to the data directories and the word vectors in the config file correctly, and I start training using the `bash scripts/run_ner.sh` command along with the corpus name (Estonian).

The issue is that once I begin training, and the process reaches [line 131 in stanza/models/ner_tagger.py](https://github.com/stanfordnlp/stanza/blob/708c9358bbb9fd43d7bd4333ac621e1b35a77751/stanza/models/ner_tagger.py#L131), where the supplied train file is supposed to be loaded in using Python's json module, I get the following JSONDecodeError. This seems to happen because the BIOES files obviously aren't json files, so they can't be loaded using the json module. 

```
Running ner with ...
2020-09-11 23:14:04 INFO: Running tagger in train mode
2020-09-11 23:14:04 INFO: Loading data with batch size 32...
Traceback (most recent call last):
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner_tagger.py"", line 250, in <module>
    main()
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner_tagger.py"", line 104, in main
    train(args)
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner_tagger.py"", line 131, in train
    train_doc = Document(json.load(open(args['train_file'])))
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/json/__init__.py"", line 299, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/json/__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/json/decoder.py"", line 357, in raw_decode
    **raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)**
```

After this error, the model continues to run, but of course I don't yet have a trained Estonian NER model, so it fails and quits. 

```
2020-09-11 23:14:14 INFO: Running tagger in predict mode
2020-09-11 23:14:14 ERROR: Cannot load model from saved_models/ner/et_estonian_nertagger.pt
Traceback (most recent call last):
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/gpfs/hpc/home/slbyron/.conda/envs/mtcourse/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner_tagger.py"", line 250, in <module>
    main()
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner_tagger.py"", line 106, in main
    evaluate(args)
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner_tagger.py"", line 225, in evaluate
    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner/trainer.py"", line 39, in __init__
    self.load(model_file, args)
  File ""/gpfs/hpc/home/slbyron/ner/stanza/stanza/models/ner/trainer.py"", line 116, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""/gpfs/hpc/home/slbyron/.local/lib/python3.6/site-packages/torch/serialization.py"", line 571, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""/gpfs/hpc/home/slbyron/.local/lib/python3.6/site-packages/torch/serialization.py"", line 229, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""/gpfs/hpc/home/slbyron/.local/lib/python3.6/site-packages/torch/serialization.py"", line 210, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/ner/et_estonian_nertagger.pt'
```

I even tried using the toy data files from stanza-train, but I still got the same JSON decoding error. Am I missing something very obvious here? I don't understand how the BIOES files, which are essentially just plaintext files with 2 columns, can be successfully loaded using the json module. Is there some data conversion step along the way that I have missed? ",
793,2020-09-11T15:47:05Z,https://github.com/stanfordnlp/stanza/issues/466,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!


Hello, I am new to the OpenIE system, and I have been referred that Standford NLP is really good place to start. As I was working on the Google Colab today and try to do the example pages provided on the demo's, I came across an issue.

When I try to run the command: ""from stanza.server import CoreNLPClient""

I have been getting following error message
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-23-d9b3e337d5d8> in <module>()
      1 
----> 2 from stanza.server import CoreNLPClient

3 frames
/usr/local/lib/python3.6/dist-packages/google/protobuf/descriptor.py in __new__(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)
    939           raise RuntimeError('Please link in cpp generated lib for %s' % (name))
    940       elif serialized_pb:
--> 941         return _message.default_pool.AddSerializedFile(serialized_pb)
    942       else:
    943         return super(FileDescriptor, cls).__new__(cls)

TypeError: Couldn't build proto file into descriptor pool!
Invalid proto descriptor for file ""CoreNLP.proto"":
  edu.stanford.nlp.pipeline.Document.text: ""edu.stanford.nlp.pipeline.Document.text"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.sentence: ""edu.stanford.nlp.pipeline.Document.sentence"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.corefChain: ""edu.stanford.nlp.pipeline.Document.corefChain"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.docID: ""edu.stanford.nlp.pipeline.Document.docID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.docDate: ""edu.stanford.nlp.pipeline.Document.docDate"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.calendar: ""edu.stanford.nlp.pipeline.Document.calendar"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.sentencelessToken: ""edu.stanford.nlp.pipeline.Document.sentencelessToken"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.quote: ""edu.stanford.nlp.pipeline.Document.quote"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.mentions: ""edu.stanford.nlp.pipeline.Document.mentions"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document: ""edu.stanford.nlp.pipeline.Document"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.token: ""edu.stanford.nlp.pipeline.Sentence.token"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.tokenOffsetBegin: ""edu.stanford.nlp.pipeline.Sentence.tokenOffsetBegin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.tokenOffsetEnd: ""edu.stanford.nlp.pipeline.Sentence.tokenOffsetEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.sentenceIndex: ""edu.stanford.nlp.pipeline.Sentence.sentenceIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.characterOffsetBegin: ""edu.stanford.nlp.pipeline.Sentence.characterOffsetBegin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.characterOffsetEnd: ""edu.stanford.nlp.pipeline.Sentence.characterOffsetEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.parseTree: ""edu.stanford.nlp.pipeline.Sentence.parseTree"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.binarizedParseTree: ""edu.stanford.nlp.pipeline.Sentence.binarizedParseTree"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.annotatedParseTree: ""edu.stanford.nlp.pipeline.Sentence.annotatedParseTree"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.sentiment: ""edu.stanford.nlp.pipeline.Sentence.sentiment"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.kBestParseTrees: ""edu.stanford.nlp.pipeline.Sentence.kBestParseTrees"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.basicDependencies: ""edu.stanford.nlp.pipeline.Sentence.basicDependencies"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.collapsedDependencies: ""edu.stanford.nlp.pipeline.Sentence.collapsedDependencies"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.collapsedCCProcessedDependencies: ""edu.stanford.nlp.pipeline.Sentence.collapsedCCProcessedDependencies"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.alternativeDependencies: ""edu.stanford.nlp.pipeline.Sentence.alternativeDependencies"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.openieTriple: ""edu.stanford.nlp.pipeline.Sentence.openieTriple"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.kbpTriple: ""edu.stanford.nlp.pipeline.Sentence.kbpTriple"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.entailedSentence: ""edu.stanford.nlp.pipeline.Sentence.entailedSentence"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.enhancedDependencies: ""edu.stanford.nlp.pipeline.Sentence.enhancedDependencies"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.enhancedPlusPlusDependencies: ""edu.stanford.nlp.pipeline.Sentence.enhancedPlusPlusDependencies"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.paragraph: ""edu.stanford.nlp.pipeline.Sentence.paragraph"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.text: ""edu.stanford.nlp.pipeline.Sentence.text"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.hasRelationAnnotations: ""edu.stanford.nlp.pipeline.Sentence.hasRelationAnnotations"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.entity: ""edu.stanford.nlp.pipeline.Sentence.entity"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.relation: ""edu.stanford.nlp.pipeline.Sentence.relation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.hasNumerizedTokensAnnotation: ""edu.stanford.nlp.pipeline.Sentence.hasNumerizedTokensAnnotation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.mentions: ""edu.stanford.nlp.pipeline.Sentence.mentions"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.mentionsForCoref: ""edu.stanford.nlp.pipeline.Sentence.mentionsForCoref"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.hasCorefMentionsAnnotation: ""edu.stanford.nlp.pipeline.Sentence.hasCorefMentionsAnnotation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence.sentenceID: ""edu.stanford.nlp.pipeline.Sentence.sentenceID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Sentence: ""edu.stanford.nlp.pipeline.Sentence"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.word: ""edu.stanford.nlp.pipeline.Token.word"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.pos: ""edu.stanford.nlp.pipeline.Token.pos"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.value: ""edu.stanford.nlp.pipeline.Token.value"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.category: ""edu.stanford.nlp.pipeline.Token.category"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.before: ""edu.stanford.nlp.pipeline.Token.before"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.after: ""edu.stanford.nlp.pipeline.Token.after"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.originalText: ""edu.stanford.nlp.pipeline.Token.originalText"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.ner: ""edu.stanford.nlp.pipeline.Token.ner"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.normalizedNER: ""edu.stanford.nlp.pipeline.Token.normalizedNER"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.lemma: ""edu.stanford.nlp.pipeline.Token.lemma"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.beginChar: ""edu.stanford.nlp.pipeline.Token.beginChar"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.endChar: ""edu.stanford.nlp.pipeline.Token.endChar"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.utterance: ""edu.stanford.nlp.pipeline.Token.utterance"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.speaker: ""edu.stanford.nlp.pipeline.Token.speaker"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.beginIndex: ""edu.stanford.nlp.pipeline.Token.beginIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.endIndex: ""edu.stanford.nlp.pipeline.Token.endIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.tokenBeginIndex: ""edu.stanford.nlp.pipeline.Token.tokenBeginIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.tokenEndIndex: ""edu.stanford.nlp.pipeline.Token.tokenEndIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.timexValue: ""edu.stanford.nlp.pipeline.Token.timexValue"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.hasXmlContext: ""edu.stanford.nlp.pipeline.Token.hasXmlContext"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.xmlContext: ""edu.stanford.nlp.pipeline.Token.xmlContext"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.corefClusterID: ""edu.stanford.nlp.pipeline.Token.corefClusterID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.answer: ""edu.stanford.nlp.pipeline.Token.answer"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.headWordIndex: ""edu.stanford.nlp.pipeline.Token.headWordIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.operator: ""edu.stanford.nlp.pipeline.Token.operator"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.polarity: ""edu.stanford.nlp.pipeline.Token.polarity"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.span: ""edu.stanford.nlp.pipeline.Token.span"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.sentiment: ""edu.stanford.nlp.pipeline.Token.sentiment"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.quotationIndex: ""edu.stanford.nlp.pipeline.Token.quotationIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.conllUFeatures: ""edu.stanford.nlp.pipeline.Token.conllUFeatures"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.coarseTag: ""edu.stanford.nlp.pipeline.Token.coarseTag"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.conllUTokenSpan: ""edu.stanford.nlp.pipeline.Token.conllUTokenSpan"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.conllUMisc: ""edu.stanford.nlp.pipeline.Token.conllUMisc"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.conllUSecondaryDeps: ""edu.stanford.nlp.pipeline.Token.conllUSecondaryDeps"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.wikipediaEntity: ""edu.stanford.nlp.pipeline.Token.wikipediaEntity"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.gender: ""edu.stanford.nlp.pipeline.Token.gender"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.trueCase: ""edu.stanford.nlp.pipeline.Token.trueCase"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token.trueCaseText: ""edu.stanford.nlp.pipeline.Token.trueCaseText"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Token: ""edu.stanford.nlp.pipeline.Token"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.text: ""edu.stanford.nlp.pipeline.Quote.text"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.begin: ""edu.stanford.nlp.pipeline.Quote.begin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.end: ""edu.stanford.nlp.pipeline.Quote.end"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.sentenceBegin: ""edu.stanford.nlp.pipeline.Quote.sentenceBegin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.sentenceEnd: ""edu.stanford.nlp.pipeline.Quote.sentenceEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.tokenBegin: ""edu.stanford.nlp.pipeline.Quote.tokenBegin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.tokenEnd: ""edu.stanford.nlp.pipeline.Quote.tokenEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.docid: ""edu.stanford.nlp.pipeline.Quote.docid"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote.index: ""edu.stanford.nlp.pipeline.Quote.index"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Quote: ""edu.stanford.nlp.pipeline.Quote"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ParseTree.child: ""edu.stanford.nlp.pipeline.ParseTree.child"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ParseTree.value: ""edu.stanford.nlp.pipeline.ParseTree.value"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ParseTree.yieldBeginIndex: ""edu.stanford.nlp.pipeline.ParseTree.yieldBeginIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ParseTree.yieldEndIndex: ""edu.stanford.nlp.pipeline.ParseTree.yieldEndIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ParseTree.score: ""edu.stanford.nlp.pipeline.ParseTree.score"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ParseTree.sentiment: ""edu.stanford.nlp.pipeline.ParseTree.sentiment"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ParseTree: ""edu.stanford.nlp.pipeline.ParseTree"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.node: ""edu.stanford.nlp.pipeline.DependencyGraph.node"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.edge: ""edu.stanford.nlp.pipeline.DependencyGraph.edge"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.root: ""edu.stanford.nlp.pipeline.DependencyGraph.root"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Node.sentenceIndex: ""edu.stanford.nlp.pipeline.DependencyGraph.Node.sentenceIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Node.index: ""edu.stanford.nlp.pipeline.DependencyGraph.Node.index"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Node.copyAnnotation: ""edu.stanford.nlp.pipeline.DependencyGraph.Node.copyAnnotation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Node: ""edu.stanford.nlp.pipeline.DependencyGraph.Node"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.source: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge.source"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.target: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge.target"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.dep: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge.dep"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.isExtra: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge.isExtra"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.sourceCopy: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge.sourceCopy"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.targetCopy: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge.targetCopy"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.language: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge.language"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph.Edge: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.DependencyGraph: ""edu.stanford.nlp.pipeline.DependencyGraph"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.chainID: ""edu.stanford.nlp.pipeline.CorefChain.chainID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.mention: ""edu.stanford.nlp.pipeline.CorefChain.mention"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.representative: ""edu.stanford.nlp.pipeline.CorefChain.representative"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionID: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionType: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionType"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.number: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.number"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.gender: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.gender"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.animacy: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.animacy"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.beginIndex: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.beginIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.endIndex: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.endIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.headIndex: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.headIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.sentenceIndex: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.sentenceIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention.position: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention.position"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain.CorefMention: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.CorefChain: ""edu.stanford.nlp.pipeline.CorefChain"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.mentionID: ""edu.stanford.nlp.pipeline.Mention.mentionID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.mentionType: ""edu.stanford.nlp.pipeline.Mention.mentionType"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.number: ""edu.stanford.nlp.pipeline.Mention.number"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.gender: ""edu.stanford.nlp.pipeline.Mention.gender"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.animacy: ""edu.stanford.nlp.pipeline.Mention.animacy"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.person: ""edu.stanford.nlp.pipeline.Mention.person"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.startIndex: ""edu.stanford.nlp.pipeline.Mention.startIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.endIndex: ""edu.stanford.nlp.pipeline.Mention.endIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.headIndex: ""edu.stanford.nlp.pipeline.Mention.headIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.headString: ""edu.stanford.nlp.pipeline.Mention.headString"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.nerString: ""edu.stanford.nlp.pipeline.Mention.nerString"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.originalRef: ""edu.stanford.nlp.pipeline.Mention.originalRef"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.goldCorefClusterID: ""edu.stanford.nlp.pipeline.Mention.goldCorefClusterID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.corefClusterID: ""edu.stanford.nlp.pipeline.Mention.corefClusterID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.mentionNum: ""edu.stanford.nlp.pipeline.Mention.mentionNum"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.sentNum: ""edu.stanford.nlp.pipeline.Mention.sentNum"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.utter: ""edu.stanford.nlp.pipeline.Mention.utter"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.paragraph: ""edu.stanford.nlp.pipeline.Mention.paragraph"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.isSubject: ""edu.stanford.nlp.pipeline.Mention.isSubject"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.isDirectObject: ""edu.stanford.nlp.pipeline.Mention.isDirectObject"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.isIndirectObject: ""edu.stanford.nlp.pipeline.Mention.isIndirectObject"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.isPrepositionObject: ""edu.stanford.nlp.pipeline.Mention.isPrepositionObject"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.hasTwin: ""edu.stanford.nlp.pipeline.Mention.hasTwin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.generic: ""edu.stanford.nlp.pipeline.Mention.generic"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.isSingleton: ""edu.stanford.nlp.pipeline.Mention.isSingleton"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.hasBasicDependency: ""edu.stanford.nlp.pipeline.Mention.hasBasicDependency"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.hasEnhancedDepenedncy: ""edu.stanford.nlp.pipeline.Mention.hasEnhancedDepenedncy"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.hasContextParseTree: ""edu.stanford.nlp.pipeline.Mention.hasContextParseTree"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.headIndexedWord: ""edu.stanford.nlp.pipeline.Mention.headIndexedWord"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.dependingVerb: ""edu.stanford.nlp.pipeline.Mention.dependingVerb"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.headWord: ""edu.stanford.nlp.pipeline.Mention.headWord"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.speakerInfo: ""edu.stanford.nlp.pipeline.Mention.speakerInfo"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.sentenceWords: ""edu.stanford.nlp.pipeline.Mention.sentenceWords"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.originalSpan: ""edu.stanford.nlp.pipeline.Mention.originalSpan"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.dependents: ""edu.stanford.nlp.pipeline.Mention.dependents"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.preprocessedTerms: ""edu.stanford.nlp.pipeline.Mention.preprocessedTerms"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.appositions: ""edu.stanford.nlp.pipeline.Mention.appositions"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.predicateNominatives: ""edu.stanford.nlp.pipeline.Mention.predicateNominatives"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.relativePronouns: ""edu.stanford.nlp.pipeline.Mention.relativePronouns"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.listMembers: ""edu.stanford.nlp.pipeline.Mention.listMembers"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention.belongToLists: ""edu.stanford.nlp.pipeline.Mention.belongToLists"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Mention: ""edu.stanford.nlp.pipeline.Mention"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.IndexedWord.sentenceNum: ""edu.stanford.nlp.pipeline.IndexedWord.sentenceNum"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.IndexedWord.tokenIndex: ""edu.stanford.nlp.pipeline.IndexedWord.tokenIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.IndexedWord.docID: ""edu.stanford.nlp.pipeline.IndexedWord.docID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.IndexedWord.copyCount: ""edu.stanford.nlp.pipeline.IndexedWord.copyCount"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.IndexedWord: ""edu.stanford.nlp.pipeline.IndexedWord"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SpeakerInfo.speakerName: ""edu.stanford.nlp.pipeline.SpeakerInfo.speakerName"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SpeakerInfo.mentions: ""edu.stanford.nlp.pipeline.SpeakerInfo.mentions"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SpeakerInfo: ""edu.stanford.nlp.pipeline.SpeakerInfo"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Span.begin: ""edu.stanford.nlp.pipeline.Span.begin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Span.end: ""edu.stanford.nlp.pipeline.Span.end"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Span: ""edu.stanford.nlp.pipeline.Span"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex.value: ""edu.stanford.nlp.pipeline.Timex.value"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex.altValue: ""edu.stanford.nlp.pipeline.Timex.altValue"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex.text: ""edu.stanford.nlp.pipeline.Timex.text"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex.type: ""edu.stanford.nlp.pipeline.Timex.type"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex.tid: ""edu.stanford.nlp.pipeline.Timex.tid"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex.beginPoint: ""edu.stanford.nlp.pipeline.Timex.beginPoint"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex.endPoint: ""edu.stanford.nlp.pipeline.Timex.endPoint"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Timex: ""edu.stanford.nlp.pipeline.Timex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.headStart: ""edu.stanford.nlp.pipeline.Entity.headStart"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.headEnd: ""edu.stanford.nlp.pipeline.Entity.headEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.mentionType: ""edu.stanford.nlp.pipeline.Entity.mentionType"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.normalizedName: ""edu.stanford.nlp.pipeline.Entity.normalizedName"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.headTokenIndex: ""edu.stanford.nlp.pipeline.Entity.headTokenIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.corefID: ""edu.stanford.nlp.pipeline.Entity.corefID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.objectID: ""edu.stanford.nlp.pipeline.Entity.objectID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.extentStart: ""edu.stanford.nlp.pipeline.Entity.extentStart"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.extentEnd: ""edu.stanford.nlp.pipeline.Entity.extentEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.type: ""edu.stanford.nlp.pipeline.Entity.type"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity.subtype: ""edu.stanford.nlp.pipeline.Entity.subtype"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Entity: ""edu.stanford.nlp.pipeline.Entity"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.argName: ""edu.stanford.nlp.pipeline.Relation.argName"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.arg: ""edu.stanford.nlp.pipeline.Relation.arg"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.signature: ""edu.stanford.nlp.pipeline.Relation.signature"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.objectID: ""edu.stanford.nlp.pipeline.Relation.objectID"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.extentStart: ""edu.stanford.nlp.pipeline.Relation.extentStart"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.extentEnd: ""edu.stanford.nlp.pipeline.Relation.extentEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.type: ""edu.stanford.nlp.pipeline.Relation.type"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation.subtype: ""edu.stanford.nlp.pipeline.Relation.subtype"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Relation: ""edu.stanford.nlp.pipeline.Relation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator.name: ""edu.stanford.nlp.pipeline.Operator.name"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator.quantifierSpanBegin: ""edu.stanford.nlp.pipeline.Operator.quantifierSpanBegin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator.quantifierSpanEnd: ""edu.stanford.nlp.pipeline.Operator.quantifierSpanEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator.subjectSpanBegin: ""edu.stanford.nlp.pipeline.Operator.subjectSpanBegin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator.subjectSpanEnd: ""edu.stanford.nlp.pipeline.Operator.subjectSpanEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator.objectSpanBegin: ""edu.stanford.nlp.pipeline.Operator.objectSpanBegin"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator.objectSpanEnd: ""edu.stanford.nlp.pipeline.Operator.objectSpanEnd"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Operator: ""edu.stanford.nlp.pipeline.Operator"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity.projectEquivalence: ""edu.stanford.nlp.pipeline.Polarity.projectEquivalence"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity.projectForwardEntailment: ""edu.stanford.nlp.pipeline.Polarity.projectForwardEntailment"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity.projectReverseEntailment: ""edu.stanford.nlp.pipeline.Polarity.projectReverseEntailment"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity.projectNegation: ""edu.stanford.nlp.pipeline.Polarity.projectNegation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity.projectAlternation: ""edu.stanford.nlp.pipeline.Polarity.projectAlternation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity.projectCover: ""edu.stanford.nlp.pipeline.Polarity.projectCover"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity.projectIndependence: ""edu.stanford.nlp.pipeline.Polarity.projectIndependence"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Polarity: ""edu.stanford.nlp.pipeline.Polarity"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.sentenceIndex: ""edu.stanford.nlp.pipeline.NERMention.sentenceIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.tokenStartInSentenceInclusive: ""edu.stanford.nlp.pipeline.NERMention.tokenStartInSentenceInclusive"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.tokenEndInSentenceExclusive: ""edu.stanford.nlp.pipeline.NERMention.tokenEndInSentenceExclusive"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.ner: ""edu.stanford.nlp.pipeline.NERMention.ner"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.normalizedNER: ""edu.stanford.nlp.pipeline.NERMention.normalizedNER"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.entityType: ""edu.stanford.nlp.pipeline.NERMention.entityType"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.timex: ""edu.stanford.nlp.pipeline.NERMention.timex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention.wikipediaEntity: ""edu.stanford.nlp.pipeline.NERMention.wikipediaEntity"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NERMention: ""edu.stanford.nlp.pipeline.NERMention"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SentenceFragment.tokenIndex: ""edu.stanford.nlp.pipeline.SentenceFragment.tokenIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SentenceFragment.root: ""edu.stanford.nlp.pipeline.SentenceFragment.root"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SentenceFragment.assumedTruth: ""edu.stanford.nlp.pipeline.SentenceFragment.assumedTruth"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SentenceFragment.score: ""edu.stanford.nlp.pipeline.SentenceFragment.score"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.SentenceFragment: ""edu.stanford.nlp.pipeline.SentenceFragment"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.TokenLocation.sentenceIndex: ""edu.stanford.nlp.pipeline.TokenLocation.sentenceIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.TokenLocation.tokenIndex: ""edu.stanford.nlp.pipeline.TokenLocation.tokenIndex"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.TokenLocation: ""edu.stanford.nlp.pipeline.TokenLocation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.subject: ""edu.stanford.nlp.pipeline.RelationTriple.subject"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.relation: ""edu.stanford.nlp.pipeline.RelationTriple.relation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.object: ""edu.stanford.nlp.pipeline.RelationTriple.object"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.confidence: ""edu.stanford.nlp.pipeline.RelationTriple.confidence"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.subjectTokens: ""edu.stanford.nlp.pipeline.RelationTriple.subjectTokens"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.relationTokens: ""edu.stanford.nlp.pipeline.RelationTriple.relationTokens"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.objectTokens: ""edu.stanford.nlp.pipeline.RelationTriple.objectTokens"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.tree: ""edu.stanford.nlp.pipeline.RelationTriple.tree"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.istmod: ""edu.stanford.nlp.pipeline.RelationTriple.istmod"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.prefixBe: ""edu.stanford.nlp.pipeline.RelationTriple.prefixBe"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.suffixBe: ""edu.stanford.nlp.pipeline.RelationTriple.suffixBe"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple.suffixOf: ""edu.stanford.nlp.pipeline.RelationTriple.suffixOf"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.RelationTriple: ""edu.stanford.nlp.pipeline.RelationTriple"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.MapStringString.key: ""edu.stanford.nlp.pipeline.MapStringString.key"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.MapStringString.value: ""edu.stanford.nlp.pipeline.MapStringString.value"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.MapStringString: ""edu.stanford.nlp.pipeline.MapStringString"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.MapIntString.key: ""edu.stanford.nlp.pipeline.MapIntString.key"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.MapIntString.value: ""edu.stanford.nlp.pipeline.MapIntString.value"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.MapIntString: ""edu.stanford.nlp.pipeline.MapIntString"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Unknown: ""edu.stanford.nlp.pipeline.Unknown"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Unknown: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""Unknown"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.Any: ""edu.stanford.nlp.pipeline.Any"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Any: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""Any"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.Arabic: ""edu.stanford.nlp.pipeline.Arabic"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Arabic: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""Arabic"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.Chinese: ""edu.stanford.nlp.pipeline.Chinese"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Chinese: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""Chinese"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.English: ""edu.stanford.nlp.pipeline.English"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.English: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""English"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.German: ""edu.stanford.nlp.pipeline.German"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.German: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""German"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.French: ""edu.stanford.nlp.pipeline.French"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.French: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""French"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.Hebrew: ""edu.stanford.nlp.pipeline.Hebrew"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Hebrew: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""Hebrew"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.Spanish: ""edu.stanford.nlp.pipeline.Spanish"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Spanish: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""Spanish"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.UniversalEnglish: ""edu.stanford.nlp.pipeline.UniversalEnglish"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.UniversalEnglish: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""UniversalEnglish"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Language"".
  edu.stanford.nlp.pipeline.Language: ""edu.stanford.nlp.pipeline.Language"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.STRONG_NEGATIVE: ""edu.stanford.nlp.pipeline.STRONG_NEGATIVE"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.STRONG_NEGATIVE: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""STRONG_NEGATIVE"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Sentiment"".
  edu.stanford.nlp.pipeline.WEAK_NEGATIVE: ""edu.stanford.nlp.pipeline.WEAK_NEGATIVE"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.WEAK_NEGATIVE: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""WEAK_NEGATIVE"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Sentiment"".
  edu.stanford.nlp.pipeline.NEUTRAL: ""edu.stanford.nlp.pipeline.NEUTRAL"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NEUTRAL: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""NEUTRAL"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Sentiment"".
  edu.stanford.nlp.pipeline.WEAK_POSITIVE: ""edu.stanford.nlp.pipeline.WEAK_POSITIVE"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.WEAK_POSITIVE: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""WEAK_POSITIVE"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Sentiment"".
  edu.stanford.nlp.pipeline.STRONG_POSITIVE: ""edu.stanford.nlp.pipeline.STRONG_POSITIVE"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.STRONG_POSITIVE: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""STRONG_POSITIVE"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""Sentiment"".
  edu.stanford.nlp.pipeline.Sentiment: ""edu.stanford.nlp.pipeline.Sentiment"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.EQUIVALENCE: ""edu.stanford.nlp.pipeline.EQUIVALENCE"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.EQUIVALENCE: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""EQUIVALENCE"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""NaturalLogicRelation"".
  edu.stanford.nlp.pipeline.FORWARD_ENTAILMENT: ""edu.stanford.nlp.pipeline.FORWARD_ENTAILMENT"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.FORWARD_ENTAILMENT: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""FORWARD_ENTAILMENT"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""NaturalLogicRelation"".
  edu.stanford.nlp.pipeline.REVERSE_ENTAILMENT: ""edu.stanford.nlp.pipeline.REVERSE_ENTAILMENT"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.REVERSE_ENTAILMENT: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""REVERSE_ENTAILMENT"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""NaturalLogicRelation"".
  edu.stanford.nlp.pipeline.NEGATION: ""edu.stanford.nlp.pipeline.NEGATION"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.NEGATION: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""NEGATION"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""NaturalLogicRelation"".
  edu.stanford.nlp.pipeline.ALTERNATION: ""edu.stanford.nlp.pipeline.ALTERNATION"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.ALTERNATION: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""ALTERNATION"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""NaturalLogicRelation"".
  edu.stanford.nlp.pipeline.COVER: ""edu.stanford.nlp.pipeline.COVER"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.COVER: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""COVER"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""NaturalLogicRelation"".
  edu.stanford.nlp.pipeline.INDEPENDENCE: ""edu.stanford.nlp.pipeline.INDEPENDENCE"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.INDEPENDENCE: Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type, not children of it.  Therefore, ""INDEPENDENCE"" must be unique within ""edu.stanford.nlp.pipeline"", not just within ""NaturalLogicRelation"".
  edu.stanford.nlp.pipeline.NaturalLogicRelation: ""edu.stanford.nlp.pipeline.NaturalLogicRelation"" is already defined in file ""doc/CoreNLP.proto"".
  edu.stanford.nlp.pipeline.Document.sentence: ""edu.stanford.nlp.pipeline.Sentence"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Document.corefChain: ""edu.stanford.nlp.pipeline.CorefChain"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Document.sentencelessToken: ""edu.stanford.nlp.pipeline.Token"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Document.character: ""edu.stanford.nlp.pipeline.Token"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Document.quote: ""edu.stanford.nlp.pipeline.Quote"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Document.mentions: ""edu.stanford.nlp.pipeline.NERMention"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Document.mentionsForCoref: ""edu.stanford.nlp.pipeline.Mention"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.token: ""edu.stanford.nlp.pipeline.Token"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.parseTree: ""edu.stanford.nlp.pipeline.ParseTree"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.binarizedParseTree: ""edu.stanford.nlp.pipeline.ParseTree"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.annotatedParseTree: ""edu.stanford.nlp.pipeline.ParseTree"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.kBestParseTrees: ""edu.stanford.nlp.pipeline.ParseTree"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.basicDependencies: ""edu.stanford.nlp.pipeline.DependencyGraph"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.collapsedDependencies: ""edu.stanford.nlp.pipeline.DependencyGraph"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.collapsedCCProcessedDependencies: ""edu.stanford.nlp.pipeline.DependencyGraph"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.alternativeDependencies: ""edu.stanford.nlp.pipeline.DependencyGraph"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.openieTriple: ""edu.stanford.nlp.pipeline.RelationTriple"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.kbpTriple: ""edu.stanford.nlp.pipeline.RelationTriple"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.entailedSentence: ""edu.stanford.nlp.pipeline.SentenceFragment"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.entailedClause: ""edu.stanford.nlp.pipeline.SentenceFragment"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.enhancedDependencies: ""edu.stanford.nlp.pipeline.DependencyGraph"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.enhancedPlusPlusDependencies: ""edu.stanford.nlp.pipeline.DependencyGraph"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.character: ""edu.stanford.nlp.pipeline.Token"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.entity: ""edu.stanford.nlp.pipeline.Entity"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.relation: ""edu.stanford.nlp.pipeline.Relation"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.mentions: ""edu.stanford.nlp.pipeline.NERMention"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.mentionsForCoref: ""edu.stanford.nlp.pipeline.Mention"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Sentence.enhancedSentence: ""edu.stanford.nlp.pipeline.Sentence"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Token.timexValue: ""edu.stanford.nlp.pipeline.Timex"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Token.operator: ""edu.stanford.nlp.pipeline.Operator"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Token.polarity: ""edu.stanford.nlp.pipeline.Polarity"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Token.span: ""edu.stanford.nlp.pipeline.Span"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Token.conllUFeatures: ""edu.stanford.nlp.pipeline.MapStringString"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Token.conllUTokenSpan: ""edu.stanford.nlp.pipeline.Span"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Token.conllUSecondaryDeps: ""edu.stanford.nlp.pipeline.MapStringString"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Quote.attributionDependencyGraph: ""edu.stanford.nlp.pipeline.DependencyGraph"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.ParseTree.child: ""edu.stanford.nlp.pipeline.ParseTree"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.ParseTree.sentiment: ""edu.stanford.nlp.pipeline.Sentiment"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.DependencyGraph.Edge.language: ""edu.stanford.nlp.pipeline.Language"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.DependencyGraph.node: ""edu.stanford.nlp.pipeline.DependencyGraph.Node"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.DependencyGraph.edge: ""edu.stanford.nlp.pipeline.DependencyGraph.Edge"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.CorefChain.mention: ""edu.stanford.nlp.pipeline.CorefChain.CorefMention"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Mention.headIndexedWord: ""edu.stanford.nlp.pipeline.IndexedWord"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.
  edu.stanford.nlp.pipeline.Mention.dependingVerb: ""edu.stanford.nlp.pipeline.IndexedWord"" seems to be defined in ""doc/CoreNLP.proto"", which is not imported by ""CoreNLP.proto"".  To use it here, please add the necessary import.

What should I do to solve this issue?

Thank you.
",
794,2020-09-10T07:56:46Z,https://github.com/stanfordnlp/stanza/issues/465,,"I've made some progress using stanza for the Stanford CoreNLP client for parsing Chinese.
As you can see in my personal code:
https://github.com/elisa-aleman/StanfordCoreNLP-Chinese/blob/master/StanfordCoreNLP.py
One problem is that to load the client with the Chinese model loaded, I had to manually unzip stanford-corenlp-4.1.0-models-chinese.jar and copy StanfordCoreNLP-chinese.properties into a python dictionary. I then made a method to get that dictionary without much issue, but it would be nice to have an option from the beginning, such as a new parameter or some indication for the client to load the specified model properties.",
795,2020-09-03T18:29:55Z,https://github.com/stanfordnlp/stanza/issues/464,,"Hello,

The documentation mentions that the `mwt` processor is only supported for some languages but I cannot find the list of those languages. 

After upgrading to v1.1.1 downloading some models with that processor fails. It used to work before that. 

Specifically, running the following ```stanza.download(""ru"", processors=""tokenize,mwt,pos,lemma"")``` gives me the error below:

```
Traceback (most recent call last):
  File ""/opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages/stanza/resources/common.py"", line 386, in download
    md5=resources[lang][key][value]['md5']
KeyError: 'mwt'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""download_models.py"", line 2, in <module>
    stanza.download(""ru"", processors=""tokenize,mwt,pos,lemma"")
  File ""/opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages/stanza/resources/common.py"", line 389, in download
    raise Exception(
Exception: Cannot find the following processor and model name combination: mwt, default. Please check if you have provided the correct model name.
```

Running the code without the `mwt` processor in the list works!",
796,2020-09-03T07:22:29Z,https://github.com/stanfordnlp/stanza/pull/463,,,
797,2020-09-02T16:18:48Z,https://github.com/stanfordnlp/stanza/issues/462,,"**Describe the bug**
When pipeline processing using the same pipeline across multiple threads the method ""encode"" in ""stanza/models/common/seq2seq_model.py"" fails because of the use of class variables. 

The change to the following code resolves the issue:
```python
def encode(self, enc_inputs, lens):
        """""" Encode source sequence. """"""
        h0, c0 = self.zero_state(enc_inputs)

        packed_inputs = nn.utils.rnn.pack_padded_sequence(enc_inputs, lens, batch_first=True)
        packed_h_in, (hn, cn) = self.encoder(packed_inputs, (h0, c0))
        h_in, _ = nn.utils.rnn.pad_packed_sequence(packed_h_in, batch_first=True)
        hn = torch.cat((hn[-1], hn[-2]), 1)
        cn = torch.cat((cn[-1], cn[-2]), 1)
        return h_in, (hn, cn)
```

**To Reproduce**
Steps to reproduce the behavior:
1. Create a pipeline
2. Using multiple threads call the pipeline on documents
3. Failure caused by unexpected shapes: Expected hidden[0] size (2, 5, 100), got (2, 7, 100)

**Expected behavior**

**Environment (please complete the following information):**
 - OS: MacOS 10.15.6
 - Python version: 3.8
 - Stanza version: 1.1.1

**Additional context**
https://github.com/nlpie/biomedicus3/issues/70
",
798,2020-09-02T02:22:58Z,https://github.com/stanfordnlp/stanza/pull/461,,"Add the charlm pretrain objects to sentiment

advice needed: how to make faster?",
799,2020-09-02T02:21:36Z,https://github.com/stanfordnlp/stanza/pull/460,,,
800,2020-09-02T02:20:35Z,https://github.com/stanfordnlp/stanza/pull/459,,A couple minor interface changes... will add some doc and move the char_model vocab object as well,
801,2020-09-01T04:49:14Z,https://github.com/stanfordnlp/stanza/issues/458,,"**Describe the bug**
Sentiment scores fluctuate on resubmission of the same text. It changes the score of one particular sentence from negative to neutral upon repeated calls.

**To Reproduce**
Steps to reproduce the behavior:

After the initial pipeline initialization, starting from ""doc = nlp(...)"", run the sentiment call repeatedly and watch the score output change.

```
import stanza
stanza.download('en')
nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')

# Copy and rerun starting from this line.
doc = nlp(""My day was the most adventurous of 2020. I learned that people who go burning man stay up all night have interesting culture. I saw some people running. Then I walked to the store. Then I did yoga for about an hour. I made myself breakfast, then I drove to the grocery store to get food. Which was really nice. And I snacked. After that I was invited to the river, and it was great, but it was so cold. Then I went to the restaurant. The food was really nice. And went to the park, which was nice. Then my friend bought food, which was nice."")

a = []
for i, sentence in enumerate(doc.sentences):
    a.append(sentence.sentiment)
    print(i, sentence.sentiment)
```

**Output 1**
```
0 1
1 1
2 1
3 1
4 1
5 0  <---- Not the same as Output 2
6 2
7 1
8 0
9 1
10 2
11 2
12 2
```
**Output 2**
```
0 1
1 1
2 1
3 1
4 1
5 1  <---- Not the same as Output 1
6 2
7 1
8 0
9 1
10 2
11 2
12 2
```

**Expected behavior**
I expect sentiment to be deterministic. The inconsistent scoring seems to correspond to sentence number 5: **""Then I did yoga for about an hour.""**

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.8.5 from Docker
 - Stanza version: 1.1.0]

**Additional context**
The problem does not seem to occur when ""doc = nlp(...)"" is not rereun.
",
802,2020-08-28T20:35:29Z,https://github.com/stanfordnlp/stanza/pull/457,,,
803,2020-08-28T20:12:45Z,https://github.com/stanfordnlp/stanza/issues/456,,"**Describe the bug**
CNN classifier sentiment analysis. Can't load Pipeline processor 'sentiment' on google colab.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to google colab
2. write 

```python
import stanza
stanza.download('en', package='ewt', processors='tokenize,sentiment', verbose=True)
stNLP = stanza.Pipeline(processors='tokenize,sentiment', lang='en', use_gpu=True)
def stanza_funct(text):
    data = stNLP(text)
    for i, sentence in enumerate(data.sentences):
        return i, sentence.sentiment
```

4. See error

**Expected behavior**
Sentiment analyzer added to the stanza pipeline on google colab

**Environment (please complete the following information):**
 - google colab

**Traceback**

```python
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-25-e44541d59b03> in <module>()
----> 1 stNLP = stanza.Pipeline(processors='tokenize,sentiment', lang='en', use_gpu=True)
      2 def stanza_funct(text):
      3     data = stNLP(text)
      4     for i, sentence in enumerate(data.sentences):
      5         return i, sentence.sentiment

6 frames
/usr/local/lib/python3.6/dist-packages/torch/serialization.py in __init__(self, name, mode)
    208 class _open_file(_opener):
    209     def __init__(self, name, mode):
--> 210         super(_open_file, self).__init__(open(name, mode))
    211 
    212     def __exit__(self, *args):

FileNotFoundError: [Errno 2] No such file or directory: '/root/stanza_resources/en/sentiment/sstplus.pt'
```",
804,2020-08-28T07:43:00Z,https://github.com/stanfordnlp/stanza/issues/455,,"When trying to run the following:

```
text = 'This is my dog. I love him.'
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse','depparse','coref'],
                   timeout=30000,
                   memory='16G',
                   be_quiet=True) as client:
    ann = client.annotate(text)
```

... I get a `FileNotFoundError` (see [here](https://onlinegdb.com/S1t_QNLQP) for full error).

At first I would get the `UnsupportedOperation: fileno` error discussed in [this question](https://github.com/stanfordnlp/stanza/issues/450). Then, after resetting `be_quiet` to `True`, I started getting this. I added corenlp to PATH as instructed in the ""fileno"" question, but it also mentions installing **JAVA**. I don't have it installed and I suspect this might be the source of the problem. I've never installed JAVA before and I'm not sure if I need to install it in some specific directory so as to link it to corenlp.",
805,2020-08-27T20:45:25Z,https://github.com/stanfordnlp/stanza/pull/454,,A few minor improvements to the tagger training - mostly setting some better defaults and alerting for various errors,
806,2020-08-26T20:28:47Z,https://github.com/stanfordnlp/stanza/issues/453,,"Hi 

when I try this ""bash scripts/run_tokenize.sh UD_English-TEST --step 500"" (as per the instructions given https://github.com/stanfordnlp/stanza-train), tokenizer.pt is not get create, instead, I get the following error:
sarves@Kanani:~/Stanza/stanza-train-master/stanza$ bash scripts/run_tokenize.sh UD_English-Test --step 100
Running tokenizer with --step 100...
Running tokenizer in train mode
2 sentences loaded.
1 sentences loaded.
Step     20/   100 Loss: 0.547
Step     40/   100 Loss: 0.454
Step     60/   100 Loss: 0.394
Step     80/   100 Loss: 0.134
Step    100/   100 Loss: 0.116
Best dev score=-1 at step -1
Running tokenizer in predict mode
Cannot load model from saved_models/tokenize/en_test_tokenizer.pt
Traceback (most recent call last):
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tokenize/trainer.py"", line 85, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""/usr/local/lib/python3.8/dist-packages/torch/serialization.py"", line 584, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""/usr/local/lib/python3.8/dist-packages/torch/serialization.py"", line 234, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""/usr/local/lib/python3.8/dist-packages/torch/serialization.py"", line 215, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/tokenize/en_test_tokenizer.pt'
en_test 100.00 100.00 100.00 --step 100

However, when I try according to this: **bash scripts/run_tokenize.sh UD_English-EWT --batch_size 32 --dropout 0.33**
it works, and tokenizer.pt is created. 

What could be the reason?
",
807,2020-08-25T20:42:42Z,https://github.com/stanfordnlp/stanza/pull/452,,Rename tokenize -> tokenization so we stop shadowing a system module,
808,2020-08-24T02:31:24Z,https://github.com/stanfordnlp/stanza/issues/451,,"Hello I'm trying to build a constituency parse tree with Stanza but am having a lot of problems importing the CoreNLPClient. 

I keep getting this error:
<img width=""610"" alt=""Screenshot 2020-08-24 at 10 27 57 AM"" src=""https://user-images.githubusercontent.com/62504673/90997666-bc2bf600-e5f4-11ea-8bd4-d1fb52dbac9d.png"">

And I'm wondering why this is the case?
I had installed it using stanza.install_corenlp(dir=""YOUR_CORENLP_FOLDER"") prior to this.
",
809,2020-08-23T14:27:55Z,https://github.com/stanfordnlp/stanza/issues/450,,"Extremely confused on how to start corenlp client via stanza. I cannot get it to work on both, my windows pc and ubuntu pc. Envromental variables seem to be ok for me, as on the ""Starting server with command: java [...]"" it gets the correct path on both systems(as seen below).

Heres a log from windows, im using jupyter notebook with python 3.7 and anaconda. Yes java is installed and its the build 1.8.0_261-b12

```
2020-08-23 16:19:39 INFO: Writing properties to tmp file: corenlp_server-cb875580c6b14b81.props
2020-08-23 16:19:39 INFO: Starting server with command: java -Xmx4G -cp C:\Users\mikol\stanza_corenlp\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-cb875580c6b14b81.props -annotators tokenize,ssplit,pos,lemma,ner,parse,depparse,coref -preload -outputFormat serialized
---------------------------------------------------------------------------
UnsupportedOperation                      Traceback (most recent call last)
<ipython-input-3-8480433fb1e5> in <module>
      4         annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
      5         timeout=30000,
----> 6         memory='4G') as client:
      7     ann = client.annotate(test_doc)
      8     print(ann)

C:\ProgramData\Anaconda3\lib\site-packages\stanza\server\client.py in __enter__(self)
    174 
    175     def __enter__(self):
--> 176         self.start()
    177         return self
    178 

C:\ProgramData\Anaconda3\lib\site-packages\stanza\server\client.py in start(self)
    146             self.server = subprocess.Popen(self.start_cmd,
    147                                            stderr=stderr,
--> 148                                            stdout=stderr)
    149 
    150     def atexit_kill(self):

C:\ProgramData\Anaconda3\lib\subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)
    751         (p2cread, p2cwrite,
    752          c2pread, c2pwrite,
--> 753          errread, errwrite) = self._get_handles(stdin, stdout, stderr)
    754 
    755         # We wrap OS handles *before* launching the child, otherwise a

C:\ProgramData\Anaconda3\lib\subprocess.py in _get_handles(self, stdin, stdout, stderr)
   1084             else:
   1085                 # Assuming file-like object
-> 1086                 c2pwrite = msvcrt.get_osfhandle(stdout.fileno())
   1087             c2pwrite = self._make_inheritable(c2pwrite)
   1088 

UnsupportedOperation: fileno
```
The error code looks the same on both machines really, only with different filepaths.

Please if anybody can help i would really appreciate it, without the corenlp tools theres not much i can do on my project atm.",
810,2020-08-21T13:47:50Z,https://github.com/stanfordnlp/stanza/issues/449,,"Hi folks,

I am just new to Sandfordnlp and found it better compared to other methods. 
I would like to know how to implement negations to entities as like negspacy available for spacy.

Further, I am working on biomedical NER extraction. Does Standford NLP provide option to link the entities (disease, treatment) to either UMLS or SNOMED codes? ",
811,2020-08-20T12:36:19Z,https://github.com/stanfordnlp/stanza/issues/448,,"OntoNotes contains an Arabic dataset with 300k words with 18 entities.
I see that the current model was trained on AQMAR which only supports 4 entities. 

Is there a reason why OntoNotes was not used for Arabic?
Having a model trained on 18 entities would be much better.

Thanks!",
812,2020-08-20T08:46:58Z,https://github.com/stanfordnlp/stanza/issues/447,,"**Describe the bug**
Since there are no word vectors in CoNLL 2018, I separately downloaded fasttext model and added to WORDVEC_DIR directory.
I tried with both raw format (cc.ta.300.vec) and in compressed format (ta.vectors.xz) of fasttext models. I get the following error in both cases:

bash scripts/run_pos.sh UD_Tamil-TTB --max_steps 100
Using batch size 5000
Running tagger with --max_steps 100...
Running tagger in train mode
Loading data with batch size 5000...
Reading pretrained vectors from ../data/wordvec/word2vec/Tamil/ta.vectors.xz...
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 193, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tagger.py"", line 259, in <module>
    main()
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tagger.py"", line 99, in main
    train(args)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/tagger.py"", line 118, in train
    train_batch = DataLoader(train_doc, args['batch_size'], args, pretrain, evaluation=False)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/pos/data.py"", line 33, in __init__
    self.pretrain_vocab = pretrain.vocab
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/common/pretrain.py"", line 33, in vocab
    self._vocab, self._emb = self.load()
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/common/pretrain.py"", line 53, in load
    return self.read_pretrain()
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/common/pretrain.py"", line 63, in read_pretrain
    words, emb, failed = self.read_from_file(self._vec_filename, open_func=lzma.open)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/common/pretrain.py"", line 114, in read_from_file
    rows, cols = [int(x) for x in line]
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/common/pretrain.py"", line 114, in <listcomp>
    rows, cols = [int(x) for x in line]
ValueError: invalid literal for int() with base 10: 'cc.ta.300.vec\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x
Running tagger in predict mode
Loading model from: saved_models/pos/ta_ttb_tagger.pt
Cannot load model from saved_models/pos/ta_ttb_tagger.pt
Traceback (most recent call last):
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/pos/trainer.py"", line 109, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""/usr/local/lib/python3.8/dist-packages/torch/serialization.py"", line 584, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""/usr/local/lib/python3.8/dist-packages/torch/serialization.py"", line 234, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""/usr/local/lib/python3.8/dist-packages/torch/serialization.py"", line 215, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/pos/ta_ttb_tagger.pt'
ta_ttb 84.48 --max_steps 100


 
",
813,2020-08-20T04:59:08Z,https://github.com/stanfordnlp/stanza/pull/446,,"Fix an apparent bug - change_lr is not defined in mwt, and BaseTrainer was not used as expected",
814,2020-08-20T04:17:29Z,https://github.com/stanfordnlp/stanza/issues/445,,"**Describe the bug**
I tried to train Stanza for Tamil, and mwt training always (tried with different data set) breaks at 33rd epoch.
Log:
2020-08-20 09:24:11: step 360/1100 (epoch 33/100), loss = 0.286464 (0.054 sec/batch), lr: 0.001000
Evaluating on dev set...
epoch 33: train_loss = 0.271402, dev_score = 0.9390
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 193, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/mwt_expander.py"", line 255, in <module>
    main()
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/mwt_expander.py"", line 89, in main
    train(args)
  File ""/home/sarves/Stanza/stanza-train-master/stanza/stanza/models/mwt_expander.py"", line 182, in train
    trainer.change_lr(current_lr)
AttributeError: 'Trainer' object has no attribute 'change_lr'
Running MWT expander in predict mode
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.
max_dec_len: 39
Loading data with batch size 50...
3 batches created.
Running the seq2seq model...
/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
",
815,2020-08-19T20:19:31Z,https://github.com/stanfordnlp/stanza/pull/444,,Document the use of the new semgrex CLI,
816,2020-08-19T06:48:31Z,https://github.com/stanfordnlp/stanza/pull/443,,,
817,2020-08-19T06:22:46Z,https://github.com/stanfordnlp/stanza/issues/442,,"Are there any plans to support TorchScript ?
```
# get params from .pt file
model = Parser(args, vocab, emb_matrix=emb_matrix)
torch.jit.script(model)
```
runs into trouble for multiple reasons: unsupported node, variant types, non TorchScript types ...",
818,2020-08-19T05:47:03Z,https://github.com/stanfordnlp/stanza/issues/441,,"Dear Stanza team,

I try to use OpenIE from Python code and as much as possible I want to stay with Stanford pieces of code.
I see that we can use the CoreNLP java functionality after installing it from Stanza. This is great, especially with automatic install available in v.1.1.

However, despite you introduced several examples of usage in the documentation, it seems to me that the OpenIE pipeline example is not available. I have read the CoreNLP document related to OpenIE, using Java, but I cannot find anything related to a direct usage of OpenIE from Stanza lib.

Is it possible to include such example in the documentation?

Thanks a lot

Best regards

Jerome",
819,2020-08-18T16:57:52Z,https://github.com/stanfordnlp/stanza/pull/440,,,
820,2020-08-18T16:57:37Z,https://github.com/stanfordnlp/stanza/pull/439,,,
821,2020-08-18T01:49:43Z,https://github.com/stanfordnlp/stanza/pull/438,,Try to avoid using Exception when a more descriptive error would apply,
822,2020-08-17T22:36:30Z,https://github.com/stanfordnlp/stanza/pull/437,,This is more descriptive than a default FileNotFoundError,
823,2020-08-16T07:25:35Z,https://github.com/stanfordnlp/stanza/issues/436,,"Using Stanza 1.1.1 and multiple Python 3.x versions and multiple IDEs on a Mac, when I try to start the Stanford CoreNLP server, it fails to start because the classpath in the java command has no quotes around it.

Steps to reproduce the behavior:

Running this Python code (from the Stanza tutorial) fails with an io error in the console.

```
from stanza.server import CoreNLPClient
text = ""Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.""
with CoreNLPClient(
        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
        timeout=30000,
        memory='16G') as client:
    ann = client.annotate(text)
```

My expectation is that the CoreNLP server would successfully start.


 - OS: MacOS 10.15.6
 - Python version: 3.85 (experienced with 3.6.x as well)
 - Stanza version: 1.1.1
 - IDE: IDLE (experienced with Spyder on MacOS and Ubuntu 18.04 as well)

An easy workaround is just to start the CoreNLP server from the command line, but having it all integrated would be nice.
",
824,2020-08-14T01:01:34Z,https://github.com/stanfordnlp/stanza/pull/435,,,
825,2020-08-13T20:15:59Z,https://github.com/stanfordnlp/stanza/pull/434,,,
826,2020-08-13T19:39:02Z,https://github.com/stanfordnlp/stanza/pull/433,,"…s run faster, especially on travis
",
827,2020-08-13T16:18:58Z,https://github.com/stanfordnlp/stanza/pull/432,,,
828,2020-08-13T09:10:20Z,https://github.com/stanfordnlp/stanza/issues/431,,"I try to run the sample Colab notebook provided in Stanza documentation but it failed on the stage of starting CoreNLP server.
[https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb](url)

The next error message was provided:

2020-08-13 08:45:50 INFO: Writing properties to tmp file: corenlp_server-2fbfaacb43214efe.props
2020-08-13 08:45:50 INFO: Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-2fbfaacb43214efe.props -annotators tokenize,ssplit,pos,lemma,ner -preload -outputFormat serialized
<stanza.server.client.CoreNLPClient object at 0x7f62ba9b3e80>
---------------------------------------------------------------------------
UnsupportedOperation                      Traceback (most recent call last)
<ipython-input-4-d27f9d625db1> in <module>()
      5 # Start the background server and wait for some time
      6 # Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed
----> 7 client.start()
      8 import time; time.sleep(10)

2 frames
/usr/local/lib/python3.6/dist-packages/stanza/server/client.py in start(self)
    146             self.server = subprocess.Popen(self.start_cmd,
    147                                            stderr=stderr,
--> 148                                            stdout=stderr)
    149 
    150     def atexit_kill(self):

/usr/lib/python3.6/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)
    685         (p2cread, p2cwrite,
    686          c2pread, c2pwrite,
--> 687          errread, errwrite) = self._get_handles(stdin, stdout, stderr)
    688 
    689         # We wrap OS handles *before* launching the child, otherwise a

/usr/lib/python3.6/subprocess.py in _get_handles(self, stdin, stdout, stderr)
   1202             else:
   1203                 # Assuming file-like object
-> 1204                 c2pwrite = stdout.fileno()
   1205 
   1206             if stderr is None:

UnsupportedOperation: fileno


**To Reproduce**

1. Go to [https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb](url)
2. Run cells to install Stanza and download CoreNLP library
3. Construct client using provided cell
4. Try to start the client using the provided cell
5. See error

**Expected behavior**
The client starts and the next cell can be run

**Environment (please complete the following information):**
 - OS: Colab environment Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic
 - Python version: Python 3.6.9
 - Stanza version:  stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)

",
829,2020-08-13T06:43:59Z,https://github.com/stanfordnlp/stanza/issues/430,,"If an entire directory is missing, stanza should be able to warn / remind the user about download(""lang"")",
830,2020-08-13T06:34:01Z,https://github.com/stanfordnlp/stanza/pull/429,,Merging the v1.1 website.,
831,2020-08-13T01:29:21Z,https://github.com/stanfordnlp/stanza/pull/428,,,
832,2020-08-13T00:35:59Z,https://github.com/stanfordnlp/stanza/pull/427,,Fixes a couple gimpy tests,
833,2020-08-12T14:29:42Z,https://github.com/stanfordnlp/stanza/issues/426,,"**Describe the bug**

I 'm trying to download a model using ```stanza.download(""el"")```. It fails with the following error: 

```
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 120kB [00:00, 597kB/s]
2020-08-12 17:36:49 INFO: Downloading default packages for language: el (Greek)...
....
....
....
requests.exceptions.ConnectionError: HTTPConnectionPool(host='nlp.stanford.edu', port=80): Max retries exceeded with url: /software/stanza/1.0.0/el/default.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f8052e0>: Failed to establish a new connection: [Errno 60] Operation timed out'))
```

**To Reproduce**
Steps to reproduce the behavior:
1. ```stanza.download(""el"")```
2. Run your script and it will fail with the error above.

Alternatively you can try to load `https://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip`
and the same will happen.

**Expected behavior**
I would expect the model to be downloaded successfully. 

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.8.5 from Anaconda
 - Stanza version: 1.0.1

**Additional context**
I'm in Cyprus trying to load this model and after I read that people in China had similar issues I used a VPN but the same problem persists. 

Is it possible that the resources server is down? I should also mention that in the last few days I could download models just fine so I'm suspecting this is a temp issue with the server.

Thanks",
834,2020-08-12T00:28:12Z,https://github.com/stanfordnlp/stanza/pull/425,,,
835,2020-08-11T22:00:35Z,https://github.com/stanfordnlp/stanza/pull/424,,,
836,2020-08-11T20:30:01Z,https://github.com/stanfordnlp/stanza/pull/423,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Desciption
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
837,2020-08-11T19:08:02Z,https://github.com/stanfordnlp/stanza/pull/422,,,
838,2020-08-11T19:01:39Z,https://github.com/stanfordnlp/stanza/pull/421,,Merge everything,
839,2020-08-11T18:48:55Z,https://github.com/stanfordnlp/stanza/pull/420,,Update version numbers in preparation for a new version,
840,2020-08-11T16:08:02Z,https://github.com/stanfordnlp/stanza/pull/419,,"…se some citations I guess
",
841,2020-08-11T16:06:22Z,https://github.com/stanfordnlp/stanza/pull/418,,,
842,2020-08-11T02:55:41Z,https://github.com/stanfordnlp/stanza/issues/417,,"PyTorch>1.5.0 has stopped supporting tensor division ( / ) and throws a runtime error for the following line
https://github.com/stanfordnlp/stanza/blob/6271245153dbf05302f9a087e09d5cd09f7aca32/stanza/models/common/beam.py#L84

Stacktrace
```

  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 176, in __call__

    self.process(doc)

  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 170, in process

    self.processors[processor_name].process(doc)

  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/lemma_processor.py"", line 66, in process

    ps, es = self.trainer.predict(b, self.config['beam_size'])

  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/models/lemma/trainer.py"", line 88, in predict

    preds, edit_logits = self.model.predict(src, src_mask, pos=pos, beam_size=beam_size)

  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/models/common/seq2seq_model.py"", line 206, in predict

    is_done = beam[b].advance(log_probs.data[b])

  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/models/common/beam.py"", line 84, in advance

    prevK = bestScoresId / numWords
RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
```",
843,2020-08-09T20:06:49Z,https://github.com/stanfordnlp/stanza/issues/416,,"Hi, 

I'm trying to train an ancient Greek model with word embeddings, since the model provided by stanza does not have word vectors. Everything works until the pos training when I get the following error:

(stanza) jacobo@ubunta:~/stanza-train/stanza$ bash scripts/run_pos.sh UD_Ancient_Greek-PROIEL --max_steps 2000
Using batch size 5000
Running tagger with --max_steps 2000...
Running tagger in train mode
Loading data with batch size 5000...
Reading pretrained vectors from ../data/wordvec/word2vec/Ancient_Greek/grc.vectors.xz...
Saved pretrained vocab and vectors to saved_models/pos/grc_proiel.pretrain.pt
38 batches created.
3 batches created.
Training tagger...
Evaluating the model every 100 steps...
2020-08-09 12:43:19: step 20/2000, loss = 6.612632 (10.024 sec/batch), lr: 0.003000
2020-08-09 12:47:02: step 40/2000, loss = 5.959401 (11.986 sec/batch), lr: 0.003000
2020-08-09 12:51:12: step 60/2000, loss = 4.654166 (11.980 sec/batch), lr: 0.003000
scripts/run_pos.sh: line 37:  7221 Killed                  python -m stanza.models.tagger --wordvec_dir $WORDVEC_DIR --train_file $train_file --eval_file $eval_file --output_file $output_file --batch_size $batch_size --gold_file $gold_file --lang $lang --shorthand $short --mode train $args
Running tagger in predict mode
Loading model from: saved_models/pos/grc_proiel_tagger.pt
Cannot load model from saved_models/pos/grc_proiel_tagger.pt
Traceback (most recent call last):
  File ""/home/jacobo/stanza-train/stanza/stanza/models/pos/trainer.py"", line 109, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""/home/jacobo/anaconda3/envs/stanza/lib/python3.7/site-packages/torch/serialization.py"", line 571, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""/home/jacobo/anaconda3/envs/stanza/lib/python3.7/site-packages/torch/serialization.py"", line 229, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""/home/jacobo/anaconda3/envs/stanza/lib/python3.7/site-packages/torch/serialization.py"", line 210, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/pos/grc_proiel_tagger.pt'
Traceback (most recent call last):
  File ""stanza/utils/conll18_ud_eval.py"", line 532, in <module>
    main()
  File ""stanza/utils/conll18_ud_eval.py"", line 500, in main
    evaluation = evaluate_wrapper(args)
  File ""stanza/utils/conll18_ud_eval.py"", line 483, in evaluate_wrapper
    system_ud = load_conllu_file(args.system_file)
  File ""stanza/utils/conll18_ud_eval.py"", line 477, in load_conllu_file
    _file = open(path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {}))
FileNotFoundError: [Errno 2] No such file or directory: '../data/processed/pos/grc_proiel.dev.pred.conllu'
grc_proiel --max_steps 2000

Why is grc_proiel_tagger.pt not being saved?

Thanks.",
844,2020-08-08T11:14:46Z,https://github.com/stanfordnlp/stanza/issues/415,,"Hi, I have a few general questions not related to the code itself per se, so I hope this is the right place to ask:

1. Which pretrained word embeddings were used to train the Lithuanian models? There is no Lithuanian folder in the _word-embeddings-conll17.tar_ file that all _word2vec_ embeddings are downloaded from with the _download_vectors.sh_ script, nor is Lithuanian on the _fasttext_ languages list. I'm trying to train my own model right now and I would like to use the same pretrained embeddings for the process.

2. Less of a question, more of an observation: [why is 'hse' the default package for Lithuanian if 'alksnis' performs better and was trained on a larger treebank](https://stanfordnlp.github.io/stanza/available_models.html)?

",
845,2020-08-06T21:06:22Z,https://github.com/stanfordnlp/stanza/pull/414,,Use pythainlp to re-chunk sentences which are poorly segmented in the BEST corpus.,
846,2020-08-06T20:54:00Z,https://github.com/stanfordnlp/stanza/issues/413,,"**Is your feature request related to a problem? Please describe.**
It is not good enough when starting CoreNLPClient. 
Now, we require user to keep only jar files in corenlp folder.
However, this solution is not optimal.

**Describe the solution you'd like**
Use glob to pick all jar files in corenlp folder.
glob is a part of the standard lib since python 2.

**Describe alternatives you've considered**
Nil

**Additional context**
References:
https://stackoverflow.com/a/29305488/8654623

My implementation:
https://github.com/chris4540/stanza/commit/4ea2833d45b7ffc4e217670ba4b44d58d69a577d
",
847,2020-08-06T13:04:32Z,https://github.com/stanfordnlp/stanza/issues/412,,"**Describe the bug**
I'm using Stanza for Arabic ('ar') POS/Lemma tagger and it works fine with most of the texts that I've tried,
but it throws a RuntimeError when I use some specific text which I couldn't put my hands on

**To Reproduce**
Steps to reproduce the behavior:
```
import stanza
stanza.download('ar')
nlp = stanza.Pipeline(lang='ar', use_gpu=True,processors='tokenize,mwt,pos,lemma',verbose=1)
nlp('ماحبيتها وكفى')
```

I get this error:
```
RuntimeError                              Traceback (most recent call last)
<ipython-input-76-c9d3d95f8f87> in <module>()
      1 # bd = nlp(body[347])
      2 # bd_norm = nlp(body_nm[502])
----> 3 nlp('ماحبيتها وكفى')

5 frames
/usr/local/lib/python3.6/dist-packages/stanza/pipeline/core.py in __call__(self, doc)
    174         assert any([isinstance(doc, str), isinstance(doc, list),
    175                     isinstance(doc, Document)]), 'input should be either str, list or Document'
--> 176         doc = self.process(doc)
    177         return doc
    178 

/usr/local/lib/python3.6/dist-packages/stanza/pipeline/core.py in process(self, doc)
    168         for processor_name in PIPELINE_NAMES:
    169             if self.processors.get(processor_name):
--> 170                 doc = self.processors[processor_name].process(doc)
    171         return doc
    172 

/usr/local/lib/python3.6/dist-packages/stanza/pipeline/mwt_processor.py in process(self, document)
     31                 preds = []
     32                 for i, b in enumerate(batch):
---> 33                     preds += self.trainer.predict(b)
     34 
     35                 if self.config.get('ensemble_dict', False):

/usr/local/lib/python3.6/dist-packages/stanza/models/mwt/trainer.py in predict(self, batch, unsort)
     77         self.model.eval()
     78         batch_size = src.size(0)
---> 79         preds, _ = self.model.predict(src, src_mask, self.args['beam_size'])
     80         pred_seqs = [self.vocab.unmap(ids) for ids in preds] # unmap to tokens
     81         pred_seqs = utils.prune_decoded_seqs(pred_seqs)

/usr/local/lib/python3.6/dist-packages/stanza/models/common/seq2seq_model.py in predict(self, src, src_mask, pos, beam_size)
    259             done = []
    260             for b in range(batch_size):
--> 261                 is_done = beam[b].advance(log_probs.data[b])
    262                 if is_done:
    263                     done += [b]

/usr/local/lib/python3.6/dist-packages/stanza/models/common/beam.py in advance(self, wordLk, copy_indices)
     82         # bestScoreId is the integer ids, and numWords is the integer length.
     83         # Need to do integer division
---> 84      prevK = bestScoresId / numWords
     85         self.prevKs.append(prevK)

RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
```

**Expected behavior**
Classification of line vocab

**Environment (please complete the following information):**
```
 - OS: [Chrome, Colab]
 - Python 3.6.9
 - stanza in /usr/local/lib/python3.6/dist-packages (1.0.1)
 - numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)
 - protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)
 - requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)
 - torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)
 - tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)
 - setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (49.2.0)
 - six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)
 - chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)
 - certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)
 - idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)
 - urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)
 - future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)
```
**Additional context**

",
848,2020-08-06T07:27:56Z,https://github.com/stanfordnlp/stanza/pull/411,,... sorry,
849,2020-08-06T01:48:14Z,https://github.com/stanfordnlp/stanza/issues/410,,"I got the following while using spacy_stanza, but I'm reasonably confident that it's due to stanza's models:

1. Not recognizing the appos in ""Alice, my mother-in-law, is the best cook ever"": 
```
(0, Alice, 'PROPN', 'NNP', 'vocative'),
 (1, ,, 'PUNCT', ',', 'punct'),
 (2, my, 'PRON', 'PRP$', 'nmod:poss'),
 (3, mother, 'NOUN', 'NN', 'nsubj'),
 (4, -, 'PUNCT', 'HYPH', 'punct'),
 (5, in, 'ADP', 'IN', 'case'),
 (6, -, 'PUNCT', 'HYPH', 'punct'),
 (7, law, 'NOUN', 'NN', 'nmod'),
 (8, ,, 'PUNCT', ',', 'punct'),
 (9, is, 'AUX', 'VBZ', 'cop'),
 (10, the, 'DET', 'DT', 'det'),
 (11, best, 'ADJ', 'JJS', 'amod'),
 (12, cook, 'NOUN', 'NN', 'root'),
 (13, ever, 'ADV', 'RB', 'advmod'),
 (14, ., 'PUNCT', '.', 'punct')]
```
even though it does recognize the appos when ""ever"" is removed. 

I was going to say that this probably is't a big deal since ""best * ever"" is slang, but I just noticed the same issue crops up with ""Maureen, my mother-in-law, is the best developer I've ever met."" But it works if it's ""mother"" instead of ""mother-in-law"".


2. The parser sometimes incorrectly takes punctuation like periods to be part of the token; e.g. in 
""Alice is Bob.""
```
 parse = sp(""Alice is Bob."") 
[(t.i, t, t.pos_, t.tag_, t.dep_) for t  in 
     ...: parse]                                      
Out[508]:                                             
[(0, Alice, 'PROPN', 'NNP', 'nsubj'),
 (1, is, 'AUX', 'VBZ', 'cop'),
 (2, Bob., 'PROPN', 'NNP', 'root')]
```",
850,2020-08-05T07:15:18Z,https://github.com/stanfordnlp/stanza/issues/409,,"Hi,
When run basic program for demo its crashed and shows Illegal instruction.

Please provides a solution for this issue. 
![stanza_demo](https://user-images.githubusercontent.com/25786085/89382293-3cf38280-d718-11ea-83ea-0ad0810630ed.png)
",
851,2020-08-04T23:58:02Z,https://github.com/stanfordnlp/stanza/pull/407,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Desciption
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
852,2020-08-04T23:05:39Z,https://github.com/stanfordnlp/stanza/pull/406,,Fix an integer division - torch 1.6 no longer allows / since it is ambiguous,
853,2020-08-04T23:04:49Z,https://github.com/stanfordnlp/stanza/pull/405,,Fix an integer division - torch 1.6 no longer allows / since it is ambiguous,
854,2020-08-04T17:13:44Z,https://github.com/stanfordnlp/stanza/issues/404,,"I'm using Stanza with the default UD model for Spanish ('ancora') and the POS tagger works fine with most of the texts that I've tried, but it throws an AssertionError every time the input contains certain words, such as ""encontrarse"". 

**To Reproduce**

When I run this minimal example:

```
import stanza

stanza.download('es')
text = ""encontrarse""
stanza_pipeline = stanza.Pipeline(lang='es', processors='tokenize, pos')
stanza_outcome = stanza_pipeline(text)
print(stanza_outcome)
```

I get the following error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/my_user/.local/share/virtualenvs/my_env/lib/python3.6/site-packages/stanza/pipeline/core.py"", line 176, in __call__
    doc = self.process(doc)
  File ""/home/my_user/.local/share/virtualenvs/my_env/lib/python3.6/site-packages/stanza/pipeline/core.py"", line 170, in process
    doc = self.processors[processor_name].process(doc)
  File ""/home/my_user/.local/share/virtualenvs/my_env/lib/python3.6/site-packages/stanza/pipeline/pos_processor.py"", line 34, in process
    preds = unsort(preds, batch.data_orig_idx)
  File ""/home/my_user/.local/share/virtualenvs/my_env/lib/python3.6/site-packages/stanza/models/common/utils.py"", line 196, in unsort
    assert len(sorted_list) == len(oidx), ""Number of list elements must match with original indices.""
AssertionError: Number of list elements must match with original indices.
```

**Expected behavior**

I was expecting this output:

```
[
  [
    {
      ""id"": ""1"",
      ""text"": ""encontrarse"",
      ""upos"": ""VERB"",
      ""xpos"": ""VERB"",
      ""feats"": ""VerbForm=Inf"",
      ""misc"": ""start_char=0|end_char=11""
    }
  ]
]
```

**Environment**

 - OS: Ubuntu 20.04
 - Python version: Python 3.6.7
 - Stanza version: 1.0.1

**Additional context**

This error only happens with certain words, it is not a general problem. For example, if you substitute `text='encontrarse' `by `text='encontrar'` in the example above, the error disappears.",
855,2020-08-04T07:02:35Z,https://github.com/stanfordnlp/stanza/pull/403,,,
856,2020-08-04T01:58:48Z,https://github.com/stanfordnlp/stanza/pull/402,,"A change to add an option after which long sentences are put in their own batches.  Stops a single long sentence from making a huge batch which can't be processed on even the biggest GPUs.
",
857,2020-08-03T23:25:16Z,https://github.com/stanfordnlp/stanza/pull/401,,,
858,2020-08-01T05:33:19Z,https://github.com/stanfordnlp/stanza/issues/400,,"For the sentence:  ""Try this wine and these snails."", I have generated two formats:

Penn Treebank format (I think?)
```
(VP (VP (VBZ Try) (NP (NP (DT this) (NN wine)) (CC and) (NP (DT these) (NNS snails)))) (PUNCT .))
```

and Universal Dependencies format (I also think?)
```
[
  [
    {
      ""id"": ""1"",
      ""text"": ""Try"",
      ""lemma"": ""Try"",
      ""upos"": ""PROPN"",
      ""xpos"": ""NNP"",
      ""head"": 2,
      ""deprel"": ""nmod"",
      ""misc"": ""start_char=0|end_char=3""
    },
    {
      ""id"": ""2"",
      ""text"": ""this"",
      ""lemma"": ""this"",
      ""upos"": ""PROPN"",
      ""xpos"": ""NNP"",
      ""head"": 6,
      ""deprel"": ""nmod"",
      ""misc"": ""start_char=4|end_char=8""
    },
    {
      ""id"": ""3"",
      ""text"": ""wine"",
      ""lemma"": ""wine"",
      ""upos"": ""PROPN"",
      ""xpos"": ""NNP"",
      ""head"": 6,
      ""deprel"": ""nmod"",
      ""misc"": ""start_char=9|end_char=13""
    },
    {
      ""id"": ""4"",
      ""text"": ""and"",
      ""lemma"": ""and"",
      ""upos"": ""PROPN"",
      ""xpos"": ""NNP"",
      ""head"": 6,
      ""deprel"": ""nmod"",
      ""misc"": ""start_char=14|end_char=17""
    },
    {
      ""id"": ""5"",
      ""text"": ""these"",
      ""lemma"": ""these"",
      ""upos"": ""PROPN"",
      ""xpos"": ""NNP"",
      ""head"": 6,
      ""deprel"": ""nmod"",
      ""misc"": ""start_char=18|end_char=23""
    },
    {
      ""id"": ""6"",
      ""text"": ""snails"",
      ""lemma"": ""snails"",
      ""upos"": ""PROPN"",
      ""xpos"": ""NNP"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=24|end_char=30""
    }
  ]
]
```

Is it possible to convert the Universal Dependencies format to the Penn Treebank format?  Is there a theoretical difficulty with this, or would it be straightforward?

I got the first representation from stanford-tregex-4.0.0/examples/atree from [here](https://nlp.stanford.edu/software/tregex.shtml#Download). The second is by using stanza as follows:

```python
import stanza
stanza.download('en')
nlp = stanza.Pipeline('en')
ud = nlp('Try this wine and these snails.')
print(ud)
```

",
859,2020-07-31T20:58:08Z,https://github.com/stanfordnlp/stanza/issues/399,,"Is it possible to use tregex to extract phrases from Japanese sentences?  For English, I use:

```python
# shell:  export CLASSPATH=$CLASSPATH:/path/to/CoreNLP/*:/path/to/CoreNLP/lib/*:

import os
from stanza.server import CoreNLPClient
annot = ['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse', 'depparse', 'coref']
cli = CoreNLPClient(classpath=os.environ['CLASSPATH'], annotators=annot, memory='2G')
# get all noun phrases
matches = cli.tregex(english_txt, 'NP')

# get noun phrases with tregex
# taken from https://github.com/stanfordnlp/stanza/issues/345#issue-635143711
def noun_phrases(_client, _text):
    pattern = 'NP'
    matches = _client.tregex(_text,pattern)
    for sentence in matches['sentences']:
        for match_id in sentence:
            print(sentence[match_id]['spanString'])

noun_phrases(cli, matches)
```
[match_processing_function](https://github.com/stanfordnlp/stanza/issues/345#issue-635143711)

However, that flow cannot be used with Japanese, can it?  For Japanese, I can at least  generate a UD structure `Document` object with:

```python
import stanza
stanza.download('ja')
nlp = stanza.Pipeline('ja')
japanese_txt = '...'
doc = nlp(japanese_txt)

# How to use doc to get tregex matches?
```

But, I don't know how to use `CoreNLPClient`'s tregex functionality on that.  Is there some way?

Thank you!",
860,2020-07-31T19:27:29Z,https://github.com/stanfordnlp/stanza/pull/398,,,
861,2020-07-30T23:20:15Z,https://github.com/stanfordnlp/stanza/pull/397,,Gets rid of some random warnings I noticed in the PR test scripts,
862,2020-07-30T06:51:58Z,https://github.com/stanfordnlp/stanza/pull/396,,"## Description
High level, the client interface had too many options and confusing logic about how properties are set. This pull request is an attempt to simplify things without losing much functionality. Also some misc. issues have been addressed, and new features in CoreNLP 4.1.0 can be used.

## Fixes Issues
- remove references to CoreNLP defaults on the client side, these
are mainly stored with CoreNLP (some exceptions include
what are valid CoreNLP languages and output formats)

- now both the client constructor and annotate() method use a
similar interface for determing properties…annotators and
output_format can override specified properties, get rid of
the properties_key arg

- related to that last point, the properties_cache has been removed,
it didn’t seem that useful

- added some basic validation (check that output format is valid)

- preload=True will tell CoreNLP to preload the default annotators,
4.1.0 can accomplish this with just the preload flag

- CoreNLP server can directly take a list of annotators (like the cl interface),
so some logic in the server for needlessly making temp files has
been removed

- non-English languages are built on top of CoreNLP English defaults...so the
German properties should lie on top of English defaults…if you start a French
server and then send a request with German properties, the German props
would lie on top of French defaults, which could cause some odd behavior…
that is German on top of French != German on top of English…the new
resetDefault option for the server allows to totally ignore server defaults,
so one can alternate between French and German requests (this wouldn’t
be an issue if one launches a default English server)

## Unit test coverage
Need to check server 

## Known breaking changes/behaviors
removes properties_cache, removes properties_key arg from annotate() method, requires CoreNLP 4.1.0
",
863,2020-07-29T19:14:36Z,https://github.com/stanfordnlp/stanza/pull/395,,"lemma should require pos (according to the documentation, at least)",
864,2020-07-29T11:44:48Z,https://github.com/stanfordnlp/stanza/issues/394,,"Some Arabic characters are causing the stanza model to break

for example ""وجيييه""

`import stanza

_tagger = stanza.Pipeline(lang=""ar"", processors='tokenize,pos', tokenize_no_ssplit=True)

text = ""وجيييه""

doc = _tagger(text)

print(doc)
`

but I get This error

`---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-13-f866b6280d67> in <module>()
      1 text = ""وجيييه""
----> 2 doc = _tagger(text)
      3 print(doc)

3 frames
/usr/local/lib/python3.6/dist-packages/stanza/pipeline/core.py in __call__(self, doc)
    174         assert any([isinstance(doc, str), isinstance(doc, list),
    175                     isinstance(doc, Document)]), 'input should be either str, list or Document'
--> 176         doc = self.process(doc)
    177         return doc
    178 

/usr/local/lib/python3.6/dist-packages/stanza/pipeline/core.py in process(self, doc)
    168         for processor_name in PIPELINE_NAMES:
    169             if self.processors.get(processor_name):
--> 170                 doc = self.processors[processor_name].process(doc)
    171         return doc
    172 

/usr/local/lib/python3.6/dist-packages/stanza/pipeline/pos_processor.py in process(self, document)
     32         for i, b in enumerate(batch):
     33             preds += self.trainer.predict(b)
---> 34         preds = unsort(preds, batch.data_orig_idx)
     35         batch.doc.set([doc.UPOS, doc.XPOS, doc.FEATS], [y for x in preds for y in x])
     36         return batch.doc

/usr/local/lib/python3.6/dist-packages/stanza/models/common/utils.py in unsort(sorted_list, oidx)
    194     Unsort a sorted list, based on the original idx.
    195     """"""
--> 196     assert len(sorted_list) == len(oidx), ""Number of list elements must match with original indices.""
    197     _, unsorted = [list(t) for t in zip(*sorted(zip(oidx, sorted_list)))]
    198     return unsorted

AssertionError: Number of list elements must match with original indices.` ",
865,2020-07-29T07:07:59Z,https://github.com/stanfordnlp/stanza/pull/393,,"## Description
High level, the client interface had too many options and confusing logic about how properties are set. This pull request is an attempt to simplify things without losing much functionality. Also some misc. issues have been addressed, and new features in CoreNLP 4.1.0 can be used.

## Fixes Issues
- remove references to CoreNLP defaults on the client side, these
  are mainly stored with CoreNLP (some exceptions include
  what are valid CoreNLP languages and output formats)

- now both the client constructor and annotate() method use a
  similar interface for determing properties…annotators and
  output_format can override specified properties, get rid of
  the properties_key arg

- related to that last point, the properties_cache has been removed,
  it didn’t seem that useful

- added some basic validation (check that output format is valid)

- preload=True will tell CoreNLP to preload the default annotators,
  4.1.0 can accomplish this with just the preload flag

- CoreNLP server can directly take a list of annotators (like the cl interface),
  so some logic in the server for needlessly making temp files has
  been removed

- non-English languages are built on top of CoreNLP English defaults...so the
  German properties should lie on top of English defaults…if you start a French
  server and then send a request with German properties, the German props
  would lie on top of French defaults, which could cause some odd behavior…
  that is German on top of French != German on top of English…the new
  resetDefault option for the server allows to totally ignore server defaults,
  so one can alternate between French and German requests (this wouldn’t
  be an issue if one launches a default English server)

## Unit test coverage
The server tests should be passing with this new code.

## Known breaking changes/behaviors
The properties_cache has been eliminated and the properties_key arg to annotate() removed.
",
866,2020-07-29T01:05:34Z,https://github.com/stanfordnlp/stanza/pull/392,,"Add an interface to the shiny new semgrex command line interface in corenlp 4.1.0.  Can pass in a whole document and process multiple semgrex queries at once.  A similar extension will be added to the server in the next version, but it is not necessary to have the server running in order to invoke the semgrex CLI",
867,2020-07-28T23:08:22Z,https://github.com/stanfordnlp/stanza/pull/391,,Update tests to accommodate 4.1.0,
868,2020-07-28T11:04:57Z,https://github.com/stanfordnlp/stanza/issues/390,,"we can use stanza_nlp = stanza.Pipeline('en',use_gpu=True) to use GPU or not. In the lab server environment, we may want to assign the module to run on a assigned gpu such as cuda:1. how to achieve that?

 ",
869,2020-07-28T03:39:47Z,https://github.com/stanfordnlp/stanza/pull/389,,,
870,2020-07-27T13:18:34Z,https://github.com/stanfordnlp/stanza/issues/388,,"**Describe the bug**
Stanza won't find the GPU. It'll always start in CPU mode.

**Environment (please complete the following information):**
 - OS: Ubuntu 18.04.4 LTS
 - Python version: Tried with 3.8 and 3.7 from Anaconda (3.8 I installed Stanza via pip, 3.7 via conda)
 - Stanza version: 1.0.1

**Additional context**
PyTorch, CuDNN, etc, do get installed with the GPU.

GTX 1070",
871,2020-07-23T17:42:43Z,https://github.com/stanfordnlp/stanza/issues/387,,"A long sentence which can be processed by itself can crash the system if included in a large document.

For example, you can get the sentence titled ""David Foster Wallace, “The Pale King.” 1185 words.""

https://thejohnfox.com/long-sentences/

By itself, stanza can parse this sentence:

```
import stanza

text = open(""long.txt"").readlines()
text = "" "".join(text).strip()

nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')

doc = nlp(text)
```

If you put ""Unban Mox Opal!"" 49 times at the end of the file, it runs out of GPU memory on a 2080ti:

```
Traceback (most recent call last):
  File ""simple_pipeline.py"", line 12, in <module>
    doc = nlp(text)
  File ""/home/john/stanza/stanza/pipeline/core.py"", line 166, in __call__
    doc = self.process(doc)
  File ""/home/john/stanza/stanza/pipeline/core.py"", line 160, in process
    doc = self.processors[processor_name].process(doc)
  File ""/home/john/stanza/stanza/pipeline/depparse_processor.py"", line 43, in process
    preds += self.trainer.predict(b)
  File ""/home/john/stanza/stanza/models/depparse/trainer.py"", line 74, in predict
    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)
  File ""/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/john/stanza/stanza/models/depparse/model.py"", line 137, in forward
    deprel_scores = self.deprel(self.drop(lstm_outputs), self.drop(lstm_outputs))
  File ""/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/john/stanza/stanza/models/common/biaffine.py"", line 74, in forward
    return self.scorer(self.dropout(self.hidden_func(self.W1(input1))), self.dropout(self.hidden_func(self.W2(input2))))
  File ""/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/john/stanza/stanza/models/common/biaffine.py"", line 59, in forward
    return self.W_bilin(input1, input2)
  File ""/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/john/stanza/stanza/models/common/biaffine.py"", line 29, in forward
    output = intermediate.view(input1_size[0], input1_size[1] * self.output_size, input2_size[2]).bmm(input2)
RuntimeError: CUDA out of memory. Tried to allocate 16.20 GiB (GPU 0; 10.76 GiB total capacity; 7.74 GiB already allocated; 2.03 GiB free; 82.58 MiB cached)
```

I suspect the issue is the batch size results in a large block of pad words and a batch of size 1200x50.
",
872,2020-07-23T09:24:55Z,https://github.com/stanfordnlp/stanza/issues/386,,"I am using Stanford CoreNLP with Python, and I want to split a document into separate sentences. I have found Stanza (in Java) provides a `ssplit` method to realize this, but I can't find a counterpart in CoreNLP in Python. Is there any way to solve this? Many thanks.",
873,2020-07-22T02:19:14Z,https://github.com/stanfordnlp/stanza/issues/385,,"Hi, I'm utilizing stanza in the machine translation scenario. As I tokenize the raw sentences in the training set, while inference, the generated translations are also tokenized. Then how to detokenize them using stanza?",
874,2020-07-18T23:06:55Z,https://github.com/stanfordnlp/stanza/issues/384,,"**Main Problem**: when I run the following code for German sentences, my `sentence.parseTree` (which contains the constituents of the sentence) is empty. My `sentence` is not empty, but apparently it doesn't have the constituency tree. 

I downloaded the German model from https://nlp.stanford.edu/software/lex-parser.html where shift-reduce models should be included.

**Code:**
```
!pip install stanza
!unset http_proxy
!unset https_proxy

import os
os.environ[""CORENLP_HOME""] = ""/home/jovyan/work/stanford-corenlp-4.0.0""
from stanza.server import CoreNLPClient

text = ""Guten Morgen.""
with CoreNLPClient(
        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
        timeout=30000,
        memory='10G',
        properties='german',
        be_quiet=False) as client:
    ann = client.annotate(text)
sentence = ann.sentence[0]

# get the constituency parse of the first sentence
constituency_parse = sentence.parseTree
print(sentence)
print(constituency_parse)
```

**Output:**
```
Using Stanford CoreNLP default properties for: german.  Make sure to have german models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH
Setting server defaults from: StanfordCoreNLP-german.properties
Warning: Server defaults being set server side, ignoring annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse', 'depparse', 'coref']
Starting server with command: java -Xmx10G -cp /home/jovyan/work/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties StanfordCoreNLP-german.properties -preload tokenize,ssplit,pos,ner,parse
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - Warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - Setting default constituency parser to PCFG parser: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz
[main] INFO CoreNLP - To use shift reduce parser download English models jar from:
[main] INFO CoreNLP - https://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 5
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/german-ud.tagger ... done [5.0 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/german.distsim.crf.ser.gz ... done [3.1 sec].
[main] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - Using numeric classifiers: false
[main] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - Using SUTime: false
[main] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - Using fine grained: false
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/srparser/germanSR.beam.ser.gz ... done [13.2 sec].
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0.0.0.0:9000
[pool-1-thread-3] INFO CoreNLP - [/127.0.0.1:51218] API call w/annotators tokenize,ssplit,mwt,pos,depparse,lemma,ner
Guten Morgen.
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mwt
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/UD_German.gz ... 
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Done reading from disk ... Time elapsed: 2.5 sec
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99984, Elapsed Time: 16.529 (s)
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [19.0 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.

token {
  word: ""Guten""
  pos: ""ADJ""
  value: ""Guten""
  before: """"
  after: "" ""
  originalText: ""Guten""
  ner: ""O""
  lemma: ""guten""
  beginChar: 0
  endChar: 5
  tokenBeginIndex: 0
  tokenEndIndex: 1
  hasXmlContext: false
  isNewline: false
  coarseNER: ""O""
  nerLabelProbs: ""O=0.9998751755314648""
  isMWT: false
  isFirstMWT: false
}
token {
  word: ""Morgen""
  pos: ""NOUN""
  value: ""Morgen""
  before: "" ""
  after: """"
  originalText: ""Morgen""
  ner: ""O""
  lemma: ""morgen""
  beginChar: 6
  endChar: 12
  tokenBeginIndex: 1
  tokenEndIndex: 2
  hasXmlContext: false
  isNewline: false
  coarseNER: ""O""
  nerLabelProbs: ""O=0.9998345108219487""
  isMWT: false
  isFirstMWT: false
}
token {
  word: "".""
  pos: ""PUNCT""
  value: "".""
  before: """"
  after: """"
  originalText: "".""
  ner: ""O""
  lemma: "".""
  beginChar: 12
  endChar: 13
  tokenBeginIndex: 2
  tokenEndIndex: 3
  hasXmlContext: false
  isNewline: false
  coarseNER: ""O""
  nerLabelProbs: ""O=0.9999999981065812""
  isMWT: false
  isFirstMWT: false
}
tokenOffsetBegin: 0
tokenOffsetEnd: 3
sentenceIndex: 0
characterOffsetBegin: 0
characterOffsetEnd: 13
basicDependencies {
  node {
    sentenceIndex: 0
    index: 1
  }
  node {
    sentenceIndex: 0
    index: 2
  }
  node {
    sentenceIndex: 0
    index: 3
  }
  edge {
    source: 2
    target: 1
    dep: ""amod""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  edge {
    source: 2
    target: 3
    dep: ""punct""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  root: 2
}
collapsedDependencies {
  node {
    sentenceIndex: 0
    index: 1
  }
  node {
    sentenceIndex: 0
    index: 2
  }
  node {
    sentenceIndex: 0
    index: 3
  }
  edge {
    source: 2
    target: 1
    dep: ""amod""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  edge {
    source: 2
    target: 3
    dep: ""punct""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  root: 2
}
collapsedCCProcessedDependencies {
  node {
    sentenceIndex: 0
    index: 1
  }
  node {
    sentenceIndex: 0
    index: 2
  }
  node {
    sentenceIndex: 0
    index: 3
  }
  edge {
    source: 2
    target: 1
    dep: ""amod""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  edge {
    source: 2
    target: 3
    dep: ""punct""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  root: 2
}
enhancedDependencies {
  node {
    sentenceIndex: 0
    index: 1
  }
  node {
    sentenceIndex: 0
    index: 2
  }
  node {
    sentenceIndex: 0
    index: 3
  }
  edge {
    source: 2
    target: 1
    dep: ""amod""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  edge {
    source: 2
    target: 3
    dep: ""punct""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  root: 2
}
enhancedPlusPlusDependencies {
  node {
    sentenceIndex: 0
    index: 1
  }
  node {
    sentenceIndex: 0
    index: 2
  }
  node {
    sentenceIndex: 0
    index: 3
  }
  edge {
    source: 2
    target: 1
    dep: ""amod""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  edge {
    source: 2
    target: 3
    dep: ""punct""
    isExtra: false
    sourceCopy: 0
    targetCopy: 0
    language: UniversalEnglish
  }
  root: 2
}
hasRelationAnnotations: false
hasNumerizedTokensAnnotation: false
hasEntityMentionsAnnotation: true


```",
875,2020-07-18T03:44:42Z,https://github.com/stanfordnlp/stanza/issues/383,,"Hi, 
I tried to run a file in SQLova, but it gives the following error. 

Traceback (most recent call last):
  File ""annotate_ws.py"", line 8, in <module>
    from stanza.nlp.corenlp import CoreNLPClient
ModuleNotFoundError: No module named 'stanza.nlp'

I installed stanza 1.0.1 and tried to google this issue but no one seems to have the same problem. Thank you! ",
876,2020-07-17T09:39:22Z,https://github.com/stanfordnlp/stanza/issues/382,,"I am using Stanza for sentence segmentation. I faced a new issue where I need to segment bullet/numbered lists into sentences. However I see that that entire list is being segmented into a single sentence. 

The text I need to process is

```
The vector space model has the following limitations:

    * Search keywords must precisely match document terms; word substrings might result in a ""false positive match"";
    * Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a ""false negative match"";
    * The order in which the terms appear in the document is lost in the vector space representation;
    * Long documents are poorly represented because they have poor similarity values (a small scalar product and a large dimensionality).

```
I want each item in the list to be a individual sentence.

Is there any method to perform such segmentation. Thanks in advance!!!",
877,2020-07-16T18:36:31Z,https://github.com/stanfordnlp/stanza/issues/381,,"The English language tokenizer splits the following sentence into two:
`While at work today Cooper realized he didn't have his laptop.`
It appears to have problems with the uppercased name because the issue goes away when ""Cooper"" is lowercased.

**To reproduce**
```
p = stanza.Pipeline(lang='en', processors='tokenize')
doc = p(""While at work today Cooper realized he didn't have his laptop."")
assert len(doc.sentences) == 1
```

**Expected behavior**
The document should contain a single sentence.

**Environment (please complete the following information):**
 - MacOS 10.14.4 with Python 3.7.5
and
 - Linux 6ec5ff8bcbe3 4.19.76-linuxkit #1 SMP x86_64 GNU/Linux with Python 3.7.7 (from pytorch/pytorch:1.5.1-cuda10.1-cudnn7-runtime docker image)  
 - Both using Stanza version: 1.0.1
",
878,2020-07-15T20:18:24Z,https://github.com/stanfordnlp/stanza/pull/380,,"## Desciption
This PR improves the robustness of the tokenizer in noisy real-world data that often contain multiple consecutive whitespaces.

## Fixes Issues
N/A

## Unit test coverage
New unittest added in test_tokenizer

## Known breaking changes/behaviors
Not that I'm aware of
",
879,2020-07-13T21:49:36Z,https://github.com/stanfordnlp/stanza/issues/379,,"I tried to install Stanza from Conda, Pip, and source codes but in any case I get the below error when calling *stanza.download('en')*.  

    Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB 
    [00:00, 9.81MB/s]
    2020-07-13 17:38:01 INFO: Downloading default packages for language: en (English)...
    Downloading http://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip: 100%|█
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/home/----/anaconda3/lib/python3.7/site-packages/stanza/utils/resources.py"", line 236, in download
        request_file(f'{url}/{__resources_version__}/{lang}/default.zip', os.path.join(dir, lang, f'default.zip'), md5=resources[lang] ['default_md5'])
      File ""/home/----/anaconda3/lib/python3.7/site-packages/stanza/utils/resources.py"", line 84, in request_file
        assert(not md5 or is_file_existed(path, md5))
    AssertionError

I did not get any error when downloading other languages, e.g. ja.    
I have tried Both MacOS 10.14.6 and Ubuntu 18.04.4 and the problem persisted.  
The python version I use is python 3.7.6.",
880,2020-07-13T20:27:00Z,https://github.com/stanfordnlp/stanza/issues/378,,"Hi, I'm trying to use Stanza with portuguese model (pt), at some texts extracted from pdf. But it's kinda specific domain text, with some abbreviations that Stanza can't handle and sometimes breaks the text in wrong way.

What could be a good way to performe well with abbreviations? Re-training the model could be an option.
Thank you!",
881,2020-07-11T10:47:04Z,https://github.com/stanfordnlp/stanza/issues/377,,"I am new to stanza and would like to make use of this wonderful implementation for my research. I see great value in contextually-keyed word vectors that explosion's sense2vec python implementation provides. Given the greater accuracy of stanza, I am keen to use stanza instead of spacy + sense2vec. Is it possible for me to create glove (or word2vec) embeddings of latest Wikipedia corpus with pos and ner tags, and then use these word embeddings to make queries such as return all words that are most similar to 'Elon_Musk|PERSON', 'University_of_Michigan|ORGANIZATION', or 'London|PROPN'?

Thank you!
sbs",
882,2020-07-09T14:51:45Z,https://github.com/stanfordnlp/stanza/issues/376,,"Hello, i have a problem when parallelizing stanza processes with joblib.

What am I doing:

```
processed_list = Parallel(n_jobs=num_cores, batch_size=round(len(texts)/num_cores), verbose=20)(
                    delayed(self.tokenize)(i) for i in inputs)

def tokenize(self, document):

        document = document.strip()

        if len(document) < 2:
            return stanza.models.common.doc.Document([])

        return(self.nlp(document))
```

Both methods are within a class. 

What is the problem?

1. Stanza needs to use torch.save in order to serialize things
2. I can set a pickler for joblib with set_loky_pickler()
3. torch.save is not compatible with that and thus
4. Stanza is incompatible with joblib

Main question: How can i parallelize CPU processing with Stanza?",
883,2020-07-08T08:38:08Z,https://github.com/stanfordnlp/stanza/pull/375,,script for processing BEST as well,
884,2020-07-08T00:01:57Z,https://github.com/stanfordnlp/stanza/pull/374,,Some minor tokenizer changes - compiling the regexes speeds up about 10%,
885,2020-07-08T00:01:21Z,https://github.com/stanfordnlp/stanza/pull/373,,Allow specifying a different branch for downloading the resources file,
886,2020-07-07T18:05:09Z,https://github.com/stanfordnlp/stanza/pull/372,,"## Desciption
This is an attempt at reducing the string building and regex overhead in finding character offsets.

## Fixes Issues
N/A

## Unit test coverage
Covered by existing unit tests.

## Known breaking changes/behaviors
N/A
",
887,2020-07-02T14:50:39Z,https://github.com/stanfordnlp/stanza/issues/371,,"hi, i'm working in pytjon 3.7, with the CoreNlpClient, trying to do a token regex
i have a text like 
other text NOT NAMES wordx xxxxx asociates:  wordx NAME NAME NAME wordx NAME NAME NAME wordx NAME NAME NAME wordx 
wordx is actually wordx .
want to extract the names
is i use somthing like (/[A-Z]*/{2,5} wordx) it works.. but it also matches NOT NAMES
if i try to use ( wordx /[A-Z]*/{2,5} wordx) it skip the middle one. I looked for this and its seems i need to user overlapping group match.. i have searched and its looks that i should use lookahead or lookbehind ( or boths).

i have search and tryed.. but i cant make lookahead works... it DO works??

any clues?
thanks

PD: i have tryed ner:Person.. but it work for only half of the names and sometimes it recog wordx as a person

",
888,2020-07-02T12:42:09Z,https://github.com/stanfordnlp/stanza/issues/370,,"I have my french and english sentences already tokenized using moses tokenizer. Is it appropriate if I remove the  `tokenize` in the annotators for the coreNLPClient?

```
with CoreNLPClient(annotators='**tokenize**,ssplit,pos,lemma,parse', timeout=30000, memory='16G', be_quiet=False) as client:

# French properties for 4.0.0
french_properties = {""annotators"": ""**tokenize**,ssplit,mwt,pos,lemma,parse"", ""tokenize.language"":""fr"",
                     ""mwt.mappingFile"": ""edu/stanford/nlp/models/mwt/french/french-mwt.tsv"",
                     ""mwt.pos.model"": ""edu/stanford/nlp/models/mwt/french/french-mwt.tagger"",
                     ""mwt.statisticalMappingFile"": ""edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv"",
                     ""mwt.preserveCasing"": ""false"", ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/french-ud.tagger"",
                     ""parse.model"": ""edu/stanford/nlp/models/srparser/frenchSR.beam.ser.gz""}```",
889,2020-07-02T10:22:42Z,https://github.com/stanfordnlp/stanza/issues/369,,"Hello,
I have trained my own NER model for the Italian language, using charlm as well. 

I train the charlm models (using some toy raw text file) using the commands:
bash scripts/run_charlm.sh English-TEST forward --epochs 2 --cutoff 0 --batch_size 2
bash scripts/run_charlm.sh English-TEST backward --epochs 2 --cutoff 0 --batch_size 2
It successfully create the two charlm models.

I then run the NER training (using charlm), using the following command:
bash scripts/run_ner.sh Italian-TEST --max_steps 500 --charlm --charlm_shorthand it_test --char_hidden_dim 1024
(I removed --word_emb_dim 5 option since I guess I don't want the number of dimensions to be limited to 5)
It successfully creates a NER model.

I then try to use the model with the following code:

nlp = stanza.Pipeline(lang='it', processors='tokenize,ner', ner_model_path='saved_models/ner/it_test_nertagger.pt', ner_forward_charlm_path=""saved_models/charlm/it_test_forward_charlm.pt"", ner_backward_charlm_path=""saved_models/charlm/it_test_backward_charlm.pt"")
doc = nlp(""Lorella Viola e Antonio Fiscarelli sono ricercatori del C2DH all' Università del Lussemburgo"")
print(*[f'entity: {ent.text}\ttype: {ent.type}' for ent in doc.ents], sep='\n')
print(doc.ents)

but doc.ents is an empty list.
How can I fix this?

Thanks in advance for your help.",
890,2020-06-30T19:55:31Z,https://github.com/stanfordnlp/stanza/pull/368,,"A bunch of minor changes and a script for processing Orchid, a Thai dataset",
891,2020-06-30T16:01:08Z,https://github.com/stanfordnlp/stanza/issues/367,,"Can I run Stanza **inside docker container**?
I Created a container, installed all the dependencies, when the interpreter reaches the call `[word.lemma for sent in doc_stanza.sentences for word in sent.words]` the program just freezes without errors.",
892,2020-06-29T06:29:25Z,https://github.com/stanfordnlp/stanza/pull/366,,"Solution for #361 (to_dict() does not support NER).

Now NER tag will be propagated from Token to Word if the token is a single-word token.

Add two methods to serialize the document and deserialize the document. (Thanks @greasystrangler!)
",
893,2020-06-27T23:07:36Z,https://github.com/stanfordnlp/stanza/pull/365,,"## Desciption
This PR adds [SudachiPy](https://github.com/WorksApplications/SudachiPy) as an external tokenizer variant for the Japanese pipeline. Use it with `stanza.Pipeline('ja', processors={'tokenize': 'sudachipy'})`.

## Known breaking changes/behaviors
No. Although using this external tokenizer requires installing Python packages `sudachipy` and `sudachidict_core`.
",
894,2020-06-23T01:31:18Z,https://github.com/stanfordnlp/stanza/pull/364,,"Add the ability to train an embedding specific for the classifier model being trained.  Either CONCAT or SUM is available.  Also, some other minor edits.",
895,2020-06-22T22:54:27Z,https://github.com/stanfordnlp/stanza/pull/363,,"## Description
There are two main changes in this PR:
- A new function for installing CoreNLP automatically; used with `stanza.install_corenlp()`;
- All resource-related scripts are now moved to a designated folder `stanza/resource` to be separated from various utility scripts.

## Known breaking changes/behaviors
Nothing user-facing breaks.
",
896,2020-06-22T20:38:02Z,https://github.com/stanfordnlp/stanza/issues/362,,"Hi,
I'm able to use stanza to extract NER,POS and dependency tree from Arabic text. 
I know that coreference resolution is based on stanford CORENLP.  I downloaded stanford CORENLP 4.0.0 , unzip it and set the CORENLP_HOME path. I also download the Arabic model from here https://stanfordnlp.github.io/CoreNLP/
NOTE: i want to use the coreference resolution feature only.
What should i do next to run the server !
 ",
897,2020-06-22T11:04:24Z,https://github.com/stanfordnlp/stanza/issues/361,,"When using the to_dict function I would expect the resulting list of dicts to include NER information but they do not. Therefore when a stanza Document is created using the output from to_dict all NER data is lost from the reconstituted document.

I think the issue is in the  token.to_dict() function of the Document class https://github.com/stanfordnlp/stanza/blob/ebb374b76b6be18799f3f89d799cc7788f033343/stanza/models/common/doc.py#L549-L562

Unless the token is MWT (which is never true for English) then token.to_dict() simply calls word.to_dict() which does not include NER information.

**To Reproduce**
The following short code will quickly reproduce the behaviour:

```python
import stanza
text = '''Here is a small sample showing that Stanza, a great NLP tool, does not retain NER information in the data generated using ""to_dict()""'''
nlp = stanza.Pipeline('en')

doc1 = nlp(text)
print ('Doc1 ents:',', '.join(e.text for e in doc1.entities))

doc2 = stanza.Document(doc1.to_dict())  # Export doc1 and create doc2 from the imported list of dicts
print ('Doc2 ents:',', '.join(e.text for e in doc2.entities))

```

Output from above script:

```
2020-06-22 11:59:57 INFO: Loading these models for language: en (English):
=========================
| Processor | Package   |
-------------------------
| tokenize  | ewt       |
| pos       | ewt       |
| lemma     | ewt       |
| depparse  | ewt       |
| ner       | ontonotes |
=========================

2020-06-22 11:59:57 INFO: Use device: cpu
2020-06-22 11:59:57 INFO: Loading: tokenize
2020-06-22 11:59:57 INFO: Loading: pos
2020-06-22 11:59:57 INFO: Loading: lemma
2020-06-22 11:59:57 INFO: Loading: depparse
2020-06-22 11:59:58 INFO: Loading: ner
2020-06-22 11:59:59 INFO: Done loading processors!
Doc1 ents: Stanza, NLP, NER
Doc2 ents: 

Process finished with exit code 0
```

**Expected behaviour**
I would expect the output of doc.to_dict() and token.to_dict() to include NER data.

**Environment:**
 - OS: Linux
 - Python version:3.6.10
 - Stanza version: 1.0.1 (git)

",
898,2020-06-21T14:23:56Z,https://github.com/stanfordnlp/stanza/issues/360,,"Hi!

Will Stanza support fine tuned sentiment analysis similar to what is in CoreNLP (positive, negative, neutral)?

Edit: I found similar questions asking about sentiment analysis in the issue tracker. I see Stanza does not support it yet (I hope it will soon). Anyways, thanks for the library.

",
899,2020-06-20T10:02:07Z,https://github.com/stanfordnlp/stanza/issues/358,,"**Describe the bug**

The Spanish default tokenizer (_ancora_ ?) doesn't recognize **both** unicode quotes  `“` and `”` (with unicode code points 8220 and 8221)

**To Reproduce**
```
import stanza
stanza.download('es')
nlp = stanza.Pipeline(lang='es', processors='tokenize')
doc = nlp(''' “maestro” ''')
doc.sentences[0]
```
Got 
```
[
  {
    ""id"": ""1"",
    ""text"": ""“"",
    ""misc"": ""start_char=1|end_char=2""
  },
  {
    ""id"": ""2"",
    ""text"": ""maestro”"",
    ""misc"": ""start_char=2|end_char=10""
  }
]
```

Notice that the opening quote is recognized, but not the closing one.


**Expected behavior**

Either complaining that `“` and `”` are not valid characters, or accepting both quotes.

**Environment (please complete the following information):**
 - OS: MacOS 10.15.5
 - Python version: 3.7.7
 - Stanza version: 1.0.1",
900,2020-06-19T04:18:53Z,https://github.com/stanfordnlp/stanza/pull/357,,"## Desciption
Use `Tuple[int]` for token indices and `int` for word indices. 

When the token is a multi-word token, the tuple contains two elements that are the starting and ending indices (inclusive) in the original CoNLL-U format. Otherwise it's a single value tuple with the token index.

Word indices used to be string representations of single integers, so effectively this is just converting them into integers.

## Fixes Issues
#221 

## Unit test coverage
Existing unit tests should already cover almost all cases.

## Known breaking changes/behaviors
Breaking change: token and word indices now have new data types. Downstream applications will need to adjust accordingly.
",
901,2020-06-19T00:41:18Z,https://github.com/stanfordnlp/stanza/pull/356,,"A new argument `resources_url` for the download function is added, which supports customized URLs for locating the resource file. It also supports shortcut names. For now ""default"" is mapped to the default github raw URL, ""stanford"" or ""stanfordnlp"" will be mapped to a stanfordnlp server address. If a shortcut mapping is not found, it'll treat the input as a URL.

This will make it easy for users who do not have easy access to the github raw URLs (such as users in China) to download the resource file.",
902,2020-06-17T19:06:16Z,https://github.com/stanfordnlp/stanza/pull/355,,,
903,2020-06-17T00:24:25Z,https://github.com/stanfordnlp/stanza/pull/354,,"Improve documentation of the classifier

(the original intended change wound up not helping)",
904,2020-06-15T19:04:16Z,https://github.com/stanfordnlp/stanza/issues/353,,"Hi

First well done with Stanza; it's very impressive and I'll be watching it closely.

I am trying to figure out the best way to save a Stanza document for later loading and further processing.

Using the CoreNLP client is is possible to write a PB2 document using writeToSerializedString and possible to read using readFromSerializedString.

Is there an equivalent for Stanza native Documents? I even played with repr/eval but the eval creates a bunch of lists rather than a Stanza Document object.

What is the best way to dump the annotated document out where I need to reload it later?

thanks!

GS",
905,2020-06-15T18:12:35Z,https://github.com/stanfordnlp/stanza/pull/352,,Creating fewer tensor objects significantly reduces the time per iteration.  ,
906,2020-06-15T14:25:23Z,https://github.com/stanfordnlp/stanza/pull/351,,"## Desciption
With the current code it is possible to train NER model end to end. But in cases where data sets are limited and there is a need to train NER with custom classes, Transfer learning may come very handy. As was in my case.

I have patched current stanza code to allow for that with a bit manipulations on model classifier.

**Summary of modifications:**
1) 2 flags were added to ner_tagger.py
2) minor update was done to DataLoader and NERTagger classes - only necessary object properties are passed to constructors instead of full objects themselves - simplifies using those objects in other contexts

**Approach to inserting a new classifier**
My assumption for TL process was following:

Whoevere to use it will probably have good enough background to mess with network architecture. So my decision was to make all necessary network modifications outside of Stanza code base. Maybe someone else would like to use several FC layers for classifier. This means that within the Stanza code it is just required to load model with modified architecture and proceed with normal training process.

Example of the code that I used to update model classifier and define new classes for NER model. Potentially this can be included somewhere into documentation or examples within Stanza.
https://gist.github.com/gawy/2fec736e6278db6e6a083c26d3ec745b

**Example usage:**
`scripts/run_ner.sh Ukrainian-languk --finetune --train_classifier_only`

**Flags and reasoning behind them**
* `finetune` - tells ner_tagger to load exising model from file instead of creating a new model from scratch for training. Potentially this has 2nd use-case in funetuning the model - that is why the name.
* `train_classifier_only` - ner_tagger will stop gradient from popagating for all layers above classifier (code disables gradient for all layers except those with names containing ['tag_clf', 'crit'])

## Experimental results

I've stated with own trained NER model on 4 standard classes (Ukrainian-languk) with F1 score around 84. Any other language model can be equaly used in the same way.

Model was modified to have a new classifier with 2 new classes and trained on a data set that had roughly 200 and 150 examples of each class.

Initial NER model had F1 score of about 84.
Newly trained model showed decent results 
Prec.	Rec.	F1
81.40	77.78	79.55

Manual isnspection in my case also showed nice results - something good enough to be used in practice and further improved.

## Fixes Issues
none as far as I can see

## Unit test coverage
Existing NER unit tests run successfully. No additional tests were created.
NER training was tested in end-to-end training as well as Transfer learning mode

## Known breaking changes/behaviors
none just adds new features
",
907,2020-06-13T11:07:21Z,https://github.com/stanfordnlp/stanza/issues/350,,I was applying pos tagger in my project. Sometimes it was tagging wrong. I am thinking to retrain the model with some custom data. How will I retrain the POS tagger model?,
908,2020-06-11T11:29:54Z,https://github.com/stanfordnlp/stanza/issues/349,,"Hello.
I have trained my own NER model for the Italian language, using charlm as well. Everything during the training went smoothly, but when I try to use the model I get an error.
Here my code:

import os
import stanza

os.chdir('/home/antonio/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza')
stanza.download('it')
nlp = stanza.Pipeline(lang='it', processors='tokenize,ner', ner_model_path='saved_models/ner/it_test_nertagger.pt')


Error is the following:
KeyError                                  Traceback (most recent call last)
<ipython-input-1-eb8bd1bc8057> in <module>
      7 #stanza.download('it')
      8 
----> 9 nlp = stanza.Pipeline(lang='it', processors='tokenize,ner', ner_model_path='saved_models/ner/it_test_nertagger.pt')
     10 
     11 #doc = nlp(""Lorella ed Antonio sono due ricercatori del C2DH a Lussemburgo"")

~/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/pipeline/core.py in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, **kwargs)
    121                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
    122                                                                                           pipeline=self,
--> 123                                                                                           use_gpu=self.use_gpu)
    124             except ProcessorRequirementsException as e:
    125                 # if there was a requirements issue, add it to list which will be printed at end

~/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/pipeline/processor.py in __init__(self, config, pipeline, use_gpu)
    101         self._trainer = None
    102         self._vocab = None
--> 103         self._set_up_model(config, use_gpu)
    104         # run set up process
    105         # build the final config for the processor

~/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/pipeline/ner_processor.py in _set_up_model(self, config, use_gpu)
     22     def _set_up_model(self, config, use_gpu):
     23         # set up trainer
---> 24         args = {'charlm_forward_file': config['forward_charlm_path'], 'charlm_backward_file': config['backward_charlm_path']}
     25         self._trainer = Trainer(args=args, model_file=config['model_path'], use_cuda=use_gpu)
     26 

KeyError: 'forward_charlm_path'
",
909,2020-06-10T13:55:37Z,https://github.com/stanfordnlp/stanza/issues/348,,"I'm trying to access CoreNLP using staza just like described here: https://stanfordnlp.github.io/stanza/corenlp_client.html#overview but I have problem setting custom properties.

`server.props`:
```
annotators = tokenize,ssplit,truecase,pos,ner
tokenize.language = English
pos.model = edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger
ner.model = edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz
ner.applyFineGrained = false
ner.combinationMode = HIGH_RECALL

outputFormat = json

port = 9199
threads = 12
timeout = 30000
```
Then, in `stanza_test.py`:
```
import os
from time import perf_counter
from stanza.server import CoreNLPClient

os.environ[""CORENLP_HOME""] = ""./stanford-corenlp-4.0.0/""

with CoreNLPClient(properties='./server.props') as client:

    query = ""how to get from toronto to new york city""

    ann = client.annotate(query)
    t0 = perf_counter()
    sentence = ann.sentence[0]
    t1 = perf_counter()

    print(sentence.mentions)
    print(f""Time elapsed: {t1-t0}"")
```

The server starts and is working property but not with the property I specified:
```
Setting server defaults from: ./server.props
Starting server with command: java -Xmx5G -cp ./stanford-corenlp-4.0.0//* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties ./server.props -preload tokenize,ssplit,truecase,pos,ner

[output ...]
```
It's not starting with the port, timeout, threads I specified in the property file but rather the default values. However, the annotators are preloaded correctly.

Did I mess up with the format/property name in the property file? I wasn't able to find an example property file, so if there is one, it would be great you can point me to that file.

Thanks!",
910,2020-06-09T16:01:05Z,https://github.com/stanfordnlp/stanza/issues/347,,"Hello. I am trying to train my own NER model for Italian.

I have followed the instructions here https://stanfordnlp.github.io/stanza/training.html and successfully run the toy example provided here https://github.com/stanfordnlp/stanza-train for the English language.
Then, I tried to do the same for Italian. I have followed the tutorial and I have put the files train.bio and test.bio in the folder data/nerbase/Italian-TEST. They look like this

Train.bio:
Oggi B adige20041007_id413942 O
incontro SS adige20041007_id413942 O
al ES adige20041007_id413942 O
Centro SS adige20041007_id413942 O
S. YA adige20041007_id413942 O
Chiara AS adige20041007_id413942 O

Latte SS adige20041007_id413942 O
al ES adige20041007_id413942 O
seno SS adige20041007_id413942 O
, XPW adige20041007_id413942 O
sos SN adige20041007_id413942 O
di E adige20041007_id413942 O
Pedrotti SPN adige20041007_id413942 B-PER


test.bio:
La RS adige20060512_idATT4 O
truffa SS adige20060512_idATT4 O
di E adige20060512_idATT4 O
Massimo SS adige20060512_idATT4 B-PER
Bassoli SP adige20060512_idATT4 I-PER
del ES adige20060512_idATT4 O
« XPO adige20060512_idATT4 O
Giornale SS adige20060512_idATT4 B-ORG
d' E adige20060512_idATT4 I-ORG
Italia SPN adige20060512_idATT4 I-ORG
» XPO adige20060512_idATT4 O
per E adige20060512_idATT4 O
ottenere VF adige20060512_idATT4 O
i RP adige20060512_idATT4 O
contributi SP adige20060512_idATT4 O
sull' ES adige20060512_idATT4 O
editoria SS adige20060512_idATT4 O

Collaborazioni SP adige20060512_idATT4 O
gonfiate AP adige20060512_idATT4 O
, XPW adige20060512_idATT4 O
direttore SS adige20060512_idATT4 O
in E adige20060512_idATT4 O
cella SS adige20060512_idATT4 O


Then, I run the script run_ner.sh with the following command: 
bash scripts/run_ner.sh Italian-TEST --max_steps 500 --word_emb_dim 
but I get an error. As far as I can understand, the script creates the json files it_test.test.json and it_test.train.json in the folder data/processed/ner but these files are empty.
How can I solve this issue?
Tahnk you in advance.

The output I get is the following: 

Running ner with --max_steps 500 --word_emb_dim 5...
Running tagger in train mode
Loading data with batch size 32...
Traceback (most recent call last):
  File ""/home/antonio/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/antonio/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/antonio/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/models/ner_tagger.py"", line 250, in <module>
    main()
  File ""/home/antonio/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/models/ner_tagger.py"", line 104, in main
    train(args)
  File ""/home/antonio/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/models/ner_tagger.py"", line 131, in train
    train_doc = Document(json.load(open(args['train_file'])))
  File ""/home/antonio/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File ""/home/antonio/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads
    return _default_decoder.decode(s)
  File ""/home/antonio/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/antonio/anaconda3/lib/python3.7/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
Running tagger in predict mode
Cannot load model from saved_models/ner/it_test_nertagger.pt
Traceback (most recent call last):
  File ""/home/antonio/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/models/ner/trainer.py"", line 116, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 525, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 212, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 193, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/ner/it_test_nertagger.pt'
Running tagger in predict mode
Cannot load model from saved_models/ner/it_test_nertagger.pt
Traceback (most recent call last):
  File ""/home/antonio/Desktop/New PC/Uni/phd/Computer Science for Heurmeneutics - Luxemburg/kickoff start/Lorella/NER/Stanza REM training/stanza-train/stanza/stanza/models/ner/trainer.py"", line 116, in load
    checkpoint = torch.load(filename, lambda storage, loc: storage)
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 525, in load
    with _open_file_like(f, 'rb') as opened_file:
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 212, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File ""/home/antonio/anaconda3/lib/python3.7/site-packages/torch/serialization.py"", line 193, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/ner/it_test_nertagger.pt'
",
911,2020-06-09T10:57:19Z,https://github.com/stanfordnlp/stanza/issues/346,,"Hi,
We would like to use Stanza to do the pre-processing stages including stopwords/punctuation/special characters removal. We noticed that this step does not seem to be part of the pipeline. We are then performing this step with NLTK, but we found out that loading only the Stanza lemmatization processor afterwards performs very slowly. Is there a workaround to this issue? For instance, with spaCy there is a workaround to do just that. Please see below. 

```python
it_nlp = it_core_news_sm.load(disable=['tagger', 'parser', 'ner'])
# lemmatization function
def lemmatize(doc):
  lemmatized_doc = []
  for w in doc:
    w_lemma = [token.lemma_ for token in it_nlp(w)]
    lemmatized_doc.append(w_lemma[0])
  return lemmatized_doc

# add column with lemmatized tokens
sources['tokens_lemmatized'] = sources['tokens_prep_nostop'].apply(lambda x: lemmatize(x))

process used with Stanza:
nlp = stanza.Pipeline(lang='it', processors='tokenize,mwt,pos,lemma', tokenize_pretokenized=True)
# lemmatization function
def lemmatize(doc):
  nlpd_doc = nlp([doc])
  output = []
  for i, sentence in enumerate(nlpd_doc.sentences):
    w_lemma = [word.lemma for word in sentence.words]
    output.append(w_lemma)
  return output
(if I try it on one doc, it works fine):
example_doc = sources['doc_prep_nostop'].iloc[0]
test_lemma = lemmatize(example_doc)
```

on the entire sample data, works fine but needs 5 to 10 minutes to run vs a few seconds in Spacy

```python
sources['doc_lemmatized'] = sources['doc_prep_nostop'].apply(lambda x: lemmatize(x))
```

Many thanks,
Lorella.
",
912,2020-06-09T06:17:48Z,https://github.com/stanfordnlp/stanza/issues/345,,"I'm using the following piece of code to use tregex (in this case, to extract NPs from an input `_text`):

```
from stanza.server import CoreNLPClient

# get noun phrases with tregex
def noun_phrases(_client, _text):
    pattern = 'NP'
    matches = _client.tregex(_text,pattern)
    for sentence in matches['sentences']:
        for match_id in sentence:
            print(sentence[match_id]['spanString'])
```
where `_client` is an instance of `CoreNLPClient` with the full list of annotators.

It looks like all the values in a `match` dictionary are of `str` type. I wonder if there's any way that I can get the start and end indexes of the `spanString` in the sentence where spanString exists?",
913,2020-06-08T06:04:06Z,https://github.com/stanfordnlp/stanza/issues/344,,"**Describe the bug**
I am importing CoreNLPClient from stanza.server by simply adding the following line:
`from stanza.server import CoreNLPClient`. Even before getting to the point to create an instance of CoreNLPClient, I get the following error:

```
Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/Users/[my-username]/anaconda3/envs/my-env/lib/python3.6/site-packages/stanza/server/__init__.py"", line 1, in <module>
    from stanza.protobuf import to_text
  File ""/Users/[my-username]/anaconda3/envs/my-env/lib/python3.6/site-packages/stanza/protobuf/__init__.py"", line 9, in <module>
    from .CoreNLP_pb2 import *
  File ""/Users/[my-username]/anaconda3/envs/my-env/lib/python3.6/site-packages/stanza/protobuf/CoreNLP_pb2.py"", line 22, in <module>
    serialized_pb=b'....a very long string....' # can add the string if needed!
TypeError: __new__() got an unexpected keyword argument 'serialized_options'
```

**Expected behavior**
Running the CoreNLPClient without error.

**Environment:**
 - OS: MacOS 10.15.5
 - Python version: 3.6
 - Stanza version: 1.0.1
 - CoreNLP version: 4.0.0
",
914,2020-06-05T13:35:10Z,https://github.com/stanfordnlp/stanza/issues/343,,"Hi,
Is there a possibility to add new (custom) named entities during training a NER model?
Best regards",
915,2020-06-05T11:13:05Z,https://github.com/stanfordnlp/stanza/issues/342,,"Hi,

My name is Lorella Viola and I am researcher at the Centre for Contemporary and Digital History (C2DH) at the University of Luxembourg. Together with Antonio Fiscarelli (Ph.D. student), we are working on enriching a digital heritage collection of Italian newspapers. We would like to use the Stanza library to perform Named Entity Recognition and we were wondering whether it is possible at all to use Stanza to train our own NER model for Italian? If yes, where can we find some information/tutorial explaining how?
Thank you in advance for your time.

best wishes,

Lorella & Antonio
https://github.com/lorellav",
916,2020-06-04T00:53:07Z,https://github.com/stanfordnlp/stanza/issues/341,,i am a beginner.I want to add some new words to the model and when i use ner I can find them.,
917,2020-06-03T12:16:05Z,https://github.com/stanfordnlp/stanza/issues/340,,"In a try to train a multi-lingual NER model(Arabic/English), I have prepared a data set which mostly contains Arabic entities and some English entities.

- I have then trained a model using the provided pretrained Arabic vectors. To my surprise, the model has achieved **_impressive results_** on recognizing both the Arabic and English entities.

- My concern here is mainly I am looking for an interpretation for these results, I know that Stanza is designed to be **_language-agnostic and data driven that is train on char LSTM_**; but I still could not get the idea how this works!. Most probably, I am missing something crucial here.

- I would appreciate if I could get an explanation for this **(references are very welcomed)**",
918,2020-06-03T09:40:14Z,https://github.com/stanfordnlp/stanza/issues/339,,"There appears to be a miss-match of dimension size for the word embeddings for the pretrained English NER model. The model definition states that the word embedding dimension is 300, but i was under the impression that this NER model was trained using the CoNLL17 Corpus word vectors, which have dimension 100.

After loading the NER model using torch.load, we can see that the dimension is indeed 300.
However, i can resume training this NER model using dimension 100 vectors. Was this just a typo in the original model config?

I may be miss understanding something crucial here, but any verification would be helpful.",
919,2020-06-02T06:54:02Z,https://github.com/stanfordnlp/stanza/issues/338,,"**Describe the bug**
Using stanza as CoreNLP client to run QuoteAnnotator, protobuf serialization often raises error:

`stanza.server.client.AnnotationException: edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer$LossySerializationException: Keys are not being serialized: class edu.stanford.nlp.quoteattribution.QuoteAttributionUtils$EnhancedSentenceAnnotation`

Setting outputFormat to json avoids the error. In the case I checked, the json `quotes` object includes a key, `canonicalSpeaker`, not defined in CoreNLP_pb2.py 

**Environment:**
 - OS: MacOS
 - Python version: 3.8.3 homebrew
 - Stanza version: 1.0.1

",
920,2020-05-31T21:59:54Z,https://github.com/stanfordnlp/stanza/issues/337,,"**Describe the bug**

---------------------------------------------------------------------------
```python
RuntimeError                              Traceback (most recent call last)
<ipython-input-20-c7cc905871af> in <module>()
----> 1 doc = stNLP('Todas las personas son las más inteligentes de mundo')
      2 
      3 print(*[f'word: {word.text+"" ""}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')

7 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py in pack_padded_sequence(input, lengths, batch_first, enforce_sorted)
    242 
    243     data, batch_sizes = \
--> 244         _VF._pack_padded_sequence(input, lengths, batch_first)
    245     return _packed_sequence_init(data, batch_sizes, sorted_indices, None)
    246 

RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor
```

**To Reproduce**
Steps to reproduce the behavior:
1. Go to google colab
2. Write the code:

```python
!pip install stanza
import stanza

stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=True)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',
                        lang='es',
                        use_gpu=True)

doc = nlp('Barack Obama nació en Hawaii.')
print(*[f'word: {word.text+"" ""}\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\n')
```
3. Error

**Expected behavior**
Should show the lemmatization of the sentences.

**Environment (please complete the following information):**
 - Google Colab
 - Stanza version: last version (31/05/2020)",
921,2020-05-31T11:40:48Z,https://github.com/stanfordnlp/stanza/issues/336,,"Concerning training NER models,
Can I train hybird NER models? _for example models that can recognize (Arabic and English) text!_
",
922,2020-05-30T23:01:48Z,https://github.com/stanfordnlp/stanza/pull/335,,Scripts to process a few sentiment datasets.  Sneak in a fix for the multi-model test script,
923,2020-05-30T13:03:43Z,https://github.com/stanfordnlp/stanza/issues/334,,"While working with Stanford Stanza I came to the following problem. I want to initialize a `scikit-learn` `CountVectorizer` with Tokenized Features from the Stanza package. In some cases I want to set Named Entities as a single feature and want to consolidate, for example a name, to a single token. For exampe ""New York"" should be the Token `New_York`. Does anybody know what would be the best way within the Stanza Objects to realize the consolidation of entity tokens?

Example:

`[I][live][in][New][York]`

`[o][o][o][GPE-B][GPE-E]`

Should become: 

`[I][live][in][New_York]`

I know that SpaCy has a function `entity_consolidate` but can I find something like this is Stanza?

On the other hand I could use something like list indexes and `.join()` operations. But I think a lambda or list comprehention formulation would be better for performance reasons.

Thanks",
924,2020-05-29T19:04:29Z,https://github.com/stanfordnlp/stanza/issues/333,,"There seems to be a significant performance hit when performing simple parsing of text in Russian.  I have looked into previous posts and noticed a similar issue with Spanish that has since been corrected.

Are there debug settings within Stanza that I can use to track down the issue?  ",
925,2020-05-29T11:54:03Z,https://github.com/stanfordnlp/stanza/issues/332,,"**Describe the bug**
The Dutch tokenizer produces empty tokens ([]) in specific situations (text ""!!!!""). Such sequence being taken over by the POS tagger results in an RuntimeError.

**To Reproduce**
```
nlp = stanza.Pipeline('nl', processors='tokenize,mwt')
nlp('!!!!')
[
  [
    {
      ""id"": ""1"",
      ""text"": ""!"",
      ""misc"": ""start_char=0|end_char=1""
    }
  ],
  [
    {
      ""id"": ""1"",
      ""text"": ""!"",
      ""misc"": ""start_char=1|end_char=2""
    }
  ],
  [
    {
      ""id"": ""1"",
      ""text"": ""!"",
      ""misc"": ""start_char=2|end_char=3""
    }
  ],
  []
]
nlp = stanza.Pipeline('nl', processors='tokenize,mwt,pos')
nlp('!!!!')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/nikolal/miniconda3/envs/stanza/lib/python3.6/site-packages/stanza/pipeline/core.py"", line 176, in __call__
    doc = self.process(doc)
  File ""/home/nikolal/miniconda3/envs/stanza/lib/python3.6/site-packages/stanza/pipeline/core.py"", line 170, in process
    doc = self.processors[processor_name].process(doc)
  File ""/home/nikolal/miniconda3/envs/stanza/lib/python3.6/site-packages/stanza/pipeline/pos_processor.py"", line 32, in process
    for i, b in enumerate(batch):
  File ""/home/nikolal/miniconda3/envs/stanza/lib/python3.6/site-packages/stanza/models/pos/data.py"", line 122, in __iter__
    yield self.__getitem__(i)
  File ""/home/nikolal/miniconda3/envs/stanza/lib/python3.6/site-packages/stanza/models/pos/data.py"", line 114, in __getitem__
    xpos = get_long_tensor(batch[3], batch_size)
  File ""/home/nikolal/miniconda3/envs/stanza/lib/python3.6/site-packages/stanza/models/common/data.py"", line 22, in get_long_tensor
    tokens[i, :len(s)] = torch.LongTensor(s)
RuntimeError: The expanded size of the tensor (10) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [0, 10].  Tensor sizes: [0]
```

**Expected behavior**
`nlp('!!!!')`not throwing an error

**Environment (please complete the following information):**
 - OS: Ubuntu 18.04
 - Python version: Python 3.6.10 Anaconda
 - Stanza version: 1.0.0
",
926,2020-05-29T05:33:36Z,https://github.com/stanfordnlp/stanza/issues/331,,"Hi, there

Could you help me to trace this issue?
Here is my some info:
- Network is okay without limitations

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import stanza

if __name__ == '__main__':
    # https://github.com/stanfordnlp/stanza/blob/master/demo/Stanza_Beginners_Guide.ipynb
    # Note that you can use verbose=False to turn off all printed messages
    print(""Downloading Chinese model..."")
    stanza.download('zh', verbose=True)

    # Build a Chinese pipeline, with customized processor list and no logging, and force it to use CPU
    print(""Building a Chinese pipeline..."")
    zh_nlp = stanza.Pipeline('zh', processors='tokenize,lemma,pos,depparse', verbose=True, use_gpu=False)
```

```text
C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\Scripts\python.exe C:/Users/mystic/JetBrains/PycharmProjects/BuildRoleRelationship4Novel/learn_stanza.py
Downloading Chinese model...
Traceback (most recent call last):
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\connection.py"", line 159, in _new_conn
    conn = connection.create_connection(
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\util\connection.py"", line 84, in create_connection
    raise err
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\util\connection.py"", line 74, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\connectionpool.py"", line 670, in urlopen
    httplib_response = self._make_request(
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\connectionpool.py"", line 381, in _make_request
    self._validate_conn(conn)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\connectionpool.py"", line 976, in _validate_conn
    conn.connect()
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\connection.py"", line 308, in connect
    conn = self._new_conn()
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\connection.py"", line 171, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000001E5A5DE7220>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\requests\adapters.py"", line 439, in send
    resp = conn.urlopen(
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\connectionpool.py"", line 724, in urlopen
    retries = retries.increment(
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\urllib3\util\retry.py"", line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/master/resources_1.0.0.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E5A5DE7220>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/mystic/JetBrains/PycharmProjects/BuildRoleRelationship4Novel/learn_stanza.py"", line 9, in <module>
    stanza.download('zh', verbose=True)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\stanza\utils\resources.py"", line 223, in download
    request_file(f'{DEFAULT_RESOURCES_URL}/resources_{__resources_version__}.json', os.path.join(dir, 'resources.json'))
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\stanza\utils\resources.py"", line 83, in request_file
    download_file(url, path)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\stanza\utils\resources.py"", line 66, in download_file
    r = requests.get(url, stream=True)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\requests\api.py"", line 76, in get
    return request('get', url, params=params, **kwargs)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\requests\api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\requests\sessions.py"", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\requests\sessions.py"", line 643, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Users\mystic\.virtualenvs\BuildRoleRelationship4Novel\lib\site-packages\requests\adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/master/resources_1.0.0.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E5A5DE7220>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))

Process finished with exit code 1

```",
927,2020-05-28T15:30:56Z,https://github.com/stanfordnlp/stanza/issues/330,,"**Describe the bug**
Similar to issue #329, one or more processors are missing for UD treebanks `orv_torot` (Old Russion) and `swl_sslc` (Swedish Sign Language).

**To Reproduce**
Create a fresh Python environment, install stanza and run in a python session:
```
>>> import stanza
>>> stanza.download('orv', dir='models', package='torot', processors='depparse')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 70.9MB/s]                                      
2020-05-28 16:27:38 INFO: Downloading these customized packages for language: orv (Old_Russian)...
=======================
| Processor | Package |
-----------------------
| depparse  | torot   |
| pretrain  | torot   |
=======================

2020-05-28 16:27:39 INFO: File exists: models/orv/depparse/torot.pt.
Traceback (most recent call last):
  File ""/redacted/stanza/venv-stanza/lib/python3.6/site-packages/stanza/utils/resources.py"", line 249, in download
    request_file(f'{url}/{__resources_version__}/{lang}/{key}/{value}.pt', os.path.join(dir, lang, key, f'{value}.pt'), md5=resources[lang][key][value]['md5'])
KeyError: 'pretrain'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/redacted/stanza/venv-stanza/lib/python3.6/site-packages/stanza/utils/resources.py"", line 251, in download
    raise Exception(f""Cannot find the following processor and model name combination: {key}, {value}. Please check if you have provided the correct model name."") from e
Exception: Cannot find the following processor and model name combination: pretrain, torot. Please check if you have provided the correct model name.
>>> 
>>> 
>>> stanza.download('swl', dir='models', package='sslc', processors='depparse')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 88.7MB/s]                                      
2020-05-28 16:28:20 INFO: Downloading these customized packages for language: swl (Swedish_Sign_Language)...
=======================
| Processor | Package |
-----------------------
| depparse  | sslc    |
| pretrain  | sslc    |
=======================

2020-05-28 16:28:21 INFO: File exists: models/swl/depparse/sslc.pt.
Traceback (most recent call last):
  File ""/redacted/stanza/venv-stanza/lib/python3.6/site-packages/stanza/utils/resources.py"", line 249, in download
    request_file(f'{url}/{__resources_version__}/{lang}/{key}/{value}.pt', os.path.join(dir, lang, key, f'{value}.pt'), md5=resources[lang][key][value]['md5'])
KeyError: 'pretrain'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/redacted/stanza/venv-stanza/lib/python3.6/site-packages/stanza/utils/resources.py"", line 251, in download
    raise Exception(f""Cannot find the following processor and model name combination: {key}, {value}. Please check if you have provided the correct model name."") from e
Exception: Cannot find the following processor and model name combination: pretrain, sslc. Please check if you have provided the correct model name.
>>> 
```

**Expected behavior**
Models listed on https://stanfordnlp.github.io/stanza/models.html should load without error. Missing models should be listed in a separate table, e.g. under a heading ""Currently unavailable"".

**Environment (please complete the following information):**
 - OS: openSUSE Leap 15.1
 - Python version: Python 3.6.10 with a fresh virtualenv environment, stanza installed
 - Stanza version: stanza-1.0.1-py3-none-any.whl (193 kB)

**Additional context**
These two and `cop_scriptorium` (issue #329) are the only ones from the UD model table that fail.

",
928,2020-05-28T14:23:16Z,https://github.com/stanfordnlp/stanza/issues/329,,"**Describe the bug**
For language `cop`, package `scriptorium`, one or more processors are missing, including `pretrain`.

**To Reproduce**
Create a fresh Python environment, install stanza and run in a python session:
```
>>> import stanza
>>> stanza.download('cop', dir='models', package='scriptorium', processors='depparse')
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 68.9MB/s]                       
2020-05-28 15:08:32 INFO: Downloading these customized packages for language: cop (Coptic)...
===========================
| Processor | Package     |
---------------------------
| depparse  | scriptorium |
| pretrain  | scriptorium |
===========================

2020-05-28 15:08:32 INFO: File exists: models/cop/depparse/scriptorium.pt.
Traceback (most recent call last):
  File ""/redacted/stanza/venv-stanza/lib/python3.6/site-packages/stanza/utils/resources.py"", line 249, in download
    request_file(f'{url}/{__resources_version__}/{lang}/{key}/{value}.pt', os.path.join(dir, lang, key, f'{value}.pt'), md5=resources[lang][key][value]['md5'])
KeyError: 'pretrain'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/redacted/stanza/venv-stanza/lib/python3.6/site-packages/stanza/utils/resources.py"", line 251, in download
    raise Exception(f""Cannot find the following processor and model name combination: {key}, {value}. Please check if you have provided the correct model name."") from e
Exception: Cannot find the following processor and model name combination: pretrain, scriptorium. Please check if you have provided the correct model name.
```

**Expected behavior**
Models listed on https://stanfordnlp.github.io/stanza/models.html should load without error.

**Environment (please complete the following information):**
 - OS: openSUSE Leap 15.1
 - Python version: Python 3.6.10 with a fresh virtualenv environment, stanza installed
 - Stanza version: stanza-1.0.1-py3-none-any.whl (193 kB)

**Additional context**
UD models for languages alphabetically before `Coptic` loaded without problem (after handling the `/` for Chinese.)
",
929,2020-05-27T20:03:54Z,https://github.com/stanfordnlp/stanza/pull/328,,"- confusion matrix output
- reweight loss based on how many examples of a class in the training set",
930,2020-05-27T08:43:14Z,https://github.com/stanfordnlp/stanza/issues/327,,"I find the default parse level for chinese sentences is on word level?  And I tried to tokenize the sentence in character level, but fail to get the correct result?

So Does Stanza surport to parse chinese sentences on character level ? 
Thanks in advance!",
931,2020-05-27T02:21:47Z,https://github.com/stanfordnlp/stanza/issues/326,,"**Is your feature request related to a problem? Please describe.**
default.zip file contributes quite a bit to the download footprint

```
nobody@bfadd6080cd0:/.stanza_resources/en$ du -h *
 22M	backward_charlm 
384M	default.zip 
103M	depparse
 22M	forward_charlm 
3.3M	lemma 
159M	ner
 22M	pos 
149M	pretrain
 624K	tokenize
```

**Describe the solution you'd like**
We'd like to delete this file after `.download` is called as it's unzipped and not useful anymore

**Describe alternatives you've considered**
Currently we're deleting this with an `os.remove` call right after

**Additional context**
I can implement this! Should be a quick 1-liner
",
932,2020-05-26T19:56:21Z,https://github.com/stanfordnlp/stanza/issues/325,,"Is there any way to preform sentiment analysis in Stanza like there is in the older Core-NLP?  I have linked the two, but I do not know what the command is to include sentiment as an output for a sentence, or if that is even possible. 

Any help would be appreciated.
",
933,2020-05-24T19:53:40Z,https://github.com/stanfordnlp/stanza/issues/324,,"Currently, attempting to load a pipeline that isn't downloaded will trigger a generic `Exception`. This makes it difficult to use with a try/except block, as other (unintended) errors will also be caught.

A simple `Exception` class definition should be added somewhere. This could probably just be something like a `StanzaException`, and shouldn't need to much actual code in it, just something to make it easier to sort out in try/catch blocks.

Ideally there would be multiple Exception types for different errors, but one would probably be enough for now. Others could be added later if/when it made sense.

In case you were wondering why, my code will try to load a model, then download it if it doesn't exist so it can boot up faster. There is only like a 3 second difference on my PC, but it may be more on lower end devices, which I plan on deploying this on. This is my code, I couldn't figure out how to get the indentation correct, but it should be pretty easy to figure out.
`try:`
`    nlp = stanza.Pipeline('en')`
`except Exception:`
`    stanza.download('en')`
`    nlp = stanza.Pipeline('en')`

",
934,2020-05-23T05:51:41Z,https://github.com/stanfordnlp/stanza/pull/323,,"## Desciption
Provides a `add_property` interface to allow users and external processors to add properties to Stanza data objects.

## Fixes Issues
N/A

## Unit test coverage
Main use cases covered by `test_data_objects.py`.

## Known breaking changes/behaviors
N/A",
935,2020-05-22T19:59:26Z,https://github.com/stanfordnlp/stanza/pull/322,,"## Desciption
This PR introduces two decorators, `register_processor` and `register_processor_variant` that allows users and developers to load their customized processors and processor variants (e.g., different tokenizer models).

## Fixes Issues
N/A

## Unit test coverage
Core functionality shouldn't change and is already covered by existing unittests; newly added decorators are tested in `test_decorators.py`.

## Known breaking changes/behaviors
N/A
",
936,2020-05-22T08:54:31Z,https://github.com/stanfordnlp/stanza/issues/321,,"Hi to train with pretrained contextualized char embedding, what all parameters are needed to be passed to ner_tagger.py. I am passing `charlm_shorthand`, `charlm_save_dir`, `charlm`. But it keeps giving error at input_tranform
![image](https://user-images.githubusercontent.com/20723123/82646851-eb684b00-9c32-11ea-96ae-8434278ee248.png)

_Originally posted by @aishwarya-agrawal in https://github.com/stanfordnlp/stanza/issues/307#issuecomment-632564099_",
937,2020-05-19T22:02:21Z,https://github.com/stanfordnlp/stanza/pull/320,,"…a test.  Also, upgrade Exception to a builtin, FileNotFoundError
",
938,2020-05-19T16:03:32Z,https://github.com/stanfordnlp/stanza/issues/319,,"**Describe the solution you'd like**
For my own project I have trained a Stanza NER model using lang-uk community dataset and thought it could be great to include it as a part of an official package.

**Describe alternatives you've considered**
lang-uk community itself has trained NER model for https://github.com/mit-nlp/MITIE/ 

**Additional context**
Data conversion code and trained model can be found here
https://github.com/gawy/stanza-lang-uk/releases/tag/v0.9
https://github.com/gawy/stanza-lang-uk

Let me know if that up to your standards or what kind of help might be required.
",
939,2020-05-18T04:28:47Z,https://github.com/stanfordnlp/stanza/issues/318,,"1、I cannot download through code

`stanza.download('zh',r""D:\h"")`

> ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/master/resources_1.0.0.json (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 11004] getaddrinfo failed'))

2、so I download manually and I unzip the file：

`zh_nlp = stanza.Pipeline('zh',r""F:\zh"")`

> Exception: Resources file not found at: F:\zh\resources.json. Try to download the model again.

And there is no json in the unzipped file.",
940,2020-05-17T12:33:44Z,https://github.com/stanfordnlp/stanza/issues/317,,"**Describe the bug**
I was trying to optimize batch size and particularly how many sentences we can feed to `nlp()`  before running into out of memory issues. However, I found that for some reason the model can run just fine for ~6479 sentences in total (not batch sizes), but after that the memory usage suddenly doubles which - depending on the batch size - crashes the parser. It seems that this doubling only happens ones at 6479 sentences but does not continue to increase after that.

This seems to be particularly related to the dependency parser, though it also happens (less noticeably) with the other processors. The increase in memory always happens around the 6479 sentences mark.


- tokenize,mwt,pos: memory usage increase from 2.7 to 2.9GB 
- tokenize,mwt,pos,lemma: memory usage increase from 2.7 to 2.9GB 
- tokenize,mwt,pos,lemma,depparse: memory usage increase from 6.6GB to **OOM**

The same behaviour is seen when disabling `tokenize_pretokenized`, though surprisingly the script uses a lot less memory in general if you set `tokenize_pretokenized=False`. This may lead to the origin of this growing memory issue.

**To Reproduce**

Find a 10k sentences file and run the following code. On a 11GB GPU this will crash after around 6479 sentences. You can for instance use the WMT news datasets: http://data.statmt.org/news-crawl/en/

```python
import stanza
from tqdm import tqdm

nlp = stanza.Pipeline(""en"", processors=""tokenize,mwt,pos,lemma,depparse"", tokenize_pretokenized=True)


with open('your-large-file.txt', encoding='utf-8') as fhin:
    lines = []
    # change 'total' to actual number of lines
    for line_idx, line in tqdm(enumerate(fhin, 1), total=10000):
        lines.append(line.rstrip())

        if line_idx % 40 == 0:
            doc = nlp('\n'.join(lines) + '\n')
            lines = []
```

**Expected behavior**

The memory usage should not increase so dramatically. This seems like a grave bug.

**Environment (please complete the following information):**
 - OS: Windows; Ubuntu
 - Python version: vanilla 3.8.2
 - Stanza version: most recent pip

",
941,2020-05-16T07:08:16Z,https://github.com/stanfordnlp/stanza/issues/316,,"**Describe the bug**
I believe there is a bug when applying depparse and having MWT.
When building the dependencies, specifically at https://github.com/stanfordnlp/stanza/blob/1c6a8509385a23c6a17ed51b6fcbe29030b80267/stanza/models/common/doc.py#L401 the `assert` fails because the ID of the `word's` head does not match the ID of the `head` itself.

It seems to fail because the dependencies and their head indices are computed over the `tokens` and not over the `words`. MWT are not added to the `words` list, see: https://github.com/stanfordnlp/stanza/blob/1c6a8509385a23c6a17ed51b6fcbe29030b80267/stanza/models/common/doc.py#L286
 
So the indices do not match when comparing them.

**To Reproduce**
Using a MWT within a phrase. In this example, _du_ is a MWT (_de_, _le_).

```python
import stanza
NLP = stanza.Pipeline(lang='fr', processors=""tokenize,lemma,pos,depparse"", use_gpu=False)
doc = NLP(""Séance ordinaire du 15 juin 2016"")
```
**Current behavior**
A failed assertion when matching the IDs of the words:
```
  File ""<ipython-input-1-28223aa33ec0>"", line 1, in <module>
    assert(int(word.head) == int(head.id))
AssertionError
```
**Expected behavior**
Depparse to work by correctly matching the head id with the word's head id.

**Environment (please complete the following information):**
 - OS: Linux Mint 19.3
 - Python version:  3.7.7 from Anaconda
 - Stanza version: 1.0.1

**Possible Fix**
If we work over the tokens instead of over the words, by changing this: 
```python
head = self.words[int(word.head) - 1]
assert(int(word.head) == int(head.id))
```
to this:
```python
head = self.tokens[int(word.head) - 1]
assert(int(word.head) == int(head.id))
```
It seems to work on my side and `pytest tests/test_depparse.py` passes :) Still, I suspect this may be more complex.

I did a branch with this modification here https://github.com/psorianom/stanza/tree/depparse_head_id_bug

Thanks for the hard work!",
942,2020-05-15T23:59:03Z,https://github.com/stanfordnlp/stanza/pull/315,,Add a classifier model and the corresponding pipeline component,
943,2020-05-15T07:35:05Z,https://github.com/stanfordnlp/stanza/issues/314,,"- hi, I was trying to tokenize only one sentence, like this: ""The Swedish commander Klingspor continued his retreat towards Oulu after the Swedish victory at the Battle of Siikajoki."", 

```
import stanza
stanza.download('en')
nlp = stanza.Pipeline('en')
sen = ""The Swedish commander Klingspor continued his retreat towards Oulu after the Swedish victory at the Battle of Siikajoki.""
doc = nlp(sen)
```

- **but I get 2 sentence segmentations which cause a dependency parsing error, I don't know why can this happen, could you show me the solutions? Thanks a lot~**

![image](https://user-images.githubusercontent.com/11159088/82023885-899f6280-96c1-11ea-97ca-f8df97dc2a1f.png)



",
944,2020-05-15T00:26:30Z,https://github.com/stanfordnlp/stanza/issues/313,,"**Describe the bug**
I can't download the model. 

**To Reproduce**

>>>import stanza
>>>stanza.download('en')


**Expected behavior**
Expected model to be downloaded. is there any other way to get the model?

**Environment (please complete the following information):**
 - OS: Windows 10
 - Python version: Python 3.8.2 from PyPi
 - Stanza version: 1.0.1

**Additional context**
Got this error.

  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\stanza\utils\resources.py"", line 223, in download
    request_file(f'{DEFAULT_RESOURCES_URL}/resources_{__resources_version__}.json', os.path.join(dir, 'resources.json'))
  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\stanza\utils\resources.py"", line 83, in request_file
    download_file(url, path)
  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\stanza\utils\resources.py"", line 66, in download_file
    r = requests.get(url, stream=True)
  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\api.py"", line 76, in get
    return request('get', url, params=params, **kwargs)
  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\sessions.py"", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\sessions.py"", line 643, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Users\shaldar\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\adapters.py"", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/master/resources_1.0.0.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)')))
================ READY ================",
945,2020-05-14T13:21:16Z,https://github.com/stanfordnlp/stanza/issues/312,,"Hello, 

  I am working with French sentences and the sentence I have is 

> Je suis en train de dormir

which in English means

> I'm sleeping

I am using Stanza for this and my french properties file looks as follows:

```
# annotators
annotators = tokenize, ssplit, pos, depparse, parse

outputFormat = json

# tokenize
tokenize.language = fr

# pos
pos.model = edu/stanford/nlp/models/pos-tagger/french-ud.tagger

# parse
parse.model = edu/stanford/nlp/models/srparser/frenchSR.beam.ser.gz

# depparse
depparse.model = edu/stanford/nlp/models/parser/nndep/UD_French.gz
```

However, when I add a line in my code to check if there is a VERB with VerbForm=Ger as follows:

`if (word.pos == VERB) and (""VerbForm=Ger"" in word.feats):`

I can't seem to find any verb in its gerund form. I understand in English the word sleeping is a Gerund verb.

The dependency parse tree is 

```
[
  [
    {
      ""id"": ""1"",
      ""text"": ""Je"",
      ""lemma"": ""il"",
      ""upos"": ""PRON"",
      ""feats"": ""Number=Sing|Person=1|PronType=Prs"",
      ""head"": 3,
      ""deprel"": ""nsubj"",
      ""misc"": ""start_char=0|end_char=2""
    },
    {
      ""id"": ""2"",
      ""text"": ""suis"",
      ""lemma"": ""être"",
      ""upos"": ""AUX"",
      ""feats"": ""Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin"",
      ""head"": 3,
      ""deprel"": ""cop"",
      ""misc"": ""start_char=3|end_char=7""
    },
    {
      ""id"": ""3"",
      ""text"": ""en"",
      ""lemma"": ""en"",
      ""upos"": ""ADP"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=8|end_char=10""
    },
    {
      ""id"": ""4"",
      ""text"": ""train"",
      ""lemma"": ""train"",
      ""upos"": ""NOUN"",
      ""feats"": ""Gender=Masc|Number=Sing"",
      ""head"": 3,
      ""deprel"": ""fixed"",
      ""misc"": ""start_char=11|end_char=16""
    },
    {
      ""id"": ""5"",
      ""text"": ""de"",
      ""lemma"": ""de"",
      ""upos"": ""ADP"",
      ""head"": 6,
      ""deprel"": ""mark"",
      ""misc"": ""start_char=17|end_char=19""
    },
    {
      ""id"": ""6"",
      ""text"": ""dormir"",
      ""lemma"": ""dormir"",
      ""upos"": ""VERB"",
      ""feats"": ""VerbForm=Inf"",
      ""head"": 3,
      ""deprel"": ""xcomp"",
      ""misc"": ""start_char=20|end_char=26""
    }
  ]
]
```

Can you please help me understand why we don't have a Gerund Verb here?

Thanks a lot for your time! ",
946,2020-05-13T23:27:43Z,https://github.com/stanfordnlp/stanza/pull/311,,"Also, add a skeleton test for doc
",
947,2020-05-13T17:02:26Z,https://github.com/stanfordnlp/stanza/issues/310,,"Hello,

  Firstly, thanks for the amazing support for other languages in Stanza. I am using Stanza to parse Chinese sentences.

The sentence is 
> 这是我妈妈的戒指

which in English means, 

> This is my mother's ring

I have a properties file which looks as follows:

```

annotators = tokenize, ssplit, pos, lemma, ner, parse, coref

outputFormat = json

# segment
tokenize.language = zh
segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true

# sentence split
ssplit.boundaryTokenRegex = [.。]|[!?！？]+

# pos
pos.model = edu/stanford/nlp/models/pos-tagger/chinese-distsim.tagger

# ner
ner.language = chinese
ner.model = edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz
ner.applyNumericClassifiers = true
ner.useSUTime = false

# regexner
ner.fine.regexner.mapping = edu/stanford/nlp/models/kbp/chinese/gazetteers/cn_regexner_mapping.tab
ner.fine.regexner.noDefaultOverwriteLabels = CITY,COUNTRY,STATE_OR_PROVINCE

# parse
parse.model = edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz

# depparse
depparse.model    = edu/stanford/nlp/models/parser/nndep/UD_Chinese.gz
depparse.language = chinese

# coref
coref.sieves = ChineseHeadMatch, ExactStringMatch, PreciseConstructs, StrictHeadMatch1, StrictHeadMatch2, StrictHeadMatch3, StrictHeadMatch4, PronounMatch
coref.input.type = raw
coref.postprocessing = true
coref.calculateFeatureImportance = false
coref.useConstituencyTree = true
coref.useSemantics = false
coref.algorithm = hybrid
coref.path.word2vec =
coref.language = zh
coref.defaultPronounAgreement = true
coref.zh.dict = edu/stanford/nlp/models/dcoref/zh-attributes.txt.gz
coref.print.md.log = false
coref.md.type = RULE
coref.md.liberalChineseMD = false

# kbp
kbp.semgrex = edu/stanford/nlp/models/kbp/chinese/semgrex
kbp.tokensregex = edu/stanford/nlp/models/kbp/chinese/tokensregex
kbp.language = zh
kbp.model = none

# entitylink
entitylink.wikidict = edu/stanford/nlp/models/kbp/chinese/wikidict_chinese.tsv.gz

```

I get the following parse tree entities

```
id:1	text:这是	pos: AUX	xpos: VC	ufeats: None	lemma: 这是	head id: 5	head: 戒指	deprel: cop

id:2	text:我	pos: PRON	xpos: PRP	ufeats: Person=1	lemma: 我	head id: 3	head: 妈妈	deprel: nsubj

id:3	text:妈妈	pos: NOUN	xpos: NN	ufeats: None	lemma: 妈妈	head id: 5	head: 戒指	deprel: acl:relcl

id:4	text:的	pos: PART	xpos: DEC	ufeats: None	lemma: 的	head id: 3	head: 妈妈	deprel: mark:relcl

id:5	text:戒指	pos: NOUN	xpos: NN	ufeats: None	lemma: 戒指	head id: 0	head: root	deprel: root
5
```

As we see here, we don't get any demonstratives. Do I need to make some changes to the properties file to be able to see the demonstratives? I expect the word ""This"" to be a demonstrative pronoun.

I see that PronType:Dem is supported in the UD dependencies for Chinese. I may be looking at the wrong link - please correct me if I am wrong here.

> https://universaldependencies.org/lzh/index.html

Again, thanks for your time and if I need to provide more information, please do let me know.",
948,2020-05-13T06:27:31Z,https://github.com/stanfordnlp/stanza/issues/309,,"To maximize speed performance, it is essential to run the pipeline on batches of documents. The best approach at this time is to concatenate documents together, with each document separated by  `\n\n`. 

But results returned are not partitioned, How do I get results for each article, not all result. I want to get the results after entity extraction of each article. Now,I wanted to iterate through the entity extraction results to match each article, but it took too long.
```
# method 1
text = ""xxxxxx\n\nxxxxx\n\nxxxxxx""
for txt in text.split(""\n\n""):
    doc = nlp(txt)
    for sentence in doc.sentences: # This article result
         print(sentence.ents) 

# method 2
text = ""xxxxxx\n\nxxxxx\n\nxxxxxx""
doc = nlp(text)
for sentence in doc.sentences:
    print(sentence.ents)  # This sentence result but i want to article result
```",
949,2020-05-12T13:48:36Z,https://github.com/stanfordnlp/stanza/issues/307,,Is it possible to use bert pretrained embeddings to train stanza NER.,
950,2020-05-12T11:22:47Z,https://github.com/stanfordnlp/stanza/issues/306,,"Hi, and first of all,thanks in advance and sorry if this is an old question or out of place.
i have in fact three question. I will post the three here.. but maybe i should do it in differents posts
CONTEXT
i'm developing a small system where i need to do a nlp process to a document. my architecture is based on aws and is as follow (more or less)
API receive file, and leavit in a place... ""lot"" of consumers are waiting (in order to have high throughput) and finally only one read THAT file, process the file, and leave the result in another place. This is already done (with local nlp server in each).
for the ""annotators"" mostly use
['tokenize','pos','lemma','ner']
for some 
['tokenize','ssplit','pos','lemma','ner','parse','depparse','coref','tokensregex', 'tokensregexnq', 'tickerregex']
Also i do a lot of TOKENREGEX search

FIRST QUESTION
i am quite new and not yet undertand the difference between using a stanza.Pipeline vs the server.CoreNLPClient. 
- stanza uses pythorch with **some** models, and CoreNLPClient connecto to the java server instance with **other** models? is this right?
- **in my understanding, i can do the same with both? is this right?**
if both are true.. i still dont figure when use each case? which i need for the next question

SECOND QUESTION
EACH consumer.. is a diferent EC2 instance.
Now i was wondering, what should be the best approach and why.
- EC2 with ??GB ram and using pipeline
- EC2 with 4-8GB ram and using CoreNLPCLient(start true), with local server
- EC2 nano with .5GB ram and using CoreNLPCLient(start false), connectedserver to a 1 big EC2 with the stanford server running and listenning

THIRD QUESTION
**IF** doing the third option, and wanting to minimize the EC2 instance requeriments. when installing stanza it also install pytorch (800mb) which seems unnecesary for this case, is there a way to install a ""serverless"" stanza


thanks, if any is able to clarify this for me.
sorry again if this is not the place, or the questions are already there.. 

",
951,2020-05-12T08:33:22Z,https://github.com/stanfordnlp/stanza/issues/305,,"I've tried Norwegian (bokmål) model of Stanza, but it fails in tokenization:

```py
>>> import stanza
>>> nlp=stanza.Pipeline(""nb"")
>>> doc=nlp(""Maten står på bordet."")
>>> print(doc)
[
  [
    {
      ""id"": ""1"",
      ""text"": ""Maten"",
      ""lemma"": ""mat"",
      ""upos"": ""NOUN"",
      ""feats"": ""Definite=Def|Gender=Masc|Number=Sing"",
      ""head"": 2,
      ""deprel"": ""nsubj"",
      ""misc"": ""start_char=0|end_char=5""
    },
    {
      ""id"": ""2"",
      ""text"": ""står"",
      ""lemma"": ""stå"",
      ""upos"": ""VERB"",
      ""feats"": ""Mood=Ind|Tense=Pres|VerbForm=Fin"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=6|end_char=10""
    },
    {
      ""id"": ""3"",
      ""text"": ""på"",
      ""lemma"": ""på"",
      ""upos"": ""ADP"",
      ""head"": 4,
      ""deprel"": ""case"",
      ""misc"": ""start_char=11|end_char=13""
    },
    {
      ""id"": ""4"",
      ""text"": ""bordet."",
      ""lemma"": ""bordet."",
      ""upos"": ""NOUN"",
      ""feats"": ""Definite=Def|Gender=Neut|Number=Sing"",
      ""head"": 2,
      ""deprel"": ""obl"",
      ""misc"": ""start_char=14|end_char=21""
    }
  ]
]
```

The lemma of the word ""bordet"" should be ""bord"" (see [here](https://no.wiktionary.org/wiki/bord)), but it fails with the fullstop.",
952,2020-05-12T04:34:20Z,https://github.com/stanfordnlp/stanza/pull/304,,Various updates to the travis config.,
953,2020-05-11T23:59:47Z,https://github.com/stanfordnlp/stanza/issues/303,,"Is this annotation property for english correct?

```
# English properties for 4.0.0
english_properties = {""annotators"": ""tokenize,ssplit,mwt,pos,lemma,parse"", ""tokenize.language"":""en"",
                     ""mwt.mappingFile"": ""edu/stanford/nlp/models/mwt/english/english-mwt.tsv"",
                     ""mwt.pos.model"": ""edu/stanford/nlp/models/mwt/english/english-mwt.tagger"",
                     ""mwt.statisticalMappingFile"": ""edu/stanford/nlp/models/mwt/english/english-mwt-statistical.tsv"",
                     ""mwt.preserveCasing"": ""false"", ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/english-ud.tagger"",
                     ""parse.model"": ""edu/stanford/nlp/models/srparser/englishSR.beam.ser.gz""}
```

When I used the French version:
```
# French properties for 4.0.0
french_properties = {""annotators"": ""tokenize,ssplit,mwt,pos,lemma,parse"", ""tokenize.language"":""fr"",
                     ""mwt.mappingFile"": ""edu/stanford/nlp/models/mwt/french/french-mwt.tsv"",
                     ""mwt.pos.model"": ""edu/stanford/nlp/models/mwt/french/french-mwt.tagger"",
                     ""mwt.statisticalMappingFile"": ""edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv"",
                     ""mwt.preserveCasing"": ""false"", ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/french-ud.tagger"",
                     ""parse.model"": ""edu/stanford/nlp/models/srparser/frenchSR.beam.ser.gz""}
```
I observed a great speed when using `client.tregex(text,'NP')` for French, thus I decided to try out a similar configuration for English with the hope of getting a speedup. But the annotation property for English gives an error `java.io.IOException: Unable to open ""edu/stanford/nlp/models/mwt/english/english-mwt.tsv"" as class path, filename or URL`. ",
954,2020-05-11T23:27:44Z,https://github.com/stanfordnlp/stanza/pull/302,,"## Desciption
This allows the user to set `start_server` to `None` to neither force start a server (by setting it to `True`, which fails if the process can't bind the designated port) nor require an existing server (by setting it to `False`). The default is still `True` to be conservative.

This allows `CoreNLPClient` to be used in scenarios such as multiprocessing, where multiple process will attempt to start a CoreNLP server, but eventually only one will succeed in binding the designated port.

## Fixes Issues
N/A

## Unit test coverage
No explicit test exists yet.

## Known breaking changes/behaviors
N/A
",
955,2020-05-11T23:16:17Z,https://github.com/stanfordnlp/stanza/pull/301,,Add the name of the failed file to the fallback logging.  Add a small test to make sure the fallback is working.,
956,2020-05-11T12:36:49Z,https://github.com/stanfordnlp/stanza/issues/300,,"**Describe the bug**
When I run nlp(comment) for Urdu language, I got error:
[E167] Unknown morphological feature: 'ConjType' (9141427322507498425). This can happen if the tagger was trained with a different set of morphological features. If you're using a pretrained model, make sure that your models are up to date: python -m spacy validate
some of the docs work while some don't.

**To Reproduce**
Following code to get tokens and pos tags:

`snlp = stanza.Pipeline(lang='ur')
nlp = StanzaLanguage(snlp)
doc = nlp('یہ سرد اور تلخ تھا')`

**Expected behavior**
No error and a list of pos tags, tokens should have been received but instead got an error for conjunction.

**Environment (please complete the following information):**
 - Windows and CentOs
 - Python3.8 
 - Stanza version: 1.0.0

**Additional context**

",
957,2020-05-10T03:43:51Z,https://github.com/stanfordnlp/stanza/issues/299,,"I need to use the tokenizer in both components, I wonder if the output of TokenizeProcessor in stanza same as the tokenizer in Stanford CoreNLP Client?

Kind regards",
958,2020-05-09T07:01:57Z,https://github.com/stanfordnlp/stanza/issues/298,,"
",
959,2020-05-08T03:36:51Z,https://github.com/stanfordnlp/stanza/issues/297,,"Hello, I'm running a pipeline with stanza and I get an error for the MWT analysis.
WARNING: Can not find mwt: default from official model list. Ignoring it.
Do you know what it is? For some reason this model is not low in stanza_resources.",
960,2020-05-07T18:31:56Z,https://github.com/stanfordnlp/stanza/issues/296,,"**Describe the bug**
In the file stanza/models/depparse/data.py:168, in the 'chunk_batches' method, the list 'res' contains the batches. However if the very first sentence is too long, then the condition in line 167 shall be true, and hence an empty 'current' list shall be appended to 'res'. Hence, when this function is called by the __init__ method, this empty batch shall be at the front of the list 'self.data'. This shall lead to an AssertionError in the __get_item__ method (line 102) where it checks length of the batch.

",
961,2020-05-07T18:15:41Z,https://github.com/stanfordnlp/stanza/pull/295,,Stanford CoreNLP is updated to 4.0,
962,2020-05-05T21:00:33Z,https://github.com/stanfordnlp/stanza/issues/294,,HDT has a lot more data than GSD and the resulting model should be a lot more robust!,
963,2020-05-05T11:55:22Z,https://github.com/stanfordnlp/stanza/issues/293,,"**Describe the bug**
After replace stanfordnlp with stanza I am experiencing disk usage & memory increase.
Additionaly CPU usage looks more stable. 


**Expected behavior**
As I changed old library with the new one with only PyTorch upgrade (1.4 > 1.5) I expect little or no change

**Environment (please complete the following information):**
 - OS: Ubuntu 18
 - Python version: 3.8.2
 - Stanza version: 1.0.1

**Additional context**
There is a service continuously parsing some text and after some time later throws exception.
I am using stanza with spacy_stanza (previously spacy_stanfornlp), when I increase batch size (pipe) I experience problem more often.

```
Pretrained file exists but cannot be loaded from /home/user/stanza_resources/tr/pretrain/imst.pt, due to the following exception:

11:15:12.065 -  ERROR - run_jobs.py               - run_jobs_in_order - Unexpected error: Traceback (most recent call last):
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/models/common/pretrain.py"", line 45, in load
    data = torch.load(self.filename, lambda storage, loc: storage)
  File ""/home/proj_home/.env/lib/python3.8/site-packages/torch/serialization.py"", line 593, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/proj_home/.env/lib/python3.8/site-packages/torch/serialization.py"", line 773, in _legacy_load
    result = unpickler.load()
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run_jobs.py"", line 116, in run_jobs_in_order
    parse_content(batch_id)
  File ""run_jobs.py"", line 31, in parse_content
    all(batch_id)
  File ""/home/proj_home/common/PerfUtils.py"", line 56, in _wrapper
    result = f(*args, **kwargs)
  File ""/home/proj_home/runparse.py"", line 165, in all
    nlp = CustomParser.loadParser(includeDepParse=True)
  File ""/home/proj_home/common/PerfUtils.py"", line 56, in _wrapper
    result = f(*args, **kwargs)
  File ""/home/proj_home/services/parse/api/CustomParserStanza.py"", line 48, in loadParser
    snlp = stanza.Pipeline(**config)
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/pipeline/core.py"", line 121, in __init__
    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/pipeline/processor.py"", line 103, in __init__
    self._set_up_model(config, use_gpu)
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/pipeline/pos_processor.py"", line 25, in _set_up_model
    self._trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/models/pos/trainer.py"", line 35, in __init__
    self.load(model_file, pretrain)
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/models/pos/trainer.py"", line 118, in load
    emb_matrix = pretrain.emb
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/models/common/pretrain.py"", line 39, in emb
    self._vocab, self._emb = self.load()
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/models/common/pretrain.py"", line 50, in load
    return self.read_pretrain()
  File ""/home/proj_home/.env/lib/python3.8/site-packages/stanza/models/common/pretrain.py"", line 58, in read_pretrain
    raise Exception(""Vector file is not provided."")
Exception: Vector file is not provided.
```
You can see changes by the red line:
![image](https://user-images.githubusercontent.com/279289/81063038-c7d99c80-8edf-11ea-8950-82da1cdc4408.png)

",
964,2020-05-05T08:39:43Z,https://github.com/stanfordnlp/stanza/issues/292,,"**Is your feature request related to a problem? Please describe.**
I'd like to use Stanza to get named entities in various languages but I have the problem that it uses different names for the same NER in a different language. For example, it uses PERSON in the English pipeline but PER in the Spanish pipeline. This is a problem for process automatization.

**Describe the solution you'd like**
A standard NER tags for all languages will be great

**Describe alternatives you've considered**
As for now, I have to create different processes for each language.

**Additional context**

",
965,2020-05-04T16:36:54Z,https://github.com/stanfordnlp/stanza/issues/291,,"Hi! Great package!

Both [NLTK](https://www.nltk.org/data.html#command-line-installation) and [Spacy](https://spacy.io/usage/models#download-pip) offer the option to install models from local files, as with:

`pip install /Users/you/en_core_web_sm-2.2.0.tar.gz`

Do you have any thoughts on adding this to stanza? This makes it easier to deploy in an environment where the resources for the code are pre-packaged on a disk; otherwise there's a lot of file transfer over network that has to be accomodated.

Let me know if you'd be open to a pull request on this, it should be fairly simple to make a .whl file, although it would be more complicated to add the options for the [custom install](https://github.com/stanfordnlp/stanza/blob/master/stanza/utils/resources.py#L239).

A small separate, but related issue: it would be great to have a STANZA_HOME variable to specify the data location (esp. useful for multiple users/deployments referencing a shared drive).

**Describe alternatives you've considered**
Currently the best way to install from local files seems to be downloading the default.zip and writing a bash script to set up the files in $HOME.

",
966,2020-05-03T23:23:27Z,https://github.com/stanfordnlp/stanza/pull/290,,"Only do a default logger if it is not already set.

Make client.py use the default stanza logger.",
967,2020-05-03T16:07:56Z,https://github.com/stanfordnlp/stanza/issues/289,,"Specific words in Chinese (Simp) text are incorrectly segmented into separate characters or pairs and separate characters. This entirly changes the mean when the tokens are used for more than character counting.

**To Reproduce**
Steps to reproduce the behavior:
import stanza
nlp = stanza.download('zh-hans', processors='tokenize,pos')
nlp = stanza.Pipeline(**{'processors': 'tokenize', 'lang': 'zh-hans',
                                  'tokenize_no_ssplit': True})
doc = nlp(""中国官方媒体《环球时报》呼应了赵的观点。该报尽管强调，赵立坚以个人身份发表了这些言论，他的言论引起“中国公众的类似质疑”。全球研究是加拿大全球化研究中心（Centre for Research on Globalization）的网站，该中心2001年在加拿大成立。美国独立事实核查网站PolitiFact称，该中心在911、疫苗和全球变暖等话题上都提出了似是而非的阴谋论。赵立坚在推特上发布的文章出自固定撰稿人罗曼诺夫（Larry Romanoff）之笔，他后来重申了自己先前文章（现已删除）的结论，即该病毒并非源自中国。"")
[[w.text for w in s.words] for s in doc.sentences]

**Expected behavior**
I had the output reviewed by a native speaker and she highlighted issues using a gray background.
![image](https://user-images.githubusercontent.com/30701731/80919109-2b3cc080-8d60-11ea-9b4b-84964f07d2bd.png)

**Environment (please complete the following information):**
 - OS: Window10 X64
 - Python version: 3.7
 - Stanza version: latest as of 3/May/2020

**Additional context**
It is proving very difficult to find a decent Chinese tokenizer. At first I tried StanfordNlp only to find the only model is Chinese (Trad). I then discovered Stanza and was pleasantly suprised to see support for Chinese Simp and Trad witha distinct difference in the outputs using Chinese Simp text.
",
968,2020-05-02T13:35:24Z,https://github.com/stanfordnlp/stanza/issues/288,,"I did dependency parsing using StanfordCoreNLP  using the code below
```
from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('stanford-corenlp-full-2018-10-05', lang='en')

sentence = 'The clothes in the dressing room are gorgeous. Can I have one?'
tree_str = nlp.parse(sentence)
print(tree_str)
```
And I got the output:

```(ROOT
  (S
    (NP
      (NP (DT The) (NNS clothes))
      (PP (IN in)
        (NP (DT the) (VBG dressing) (NN room))))
    (VP (VBP are)
      (ADJP (JJ gorgeous)))
    (. .)))
```
How can I get this same output in Stanza??
```
import stanza
from stanza.server import CoreNLPClient
classpath='/stanford-corenlp-full-2020-04-20/*'
client = CoreNLPClient(be_quite=False, classpath=classpath, annotators=['parse'], memory='4G', endpoint='http://localhost:8900')
client.start()
text = 'The clothes in the dressing room are gorgeous. Can I have one?'
ann = client.annotate(text)
sentence = ann.sentence[0]
dependency_parse = sentence.basicDependencies
print(dependency_parse)

```
In stanza It appears I have to split the sentences that makes up the sentence. Is there something I am doing wrong?

Please note that my objective is to extract noun phrases. ",
969,2020-04-30T18:27:45Z,https://github.com/stanfordnlp/stanza/issues/287,,"Hi, thanks for making this easy-to-use package!

I have tens of millions of paragraphs that needs NER. However, the speed limit is too much of a pain. Can I ask **if the following method is the fastest way?** By current speed, I need to wait 24 hours * 300 days to complete NER'ing all my data.

```
import stanza
self.stanza_ner = stanza.Pipeline(lang='en', processors='tokenize,ner')

# txt is a string of a pretokenized paragraph
doc = self.stanza_ner(txt)
ents = [ent.text for ent in doc.ents]
```

I have 64 CPU and 1.8T RAM, but I found that only 4 processes can be run at the same time before the speeds degrade largely for each process.

Thank you in advance for tips about speeding up the NER.",
970,2020-04-30T07:28:52Z,https://github.com/stanfordnlp/stanza/pull/286,,"…keeps those fields
",
971,2020-04-29T16:46:43Z,https://github.com/stanfordnlp/stanza/issues/285,,"I have a CoreNLP server running with annotators pre-loaded.

Annotating the same sentence using Stanza's CoreNLPClient is about 20 times slower compared to using StanfordLib's CoreNLP client (version 0.1.1).   We currently use stanfordnlp.server.CoreNLP  (even though it is deprecated) because Stanza's client is too slow.   

Here's code to reproduce test:


```
#pip install stanfordnlp==0.1.1
#pip install stanza

from stanfordnlp.server import CoreNLPClient     # Takes 2.6 seconds for 100 iterations
#from stanza.server import CoreNLPClient         # Takes 81.9 seconds for 100 iterations

import timeit


if __name__ == '__main__':

    text = """"""I have a problem with Caine FM-OTG (No hardware found) so I've followed the KB# 31312 (https://www.test.com/support/answers/31312.html).""""""

    client = CoreNLPClient(start_server=False, endpoint='http://localhost:9050', annotators=['depparse', 'lemma'])
    print(timeit.timeit('client.annotate(text)', number=100, globals = globals()))
```
",
972,2020-04-29T08:23:40Z,https://github.com/stanfordnlp/stanza/issues/284,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!

If you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. That would greatly help us in locating the issue faster and help you resolve it!
",
973,2020-04-29T07:59:42Z,https://github.com/stanfordnlp/stanza/pull/283,,"## Desciption
This removes the sys.exit(1) calls from model loading, and instead re-raises the exception (so it could be caught). This burned me on a recent project with an unexpected exit

## Fixes Issues
No issue currently filed

## Unit test coverage
Not covered explicitly

## Known breaking changes/behaviors
Since the uncaught exception will likely lead to an exit, it's unlikely to change anything.
",
974,2020-04-28T22:49:06Z,https://github.com/stanfordnlp/stanza/issues/282,,"Thanks a lot for this great package :+1: 
 
The POS tagging is really useful for a project I am working on and works pretty well, but I do occasionally find problem cases (only natural, as I'm specifically looking at examples of words where their POS differs by context!)

My two questions are:
**1. Are you interested in problems with POS tagging?** (ie is it worth reporting them / would you likely do something with that knowledge?)
**2. If so, what's the best way to report them?** (I'm guessing that one by one isn't terribly helpful; plus presumably examples of cases where it breaks would be handy - any guidelines on that front? Eg only interested if it often goes wrong or is one case enough?)

I could gather details as I go and then report them in a batch, but I'm keen not to spend time if it's just going to be of casual interest and not make a difference :slightly_smiling_face: 

To give an idea of the kind of examples I've got, ""august"" the adjective is consistently picked up as a proper noun (ie as if it were August)",
975,2020-04-28T20:41:25Z,https://github.com/stanfordnlp/stanza/issues/281,,"Hi, I'm attempting to run the basic demo for stanza in Ubuntu 18.04 (VM). I have a conda env with 3.7 because pip install stanza with 3.8 did not work (process said ""Killed"" while installing one of the whl files). When I try the demo step-by-step it seems to stop after trying to load the NER package while initializing the pipeline.
If you can see the attached screenshot, I'm getting a ""Killed"" message right after the NER package tries to load. Is this a memory allocation issue? If so, how much memory is this pipeline supposed to take?

![Screenshot from 2020-04-28 16-16-13](https://user-images.githubusercontent.com/57918949/80535390-b1c35d80-896e-11ea-803b-6d823c2a53b9.png)

Thank you
",
976,2020-04-28T08:18:37Z,https://github.com/stanfordnlp/stanza/issues/280,,"requests.exceptionsConnectionError: HTTPSConnectionPool......

Why not download the Chinese module, how to solve it?
为什么下载不了中文模块，怎么解决呢？",
977,2020-04-27T08:29:46Z,https://github.com/stanfordnlp/stanza/pull/279,,"## Desciption
Removes a global logger configuration by the stanza library, because a library setting global logging configuration is unexpected.

## Fixes Issues
Fixes #278, issue originally started by commit https://github.com/stanfordnlp/stanza/commit/b6079be3d884449acee44c2a54b2afdf03cd8b11

## Known breaking changes/behaviors
If someone is relying on stanza (the library) setting a global logger, that will break. No user of this library should be doing that, but there are some CLI applications in the repo which might expect that. ",
978,2020-04-27T08:17:23Z,https://github.com/stanfordnlp/stanza/issues/278,,"`__init__.py` sets a global logger, which overrides a logger set by an application using stanza. 

To reproduce, set a logger in your application, import stanza, log something. Minimal test case:

```
import logging
import sys
LOGGER = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format=""MY CUSTOM FORMAT %(asctime)s.%(msecs)03d %(levelname)s %(message)s"",
    datefmt=""%Y-%m-%d %H:%M:%S"",
    stream=sys.stdout)
LOGGER.info(""Before stanza"")

import stanza
LOGGER.info(""After stanza"")
```

Expected output:

```
MY CUSTOM FORMAT 2020-04-27 11:14:38.102 INFO Before stanza
MY CUSTOM FORMAT 2020-04-27 11:14:38.581 INFO After stanza
```

Current output:

```
MY CUSTOM FORMAT 2020-04-27 11:16:24.534 INFO Before stanza
2020-04-27 11:16:25 INFO: After stanza
```

",
979,2020-04-26T17:19:18Z,https://github.com/stanfordnlp/stanza/issues/277,,"**Describe the bug**
Here there is a simple code ran over two different sentences (I've already downloaded the english model)

```
import stanza

nlp = stanza.Pipeline('en') 
string = ""Barack Obama, the president of United States of America, was born in Hawaii.  He was elected president in 2008. ""
# string = ""Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44.""
doc = nlp(string)  
doc.sentences[0].print_dependencies()
```

So I make a run with the first sentence, then I comment it and I make a second run with the second sentence)
For the first sentences, I get this:

`('Barack', '13', 'nsubj:pass')
('Obama', '1', 'flat')
(',', '1', 'punct')
('the', '5', 'det')
('president', '1', 'appos')
('of', '8', 'case')
('United', '8', 'compound')
('States', '5', 'nmod')
('of', '10', 'case')
('America', '8', 'nmod')
(',', '13', 'punct')
('was', '13', 'aux:pass')
('born', '0', 'root')
('in', '15', 'case')
('Hawaii', '13', 'obl')
('.', '13', 'punct')`

For the first sentences, I get just this:

`('Dana', '0', 'root')`

That's really an anomalous behavior

**Environment:**
I've tested in two different enviroments:
 - OS: [Windows, CentOS]
 - Python version: [Windows: Python 3.8, CentOS: Python 3.6.2 from Anaconda]
 - Stanza version: [1.0.0]

",
980,2020-04-26T13:52:16Z,https://github.com/stanfordnlp/stanza/issues/276,,"I've tried Korean model of Stanza, but it failed in almost all cases.

```py
>>> import stanza
>>> nlp=stanza.Pipeline(""ko"")
>>> doc=nlp(""종지부를 나눈다."")
>>> print(doc)
[
  [
    {
      ""id"": ""1"",
      ""text"": ""종지부를"",
      ""lemma"": ""종지부+를"",
      ""upos"": ""NOUN"",
      ""xpos"": ""ncn+jco"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=0|end_char=4""
    },
    {
      ""id"": ""2"",
      ""text"": ""나눈다."",
      ""lemma"": ""나눈다."",
      ""upos"": ""PUNCT"",
      ""xpos"": ""sf"",
      ""head"": 1,
      ""deprel"": ""punct"",
      ""misc"": ""start_char=5|end_char=9""
    }
  ]
]
```

The verb ""나눈다"" (divide) is labeled as PUNCT with the following fullstop.",
981,2020-04-25T17:41:46Z,https://github.com/stanfordnlp/stanza/issues/275,,"**Is your feature request related to a problem? Please describe.**
I want to use Stanza within the company laptop. However, the firewall is blocking outside connections. We have a proxy server setup, however I can only define them when using pip install and conda install.

when running **stanza.download('en')**, it gives below error:
SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with URL: /stanfordnlp/stanza-resources/master/resources_1.0.0.json (Caused by SSLError(SSLError(""bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])"")))

**Describe the solution you'd like**
Can I manually download the English model and be able to use it with my stanza package?

",
982,2020-04-24T14:50:50Z,https://github.com/stanfordnlp/stanza/issues/274,,"Hi, when I tried the tokenization for the raw text of Czech, the tokenized output will return some words that do not exist in the text. For example, the input text is ```V šestnácti jsem byla pěkný rozmazlenec a nebyla jsem zvyklá, aby mi babička někdy nevyhověla.```, but the output becomes:
```
1       V       ADP
2       šestnácti       NUM
3       jsem    AUX
4       byla    AUX
5       pěkný   ADJ
6       rozmazlenec     NOUN
7       a       CCONJ
8       nebyla  AUX
9       jsem    AUX
10      zvyklá  ADJ
11      ,       PUNCT
12      aby     SCONJ
13      by      AUX
14      mi      PRON
15      babička NOUN
16      někdy   ADV
17      nevyhověla      VERB
18      .       PUNCT
```
The output gives an unexpected extra token ```by``` at 13-th place. How to solve this problem?",
983,2020-04-24T02:18:23Z,https://github.com/stanfordnlp/stanza/issues/273,,"I want to get the result of ssplit(split the doc to sentences) ?
But I do not know how to get .
The code in stanza/utils/resources.py as follows ?
PIPELINE_NAMES = [TOKENIZE, MWT, POS, LEMMA, DEPPARSE, NER]",
984,2020-04-23T19:21:52Z,https://github.com/stanfordnlp/stanza/issues/272,,"Hello,

  I am processing the following French sentence to get the dependency graph from Stanza

`Ce site contient quatre tombeaux de la dynastie achéménide et sept des Sassanides`

I am initializing Stanza as follows:

`self.nlp = stanza.Pipeline(
           lang='fr'
        )`

`stanza_doc = self.nlp(text)`

I get the following error while it is generating the dependency graph.

`File ""/home/ubuntu/.local/share/virtualenvs/BlaBla-ITc8YKAM/lib/python3.6/site-packages/stanza/models/common/doc.py"", line 401, in build_dependencies
    assert(int(word.head) == int(head.id))
AssertionError`

I traced it to this line, may be something needs to be checked there?

https://github.com/stanfordnlp/stanza/blob/35aec3b79207c45875cb0591ee1637f207c9106b/stanza/models/common/doc.py#L401

Please do let me know if I need to provide more information.

Thanks a lot for your help.",
985,2020-04-23T15:54:34Z,https://github.com/stanfordnlp/stanza/issues/271,,"When I use CoreNLP server using Python, I get the following error when processing sentences in Spanish.

`[pool-1-thread-3] INFO CoreNLP - [/127.0.0.1:45530] API call w/annotators tokenize,ssplit,mwt,pos,lemma,ner,depparse,parse,coref,kbp
Para visitar contactar primero con la dirección
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mwt
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-3] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/spanish-ud.tagger ... done [0.7 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-3] INFO edu.stanford.nlp.sequences.SeqClassifierFlags - sutime.language=spanish
[pool-1-thread-3] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz ... done [1.2 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[pool-1-thread-3] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/spanish.sutime.txt
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - spanish.number.regexner: Read 3 unique entries out of 3 from edu/stanford/nlp/models/kbp/spanish/gazetteers/kbp_regexner_number_sp.tag, 0 TokensRegex patterns.
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 50998 unique entries out of 50999 from edu/stanford/nlp/models/kbp/spanish/gazetteers/kbp_regexner_mapping_sp.tag, 0 TokensRegex patterns.
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - Using numeric classifiers: true
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - Using SUTime: true
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - Using fine grained: true
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz ...
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Done reading from disk ... Time elapsed: 2.2 sec

[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99990, Elapsed Time: 14.54 (s)
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [16.8 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[pool-1-thread-3] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/srparser/spanishSR.ser.gz ... done [5.9 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
[pool-1-thread-3] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [1.6 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: dependency
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator kbp
[pool-1-thread-3] WARN CoreNLP - java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error making document
  java.util.concurrent.FutureTask.report(FutureTask.java:122)
  java.util.concurrent.FutureTask.get(FutureTask.java:206)
  edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:923)
  com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
  sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
  com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
  sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
  com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
  sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  java.lang.Thread.run(Thread.java:748)
Caused by: class java.lang.RuntimeException: Error making document
  edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:55)
  edu.stanford.nlp.pipeline.CorefAnnotator.annotate(CorefAnnotator.java:160)
  edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
  edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:640)
  edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$0(StanfordCoreNLPServer.java:918)
  java.util.concurrent.FutureTask.run(FutureTask.java:266)
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  java.lang.Thread.run(Thread.java:748)
Caused by: class java.lang.IllegalArgumentException: No head rule defined for sn using class edu.stanford.nlp.trees.SemanticHeadFinder in (sn
  (spec (DET la))
  (grup.nom (NOUN dirección)))


  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineNonTrivialHead(AbstractCollinsHeadFinder.java:245)
  edu.stanford.nlp.trees.SemanticHeadFinder.determineNonTrivialHead(SemanticHeadFinder.java:445)
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:193)
  edu.stanford.nlp.trees.Tree.headTerminal(Tree.java:1107)
  edu.stanford.nlp.trees.Tree.headTerminal(Tree.java:1122)
  edu.stanford.nlp.coref.data.DocumentPreprocessor.fillMentionInfo(DocumentPreprocessor.java:330)
  edu.stanford.nlp.coref.data.DocumentPreprocessor.initializeMentions(DocumentPreprocessor.java:169)
  edu.stanford.nlp.coref.data.DocumentPreprocessor.preprocess(DocumentPreprocessor.java:62)
  edu.stanford.nlp.coref.data.DocumentMaker.makeDocument(DocumentMaker.java:92)
  edu.stanford.nlp.coref.data.DocumentMaker.makeDocument(DocumentMaker.java:64)
  edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:53)
  ...8 more
Traceback (most recent call last):
  File ""/home/ubuntu/.local/share/virtualenvs/BlaBla-ITc8YKAM/lib/python3.6/site-packages/stanza/server/client.py"", line 361, in _request
    r.raise_for_status()
  File ""/home/ubuntu/.local/share/virtualenvs/BlaBla-ITc8YKAM/lib/python3.6/site-packages/requests/models.py"", line 941, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9001/?properties=%7B%27outputFormat%27%3A+%27+json%27%7`

Can someone please help me resolve this? I dont seem to know what HeadFinder is unfortunately.

Thanks!",
986,2020-04-23T14:57:53Z,https://github.com/stanfordnlp/stanza/issues/270,,"I have the following messages printed on screen

> Downloading http://nlp.stanford.edu/software/stanza/1.0.0/fr/default.zip: 100%|██████████████████████████████████████████████████████████████| 589M/589M [00:41<00:00, 14.1MB/s]
2020-04-23 14:48:54 INFO: Finished downloading models and saved to stanza_resources.
2020-04-23 14:48:54 INFO: Loading these models for language: fr (French):
    =======================
    | Processor | Package |
    -----------------------
    | tokenize  | gsd     |
    | pos       | gsd     |
    | lemma     | gsd     |
    | depparse  | gsd     |
    | ner       | wikiner |
    =======================

2020-04-23 14:48:54 INFO: Use device: gpu
2020-04-23 14:48:54 INFO: Loading: tokenize
2020-04-23 14:48:56 INFO: Loading: pos
2020-04-23 14:48:57 INFO: Loading: lemma
2020-04-23 14:48:57 INFO: Loading: depparse
2020-04-23 14:48:58 INFO: Loading: ner
2020-04-23 14:49:00 INFO: Done loading processors!
Setting server defaults from: stanza_config/french_properties.txt
Starting server with command: java -Xmx4G -cp ./stanza_resources/corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties stanza_config/french_properties.txt -preload tokenize, ssplit, pos, depparse, parse
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - Setting default constituency parser to SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP -     Threads: 5
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9001
[pool-1-thread-3] INFO CoreNLP - [/127.0.0.1:45402] API call w/annotators tokenize,ssplit,pos,depparse,parse
La famille devra alors tout réapprendre
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-3] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/french-ud.tagger ... done [0.2 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/UD_French.gz ...
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Done reading from disk ... Time elapsed: 2.3 sec



[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99984, Elapsed Time: 15.277 (s)
[pool-1-thread-3] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [17.5 sec].
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[pool-1-thread-3] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/srparser/frenchSR.beam.ser.gz ... done [12.9 sec].

In the above messages I still see this

> [main] INFO CoreNLP - Setting default constituency parser to SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz

Is it really loading the English constituency parser or somewhere later is it loading the correct French parser?

I have pointed it to the French properties file which has the following:

> annotators = tokenize, ssplit, pos, depparse, parse
outputFormat = json
tokenize.language = fr
pos.model = edu/stanford/nlp/models/pos-tagger/french-ud.tagger
parse.model = edu/stanford/nlp/models/srparser/frenchSR.beam.ser.gz
depparse.model = edu/stanford/nlp/models/parser/nndep/UD_French.gz",
987,2020-04-23T13:53:05Z,https://github.com/stanfordnlp/stanza/issues/269,,"
>>> stanza.download(""en"")
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 100%|████████████████████████████████████████████| 449/449 [00:00<00:00, 120kB/s]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/stanza/utils/resources.py"", line 224, in download
    resources = json.load(open(os.path.join(dir, 'resources.json')))
  File ""/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py"", line 296, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File ""/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py"", line 348, in loads
    return _default_decoder.decode(s)
  File ""/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 2 column 1 (char 1)",
988,2020-04-23T00:50:45Z,https://github.com/stanfordnlp/stanza/pull/268,,"## Desciption
Support jieba as an alternative to the default neural tokenizer.

## Fixes Issues
N/A

## Known breaking changes/behaviors
N/A",
989,2020-04-22T18:03:52Z,https://github.com/stanfordnlp/stanza/issues/267,,"I am getting this exception even though I downloaded the English CoreNLP v4.0 into the corenlp folder. 

```
edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:925)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:823)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:797)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:320)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:273)
	at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:85)
	at edu.stanford.nlp.pipeline.POSTaggerAnnotator.<init>(POSTaggerAnnotator.java:73)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.posTagger(AnnotatorImplementations.java:53)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$3(StanfordCoreNLP.java:521)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:251)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:192)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:188)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(StanfordCoreNLPServer.java:368)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$800(StanfordCoreNLPServer.java:50)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:855)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.tagger.maxent.ExtractorNonAlphanumeric
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:720)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1923)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1806)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2097)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:628)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:874)
	... 27 more
Traceback (most recent call last):
  File ""/home/ubuntu/.local/share/virtualenvs/BlaBla-ITc8YKAM/lib/python3.6/site-packages/stanza/server/client.py"", line 361, in _request
    r.raise_for_status()
  File ""/home/ubuntu/.local/share/virtualenvs/BlaBla-ITc8YKAM/lib/python3.6/site-packages/requests/models.py"", line 941, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9001/?properties=%7B%27outputFormat%27%3A+%27+json%27%7D

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ubuntu/codebase/BlaBla/bla_bla/sentence_processor/sentence_processing_engine.py"", line 197, in setup
    self._const_pt = self.const_parse_tree()
  File ""/home/ubuntu/codebase/BlaBla/bla_bla/sentence_processor/sentence_processing_engine.py"", line 99, in const_parse_tree
    document = self.client.annotate(self._sent)
  File ""/home/ubuntu/.local/share/virtualenvs/BlaBla-ITc8YKAM/lib/python3.6/site-packages/stanza/server/client.py"", line 431, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/home/ubuntu/.local/share/virtualenvs/BlaBla-ITc8YKAM/lib/python3.6/site-packages/stanza/server/client.py"", line 367, in _request
    raise AnnotationException(r.text)
stanza.server.client.AnnotationException: edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
```",
990,2020-04-22T15:52:54Z,https://github.com/stanfordnlp/stanza/issues/266,,"I am currently using Stanza and I would like to mark all vowels and consonants in an input text given I know the languages. Does Stanza output this somewhere in some data structure?
",
991,2020-04-22T09:14:17Z,https://github.com/stanfordnlp/stanza/issues/265,,"PyTorch has a new way of serving models quickly via [TorchServe](https://pytorch.org/serve/) which presume is a way to serve binarized models just like in TF-Serving. I had earlier built a TF model and was able to get good prediction speed when served via TF-serving and writing my transformation wrappers on top of it in python. 

I was wondering if you guys have any way or advice to go about doing it for Stanza, and maybe we'll get better prediction speed here too? ",
992,2020-04-21T19:01:51Z,https://github.com/stanfordnlp/stanza/pull/264,,Update everything to use CoreNLP 400,
993,2020-04-21T08:54:14Z,https://github.com/stanfordnlp/stanza/issues/263,,"Hello,

  Just wanted to confirm if the whole CoreNLP based in Java is being converted to neural based in Python? If so, any idea when this is expected to be ready?

I love the Stanza Python support and neural networks based models and so was curious of how the CoreNLP will shape up in this direction in future.

Thanks",
994,2020-04-21T00:21:27Z,https://github.com/stanfordnlp/stanza/pull/262,,,
995,2020-04-20T19:13:31Z,https://github.com/stanfordnlp/stanza/pull/261,,"Make this test more readable

I need to figure out this interface a little better",
996,2020-04-20T19:12:57Z,https://github.com/stanfordnlp/stanza/pull/260,,Make this test more readable,
997,2020-04-20T14:56:47Z,https://github.com/stanfordnlp/stanza/issues/259,,"I am using the following piece of code to get the constituence parse tree output for the following French sentence:

> Je m'appelle Mondly

My code:

```
import stanza
import os
stanza.download('french', dir='.')
from stanza.server import CoreNLPClient
os.environ[""CORENLP_HOME""] = ""./corenlp""

FRENCH_CUSTOM_PROPS = {'annotators': 'tokenize,ssplit,pos,parse', 'tokenize.language': 'fr',
                       'pos.model': 'edu/stanford/nlp/models/pos-tagger/french/french.tagger',
                       'parse.model': 'edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz',
                       'outputFormat': 'text'}
client.register_properties_key('fr-custom', FRENCH_CUSTOM_PROPS)
client = CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','parse','depparse','coref'], timeout=30000, memory='4G', output_format = 'json', endpoint='http://localhost:9004')

text = ""Je m'appelle Bill Gates. Je m'appelle Bill Gates""

document = client.annotate(text, properties_key='fr-custom')
document[""sentences""][0][""parse""]
```

I get the following output for the last statement

> '(ROOT\n  (NP\n    (NP\n      (NP (NN Je) (NN m))\n      (`` `)\n      (NP (FW appelle)))\n    (NP (NNP Bill) (NNP Gates))))''

Is this the right usage for any non-English language to get the constituency parse tree?

If Yes, why do I see ""FW"" tag for the word ""appelle""? I couldn't find a matching POS tag for ""FW"" in the Universal Dependency Tags. Shouldn't be tagged with one of the POS tags?

Otherwise, am I doing something wrong in the code? ",
998,2020-04-17T17:25:41Z,https://github.com/stanfordnlp/stanza/pull/258,,,
999,2020-04-17T17:24:59Z,https://github.com/stanfordnlp/stanza/pull/257,,"**BEFORE YOU START**: please make sure your pull request is against the `dev` branch. 
We cannot accept pull requests against the `master` branch. 
See our [contributing guide](https://github.com/stanfordnlp/stanza/blob/master/CONTRIBUTING.md) for details.

## Desciption
A brief and concise description of what your pull request is trying to accomplish.

## Fixes Issues
A list of issues/bugs with # references. (e.g., #123)

## Unit test coverage
Are there unit tests in place to make sure your code is functioning correctly?
(see [here](https://github.com/stanfordnlp/stanza/blob/master/tests/test_tagger.py) for a simple example)

## Known breaking changes/behaviors
Does this break anything in Stanza's existing user interface? If so, what is it and how is it addressed?
",
1000,2020-04-17T11:18:39Z,https://github.com/stanfordnlp/stanza/issues/256,,When can I get the docker version of stanza？THK,
1001,2020-04-17T06:43:11Z,https://github.com/stanfordnlp/stanza/issues/255,,"This is my problem, when trying to download 'en' module.
Python 3.7.0   Apr 17 2020
Traceback (most recent call last):
  File ""stanza_ner.py"", line 3, in <module>
    stanza.download('en')
  File ""/usr/local/lib/python3.7/site-packages/stanza/utils/resources.py"", line 223, in download
    request_file(f'{DEFAULT_RESOURCES_URL}/resources_{__resources_version__}.json', os.path.join(dir, 'resources.json'))
  File ""/usr/local/lib/python3.7/site-packages/stanza/utils/resources.py"", line 83, in request_file
    download_file(url, path)
  File ""/usr/local/lib/python3.7/site-packages/stanza/utils/resources.py"", line 66, in download_file
    r = requests.get(url, stream=True)
  File ""/usr/local/lib/python3.7/site-packages/requests/api.py"", line 76, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 643, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/master/resources_1.0.0.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f5c7d581dd8>: Failed to establish a new connection: [Errno 111] Connection refused'))",
1002,2020-04-17T01:29:32Z,https://github.com/stanfordnlp/stanza/issues/254,,"Thank you for the great tools. I'm trying stanza in Python 3.6.9. When following the steps on guide try to download 'en' module, I'm encounter the problem below:
`Python 3.6.9 (default, Nov  7 2019, 10:44:02)
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import stanza
>>> stanza.download('en')
Traceback (most recent call last):
  File ""/home/xxxxxxx/.local/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py"", line 488, in wrap_socket
    cnx.do_handshake()
  File ""/usr/lib/python3/dist-packages/OpenSSL/SSL.py"", line 1808, in do_handshake
    self._raise_ssl_error(self._ssl, result)
  File ""/usr/lib/python3/dist-packages/OpenSSL/SSL.py"", line 1540, in _raise_ssl_error
    raise SysCallError(errno, errorcode.get(errno))
OpenSSL.SSL.SysCallError: (104, 'ECONNRESET')

During handling of the above exception, another exception occurred:
........`

How can I fix this?
Thanks",
1003,2020-04-16T21:21:32Z,https://github.com/stanfordnlp/stanza/pull/253,,,
1004,2020-04-16T21:21:03Z,https://github.com/stanfordnlp/stanza/pull/252,,,
1005,2020-04-16T07:02:07Z,https://github.com/stanfordnlp/stanza/pull/251,,"…ct if the port is available in the first place
",
1006,2020-04-16T06:29:24Z,https://github.com/stanfordnlp/stanza/issues/250,,"I want to have a image visualization for my dependencies Results but I do not know which api I can used .
thanks ",
1007,2020-04-15T18:48:08Z,https://github.com/stanfordnlp/stanza/issues/249,,"I've made an experiment that does the following:

1. Run the pipeline **""tokenize,pos,mwt""** on a pretokenized English text using the **CPU**
2. Run the pipeline **""tokenize,pos,mwt""** on a pretokenized English  text using the **GPU**
3. Run the pipeline **""tokenize,pos,mwt,lemma""** on a pretokenized English  text using the **CPU**
4. Run the pipeline **""tokenize,pos,mwt,lemma""** on a pretokenized English  text using the **GPU**

The second experiment (gpu+no_lemma) ran faster than the first one (cpu+no_lemma) which is expected as gpu is being used. However, the 4th experiment (gpu_lemma) ran slower than the third one(cpu+lemma) which is unexpected, is this normal? If yes then why? Is there something I can change in configuration that can make GPU performance comparable to the CPU? (I tried increasing lemma_batch_size to 5000 but still the same behavior) 

The pretokenized text can be downloaded from [here](https://drive.google.com/open?id=1tQqvcS7h3_x-vrUral4oTEz4C8P35voM)

The code I used for this experiment:


```
import sys
import time
import stanza
import numpy as np


def text_from_file(filename):
    f = open(filename, ""r"", encoding=""utf8"")
    text = f.read()
    f.close()
    return text


def print_result(times):
    print(""--- Average: %s seconds ---"" % np.average(times))
    print(""--- Minimum: %s seconds ---"" % np.min(times))
    print(""--- Maximum: %s seconds ---"" % np.max(times))
    print(""--- Standard Deviation: %s seconds ---"" % np.std(times))


def get_pipeline_stanza(models_dir, use_gpu=True, include_lemma=True):
    lemma_str = "",lemma"" if include_lemma else """"
    return stanza.Pipeline(
        dir=models_dir,
        tokenize_pretokenized=True,
        processors=""tokenize,pos,mwt{}"".format(lemma_str),
        use_gpu=use_gpu,
        package='ewt'
    )


def measure_runtime(nlp, num_runs):
    times = []
    for run_idx in range(num_runs):
        start = time.time()
        doc = nlp(text)
        end = time.time()
        times.append(end - start)
        print(""\rFinished {}/{} Runs... "".format(run_idx + 1, num_runs), end="""")
    print("""")
    print_result(times)


if __name__ == '__main__':
    models_dir = sys.argv[1]
    document_path = sys.argv[2]

    add_lemma_options = [False, False, True, True]
    use_gpu_options = [False, True, False, True]

    pipelines = [get_pipeline_stanza(models_dir=models_dir, use_gpu=use_gpu, include_lemma=include_lemma)
                      for use_gpu, include_lemma in zip(use_gpu_options, add_lemma_options)]
    text = text_from_file(filename=document_path)

    index = 0
    while index < 4:
        print('Measuring time with use_gpu={} and include_lemma={}'.
               format(use_gpu_options[index], add_lemma_options[index]))
        measure_runtime(pipelines[index], 5)
        print('')
        index += 1
```

And here is the output from this code:

```
Measuring time with use_gpu=False and include_lemma=False
Finished 5/5 Runs... 
--- Average: 1.960188388824463 seconds ---
--- Minimum: 1.9383649826049805 seconds ---
--- Maximum: 1.9839963912963867 seconds ---
--- Standard Deviation: 0.016525809209641883 seconds ---

Measuring time with use_gpu=True and include_lemma=False
Finished 5/5 Runs... 
--- Average: 0.7326192378997802 seconds ---
--- Minimum: 0.6423196792602539 seconds ---
--- Maximum: 1.000077247619629 seconds ---
--- Standard Deviation: 0.13569304248733163 seconds ---

Measuring time with use_gpu=False and include_lemma=True
Finished 5/5 Runs... 
--- Average: 4.689572477340699 seconds ---
--- Minimum: 4.63610315322876 seconds ---
--- Maximum: 4.723896265029907 seconds ---
--- Standard Deviation: 0.03234981515817973 seconds ---

Measuring time with use_gpu=True and include_lemma=True
Finished 5/5 Runs... 
--- Average: 12.985066032409668 seconds ---
--- Minimum: 12.798763513565063 seconds ---
--- Maximum: 13.09037709236145 seconds ---
--- Standard Deviation: 0.10000714443175517 seconds ---

```
**System:** Microsoft Windows 10 Pro
**CPU:** Intel Core i7-9750H @2.6 GHz 6 physical cores
**GPU:** Nvidia RTX 2060 (Laptop Version)
**RAM:** 16 GB
**Python:** Python 3.7.5 (tags/v3.7.5:5c02a39a0b, Oct 15 2019, 00:11:34) [MSC v.1916 64 bit (AMD64)] on win32
**Stanza:** 1.0.0
I installed torch using this command
`pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html
`
**CUDA:** 10.1
",
1008,2020-04-14T20:46:55Z,https://github.com/stanfordnlp/stanza/issues/248,,"**Describe the bug**
When the CoreNLP server fails to start (usually because the designated port 9000) is occupied, the CoreNLPClient usually throws obscure errors that make it look like the error is from some other source.

**To Reproduce**
Steps to reproduce the behavior:
1. Launch something that occupies the 9000 port, e.g., a dockerized version of the CoreNLP server;
2. Run
```python
from stanza.server import CoreNLPClient
client = CoreNLPClient(be_quiet=False, annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], memory='16G')
text = ""Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.""
document = client.annotate(text)
```
This prints something that looks like a serialization issue, but as you can see in the CoreNLP log it's a port binding failure.
```
Starting server with command: java -Xmx16G -cp /.../stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-7851276e01554842.props -preload tokenize,ssplit,pos,lemma,ner
Traceback (most recent call last):
  File ""/.../stanza/stanza/server/client.py"", line 362, in _request
    r.raise_for_status()
  File ""/.../python3.7/site-packages/requests/models.py"", line 941, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%27outputFormat%27%3A+%27serialized%27%7D

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../stanza/stanza/server/client.py"", line 432, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/.../stanza/stanza/server/client.py"", line 368, in _request
    raise AnnotationException(r.text)
stanza.server.client.AnnotationException: edu.stanford.nlp.io.RuntimeIOException: java.io.NotSerializableException: edu.stanford.nlp.naturalli.Polarity
>>> [main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 5
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.3 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
[main] INFO CoreNLP - Starting server...
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at sun.net.httpserver.ServerImpl.<init>(ServerImpl.java:100)
	at sun.net.httpserver.HttpServerImpl.<init>(HttpServerImpl.java:50)
	at sun.net.httpserver.DefaultHttpServerProvider.createHttpServer(DefaultHttpServerProvider.java:35)
	at com.sun.net.httpserver.HttpServer.create(HttpServer.java:130)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.run(StanfordCoreNLPServer.java:1427)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1523)
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.
```

**Expected behavior**
The client should throw a helpful error message that tells the user the root cause of the issue -- another program is bound to the port and the user should either terminate it or use another port to start the client.

**Environment (please complete the following information):**
 - OS: MacOS 10.15.4
 - Python version: Python 3.7.2 from Anaconda
 - Stanza version: 1.0.0 (also reproducible from head of `dev`)
",
1009,2020-04-14T19:13:04Z,https://github.com/stanfordnlp/stanza/issues/247,,"**Background and related problem**
For NLP there are clear generic tasks that almost all different researching projects have to do (POS tagging, dependency tagging, etc.) However, in some instances, there is the problem that no standardized implementation is available for different research (e.g. automatic summarization, essay scoring, text readability, and such).

**Possible solution architecture**
A plugin-like interface for some specific NLP projects. e.g. text readability, we add the plugin, then we can compute metrics such as: sentence count, word count, syllable count, lexical-semantic averages, emotion lexicon metrics, and such.

**Alternatives**
An alternative, would be taking advantage of the Doc interface provided by `stanza`. Maybe, we can add some capability, for different NLP specific problems based on the current architecture.

**Additional context**
For instance, I wrote a library built on spaCy (https://github.com/dpalmasan/TRUNAJOD2.0) for text complexity assessment; There are several other problems in which having kind of a benchmark would be cool.",
1010,2020-04-14T06:55:58Z,https://github.com/stanfordnlp/stanza/issues/246,,"I would like to see a SpaCy-style [Matcher](https://spacy.io/api/matcher) for Stanza. Is this in the works or is there a workaround, for now, to use the SpaCy API with Stanza Document objects? ",
1011,2020-04-13T06:38:30Z,https://github.com/stanfordnlp/stanza/issues/245,,"Hi! I know this is similar to #52 and #91 but I am unable to understand how that was solved.

When I run it on the commandline (Ubuntu : Ubuntu 16.04.6 LTS), it runs with success as below: 
```
java -Xmx16G -cp ""/home/naive/Documents/shrikant/Dialogue_Implement/DST/stanford-corenlp-full-2018-10-05/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-34d0c1fe4d724a56.props -preload tokenize,ssplit,pos,lemma,ner

[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP -     Threads: 5
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000

```

But when I run it with python script, it fail with error as below:
```

import os
os.environ[""CORENLP_HOME""] = '/home/naive/Documents/shrikant/Dialogue_Implement/DST/stanford-corenlp-full-2018-10-05'

# Import client module
from stanza.server import CoreNLPClient


client = CoreNLPClient(be_quite=False, classpath='""/home/naive/Documents/shrikant/Dialogue_Implement/DST/stanford-corenlp-full-2018-10-05/*""', annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], memory='16G', endpoint='http://localhost:9000')
print(client)

client.start()
#import time; time.sleep(10)

text = ""Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.""
print (text)
document = client.annotate(text)
print ('malviya')
print(type(document))
```
Error:
```
<stanza.server.client.CoreNLPClient object at 0x7fd296e40d68>
Starting server with command: java -Xmx4G -cp ""/home/naive/Documents/shrikant/Dialogue_Implement/DST/stanford-corenlp-full-2018-10-05""/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-9a4ccb63339146d0.props -preload tokenize,ssplit,pos,lemma,ner
Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.

Traceback (most recent call last):
  File ""stanza_eng.py"", line 18, in <module>
    document = client.annotate(text)
  File ""/home/naive/.conda/envs/torch_gpu36/lib/python3.6/site-packages/stanza/server/client.py"", line 431, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/home/naive/.conda/envs/torch_gpu36/lib/python3.6/site-packages/stanza/server/client.py"", line 342, in _request
    self.ensure_alive()
  File ""/home/naive/.conda/envs/torch_gpu36/lib/python3.6/site-packages/stanza/server/client.py"", line 161, in ensure_alive
    raise PermanentlyFailedException(""Timed out waiting for service to come alive."")
stanza.server.client.PermanentlyFailedException: Timed out waiting for service to come alive.

```
Python 3.6.10 
asn1crypto==1.3.0
certifi==2020.4.5.1
cffi==1.14.0
chardet==3.0.4
cryptography==2.8
embeddings==0.0.8
gast==0.2.2
idna==2.9
numpy==1.18.2
protobuf==3.11.3
pycparser==2.20
pyOpenSSL==19.1.0
PySocks==1.7.1
requests==2.23.0
six==1.14.0
stanza==1.0.0
torch==1.4.0
tqdm==4.44.1
urllib3==1.25.8
vocab==0.0.4

I am unable to understand the issue here...",
1012,2020-04-13T01:11:01Z,https://github.com/stanfordnlp/stanza/issues/244,,"**Describe the bug**
My program is raising an exception when I import stanza and try to set a value to stanza.Pipeline('en'): 

**The full exception follows:**

c:\python38\lib\site-packages\stanza\pipeline\core.py in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, **kwargs)
     75         resources_filepath = os.path.join(dir, 'resources.json')
     76         if not os.path.exists(resources_filepath):
---> 77             raise Exception(f""Resources file not found at: {resources_filepath}. Try to download the model again."")
     78         with open(resources_filepath) as infile:
     79             resources = json.load(infile)
**""Exception: Resources file not found at: {userPATH}\stanza_resources\resources.json. Try to download the model again.""**

I have uninstalled and reinstalled without any success.

**To Reproduce**
Steps to reproduce the behavior:
1. import stanza
2. stanza_nlp = stanza.Pipeline('en')
3. run program
4. see error

**Expected behavior**
The model should have started extracting entities from a list of sentences that I already have pre-processed. (I did this successfully on a different machine)

**Environment (please complete the following information):**
 - OS: Windows 10
 - Python version:Python 3.8
 - Stanza version: 1.0.0

**Additional context**
I have already run stanza successfully on a different machine following the same steps. I did not get this issue last time. I am still relatively new to python. Could be staring me in the face, but hopefully solving this will help the next newcomer.
",
1013,2020-04-12T16:32:35Z,https://github.com/stanfordnlp/stanza/issues/243,,"I'd like to use xpos tagset on a Spanish text. So according to documentation I should be able to access it using .xpos property of a Word object. Using a Pipeline builded with lang = ""en"" everything is ok.
The problem is that it doesn't work using lang = ""es"", where .pos and .xpos use the same tagset (that is Universal POS). 
I'd like to understand whether it's a missing feature, a bug, or I'm missing something.

Example
```
    import stanza
    stanza.download(""es"")
    es_nlp = stanza.Pipeline(""es"")
    es_doc = es_nlp(""Los expertos han tirado los planes a la papelera a la espera de una fecha de vuelta indeterminada"")
    for sent in es_doc.sentences:
        for word in sent.words:
            print(word.text + "" "" + word.xpos)
```
",
1014,2020-04-09T21:30:50Z,https://github.com/stanfordnlp/stanza/issues/242,,"While using stanza I noticed that some tokens were skipped for no apparent reason. For example, consider the following code:
```python
import stanza
example = ""cascare al suolo.""
p = stanza.Pipeline(processors=""tokenize"", lang=""it"")
p(example)
```
It outputs 
```json
[
  [
    {
      ""id"": ""1"",
      ""text"": ""cascare"",
      ""misc"": ""start_char=0|end_char=7""
    },
    {
      ""id"": ""3"",
      ""text"": ""suolo"",
      ""misc"": ""start_char=11|end_char=16""
    },
    {
      ""id"": ""4"",
      ""text"": ""."",
      ""misc"": ""start_char=16|end_char=17""
    }
  ]
]
```
where, as you may notice, the token ""al"" is missing. The sentence translates as follow:
falling (cascare) al (to) the ground (suolo).

Similarly this other code:
```python
import stanza
example = ""andargli incontro.""
p = stanza.Pipeline(processors=""tokenize"", lang=""it"")
p(example)
```
outputs the following:
```json
[
  [
    {
      ""id"": ""2"",
      ""text"": ""incontro"",
      ""misc"": ""start_char=9|end_char=17""
    },
    {
      ""id"": ""3"",
      ""text"": ""."",
      ""misc"": ""start_char=17|end_char=18""
    }
  ]
]
```
where the first word (andargli - going to him) is missing.

This looks weird to me but I haven't found any other open issue nor documented behaviour.
Do you guys have any thought about this or are aware on any possible workaround?

Thanks",
1015,2020-04-07T10:07:08Z,https://github.com/stanfordnlp/stanza/issues/241,,"Great work, do you have any plan for chinese document? I am glad to help.",
1016,2020-04-06T17:52:51Z,https://github.com/stanfordnlp/stanza/issues/240,,"As asked in the title, is there a redundant ""r"" in the following code?
https://github.com/stanfordnlp/stanza/blob/master/stanza/server/client.py#L475",
1017,2020-04-06T15:28:49Z,https://github.com/stanfordnlp/stanza/issues/239,,"Sorry to bother. But the instruction of '**stanza.download('**')**' is not working.

And the site of
1. http://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip
2. http://nlp.stanford.edu/software/stanza/1.0.0/zh-hans/default.zip
is unable to accesses.

Would the author you please share the files uploading to Google Driver or something?
Really sorry to bother you honor.",
1018,2020-04-05T16:48:05Z,https://github.com/stanfordnlp/stanza/issues/238,,"**Describe the bug**
Assertion error when extracting dependency parses for text in Finnish and Arabic languages.

Error is coming from this file: stanza/models/common/doc.py(401)build_dependencies()
and due to this assert statement: assert(int(word.head) == int(head.id))

**To Reproduce**
Steps to reproduce the behavior:

config = {'dir': 'saved_models/stanza_resources/',
              'lang': 'fi',
              'package': 'ftb',
              'processors': 'tokenize,pos,lemma,depparse',
              'tokenize_pretokenized': False,
              'depparse_batch_size': 1000}

Input text:
' Uppopallo on taktinen ja nopea kontaktilaji, jota pelataan syvissä, useimmiten uimahallien hyppyaltaissa. Uppopalloa pidetään kovana lajina, koska pääosin peli tapahtuu uima-al
taan pohjassa noin 4 metrin syvyydessä. Pelaaminen kuitenkin onnistuu, vaikkei olisi kovakuntoinen. Yksinkertaistettuna päämääränä pelissä on tarkoitus saada uppoava pallo allasp
äätyjen pohjassa oleviin koreihin.'

Here is the erroneous word which I found from debugging
word: {
  ""id"": ""5"",
  ""text"": ""olisi"",
  ""lemma"": ""olla"",
  ""upos"": ""AUX"",
  ""xpos"": ""V,Act,Cond,Sg3"",
  ""feats"": ""Mood=Cnd|Number=Sing|Person=3|VerbForm=Fin|Voice=Act"",
  ""head"": 5,
  ""deprel"": ""cop"",
  ""misc"": ""start_char=256|end_char=261""
}

head: {
  ""id"": ""6"",
  ""text"": ""kovakuntoinen."",
  ""lemma"": ""kovakuntoinen"",
  ""upos"": ""ADJ"",
  ""xpos"": ""A,Sg,Nom"",
  ""feats"": ""Case=Nom|Number=Sing"",
  ""head"": 3,
  ""deprel"": ""xcomp"",
  ""misc"": ""start_char=262|end_char=276""
}


**Expected behavior**
The expectation is that the head id should match with the information of it in the word.

**Environment (please complete the following information):**
 - OS: Ubuntu 18.04
 - Python version: python 3.6.8
 - Stanza version: 1.0.0

**Additional context**
A similar error occurs for 'Arabic' as well.
",
1019,2020-04-03T14:58:23Z,https://github.com/stanfordnlp/stanza/issues/237,,"I am trying to start with basic example that I found on this site: https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html and I get ModuleNotFoundError: No module named 'stanza.server'; 'stanza' is not a package

Although I change start of code to this:

```
import os
import stanza
from stanza.server import CoreNLPClient
os.environ[""CORENLP_HOME""] = r'StanfordCoreNLP'

print('---')
print('input text')
print('')

text = ""Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.""

print(text)

# set up the client
print('---')
print('starting up Java Stanford CoreNLP Server...')

# set up the client
with CoreNLPClient(annotators=['ssplit'], timeout=30000, memory='4G') as client:
etc... Like in example.
```
I am runnig Windows 10 64bit, 8gb ram. I did install PyTorch and stanza library through pip.",
1020,2020-04-01T20:07:04Z,https://github.com/stanfordnlp/stanza/issues/236,,"**Describe the bug**
I am able to run the example mentioned here under the header `Steps to setup from the official release`, I am able to store the output in the json file. Everything works fine.

But when I run the Stanza example mentioned in the documentation, it is always stuck at `[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000`. I checked my `$CORENLP_HOME` path is correct when I do `echo $CORENLP_HOME`. 

I did ask somebody to run the example on a linux system and works just fine, and also tried running it on 3 Mac systems and it runs into the same error every time. 

I did run it in the terminal. How else can I troubleshoot this?

**To Reproduce**
Steps to reproduce the behavior:

Run the example on a MacOS system. 

**Environment (please complete the following information):**
 - OS: MacOS Catalina 10.15.4
 - Python version:3.7.7
 - Stanza version: 1.0.0
 - Java: 1.8

**Additional context**
I am able to run a basic example mentioned in the Java CoreNLP documentation where it saves the output of the string into a json format text file. But when I run it through the python wrapper it is somehow stuck on listening and not responding, only on MacOS.
",
1021,2020-03-31T14:49:17Z,https://github.com/stanfordnlp/stanza/issues/235,,"I have the following piece of code:

```
import stanza

stanza.download('en', dir='.')
```

Now, when I do

```
self.config = {
		'processors': 'tokenize,lemma,pos,mwt,depparse,ner'
		}
		self.nlp = stanza.Pipeline(**self.config)
```

I get an error saying

`Exception: Resources file not found at: /home/ubuntu/stanza_resources/resources.json. Try to download the model again.`

Obviously it is trying to look into my home directory and not the current directory. How can I specify the correct path?",
1022,2020-03-31T10:24:22Z,https://github.com/stanfordnlp/stanza/issues/234,,"Before you start, make sure to check out:
* Our documentation: https://stanfordnlp.github.io/stanza/
* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html
* Github issues (especially closed ones)
Your question might have an answer in these places!

If you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!
",
1023,2020-03-30T11:57:51Z,https://github.com/stanfordnlp/stanza/issues/233,,"what should be the format of the NER data for training and evaluation?
",
1024,2020-03-29T22:11:04Z,https://github.com/stanfordnlp/stanza/issues/232,,"[removed]
",
1025,2020-03-28T08:14:23Z,https://github.com/stanfordnlp/stanza/issues/231,,"**Describe the bug**
When I wanna run the example code , there is something wrong about the 'depparse'.
```
    stanza.download('en')
    nlp = stanza.Pipeline('en')
    doc = nlp(""Barack Obama was born in Hawaii.  He was elected president in 2008."")
    print(doc.sentences[0].print_dependencies())
```

The console shows me the following things, and I don't know how to fix it :
```
2020-03-28 16:05:52 INFO: Downloading default packages for language: en (English)...
2020-03-28 16:05:52 INFO: File exists: /home/vodka/stanza_resources/en/default.zip.
2020-03-28 16:05:55 INFO: Finished downloading models and saved to /home/vodka/stanza_resources.
2020-03-28 16:05:55 INFO: Loading these models for language: en (English):
=========================
| Processor | Package   |
-------------------------
| tokenize  | ewt       |
| pos       | ewt       |
| lemma     | ewt       |
| depparse  | ewt       |
| ner       | ontonotes |
=========================

2020-03-28 16:05:55 INFO: Use device: cpu
2020-03-28 16:05:55 INFO: Loading: tokenize
2020-03-28 16:05:55 INFO: Loading: pos
2020-03-28 16:05:56 INFO: Loading: lemma
2020-03-28 16:05:56 INFO: Loading: depparse
2020-03-28 16:05:57 INFO: Loading: ner
2020-03-28 16:05:57 INFO: Done loading processors!
Traceback (most recent call last):
  File ""/home/vodka/Desktop/Linking/help_files/StanfordCoreNLP_Usage.py"", line 25, in <module>
    doc = nlp(""Barack Obama was born in Hawaii.  He was elected president in 2008."")
  File ""/home/vodka/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 173, in __call__
    doc = self.process(doc)
  File ""/home/vodka/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 167, in process
    doc = self.processors[processor_name].process(doc)
  File ""/home/vodka/anaconda3/lib/python3.7/site-packages/stanza/pipeline/depparse_processor.py"", line 42, in process
    preds += self.trainer.predict(b)
  File ""/home/vodka/anaconda3/lib/python3.7/site-packages/stanza/models/depparse/trainer.py"", line 74, in predict
    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)
  File ""/home/vodka/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/vodka/anaconda3/lib/python3.7/site-packages/stanza/models/depparse/model.py"", line 156, in forward
    diag = torch.eye(head.size(-1)+1, dtype=torch.bool, device=head.device).unsqueeze(0)
RuntimeError: ""eye"" not implemented for 'Bool'
```



**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: Python 3.7.3
 - Pytorch version: 1.2.0
 - Stanza version: 1.0.0",
1026,2020-03-28T00:21:21Z,https://github.com/stanfordnlp/stanza/issues/230,,"**Describe the bug**
I am trying to run the CoreNLP example as mentioned here - https://stanfordnlp.github.io/stanza/corenlp_client.html#setup. I set the  ```CORENLP_HOME``` by using the command 

```export CORENLP_HOME=/path/to/stanford-corenlp-full-2018-10-05```

when I do ```echo $CORENLP_HOME``` from my terminal I am able to see the path to the corenlp folder. 

But for some reason, while initiating the client in ```client.py``` the command ```os.getenv(""CORENLP_HOME"") was returning ```None``` while I did not understand why. So to get around that I manually set the env variable through a python command by doing:

```os.environ[""CORENLP_HOME""] = ""fullpath/to/my/stanford-corenlp-full-2018-10-05""```

Now it is able to start the server with the default arguments in the example, but gets stuck on this:

```
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
```
I am not sure why this is happening, did I miss something?

**To Reproduce**
Steps to reproduce the behavior:
1. Go to the example mentioned here https://stanfordnlp.github.io/stanza/corenlp_client.html#setup and try running it


**Expected behavior**
I would expect it to go ahead talk to the server and print everything mentioned in the example. 

**Environment (please complete the following information):**
 - OS: MacOS 10.15.4 
 - Python version: Python 3.7.6
 - Stanza version: 1.0.0
 - Java version: 1.8.0_241
",
1027,2020-03-27T18:44:13Z,https://github.com/stanfordnlp/stanza/issues/229,,"**Describe the bug**
Using the `tokenize_pretokenized=True` parameter when initializing a `Pipeline` object, and proceeding to use `processors='tokenize, ner'` to extract named entities results in inconsistent output, even when the tokenizations are identical.

**To Reproduce**
The following code (using pre-tokenized input)
```python3
stanza.download('en')
nlp = stanza.Pipeline(lang='en', processors='tokenize, ner', tokenize_pretokenized=True)

sents = [['Hello', 'John', 'Smith', ',', 'this', 'is', 'a', 'nifty', 'sentence', '.'], ['Farrell', 'Smyth', 'is', 'my', 'landlord', '.']]
doc = nlp(sents)

print([token for sent in doc.sentences for token in sent.ents])
```
produces the following output:
```
[{
  ""text"": ""John Smith"",
  ""type"": ""PERSON"",
  ""start_char"": 6,
  ""end_char"": 16
}, {
  ""text"": ""arrell Smyth "",
  ""type"": ""PERSON"",
  ""start_char"": 47,
  ""end_char"": 60
}]
```
**Note the off-by-one in `arrell Smyth `**


While the following code
```python3
stanza.download('en')
nlp = stanza.Pipeline(lang='en', processors='tokenize, mwt, pos, lemma, depparse, ner')

sents = 'Hello John Smith, this is a nifty sentence. Farrell Smyth is my landlord.'
doc = nlp(sents)

print([token for sent in doc.sentences for token in sent.ents])
```
Produces:
```
[{
  ""text"": ""John Smith"",
  ""type"": ""PERSON"",
  ""start_char"": 6,
  ""end_char"": 16
}, {
  ""text"": ""Farrell Smyth"",
  ""type"": ""PERSON"",
  ""start_char"": 44,
  ""end_char"": 57
}]
```

**Expected behavior**
One would expect the same output for both instances.

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.7.4
 - Stanza version: 1.0.0
",
1028,2020-03-27T11:03:56Z,https://github.com/stanfordnlp/stanza/pull/228,,"## Desciption
See issue #227.  Basically, it'd be nice to override the default resources dir and it's super easy to do.

## Fixes Issues
this fixes #227.  Basically fixes unwanted behavior when using `pip install -e .` and also makes the default resources dir user changeable. 
",
1029,2020-03-27T10:43:56Z,https://github.com/stanfordnlp/stanza/issues/227,,"**Is your feature request related to a problem? Please describe.**

1. I want to be able to specify the default dir for the stanza resources.  Specifically, I kind of hate that it's a visible dir in my home folder, so I wanted to put it in the `.cache` folder with a bunch of other models.
2. I wanted to install this library as a development version to make the above change, but then the `.egg_info` dir was not ignored when I did `git add`.

**Describe the solution you'd like**
1. allow to override the default location `$HOME/stanza_resources` by setting an environmental variable called `STANZA_RESOURCES_DIR`.
2. add the github default python .gitignore to the current .gitignore
",
1030,2020-03-27T03:47:15Z,https://github.com/stanfordnlp/stanza/issues/226,,"I ran code and it printed to me the following:

```
2020-03-27 05:43:52 INFO: Loading these models for language: ru (Russian):
=========================
| Processor | Package   |
-------------------------
| tokenize  | syntagrus |
| ner       | wikiner   |
=========================

2020-03-27 05:43:52 INFO: Use device: gpu
2020-03-27 05:43:52 INFO: Loading: tokenize
2020-03-27 05:43:54 INFO: Loading: ner
2020-03-27 05:43:55 INFO: Done loading processors!
2020-03-27 05:43:55 WARNING: Can not find ner: default from official model list. Ignoring it.
2020-03-27 05:43:55 INFO: Loading these models for language: uk (Ukrainian):
=======================
| Processor | Package |
-----------------------
| tokenize  | iu      |
=======================

2020-03-27 05:43:55 INFO: Use device: gpu
2020-03-27 05:43:55 INFO: Loading: tokenize
2020-03-27 05:43:55 INFO: Done loading processors!
```

As I can see it does not seem to support NER for the Ukrainian language. Is it correct?",
1031,2020-03-26T19:43:14Z,https://github.com/stanfordnlp/stanza/issues/225,,"I trained NER model on my text dataset. 
It was saved in ""saved_models"" directory. How to use that model?
What are parameters for nlp = stanza.Pipeline('ru',processors='tokenize,ner')?",
1032,2020-03-26T13:27:00Z,https://github.com/stanfordnlp/stanza/issues/224,,">>> import stanza
>>> nlp = stanza.Pipeline(lang='zh', processors='tokenize,ner')
2020-03-26 21:20:36 INFO: ""zh"" is an alias for ""zh-hans""
2020-03-26 21:20:36 INFO: Loading these models for language: zh-hans (Simplified_Chinese):
=========================
| Processor | Package   |
-------------------------
| tokenize  | gsdsimp   |
| ner       | ontonotes |
=========================

2020-03-26 21:20:36 INFO: Use device: cpu
2020-03-26 21:20:36 INFO: Loading: tokenize
2020-03-26 21:20:36 INFO: Loading: ner
2020-03-26 21:20:36 INFO: Done loading processors!
>>> doc = nlp(""我 爱 北京 天安门 ， 我 是 中国人"")
[1]    1684 segmentation fault  python
(base) ➜  ~ python",
1033,2020-03-26T12:14:34Z,https://github.com/stanfordnlp/stanza/issues/223,,"Hi,

I was just trying to run NERProcessor on below example:

>>> import stanza
>>> nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')
>>> doc = nlp(""Currently working at Leoforce as Senior Software Engineer."")
>>> print(*[f'entity: {ent.text}\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\n')
entity: Leoforce    type: ORG
entity: Senior Software Engineer    type: ORG

It's returning Senior Software Engineer as ORG, which is incorrect.

Can you please tell if I'm missing anything here ? 

Thanks.",
1034,2020-03-26T01:57:14Z,https://github.com/stanfordnlp/stanza/issues/222,,"
```
import stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize',use_gpu=True)
```
**When I run the code above,  runtime error happened, sometimes it is `CUDA error: out of memory`.**
```
2020-03-26 09:41:41 INFO: Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
=======================

2020-03-26 09:41:41 INFO: Use device: gpu
2020-03-26 09:41:41 INFO: Loading: tokenize
Traceback (most recent call last):
  File ""/home/xxx/Codes/check_stanza.py"", line 5, in <module>
    nlp=stanza.Pipeline(lang='en', processors='tokenize',use_gpu=True)
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/stanza/pipeline/core.py"", line 120, in __init__
    use_gpu=self.use_gpu)
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/stanza/pipeline/processor.py"", line 103, in __init__
    self._set_up_model(config, use_gpu)
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/stanza/pipeline/tokenize_processor.py"", line 38, in _set_up_model
    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/stanza/models/tokenize/trainer.py"", line 27, in __init__
    self.model.cuda()
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 260, in cuda
    return self._apply(lambda t: t.cuda(device))
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 187, in _apply
    module._apply(fn)
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 193, in _apply
    param.data = fn(param.data)
  File ""/home/xxx/anaconda3/envs/pt/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 260, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
```
but sometimes it is:
```
2020-03-26 09:50:12 INFO: Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
=======================

2020-03-26 09:50:12 INFO: Use device: gpu
2020-03-26 09:50:12 INFO: Loading: tokenize
[1]    84111 segmentation fault (core dumped)  /home/xxx/anaconda3/envs/pt/bin/python 
```
Could you tell me what happened?
I will be appreciate for your help. Thank you very much.",
1035,2020-03-25T17:27:04Z,https://github.com/stanfordnlp/stanza/issues/221,,"The following line in the README seems to be unnecessary or deprecated. Using a Jupyter notebook, I didn't need to use force=True and it downloaded just fine with no prompt. In fact, when I did include the flag I got a message: unexpected argument 'force'. 

```
# IMPORTANT: The above line prompts you before downloading, which doesn't work well in a Jupyter notebook.
# To avoid a prompt when using notebooks, instead use: >>> stanza.download('en', force=True)
```

**Environment (please complete the following information):**
 - OS: MacOS
 - Python version: Python 3.8 from Anaconda
 - Stanza version: stanza-1.0.0-py3
",
1036,2020-03-25T14:41:29Z,https://github.com/stanfordnlp/stanza/issues/220,,"**Describe the bug**
I am experiencing improper behavior of stanza pos tagger for polish compared to other packages (flair, spacy-pl, udpipe). I performed experiment of 178299 polish search queries (google) that range between 1 to 14 words. All of them do end with ""."" which may signal the end of sentence. From all of them `stanza` produces largest number of PUNCT tags for last word (few examples below). I get something like this:

| Model | #PUNCT |
| -------- | ------------ |
| Flair    | 40 |
| Stanza | 29813 |
| UDPipe | 320 |
| spaCy-pl (sigmoidal) | 125 |  

Stanza has significantly larger number of PUNCT tags than other methods. As far as I know, stanza, flair and udpipe are trained on Universal Dependencies.

**To Reproduce**
I am listing here 5 examples of different length to demonstrate improper behavior:
```
import stanza
stanza.download('pl')

nlp = stanza.Pipeline(lang='pl', processors='tokenize,pos')

queries = [
   'facebook',
   'poczta onet',
   'nowoczesna diagnostyka raka prostaty',
   'pkp rozkład jazdy',
   'boli mnie prawe jądro i podbrzusze'
]

batch = ""\n\n"".join(queries)

doc = nlp(batch)

for sent in doc.sentences:
   for word in sent.words:
      print(sent.text, word.text, word.upos)
```
**Expected behavior**
Last word in sentence should not be labeled as PUNCT.

**Environment (please complete the following information):**
 - OS: Ubuntu 18.04
 - Python 3.6.9
 - stanza==1.0.0
",
1037,2020-03-25T12:12:54Z,https://github.com/stanfordnlp/stanza/issues/219,,"In this sentence below, ""İzmit"" which is a city name in Turkish gets lemmatized mistakenly as ""**_izmitat_**""

**To Reproduce**
Sentence: ""İyi güzel de **İzmit**’te henüz yeni i20 üretimi bile başlayamadı ki.""

Response:

```

[
  [
    {
      ""id"": ""1"",
      ""text"": ""İyi"",
      ""lemma"": ""iyi"",
      ""upos"": ""ADJ"",
      ""xpos"": ""Adj"",
      ""head"": 2,
      ""deprel"": ""amod"",
      ""misc"": ""start_char=0|end_char=3""
    },
    {
      ""id"": ""2"",
      ""text"": ""güzel"",
      ""lemma"": ""güzel"",
      ""upos"": ""ADJ"",
      ""xpos"": ""Adj"",
      ""head"": 4,
      ""deprel"": ""amod"",
      ""misc"": ""start_char=4|end_char=9""
    },
    {
      ""id"": ""3"",
      ""text"": ""de"",
      ""lemma"": ""de"",
      ""upos"": ""CCONJ"",
      ""xpos"": ""Conj"",
      ""head"": 2,
      ""deprel"": ""advmod:emph"",
      ""misc"": ""start_char=10|end_char=12""
    },
    {
      ""id"": ""4"",
      ""text"": ""İzmit’te"",
      ""lemma"": ""izmitat"",
      ""upos"": ""NOUN"",
      ""xpos"": ""Noun"",
      ""feats"": ""Case=Loc|Number=Sing|Person=3"",
      ""head"": 10,
      ""deprel"": ""obl"",
      ""misc"": ""start_char=13|end_char=21""
    },
    {
      ""id"": ""5"",
      ""text"": ""henüz"",
      ""lemma"": ""henüz"",
      ""upos"": ""ADV"",
      ""xpos"": ""Adverb"",
      ""head"": 8,
      ""deprel"": ""advmod"",
      ""misc"": ""start_char=22|end_char=27""
    },
    {
      ""id"": ""6"",
      ""text"": ""yeni"",
      ""lemma"": ""yeni"",
      ""upos"": ""ADJ"",
      ""xpos"": ""Adj"",
      ""head"": 7,
      ""deprel"": ""amod"",
      ""misc"": ""start_char=28|end_char=32""
    },
    {
      ""id"": ""7"",
      ""text"": ""i20"",
      ""lemma"": ""i20"",
      ""upos"": ""NOUN"",
      ""xpos"": ""Abr"",
      ""feats"": ""Case=Nom|Number=Sing|Person=3"",
      ""head"": 8,
      ""deprel"": ""nmod:poss"",
      ""misc"": ""start_char=33|end_char=36""
    },
    {
      ""id"": ""8"",
      ""text"": ""üretimi"",
      ""lemma"": ""üretim"",
      ""upos"": ""NOUN"",
      ""xpos"": ""Noun"",
      ""feats"": ""Case=Nom|Number=Sing|Number[psor]=Sing|Person=3|Person[psor]=3"",
      ""head"": 10,
      ""deprel"": ""nsubj"",
      ""misc"": ""start_char=37|end_char=44""
    },
    {
      ""id"": ""9"",
      ""text"": ""bile"",
      ""lemma"": ""bile"",
      ""upos"": ""ADV"",
      ""xpos"": ""Adverb"",
      ""head"": 8,
      ""deprel"": ""advmod:emph"",
      ""misc"": ""start_char=45|end_char=49""
    },
    {
      ""id"": ""10"",
      ""text"": ""başlayamadı"",
      ""lemma"": ""başla"",
      ""upos"": ""VERB"",
      ""xpos"": ""Verb"",
      ""feats"": ""Aspect=Perf|Mood=Ind|Number=Sing|Person=3|Polarity=Neg|Tense=Past"",
      ""head"": 0,
      ""deprel"": ""root"",
      ""misc"": ""start_char=50|end_char=61""
    },
    {
      ""id"": ""11"",
      ""text"": ""ki"",
      ""lemma"": ""ki"",
      ""upos"": ""CCONJ"",
      ""xpos"": ""Conj"",
      ""head"": 10,
      ""deprel"": ""advmod:emph"",
      ""misc"": ""start_char=62|end_char=64""
    },
    {
      ""id"": ""12"",
      ""text"": ""."",
      ""lemma"": ""."",
      ""upos"": ""PUNCT"",
      ""xpos"": ""Punc"",
      ""head"": 10,
      ""deprel"": ""punct"",
      ""misc"": ""start_char=64|end_char=65""
    }
  ]
]

```",
1038,2020-03-24T22:29:52Z,https://github.com/stanfordnlp/stanza/issues/218,,"The link here:

https://stanfordnlp.github.io/stanza/tutorials.html

Interactive tutorial - ""A Beginner’s Guide to Stanza"" is broken.

It points to 

https://github.com/stanfordnlp/stanza/blob/master/demo/StanfordNLP_Beginners_Guide.ipynb

while the correct URL is

https://github.com/stanfordnlp/stanza/blob/master/demo/Stanza_Beginners_Guide.ipynb",
1039,2020-03-24T19:53:29Z,https://github.com/stanfordnlp/stanza/issues/217,,"If there's leading punctuation in the string, the Vietnamese tokenizer raises an AssertionError.

**To Reproduce**
Steps to reproduce the behavior:
```python
import stanza
stanza.download(lang=""vi"")
s = stanza.Pipeline(lang=""vi"")
s(""- tuyệt vời!"")
```
This gives:
```
Traceback (most recent call last):
    ...
    doc = self.nlp(s)
  File ""/Users/arya/anaconda3/envs/staple/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 173, in __call__
    doc = self.process(doc)
  File ""/Users/arya/anaconda3/envs/staple/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 167, in process
    doc = self.processors[processor_name].process(doc)
  File ""/Users/arya/anaconda3/envs/staple/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py"", line 81, in process
    data = paras_to_chunks(text, dummy_labels)
  File ""/Users/arya/anaconda3/envs/staple/lib/python3.7/site-packages/stanza/utils/postprocess_vietnamese_tokenizer_data.py"", line 38, in paras_to_chunks
    return [para_to_chunks(re.sub('\s', ' ', pt.rstrip()), pc) for pt, pc in zip(text.split('\n\n'), char_level_pred.split('\n\n'))]
  File ""/Users/arya/anaconda3/envs/staple/lib/python3.7/site-packages/stanza/utils/postprocess_vietnamese_tokenizer_data.py"", line 38, in <listcomp>
    return [para_to_chunks(re.sub('\s', ' ', pt.rstrip()), pc) for pt, pc in zip(text.split('\n\n'), char_level_pred.split('\n\n'))]
  File ""/Users/arya/anaconda3/envs/staple/lib/python3.7/site-packages/stanza/utils/postprocess_vietnamese_tokenizer_data.py"", line 24, in para_to_chunks
    assert len(lastpred) > 0
AssertionError

```

**Expected behavior**
Not crashing.

**Environment (please complete the following information):**
 - OS: macOS
 - Python version: Anaconda Python 3.7
 - Stanza version: latest

**Additional context**
The issue stems from lastpred being empty because you haven't ever reached this line: https://github.com/stanfordnlp/stanza/blob/b73fb996b1cc2ea339acf4668f484a9c3e298434/stanza/utils/postprocess_vietnamese_tokenizer_data.py#L29

This issue also affects StanfordNLP.",
1040,2020-03-24T13:24:10Z,https://github.com/stanfordnlp/stanza/issues/216,,"I don't set up the client. You can help me.
  File ""C:\Users\HP\Anaconda3\lib\site-packages\stanza\server\client.py"", line 240, in __init__
    super().__init__(start_cmd, stop_cmd, endpoint,
TypeError: super(type, obj): obj must be an instance or subtype of type
 Please help me,",
1041,2020-03-20T10:38:47Z,https://github.com/stanfordnlp/stanza/issues/215,,"Hey~ Since the simplified Chinese Version of UD has been built up, I'm wondering if related models can be added for the pipeline. We're appriciated for your work.
AND by the way, is it possible to use pre-tokenized sentences for the pipeline?
https://universaldependencies.org/treebanks/zh_gsdsimp/index.html",
1042,2020-03-18T19:48:42Z,https://github.com/stanfordnlp/stanza/issues/214,,Stanza 0.3 allowed CoreNLPClient to open a remote connection to a CoreNLP server on any remote machine. This feature does not seem to be available any more. Is there any way to make a remote connection now?,
1043,2020-03-18T16:19:22Z,https://github.com/stanfordnlp/stanza/issues/213,,"In my code, I have some code like this

```
def load_nlp(lang: str, tokenize_pretokenized: bool = True, use_gpu: bool = True):
    stanza.download(lang)
    return stanza.Pipeline(
        processors=""tokenize,pos,lemma,depparse"",
        lang=lang,
        tokenize_pretokenized=tokenize_pretokenized,
        use_gpu=use_gpu,
        logging_level='WARN'
    )
```

This is called in multiple concurrent processes, but for some reason it leads to errors _even if the models have been download prior to running the code in parallel_. The error goes away when I remove the download statement. Trace:

```
File ""c:\dev\python\syntactic-equivalence-metrics\astred\utils.py"", line 42, in load_nlp
    stanza.download(lang)
  File ""C:\Users\bramv\.virtualenvs\modelling-tree-edit-distance-KGHJ0GSx\lib\site-packages\stanza\utils\resources.py"", line 224, in download
    resources = json.load(open(os.path.join(dir, 'resources.json')))
  File ""c:\users\bramv\appdata\local\programs\python\python36\Lib\json\__init__.py"", line 299, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File ""c:\users\bramv\appdata\local\programs\python\python36\Lib\json\__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""c:\users\bramv\appdata\local\programs\python\python36\Lib\json\decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""c:\users\bramv\appdata\local\programs\python\python36\Lib\json\decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```



**Environment (please complete the following information):**
 - OS: Ubuntu 18.10 and Windows 10
 - Python version: Python 3.7
 - StanfordNLP version: Stanza 1.0.0

**Additional context**
I don't think that this problem occurred with `stanfordnlp`.
",
1044,2020-03-18T15:39:16Z,https://github.com/stanfordnlp/stanza/pull/212,,"Currently, it seems that there is a lot of older code mixed with new code. There are some design inconsistencies. One thing that is quite important, though, is the closing of file handles. 

I first made this PR because I am running into JSON decoding errors when using multiple processes and I thought this would fix that (related to locked files), but that is not the case. I'll make a separate issue.",
1045,2020-03-18T12:04:04Z,https://github.com/stanfordnlp/stanza/issues/211,,"**Describe the bug**

It seems quite strange that the a Word's index is a string whereas its head is an int. This is documented as well (https://stanfordnlp.github.io/stanza/data_objects.html), but I don't quite understand why. Why is the id a str? This makes comparisons and tracking of head/id a bit annoying because we need to cast the values manually.


**Environment (please complete the following information):**
 - OS: Windows
 - Python version: 3.7
 - StanfordNLP version: Stanza 1.0.0

",
1046,2020-03-18T09:40:15Z,https://github.com/stanfordnlp/stanza/issues/210,,"I have installed new stanza package in a clean environment.
Previous version have lemmatization problems like numbers lemmatized as ""2n19"" (for text:2019)
These problems are solved as far as I can see from few samples but current problem is if I do not add mtw in processor it either throw exception or gave empty token. (In Turkish language model)

Steps to reproduce the behavior:
After installing package and downloading model with ""stanza.downlaod('tr')""
```python
import stanza

config = {
    #'processors': 'tokenize,mwt,pos,lemma,depparse',
    'processors': 'tokenize,pos,lemma,depparse',
    'lang': 'tr',
}

nlp = stanza.Pipeline(**config)  # This sets up a default neural pipeline in English

text = ""Bu bir cümledir.""
#text = ""29 bin 689 sağlık personeli alınacak.""
doc = nlp(text)

for sent in doc.sentences:
    sent.print_tokens()
```
For the first sentence result is as follow:
```
<Token id=1;words=[<Word id=1;text=Bu;lemma=bu;upos=PRON;xpos=Demons;feats=Case=Nom|Number=Sing|Person=3|PronType=Dem;head=2;deprel=nsubj>]>
<Token id=2;words=[<Word id=2;text=bir;lemma=bir;upos=ADV;xpos=Adverb;head=0;deprel=root>]>
<Token id=3;words=[]>
<Token id=4;words=[<Word id=4;text=.;lemma=.;upos=PUNCT;xpos=Punc;head=2;deprel=punct>]>
```
if I add mwt to the processor I got token 3 as follows:
```
<Token id=3-4;words=[<Word id=3;text=cümle;lemma=cümle;upos=NOUN;xpos=Noun;feats=Case=Nom|Number=Sing|Person=3;head=0;deprel=root>, <Word id=4;text=dir;lemma=i;upos=AUX;xpos=Zero;feats=Aspect=Perf|Mood=Gen|Number=Sing|Person=3|Tense=Pres;head=3;deprel=cop>]>
```

**Expected behavior**
If I don't use mwt I should be able to see 3. word as a single token as text:cümledir lemma: cümle

**Environment (please complete the following information):**
 - OS: Windows 10
 - Python version: 3.7.6
 - StanfordNLP version: stanza 1.0

**Additional context**
Another but related problem is with 2. sentence in the example code above.
If you try it without mwt it will throw exception as follows:

```python
Traceback (most recent call last):
  File ""c:/StanzaTest/t.py"", line 16, in <module>
    doc = nlp(text)
  File ""c:\StanzaTest\.env\lib\site-packages\stanza\pipeline\core.py"", line 173, in __call__
    doc = self.process(doc)
  File ""c:\StanzaTest\.env\lib\site-packages\stanza\pipeline\core.py"", line 167, in process
    doc = self.processors[processor_name].process(doc)
  File ""c:\StanzaTest\.env\lib\site-packages\stanza\pipeline\depparse_processor.py"", line 47, in process
    sentence.build_dependencies()
  File ""c:\StanzaTest\.env\lib\site-packages\stanza\models\common\doc.py"", line 401, in build_dependencies
    assert(int(word.head) == int(head.id))
AssertionError
```

If you add mwt you will get 5. token as follows (which is wrong lemmazated as well and it is not multiword)
```
text: personeli lemma: personel but parsed as multiword and li became another token (but this can be related with model - it is parsed right previous version)
<Token id=5-6;words=[<Word id=5;text=persone;lemma=person;upos=NOUN;xpos=Noun;feats=Case=Nom|Number=Sing|Person=3;head=7;deprel=obl>, <Word id=6;text=li;lemma=li;upos=ADP;xpos=With;head=5;deprel=case>]>
```

",
1047,2020-03-18T09:33:25Z,https://github.com/stanfordnlp/stanza/issues/209,,"I want to segment sentences,  I thought there should be a processor named ""ssplit"".
But I'm wrong.
when I run this command, it returned ""AssertError"".
`>>> stanza.download(lang='en',processors='ssplit')`

I wish I could use stanza to segment sentences, like:
```
input: 'First tree. Second tree. Third tree?'
output: ['First tree.', 'Second tree.', 'Third tree?']
```
How could I do it with stanza? Thank you",
1048,2020-03-18T03:19:58Z,https://github.com/stanfordnlp/stanza/pull/208,,"Hi,

I think it was supposed to be `START_CHAR`, not `END_CHAR` (duplicated key in that dict)?

Cheers
Bruno",
1049,2020-03-18T03:18:19Z,https://github.com/stanfordnlp/stanza/pull/207,,"Probably not important as part of the demo, and not breaking anything really.",
1050,2020-03-18T02:59:13Z,https://github.com/stanfordnlp/stanza/issues/206,,"Hello, 
I tried to download resources with the new stanza but failed with "" module 'stanza' has no attribute 'download'"". I creatde a new env and only install stanza through pip, but failed to run ""stanza.download('en')"" too.  

",
1051,2020-03-17T17:02:52Z,https://github.com/stanfordnlp/stanza/issues/205,,"Hi there

I am comparing the model pages of [stanfordnlp](https://stanfordnlp.github.io/stanfordnlp/models.html) and [stanza](https://stanfordnlp.github.io/stanza/models.html) and I see that for stanfordnlp the version is 0.2.0 and for stanza it is 1.0.0. Does that mean that the models have been changed? Does that imply different performance?

Thanks",
1052,2020-03-16T15:20:37Z,https://github.com/stanfordnlp/stanza/issues/204,,"StanfordCoreNLP provides ""prase"" method and ""dependency_parse"" method, but I can only find ""dependency_parse"" method in stanfordnlp. How can I get a tree like the one in the picture below？
![20190524214347936](https://user-images.githubusercontent.com/29458631/76772954-ae1fb100-67dc-11ea-98b4-6dae69da50b6.png)
",
1053,2020-03-07T04:06:13Z,https://github.com/stanfordnlp/stanza/issues/203,,"My python version is 3.7.5 and i also tried on 3.6 on windows and linux both but they are giving the same issue as:
ERROR: Could not find a version that satisfies the requirement torch==1.4.0+cpu (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch==1.4.0+cpu
I tried to install torch also but giving the same result.",
1054,2020-03-03T13:19:40Z,https://github.com/stanfordnlp/stanza/issues/202,,"Hi there,
I'm trying to use stanfordNLP on a column of dataframe which has around 4546 rows. What I am trying to achieve is to have each sentences(each row) cut and get a new column with processed text contents.
I keep getting error message in jupyter notebook:
The kernel appears to have died. It will restart automatically.

I could not even run on a single string obejct.

The import and constructing pipeline are successful with no error. Whenever i feed text into en_nlp or zh_nlp, its showing the kernel issue. Would like to know how can i resolve?

Environment:
MacOS 10.14.6
stanfordnlp 0.2.0
conda 4.8.2
jupyter 1.0.0
jupyter_client 5.3.4
jupyter_console 6.1.0
jupyter_core 4.6.1

Thanks a lot!",
1055,2020-03-03T04:25:28Z,https://github.com/stanfordnlp/stanza/issues/201,,"I'm trying to use the un-tokenizer from PTBTokenizer with this library and trying to recreate the below `java` command using a `CoreNLPClient` / other call:
`java -cp ""*"" edu.stanford.nlp.process.PTBTokenizer -untok < input_fn.txt > sample-untok.txt`

I tried running it as properties supplied with the CoreNLP `tokenize` annotator, but no luck.

Is there another way to do it or is the best option to write a wrapper that executes the PTB commands?",
1056,2020-03-01T17:05:17Z,https://github.com/stanfordnlp/stanza/issues/200,,"Hi, thanks for your library.

It would be great if we could suppress the print statement that the library does. In particular, the processors' loading message are a disturbance. On a single-process it's already a bit annoying, but in my case I sometimes run 20+ processes in parallel which, you can imagine, is a pain to look at it.

Better would be to use Python's logging module. That allows you to print these messages on an `info` level, which the users can then choose the show or hide through the logging API.",
1057,2020-02-26T10:24:47Z,https://github.com/stanfordnlp/stanza/issues/199,,"I want to output the probability of each POS of its word behind its POS，but I don't find related command in the mannual and I find it is hard to debug the related code in jar package .Could you give me some suggestions about how to get the probability or whether there is command that I can get the  probability.
Thanks!",
1058,2020-02-26T08:48:30Z,https://github.com/stanfordnlp/stanza/issues/198,,"**Describe the solution you'd like**
A lot of times, analysis and word relation become easier due to the RDF triples format or triplese format of the dependency parser.  It is available in multiple ways to achieve the same . so Stanfordnlp should also be supporting it no ?
",
1059,2020-02-25T02:36:26Z,https://github.com/stanfordnlp/stanza/issues/197,,"Hi,

Do you have any suggestions on the workflow I should follow to train and use my own ner model with the python interface? Is there a way to achieve this only using python?

Thanks!",
1060,2020-02-21T21:47:06Z,https://github.com/stanfordnlp/stanza/pull/196,,,
1061,2020-02-21T00:21:38Z,https://github.com/stanfordnlp/stanza/issues/195,,"I modify the `max_char_length` option within the `CoreNLPClient` object, but it des not seem to have an effect. 

In my code, I specify the client as:

```
with CoreNLPClient(
        properties={
            ""ner.applyFineGrained"": ""false"",
            ""annotators"": ""tokenize, ssplit, pos, lemma, ner, depparse"",
        },
        memory=global_options.RAM_CORENLP,
        threads=global_options.N_CORES,
        timeout=12000000,
        max_char_length = 300000,
    ) as client:
```
but I keep getting the error:

```
stanfordnlp.server.client.AnnotationException: Request is too long to be handled by server: 201703 characters. Max length is 100000 characters.
```

**Environment:**
 - OS: MacOS
 - Python version: Python 3.7.3 from Anaconda
 - StanfordNLP version: 0.2.0

Thank you!
",
1062,2020-02-18T23:41:09Z,https://github.com/stanfordnlp/stanza/pull/194,,"1. Generate the latest resources file from new training dir
2. Unifies loading function (now charlm is split into forward_charlm and backward_charlm)
3. Dependencies from List of List to List of Dict
4. Adjust pipeline order: pos before lemma
",
1063,2020-02-12T15:42:23Z,https://github.com/stanfordnlp/stanza/issues/193,,"Hi, I try to use the Stanford Corenlp through Python (Jupyter notebook). I'm not sure whether it's a bug or just something wrong with my directories.

When I try to execute the first line of the [demo ](https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/corenlp.py)script, namely:

`from stanfordnlp.server import CoreNLPClient`

I get this error message:
```
`---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-43-371e22dbc359> in <module>
      1 import nltk
      2 import stanfordnlp
----> 3 from stanfordnlp.server import CoreNLPClient

~\Anaconda3\lib\site-packages\stanfordnlp\server\__init__.py in <module>
----> 1 from stanfordnlp.protobuf import to_text
      2 from stanfordnlp.protobuf import Document, Sentence, Token, IndexedWord, Span
      3 from stanfordnlp.protobuf import ParseTree, DependencyGraph, CorefChain
      4 from stanfordnlp.protobuf import Mention, NERMention, Entity, Relation, RelationTriple, Timex
      5 from stanfordnlp.protobuf import Quote, SpeakerInfo

~\Anaconda3\lib\site-packages\stanfordnlp\protobuf\__init__.py in <module>
      5 from google.protobuf.internal.encoder import _EncodeVarint
      6 from google.protobuf.internal.decoder import _DecodeVarint
----> 7 from .CoreNLP_pb2 import *
      8 
      9 def parseFromDelimitedString(obj, buf, offset=0):

~\Anaconda3\lib\site-packages\stanfordnlp\protobuf\CoreNLP_pb2.py in <module>
     80   serialized_end=9486,
     81 )
---> 82 _sym_db.RegisterEnumDescriptor(_LANGUAGE)
     83 
     84 Language = enum_type_wrapper.EnumTypeWrapper(_LANGUAGE)

~\Anaconda3\lib\site-packages\google\protobuf\symbol_database.py in RegisterEnumDescriptor(self, enum_descriptor)
    105     if api_implementation.Type() == 'python':
    106       # pylint: disable=protected-access
--> 107       self.pool._AddEnumDescriptor(enum_descriptor)
    108     return enum_descriptor
    109 

~\Anaconda3\lib\site-packages\google\protobuf\descriptor_pool.py in _AddEnumDescriptor(self, enum_desc)
    261 
    262     file_name = enum_desc.file.name
--> 263     self._CheckConflictRegister(enum_desc, enum_desc.full_name, file_name)
    264     self._enum_descriptors[enum_desc.full_name] = enum_desc
    265 

~\Anaconda3\lib\site-packages\google\protobuf\descriptor_pool.py in _CheckConflictRegister(self, desc, desc_name, file_name)
    189                           'children of it.')
    190 
--> 191           raise TypeError(error_msg)
    192 
    193         return

TypeError: Conflict register for file ""CoreNLP.proto"": edu.stanford.nlp.pipeline.Language is already defined in file ""doc/CoreNLP.proto"". Please fix the conflict by adding package name on the proto file, or use different name for the duplication.`
```

I looked on the internet for possible solutions but didn't find any.
Thanks in advance for the help!",
1064,2020-01-15T20:16:18Z,https://github.com/stanfordnlp/stanza/issues/192,,"When running the demo script provided the code I have breaks:

The demo I am talking about is [https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/corenlp.py](url) 

My the code breaks on line 17 and i get the error `""C:\Users\vinhl\AppData\Local\Programs\Python\Python37\lib\subprocess.py"", line 1179, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified`

I already added the classpath to the environment variables and it is able to call upon client but as soon as it hits client.start() it breaks.

I'm sorry if this is a trivial issue I am new to this kind of stuff and I have no idea what to do I have been at this for hours.",
1065,2020-01-15T04:43:00Z,https://github.com/stanfordnlp/stanza/issues/191,,"I notice that a ""compound"" for an edge in print_dependencies shows ""nmod"" in word.dependency_relation.",
1066,2020-01-14T01:31:40Z,https://github.com/stanfordnlp/stanza/issues/190,,"Tested a few sentences on simplified Chinese and found annoying errors:

text = ""我的朋友。""
def run_stnlp():
    with CoreNLPClient(timeout=60000, memory='8G', properties='zh', annotators=['tokenize', 'ssplit', 'pos', 'depparse']) as client:
        # submit the request to the server
        doc = client.annotate(text, output_format='json', annotators=['tokenize', 'ssplit', 'pos', 'depparse'])
        for sentence in doc['sentences']:
            tokens = sentence['tokens']
            for token in tokens:
                print(token)

if __name__ == ""__main__"":
    run_stnlp()

我的朋友：always tags ""我的"" as one token. 
我的狗吃苹果： ‘我的狗’ tagged as one token.
他的狗吃苹果：'狗吃' tagged as one token.

This looks very bad. I tested those on http://nlp.stanford.edu:8080/parser/index.jsp, and they all work fine.  Can someone verify this is a model issue, which is different from the online Stanford parser demo, or some issue coming from the CoreNLPClient? 

Also, for this one:
高质量就业成时代: '就业' tagged as VV, but the online Stanford parser demo tags it as ""NN"", which is right. 

My model is the latest from https://stanfordnlp.github.io/CoreNLP/ by following instruction of the latest distribution from here:
https://stanfordnlp.github.io/stanfordnlp/ 

",
1067,2020-01-13T23:19:41Z,https://github.com/stanfordnlp/stanza/issues/189,,"For this method, I don't quite understand what's it is computing for the list heads[]. For two examples:
My dog eats food => [1, 1, 0, -1, -2]
Is it a smart dog, right? => [4, 3, 2, 1, 0, -1, -2, -3]

What does the ""heads"" mean, given these two examples? The original comments are:
 Here, we're calculating the absolute token index in the doc,
 then the *relative* index of the head, -1 for zero-indexed

    def get_tokens_with_heads(self, snlp_doc):
        """"""Flatten the tokens in the StanfordNLP Doc and extract the token indices
        of the sentence start tokens to set is_sent_start.

        snlp_doc (stanfordnlp.Document): The processed StanfordNLP doc.
        RETURNS (list): The tokens (words).
        """"""
        tokens = []
        heads = []
        offset = 0
        for sentence in snlp_doc.sentences:
            for token in sentence.tokens:
                for word in token.words:
                    # Here, we're calculating the absolute token index in the doc,
                    # then the *relative* index of the head, -1 for zero-indexed
                    # and if the governor is 0 (root), we leave it at 0
                    if word.governor:
                        head = word.governor + offset - len(tokens) - 1
                    else:
                        head = 0
                    heads.append(head)
                    tokens.append(word)
            offset += sum(len(token.words) for token in sentence.tokens)
        return tokens, heads",
1068,2020-01-11T22:35:05Z,https://github.com/stanfordnlp/stanza/pull/188,,"There's an annoying warning whenever the dependency parser runs, about using an unsigned byte type for a mask.  Replacing this with a `torch.bool` seems to fix the problem.",
1069,2020-01-11T03:53:31Z,https://github.com/stanfordnlp/stanza/issues/187,,"I'm sorry. Forgive my bad english.
Both english and frence are seperate by space, so i wander it is the same or not when i just need keywords and ner ?
",
1070,2020-01-10T20:17:16Z,https://github.com/stanfordnlp/stanza/issues/186,,"I want to do something like this:

with CoreNLPClient(timeout=60000, memory='8G', properties={'lang_name': 'zh', 'path': '../../../model-files'}) as client:

But it didn't work. My model files are out of the ""stanford-corenlp-full-2018-10-05"" directory.",
1071,2020-01-10T00:23:24Z,https://github.com/stanfordnlp/stanza/issues/185,,"I tested CoreNLPClient with StanfordNLP and found that the object returned by the 'annotate' function doesn't produce valid Chinese characters, below:

text = ""我吃苹果""
with CoreNLPClient(annotators=['tokenize', 'ssplit', 'pos'],
                   timeout=60000, memory='8G', properties='zh') as client:
    # submit the request to the server
    doc = client.annotate(text)

    # get the first sentence
    sentence = doc.sentence[0]
    
    print('---')
    print('first token of first sentence')
    token = sentence.token[0]
    print(token)
   
The output:
first token of first sentence
word: ""\346\210\221""
pos: ""PN""
value: ""\346\210\221""
originalText: ""\346\210\221""
ner: ""O""
lemma: ""\346\210\221""
beginChar: 0
endChar: 1
utterance: 0
speaker: ""PER0""
tokenBeginIndex: 0
tokenEndIndex: 1
hasXmlContext: false
isNewline: false
coarseNER: ""O""
fineGrainedNER: ""O""
corefMentionIndex: 0

How to change that output to valid Chinese characters?",
1072,2020-01-09T05:27:04Z,https://github.com/stanfordnlp/stanza/issues/184,,"UPDATE: yes we can!

[Just as we can in JAVA](https://github.com/stanfordnlp/CoreNLP/issues/983), I am now able to implement RegexNER tagging (blended with the default CoreNLP NER tagging) using the `stanfordnlp` Python interface.

After some difficulty, I sorted it out (solution below), but that raised a question.

For CoreNLP I have both

* the coned [GitHub repository](https://github.com/stanfordnlp/stanfordnlp), and also
* the [precompiled binary](https://stanfordnlp.github.io/CoreNLP/#download) (zip).

 If I set `$CORENLP_HOME` to the former I do not get finegrained / additional NER tagging, in `standordnlp` (Python).

If I set `$CORENLP_HOME` to the latter, everything works.  

**Question:** What is missing in the former (GitHub repo), vs. the latter (precompiled package)?

---

**stanfordnlp_test.py**

```python
import stanfordnlp
from stanfordnlp.server import CoreNLPClient

props = {
        ""ner.additional.regexner.mapping"": ""custom_entities.tsv""
        }
annotators = ['tokenize', 'ssplit', 'pos', 'lemma', 'ner']

text = 'Victoria lives in Vancouver, Canada. \
        She was born in Nova Scotia. \
        Victoria likes apples and bananas.'

with CoreNLPClient(properties=props,
        annotators=annotators,
        timeout=30000, memory='16G',
        output_format=""text"",    ## `outputFormat`, if used in `props` file
        threads='7',
        be_quiet=False) as client:

    ann = client.annotate(text)
    print(ann)
```

**custom_entities.tsv** [tab-separated values]

```
Victoria	PERSON	LOCATION,ORGANIZATION,CITY	2
Vancouver	CITY	LOCATION,ORGANIZATION	2
Canada	COUNTRY	LOCATION,ORGANIZATION,CITY	2
apple(s)	FRUIT		2
banana(s)	FRUIT		2
```
---

**Test 1:** [""incorrect"" `$CORENLP_HOME` (no blended tagging)]

*Note that I am tagged as a `LOCATION` in Sentence 1; ""apples"" and ""bananas"" in Sentence 3 are not tagged.*

```bash
$ echo $CORENLP_HOME    ## GitHub-cloned repo
/mnt/Vancouver/apps/CoreNLP/target

$ python stanfordnlp_test.py

Starting server with command: java -Xmx16G -cp /mnt/Vancouver/apps/CoreNLP/target/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 7 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-ca0e7c614fb9409f.props -preload tokenize,ssplit,pos,lemma,ner
--- StanfordCoreNLPServer#main() called ---
setting default constituency parser
using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
    Threads: 7
Adding annotator tokenize
No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Starting server...
StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[/0:0:0:0:0:0:0:1:35282] API call w/annotators tokenize,ssplit,pos,lemma,ner
Victoria lives in Vancouver, Canada. She was born in Nova Scotia. Victoria likes apples and bananas.
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Adding annotator lemma
Adding annotator ner
```
```bash
: Sentence #1 (7 tokens):
Victoria lives in Vancouver, Canada.
[Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=LOCATION]
[Text=lives CharacterOffsetBegin=9 CharacterOffsetEnd=14 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
[Text=in CharacterOffsetBegin=15 CharacterOffsetEnd=17 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Vancouver CharacterOffsetBegin=18 CharacterOffsetEnd=27 PartOfSpeech=NNP Lemma=Vancouver NamedEntityTag=LOCATION]
[Text=, CharacterOffsetBegin=27 CharacterOffsetEnd=28 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=Canada CharacterOffsetBegin=29 CharacterOffsetEnd=35 PartOfSpeech=NNP Lemma=Canada NamedEntityTag=LOCATION]
[Text=. CharacterOffsetBegin=35 CharacterOffsetEnd=36 PartOfSpeech=. Lemma=. NamedEntityTag=O]
```
```bash
Sentence #2 (7 tokens):
She was born in Nova Scotia.
[Text=She CharacterOffsetBegin=37 CharacterOffsetEnd=40 PartOfSpeech=PRP Lemma=she NamedEntityTag=O]
[Text=was CharacterOffsetBegin=41 CharacterOffsetEnd=44 PartOfSpeech=VBD Lemma=be NamedEntityTag=O]
[Text=born CharacterOffsetBegin=45 CharacterOffsetEnd=49 PartOfSpeech=VBN Lemma=bear NamedEntityTag=O]
[Text=in CharacterOffsetBegin=50 CharacterOffsetEnd=52 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Nova CharacterOffsetBegin=53 CharacterOffsetEnd=57 PartOfSpeech=NNP Lemma=Nova NamedEntityTag=LOCATION]
[Text=Scotia CharacterOffsetBegin=58 CharacterOffsetEnd=64 PartOfSpeech=NNP Lemma=Scotia NamedEntityTag=LOCATION]
[Text=. CharacterOffsetBegin=64 CharacterOffsetEnd=65 PartOfSpeech=. Lemma=. NamedEntityTag=O]
```
```bash
Sentence #3 (6 tokens):
Victoria likes apples and bananas.
[Text=Victoria CharacterOffsetBegin=66 CharacterOffsetEnd=74 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=PERSON]
[Text=likes CharacterOffsetBegin=75 CharacterOffsetEnd=80 PartOfSpeech=VBZ Lemma=like NamedEntityTag=O]
[Text=apples CharacterOffsetBegin=81 CharacterOffsetEnd=87 PartOfSpeech=NNS Lemma=apple NamedEntityTag=O]
[Text=and CharacterOffsetBegin=88 CharacterOffsetEnd=91 PartOfSpeech=CC Lemma=and NamedEntityTag=O]
[Text=bananas CharacterOffsetBegin=92 CharacterOffsetEnd=99 PartOfSpeech=NNS Lemma=banana NamedEntityTag=O]
[Text=. CharacterOffsetBegin=99 CharacterOffsetEnd=100 PartOfSpeech=. Lemma=. NamedEntityTag=O]
```

---
**Test 2:** [""correct"" `$CORENLP_HOME` (finegrained, blended tagging)]

```bash
$ export CORENLP_HOME=/mnt/Vancouver/apps/stanford-corenlp-full/stanford-corenlp-full-2018-10-05

$ echo $CORENLP_HOME    ## precompiled binary
/mnt/Vancouver/apps/stanford-corenlp-full/stanford-corenlp-full-2018-10-05

$ python stanfordnlp_test.py

Starting server with command: java -Xmx16G -cp /mnt/Vancouver/apps/stanford-corenlp-full/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 7 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-8c8455b24d4d4a3c.props -preload tokenize,ssplit,pos,lemma,ner
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 7
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.additional.regexner: Read 5 unique entries out of 5 from custom_entities.tsv, 0 TokensRegex patterns.
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[pool-1-thread-3] INFO CoreNLP - [/0:0:0:0:0:0:0:1:35364] API call w/annotators tokenize,ssplit,pos,lemma,ner
Victoria lives in Vancouver, Canada. She was born in Nova Scotia. Victoria likes apples and bananas.
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
```
```bash
Sentence #1 (7 tokens):
Victoria lives in Vancouver, Canada.

Tokens:
[Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=PERSON]
[Text=lives CharacterOffsetBegin=9 CharacterOffsetEnd=14 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
[Text=in CharacterOffsetBegin=15 CharacterOffsetEnd=17 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Vancouver CharacterOffsetBegin=18 CharacterOffsetEnd=27 PartOfSpeech=NNP Lemma=Vancouver NamedEntityTag=CITY]
[Text=, CharacterOffsetBegin=27 CharacterOffsetEnd=28 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=Canada CharacterOffsetBegin=29 CharacterOffsetEnd=35 PartOfSpeech=NNP Lemma=Canada NamedEntityTag=COUNTRY]
[Text=. CharacterOffsetBegin=35 CharacterOffsetEnd=36 PartOfSpeech=. Lemma=. NamedEntityTag=O]

Extracted the following NER entity mentions:
Victoria	PERSON
Vancouver	CITY
Canada	COUNTRY
```

```bash
Sentence #2 (7 tokens):
She was born in Nova Scotia.

Tokens:
[Text=She CharacterOffsetBegin=37 CharacterOffsetEnd=40 PartOfSpeech=PRP Lemma=she NamedEntityTag=O]
[Text=was CharacterOffsetBegin=41 CharacterOffsetEnd=44 PartOfSpeech=VBD Lemma=be NamedEntityTag=O]
[Text=born CharacterOffsetBegin=45 CharacterOffsetEnd=49 PartOfSpeech=VBN Lemma=bear NamedEntityTag=O]
[Text=in CharacterOffsetBegin=50 CharacterOffsetEnd=52 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Nova CharacterOffsetBegin=53 CharacterOffsetEnd=57 PartOfSpeech=NNP Lemma=Nova NamedEntityTag=STATE_OR_PROVINCE]
[Text=Scotia CharacterOffsetBegin=58 CharacterOffsetEnd=64 PartOfSpeech=NNP Lemma=Scotia NamedEntityTag=STATE_OR_PROVINCE]
[Text=. CharacterOffsetBegin=64 CharacterOffsetEnd=65 PartOfSpeech=. Lemma=. NamedEntityTag=O]

Extracted the following NER entity mentions:
Nova Scotia	STATE_OR_PROVINCE
She	PERSON
```
```bash
Sentence #3 (6 tokens):
Victoria likes apples and bananas.

Tokens:
[Text=Victoria CharacterOffsetBegin=66 CharacterOffsetEnd=74 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=PERSON]
[Text=likes CharacterOffsetBegin=75 CharacterOffsetEnd=80 PartOfSpeech=VBZ Lemma=like NamedEntityTag=O]
[Text=apples CharacterOffsetBegin=81 CharacterOffsetEnd=87 PartOfSpeech=NNS Lemma=apple NamedEntityTag=FRUIT]
[Text=and CharacterOffsetBegin=88 CharacterOffsetEnd=91 PartOfSpeech=CC Lemma=and NamedEntityTag=O]
[Text=bananas CharacterOffsetBegin=92 CharacterOffsetEnd=99 PartOfSpeech=NNS Lemma=banana NamedEntityTag=FRUIT]
[Text=. CharacterOffsetBegin=99 CharacterOffsetEnd=100 PartOfSpeech=. Lemma=. NamedEntityTag=O]

Extracted the following NER entity mentions:
Victoria	PERSON
apples	FRUIT
bananas	FRUIT
```",
1073,2020-01-08T14:00:46Z,https://github.com/stanfordnlp/stanza/issues/183,,"**Describe the bug**
I am trying to download the English language model using the piece of code on the ""Getting started"" page, but my proxy seems to be blocking the download. Is it possible to setup proxy setttings for the package? I have tried using global variables, but it doesn't seem to work.

Here is the traceback (pardon the french):

``` python
Traceback (most recent call last):
  File ""C:\Users\karadimt\Documents\Python Scripts\nlptest.py"", line 2, in <module>
    stanfordnlp.download('en')   # This downloads the English models for the neural pipeline
  File ""c:\users\karadimt\appdata\local\continuum\anaconda3\lib\site-packages\stanfordnlp\utils\resources.py"", line 137, in download
    confirm_if_exists=confirm_if_exists, force=force, version=version)
  File ""c:\users\karadimt\appdata\local\continuum\anaconda3\lib\site-packages\stanfordnlp\utils\resources.py"", line 101, in download_ud_model
    r = requests.get(download_url, stream=True)
  File ""c:\users\karadimt\appdata\local\continuum\anaconda3\lib\site-packages\requests\api.py"", line 75, in get
    return request('get', url, params=params, **kwargs)
  File ""c:\users\karadimt\appdata\local\continuum\anaconda3\lib\site-packages\requests\api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""c:\users\karadimt\appdata\local\continuum\anaconda3\lib\site-packages\requests\sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File ""c:\users\karadimt\appdata\local\continuum\anaconda3\lib\site-packages\requests\sessions.py"", line 646, in send
    r = adapter.send(request, **kwargs)
  File ""c:\users\karadimt\appdata\local\continuum\anaconda3\lib\site-packages\requests\adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='nlp.stanford.edu', port=80): Max retries exceeded with url: /software/stanfordnlp_models/latest/en_ewt_models.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000197BBF97B48>: Failed to establish a new connection: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu'))
```


**To Reproduce**
Steps to reproduce the behavior:
1. Use a proxy network
2. Execute following code : 

``` python
import stanfordnlp
stanfordnlp.download('en')   # This downloads the English models for the neural pipeline
nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English
doc = nlp(""Barack Obama was born in Hawaii.  He was elected president in 2008."")
doc.sentences[0].print_dependencies()
```

3. Accept what the console proposes
4. See error

**Expected behavior**
It should normally download the model and execute the piece of code successful.

**Environment (please complete the following information):**
 - OS: Windows 10
 - Python version: Python 3.7.4
 - StanfordNLP version: 0.2.0
",
1074,2020-01-07T23:17:39Z,https://github.com/stanfordnlp/stanza/issues/182,,"For general reference.  Refer also to #184 where I specify the properties as a dictionary, inside the script.

I also wanted the option to load properties from an external file, e.g. formatted as a dictionary.  Here is my solution.

**regexner.props** [properties file; note: formatted as a dictionary]

```python
{
    ""outputFormat"": ""text"",
    ""be_quiet"": ""true"",
    ""annotators"": ""tokenize, ssplit, pos, lemma, ner"",
    ""ner.additional.regexner.mapping"": ""custom_entities.tsv""
}
```

Note also that `outputFormat` (above, but `output_format` if defined internally in the `CoreNLPClient()` function inside a Python script) can be one of:

`text`, `xml`, `json`, `conll`, ...

**$CORENLP_HOME/**

```bash
/mnt/Vancouver/apps/stanford-corenlp-full/stanford-corenlp-full-2018-10-05/
```

**custom_entities.tsv**

```
Victoria	PERSON	LOCATION,ORGANIZATION,CITY	2
Vancouver	CITY	LOCATION,ORGANIZATION	2
Canada	COUNTRY	LOCATION,ORGANIZATION,CITY	2
apple(s)	FRUIT		2
banana(s)	FRUIT		2
```

**stanfordnlp_test.py**

```python
import stanfordnlp, json
from stanfordnlp.server import CoreNLPClient

## https://stackoverflow.com/questions/11026959/writing-a-dict-to-txt-file-and-reading-it-back

props_path = '/mnt/Vancouver/apps/CoreNLP/target/regexner.props'
props = json.load(open(props_path))
# print('\n', props, '\n')

text = 'Victoria lives in Vancouver, Canada. \
        She was born in Nova Scotia. \
        Victoria likes apples and bananas.'

with CoreNLPClient(properties=props,
        timeout=30000, memory='16G',
        # output_format='text',   ## `outputFormat` in `regexner.props`
        threads='7') as client:

    ann = client.annotate(text)
    print(ann)
```

**Output**

```
(py3.7) [victoria@victoria target]$ python stanfordnlp_test.py
Starting server with command: java -Xmx16G -cp /mnt/Vancouver/apps/stanford-corenlp-full/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 7 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-83983046a739473e.props -preload tokenize, ssplit, pos, lemma, ner

Sentence #1 (7 tokens):
Victoria lives in Vancouver, Canada.

Tokens:
[Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=PERSON]
[Text=lives CharacterOffsetBegin=9 CharacterOffsetEnd=14 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
[Text=in CharacterOffsetBegin=15 CharacterOffsetEnd=17 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Vancouver CharacterOffsetBegin=18 CharacterOffsetEnd=27 PartOfSpeech=NNP Lemma=Vancouver NamedEntityTag=CITY]
[Text=, CharacterOffsetBegin=27 CharacterOffsetEnd=28 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=Canada CharacterOffsetBegin=29 CharacterOffsetEnd=35 PartOfSpeech=NNP Lemma=Canada NamedEntityTag=COUNTRY]
[Text=. CharacterOffsetBegin=35 CharacterOffsetEnd=36 PartOfSpeech=. Lemma=. NamedEntityTag=O]

Extracted the following NER entity mentions:
Victoria	PERSON
Vancouver	CITY
Canada	COUNTRY
```
```
Sentence #2 (7 tokens):
She was born in Nova Scotia.

Tokens:
[Text=She CharacterOffsetBegin=41 CharacterOffsetEnd=44 PartOfSpeech=PRP Lemma=she NamedEntityTag=O]
[Text=was CharacterOffsetBegin=45 CharacterOffsetEnd=48 PartOfSpeech=VBD Lemma=be NamedEntityTag=O]
[Text=born CharacterOffsetBegin=49 CharacterOffsetEnd=53 PartOfSpeech=VBN Lemma=bear NamedEntityTag=O]
[Text=in CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Nova CharacterOffsetBegin=57 CharacterOffsetEnd=61 PartOfSpeech=NNP Lemma=Nova NamedEntityTag=STATE_OR_PROVINCE]
[Text=Scotia CharacterOffsetBegin=62 CharacterOffsetEnd=68 PartOfSpeech=NNP Lemma=Scotia NamedEntityTag=STATE_OR_PROVINCE]
[Text=. CharacterOffsetBegin=68 CharacterOffsetEnd=69 PartOfSpeech=. Lemma=. NamedEntityTag=O]

Extracted the following NER entity mentions:
Nova Scotia	STATE_OR_PROVINCE
She	PERSON
```
```
Sentence #3 (6 tokens):
Victoria likes apples and bananas.

Tokens:
[Text=Victoria CharacterOffsetBegin=74 CharacterOffsetEnd=82 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=PERSON]
[Text=likes CharacterOffsetBegin=83 CharacterOffsetEnd=88 PartOfSpeech=VBZ Lemma=like NamedEntityTag=O]
[Text=apples CharacterOffsetBegin=89 CharacterOffsetEnd=95 PartOfSpeech=NNS Lemma=apple NamedEntityTag=FRUIT]
[Text=and CharacterOffsetBegin=96 CharacterOffsetEnd=99 PartOfSpeech=CC Lemma=and NamedEntityTag=O]
[Text=bananas CharacterOffsetBegin=100 CharacterOffsetEnd=107 PartOfSpeech=NNS Lemma=banana NamedEntityTag=FRUIT]
[Text=. CharacterOffsetBegin=107 CharacterOffsetEnd=108 PartOfSpeech=. Lemma=. NamedEntityTag=O]

Extracted the following NER entity mentions:
Victoria	PERSON
apples	FRUIT
bananas	FRUIT
```

* See also #12 [Using the corenlp client with different languages]",
1075,2020-01-04T20:28:38Z,https://github.com/stanfordnlp/stanza/pull/181,,Closes #179,
1076,2020-01-04T06:00:52Z,https://github.com/stanfordnlp/stanza/pull/180,,"Closes #179

",
1077,2020-01-04T06:00:39Z,https://github.com/stanfordnlp/stanza/issues/179,,"# Issue Type

[x] Bug (Typo)

# Steps to Replicate

1. Examine stanfordnlp/models/lemma/trainer.py.
2. Search for `statitical`.

# Expected Behaviour

1. Should read `statistical`.

",
1078,2020-01-02T13:59:59Z,https://github.com/stanfordnlp/stanza/issues/178,,"I only found the function of dependency parsing, but I'm not sure if this tool supports semantic dependency parsing, how do I use it?",
1079,2019-12-25T01:51:33Z,https://github.com/stanfordnlp/stanza/issues/176,,"I see that the `client.annotate` method is used in the demo script. However, I'm confused as to what the `client.update` method does?

https://github.com/stanfordnlp/stanfordnlp/blob/f678a6c4003c2e48caf7957b9aba8bc8fa9a3327/stanfordnlp/server/client.py#L439-L456

Here is the associated test:

https://github.com/stanfordnlp/stanfordnlp/blob/f678a6c4003c2e48caf7957b9aba8bc8fa9a3327/tests/test_client.py#L65-L68

Thanks!

Related: https://github.com/stanfordnlp/stanfordnlp/issues/69",
1080,2019-12-18T23:51:21Z,https://github.com/stanfordnlp/stanza/issues/175,,"First off: I have limited knowledge of python venvs and I am not sure if this error is caused by stanfordnlp or by pipenv.

When I tried using stanfordnlp in a python virtual environment using pipenv together with scikit-learn, i got a SIGSEGV when i import scikit-learn first. 

- python 3.7.5
- Linux 5.4.1
- pipenv 2018.11.26

Minimal example: 
```
pipenv --three
pipenv install sklearn stanfordnlp
pipenv run python
> import sklearn
> import stanfordnlp
```

If i swap the order of the imports, no error is thrown.",
1081,2019-12-16T20:29:02Z,https://github.com/stanfordnlp/stanza/issues/174,,"Hello, 
I have been trying to include cleanxml in properties but did not work as I am using Stanford client. 
for example: for this text:
```
text='The Navy has changed its account of the attack on the USS Cole in Yemen.\
 Officials <TIMEX3 tid=""t1"" type=""DATE"" value=""PRESENT_REF"" temporalFunction=""true"" anchorTimeID=""t0"">now</TIMEX3> say the ship was hit <TIMEX3 tid=""t2"" type=""DURATION"" value=""PT2H"">nearly two hours </TIMEX3>after it had docked. '
```
I want to obtain the annotation and nnot including xml elements. 
Any idea? 
 I looked at [https://stanfordnlp.github.io/CoreNLP/cleanxml.html#description](url) but did not know how to include it in the properities. Here is my initial start. I really appreciate your work.
```
from stanfordnlp.server import CoreNLPClient
properties={'annotators':'tokenize,ssplit',
            'cleanxml':'true',
           }  
with CoreNLPClient(properties=properties,output_format='json') as client:
    ann = client.annotate(text)
    
```",
1082,2019-12-13T22:00:34Z,https://github.com/stanfordnlp/stanza/issues/173,,"Unable to install on Mac OSx Catalina,

ImportError: dlopen

------
**To Reproduce**

- fresh install of brew
- fresh install of python 3.7.5

>>> pip3 install stanfordnlp
>>> import stanfordnlp
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/__init__.py"", line 1, in <module>
    from stanfordnlp.pipeline.core import Pipeline
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 7, in <module>
    import torch
  File ""/usr/local/lib/python3.7/site-packages/torch/__init__.py"", line 81, in <module>
    from torch._C import *
ImportError: dlopen(/usr/local/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so, 9): Library not loaded: @rpath/libc++.1.dylib
  Referenced from: /usr/local/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so
  Reason: image not found



**Environment (please complete the following information):**
 - OS: [MacOS- catalina]
 - Python version: [e.g. Python 3.7.5 from brew]
 - StanfordNLP version: [e.g., 0.2.0]

**Additional context**

I can install and run on Mac OSx Sierra
",
1083,2019-12-13T10:17:46Z,https://github.com/stanfordnlp/stanza/issues/172,,"we knew that stanfoldcorenlp already trained many models many langurages 

https://stanfordnlp.github.io/CoreNLP/index.html#download

but, to me that these two packages cannot share the model??

anyone point out a solution here ,,,thanks!",
1084,2019-12-11T23:45:59Z,https://github.com/stanfordnlp/stanza/issues/171,,"A quick question: do any of you folks have code for finding the shortest dependency path in a dependency parse?

I found these discussions (that build a NetworkX graph and find the shortest path) -- petty neat, but a bit unwieldy.  I'd prefer a fully automated procedure, where you don't have to provide the two arguments (e.g. subject, object).

* https://stackoverflow.com/questions/51252914
* https://stackoverflow.com/questions/32835291

Thanks!  :-)",
1085,2019-12-07T19:10:44Z,https://github.com/stanfordnlp/stanza/issues/170,,"The Russian text is not broken down into sentences. Help me to solve this problem

## test code

text = ""Это первое предложение. Это тестовая строка. Почему бы не написать что-нибудь еще.""
pipeline = stanfordnlp.Pipeline(lang=""ru"")
doc = pipeline(text.lower())
print(""The tokenizer split the input into {} sentences."".format(len(doc.sentences)))

## result

The tokenizer split the input into 1 sentences.",
1086,2019-12-06T21:02:22Z,https://github.com/stanfordnlp/stanza/issues/169,,"I am hitting a performance issue when comparing to StanfordCoreNLP and was wondering if anyone has some advice?

This is the code I am using

```
from nltk.parse.corenlp import CoreNLPDependencyParser
import stanfordnlp
import os
import sys
from abc import ABC, abstractmethod

class NLP_API(ABC):
    def __init__(self, text):
        self.text=text
        self.triples=None
        self.lemmas=None
        self.words=None

    @abstractmethod
    def get_triples(self):
        pass

    @abstractmethod
    def get_lemmas(self):
        pass

    @abstractmethod
    def get_words(self):
        pass

class CoreNLP_API(NLP_API):
    def __init__(self, text, dparser):
        super().__init__(text)
        self.dparser = dparser
        self.gss = list(self.dparser.parse_text(self.text))
        self.get_triples()
        self.get_lemmas()
        self.get_words()

    def get_triples(self):
        if not self.triples :
          self.triples = []
        for gs in self.gss:
          self.triples.append(list(gs.triples()))
        return self.triples

    def _extract_key(gss,key):
        wss = []
        for gs in gss:
            ns=list(gs.nodes.items())
            ws=[None]*(len(ns)-1)
            for k,v in ns:
              ws[k-1]=v[key]
            wss.append(ws)
        return wss

    def get_lemmas(self):
        if not self.lemmas:
            self.lemmas = CoreNLP_API._extract_key(self.gss,'lemma')
        return self.lemmas

    def get_words(self):
        if not self.words:
           self.words = CoreNLP_API._extract_key(self.gss, 'word')
        return self.words

class StanTorch_API(NLP_API):
    def __init__(self, text, nlp):
        super().__init__(text)
        self.dparser = nlp(self.text)

    def get_triples(self):
        if not self.triples:
            tss=[]
            for s in self.dparser.sentences:
                ts=[]
                for dep_edge in s.dependencies:
                    source=(dep_edge[0].text,dep_edge[0].pos)
                    target=(dep_edge[2].text,dep_edge[2].pos)
                    t= (source,  dep_edge[1], target)
                    ts.append(t)
                tss.append(ts)
            self.tuples=tss
        return self.tuples

    def get_words_and_lemmas(self):
        if not self.lemmas or not self.words :
            wss=[]
            lss=[]
            for s in self.dparser.sentences:
                ws = []
                ls = []
                for w in s.words:
                    ws.append(w.text)
                    ls.append(w.lemma)
                wss.append(ws)
                lss.append(ls)
            self.words=wss
            self.lemmas=lss

    def get_words(self):
        self.get_words_and_lemmas()
        return self.words

    def get_lemmas(self):
        self.get_words_and_lemmas()
        return self.lemmas


def t1(text, dparser) :
    print('with coreNLP')
    print('')
    p=CoreNLP_API(text, dparser)
    print(p.get_triples())
    print('')
    print(p.get_lemmas())
    print('')
    print(p.get_words())
    print('')

def t2(text, nlp) :
    print('with stanfordnlp - torch based')
    print('')
    p = StanTorch_API(text, nlp)
    print(p.get_triples())
    print('')
    print(p.get_lemmas())
    print('')
    print(p.get_words())
    print('')


if __name__ == '__main__':
    text = """"""
    Justin Drew Bieber (/ˈbiːbər/; born March 1, 1994) is a Canadian singer-songwriter and actor.[6] Encountered at 13 years old by talent manager Scooter Braun after he had watched the boy's YouTube cover song videos, Bieber was signed to RBMG Records in 2008. Bieber released his debut EP, My World, in late 2009. It was certified Platinum in the United States. With the EP, Bieber became the first artist to have seven songs from a debut record chart on the Billboard Hot 100.[7]
Bieber released his first studio album, My World 2.0, in 2010. It debuted at number one in several countries, was certified triple Platinum in the US,[8] and contained his single ""Baby"", which debuted at number five on the Billboard Hot 100 and sold 12 million units. Following his debut album and promotional tours, he released his 3D biopic-concert film Justin Bieber: Never Say Never and his second studio album, Under the Mistletoe (2011), which debuted at number one on the Billboard 200. His third studio album, Believe (2012) generated the single ""Boyfriend"", which reached number one in Canada. His fourth studio album Purpose was released in 2015, spawning three number one singles: ""What Do You Mean?"", ""Sorry"", and ""Love Yourself"". Bieber has not released a studio album following Purpose, but has since been featured on several successful collaborations, including ""Cold Water"", ""Let Me Love You"", ""Despacito (Remix)"", ""I'm the One"", ""I Don't Care"" and ""10,000 Hours"".
    """"""

    if sys.argv[1]=='t1':
        print(80*'-')
        dparser = CoreNLPDependencyParser(url='http://localhost:9000')
        for _ in range(int(sys.argv[2])):
            t1(text, dparser)
    if sys.argv[1]=='t2':
        print(80*'-')
        mfile = os.getenv(""HOME"") + \
            '/stanfordnlp_resources/en_ewt_models'
        if not os.path.exists(mfile):
            stanfordnlp.download('en',confirm_if_exists=True)
        nlp = stanfordnlp.Pipeline()
        for _ in range(int(sys.argv[2])):
            t2(text, nlp)
```

Running the code for the StanfordCoreNLP server test yields

```
>>> time python ./test.py t1 10

[...]

real	0m5.074s
user	0m1.714s
sys	0m0.629s
```

whereas for the case of the stanfordnlp python package we have

```
>>> time python ./test.py t2 10
--------------------------------------------------------------------------------

Use device: gpu
---

[...]

real	0m13.954s
user	0m12.442s
sys	0m2.493s
```

The gpu stats are:

```
Fri Dec  6 20:18:00 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00000000:00:04.0 Off |                    0 |
| N/A   72C    P0    75W / 149W |    837MiB / 11441MiB |     20%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      9391      C   python3                                      826MiB |
+-----------------------------------------------------------------------------+
```

The stanfordnlp is about 3 times slower than the corresponding java-server based code: any thoughts?",
1087,2019-12-04T00:07:49Z,https://github.com/stanfordnlp/stanza/pull/168,,remove all spaces in keys and values. have passed the test.,
1088,2019-11-29T14:10:59Z,https://github.com/stanfordnlp/stanza/issues/167,,"Running the pipeline with PyTorch 1.3.1 will give a deprecation warning:

> /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.

Masking has changed because they now use `torch.bool` dtypes (which makes much more sense). Might be a good idea to prepare for this future change. (I marked this as bug because ""feature"" didn't really seem to cut it.)",
1089,2019-11-27T09:00:52Z,https://github.com/stanfordnlp/stanza/issues/165,,,
1090,2019-11-23T20:12:54Z,https://github.com/stanfordnlp/stanza/issues/164,,"Hello,

Is there a way to choose on which GPU `stanfordnlp.Pipeline` will run on?

Thanks!",
1091,2019-11-22T07:42:57Z,https://github.com/stanfordnlp/stanza/issues/163,,"There seem to be a number of features currently missing in the Python implementation
relative to the Java one. For example:

- [Quote Attribution](https://stanfordnlp.github.io/CoreNLP/quote.html#quote-attribution)
- [Coreference resolution](https://stanfordnlp.github.io/CoreNLP/coref.html)
- [OpenIE](https://stanfordnlp.github.io/CoreNLP/openie.html)

I was wondering if there were plans to incorporate these into Python, and if so whether
there is a timeline for doing so.",
1092,2019-11-21T18:56:32Z,https://github.com/stanfordnlp/stanza/issues/162,,"**Describe the bug**
When creating an instance of a Pipeline that uses the `depparse` processor, either on CPU or GPU, on every call to that instance I'm getting the follwing warning:
```
/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.
```
The warning does not trigger when not using the `depparse` processor.

**To Reproduce**
Run the following code:
```python
>>> import stanfordnlp
>>> nlp = stanfordnlp.Pipeline(processors=""tokenize,pos,depparse"", lang=""en"")
>>> nlp(""Hello, world!"")
```

**Environment (please complete the following information):**
 - OS: Ubuntu 18.04.3
 - Python version: 3.7.5
 - StanfordNLP version: 0.2.0",
1093,2019-11-21T12:15:28Z,https://github.com/stanfordnlp/stanza/issues/161,,"Greetings!

I'm not sure whether it's from my inexperience with PyTorch or conll (if it's even relevant), but I'm having trouble with understanding the input/output files necessary in training a new lemmatizer model and the evaluation process in general.

**_et_edt.train.in.conllu**  - From what I understand, this is just the training data used to train the model.
**_et_edt.dev.in.conllu** - This seems to be the test dataset used to evaluate/test everything. Am I correct in assuming that it's evaluated by comparing the predictions of the trained model with the true values in this file, thus generating a score from the correct/wrong predictions?
**_et_edt.dev.pred.conllu** - Is this the output of every prediction that gets evaluated during the training proccess?
**_et_edt.dev.gold.conllu** - This one confuses me the most as I'm not familiar what the gold stands for and after some experimenting it seems this has to be the same as dev.in.conllu?

I'd greatly appreciate your help!",
1094,2019-11-19T03:13:55Z,https://github.com/stanfordnlp/stanza/issues/160,,"**Describe the bug**
I want to work on pre-tokenized input so I specified the following properties
```
properties = {
        'tokenize.whitespace': True,
        'tokenize.keepeol': True,
        'ssplit.eolonly': True
        }
```
I received the following NullPointer error:
```
starting up Java Stanford CoreNLP Server...
Starting server with command: java -Xmx16G -cp /homes/lee2226/scratch2/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-5074c5d23e934dbf.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref
Traceback (most recent call last):
  File ""/homes/lee2226/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py"", line 330, in _request
    r.raise_for_status()
  File ""/scratch2/lee2226/github_repo/requests/requests/models.py"", line 840, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%27outputFormat%27%3A+%27serialized%27%7D

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 24, in <module>
    ann = client.annotate(text)
  File ""/homes/lee2226/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py"", line 398, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/homes/lee2226/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py"", line 336, in _request
    raise AnnotationException(r.text)
stanfordnlp.server.client.AnnotationException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
```

```
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:870)
        at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)
        at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)
        at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)
        at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692)
        at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)
        at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.NullPointerException
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:322)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$0(StanfordCoreNLPServer.java:857)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        ... 3 more
```



**To Reproduce**
Run the following example, which is modified from the document example by adding the properties and change the input texts with EOL.

```python
from stanfordnlp.server import CoreNLPClient

# example text
print('---')
print('input text')
print('')

text = ""Chris Manning is a nice person.\nChris wrote a simple sentence. He also gives oranges to people.\ntest it.""

print(text)

# set up the client
print('---')
print('starting up Java Stanford CoreNLP Server...')

# set up the client
properties = {
        'tokenize.whitespace': True,
        'tokenize.keepeol': True,
        'ssplit.eolonly': True
        }
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G', properties=properties) as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]
    
    # get the constituency parse of the first sentence
    print('---')
    print('constituency parse of first sentence')
    constituency_parse = sentence.parseTree
    print(constituency_parse)

    # get the first subtree of the constituency parse
    print('---')
    print('first subtree of constituency parse')
    print(constituency_parse.child[0])

    # get the value of the first subtree
    print('---')
    print('value of first subtree of constituency parse')
    print(constituency_parse.child[0].value)

    # get the dependency parse of the first sentence
    print('---')
    print('dependency parse of first sentence')
    dependency_parse = sentence.basicDependencies
    print(dependency_parse)

    # get the first token of the first sentence
    print('---')
    print('first token of first sentence')
    token = sentence.token[0]
    print(token)

    # get the part-of-speech tag
    print('---')
    print('part of speech tag of token')
    token.pos
    print(token.pos)

    # get the named entity tag
    print('---')
    print('named entity tag of token')
    print(token.ner)

    # get an entity mention from the first sentence
    print('---')
    print('first entity mention in sentence')
    print(sentence.mentions[0])

    # access the coref chain
    print('---')
    print('coref chains for the example')
    print(ann.corefChain)

    # Use tokensregex patterns to find who wrote a sentence.
    pattern = '([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/'
    matches = client.tokensregex(text, pattern)
    # sentences contains a list with matches for each sentence.
    assert len(matches[""sentences""]) == 3
    # length tells you whether or not there are any matches in this
    assert matches[""sentences""][1][""length""] == 1
    # You can access matches like most regex groups.
    matches[""sentences""][1][""0""][""text""] == ""Chris wrote a simple sentence""
    matches[""sentences""][1][""0""][""1""][""text""] == ""Chris""

    # Use semgrex patterns to directly find who wrote what.
    pattern = '{word:wrote} >nsubj {}=subject >dobj {}=object'
    matches = client.semgrex(text, pattern)
    # sentences contains a list with matches for each sentence.
    assert len(matches[""sentences""]) == 3
    # length tells you whether or not there are any matches in this
    assert matches[""sentences""][1][""length""] == 1
    # You can access matches like most regex groups.
    matches[""sentences""][1][""0""][""text""] == ""wrote""
    matches[""sentences""][1][""0""][""$subject""][""text""] == ""Chris""
    matches[""sentences""][1][""0""][""$object""][""text""] == ""sentence""

```


**Expected behavior**
Serialized parsing results

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: 3.6.x
 - StanfordNLP version: 0.2.0
 - CoreNLP version: 3.9.2

**Additional context**
None",
1095,2019-11-18T02:19:58Z,https://github.com/stanfordnlp/stanza/issues/159,,"A user tried to run on a large document, 1 paragraph per line.  Two issues seemed to emerge...

1.) It crashed with this:

```
Traceback (most recent call last):
  File ""dependency_parse.py"", line 38, in <module>
    doc = nlp(text)
  File ""/u/nlp/anaconda/main/anaconda3/envs/dora-hungarian-2/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 176, in __call__
    self.process(doc)
  File ""/u/nlp/anaconda/main/anaconda3/envs/dora-hungarian-2/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 170, in process
    self.processors[processor_name].process(doc)
  File ""/u/nlp/anaconda/main/anaconda3/envs/dora-hungarian-2/lib/python3.7/site-packages/stanfordnlp/pipeline/pos_processor.py"", line 31, in process
    for i, b in enumerate(batch):
  File ""/u/nlp/anaconda/main/anaconda3/envs/dora-hungarian-2/lib/python3.7/site-packages/stanfordnlp/models/pos/data.py"", line 120, in __iter__
    yield self.__getitem__(i)
  File ""/u/nlp/anaconda/main/anaconda3/envs/dora-hungarian-2/lib/python3.7/site-packages/stanfordnlp/models/pos/data.py"", line 91, in __getitem__
    assert len(batch) == 6
AssertionError
```

when trying to just run on the whole file

2.) There isn't really a good set up for processing one paragraph at a time and appending to a file when trying to produce *.conllu output.",
1096,2019-11-18T02:10:59Z,https://github.com/stanfordnlp/stanza/issues/158,,It doesn't look like Sentence has a text option...,
1097,2019-11-15T06:49:43Z,https://github.com/stanfordnlp/stanza/issues/157,,"**Describe the bug**
When calling the default Old French NLP models (`stanfordnlp.Pipeline(lang=""fro"")`), then following error arises:

``` python
FileNotFoundError: [Errno 2] No such file or directory: '/Users/kyle.p.johnson/stanfordnlp_resources/fro_srcmf_models/fro_srcmf_lemmatizer.pt'
```

**To Reproduce**

See gist here for commands and full traceback: https://gist.github.com/kylepjohnson/d40215b380be4b050b5cc1ceac09e369

**Expected behavior**
Expect the `Pipeline` object to become instantiated.

**Environment (please complete the following information):**
 - OS: MacOS Mojave 10.14.6
 - Python version: Python 3.7.5 from pyenv
 - StanfordNLP version: 0.2.0

**Additional context**
I've tried several other languages (Ancient Greek `grc` and Latin `lat`) and the Pipeline works fine.",
1098,2019-11-14T13:03:51Z,https://github.com/stanfordnlp/stanza/pull/156,,"Replacing PR #155.

For some applications the comments carry important information, such as the sentence ID, which otherwise has to be inferred from the order of sentences. This is not always easy, and furthermore, comments may carry other kinds of metadata.

This change makes comments be carried over from the input CoNLL to the output CoNLL, regardless of any manipulations to the data itself.",
1099,2019-11-13T13:38:03Z,https://github.com/stanfordnlp/stanza/pull/155,,,
1100,2019-10-31T17:29:55Z,https://github.com/stanfordnlp/stanza/issues/154,,"**Description**
I think this is similar to a bug in the old python library: 
[python-stanford-corenlp](https://github.com/stanfordnlp/python-stanford-corenlp/issues/8).
I'm trying to copy the demo for the client [here ](https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/corenlp.py)or [here](https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html).
but with my own texts... text2 works and text3 doesn't, the only differemce between them in the very last word.

The error I get is:
```
Traceback (most recent call last):
  File ""C:/gitProjects/patentmoto2/scratch4.py"", line 23, in <module>
    ann = client.annotate(text)
  File ""C:\gitProjects\patentmoto2\venv\lib\site-packages\stanfordnlp\server\client.py"", line 403, in annotate
    parseFromDelimitedString(doc, r.content)
  File ""C:\gitProjects\patentmoto2\venv\lib\site-packages\stanfordnlp\protobuf\__init__.py"", line 18, in parseFromDelimitedString
    obj.ParseFromString(buf[offset+pos:offset+pos+size])
google.protobuf.message.DecodeError: Error parsing message
```

**To Reproduce**

Steps to reproduce the behavior:

```from stanfordnlp.server import CoreNLPClient

print('---')
print('input text')
print('')

text = ""Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.""
text2 = ""We claim:1. A photographic camera for three dimension photography comprising:a housing having an opening to the interior for light rays;means for immovably locating photosensitive material in communication with the interior of the housing at a location during a time for exposure;optical means in said housing for projecting light rays, which are received through said opening from a scene to be photographed, along an optical path to said location, said path having a first position therealong extending transversely to the direction of the path from a first side to a second side of the path, the optical means comprisinga lenticular screen extending across said path at a second position farther along said path from the first position and having, on one side, a plurality of elongated lenticular elements of width P which face in the direction from which the light rays are being projected and having an opposite side facing and positioned for contact with the surface of such located photosensitive material,the optical means being characterized in that it changes, by a predetermined distance Y, on such surface of the photosensitive material, the position of light rays which come from a substantially common point on such scene and which extend along said first and second sides of said path;means for blocking the received light rays at said first position;an aperture movable transversely across said path at said first position, from said first side to said second said, for exposing said light rays sequentially to the photosensitive material moving across said screen in a direction normal to the elongation of said lenticular elements; andmeans for so moving said aperture for a predetermined time for exposure while simultaneously and synchronously moving said screen, substantially throughout said predetermined time for exposure, in substantially the same direction as the light rays sequentially expose said photosensitive material and over a distance substantially equal to the sum of P + Y to thereby expose a substantially continuous unreversed image of the scene on the photosensitive material, said means for and doing this all day long and.""
text3 = ""We claim:1. A photographic camera for three dimension photography comprising:a housing having an opening to the interior for light rays;means for immovably locating photosensitive material in communication with the interior of the housing at a location during a time for exposure;optical means in said housing for projecting light rays, which are received through said opening from a scene to be photographed, along an optical path to said location, said path having a first position therealong extending transversely to the direction of the path from a first side to a second side of the path, the optical means comprisinga lenticular screen extending across said path at a second position farther along said path from the first position and having, on one side, a plurality of elongated lenticular elements of width P which face in the direction from which the light rays are being projected and having an opposite side facing and positioned for contact with the surface of such located photosensitive material,the optical means being characterized in that it changes, by a predetermined distance Y, on such surface of the photosensitive material, the position of light rays which come from a substantially common point on such scene and which extend along said first and second sides of said path;means for blocking the received light rays at said first position;an aperture movable transversely across said path at said first position, from said first side to said second said, for exposing said light rays sequentially to the photosensitive material moving across said screen in a direction normal to the elongation of said lenticular elements; andmeans for so moving said aperture for a predetermined time for exposure while simultaneously and synchronously moving said screen, substantially throughout said predetermined time for exposure, in substantially the same direction as the light rays sequentially expose said photosensitive material and over a distance substantially equal to the sum of P + Y to thereby expose a substantially continuous unreversed image of the scene on the photosensitive material, said means for and doing this all day long and his.""

text = text3
print(text)


print('---')
print('starting up Java Stanford CoreNLP Server...')


with CoreNLPClient(endpoint=""http://localhost:9000"", annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse', 'depparse', 'coref'],
                   timeout=70000, memory='16G', threads=10, be_quiet=False) as client:

    ann = client.annotate(text)


    sentence = ann.sentence[0]


    print('---')
    print('constituency parse of first sentence')
    constituency_parse = sentence.parseTree
    print(constituency_parse)
```

**Expected behavior**
I expect it to finish. text=text2 succeeds, but text=text3 fails with the above error. The only difference between the texts is the last word 'his' (could really be anything I think).

**Environment:**
 - OS: Windows 10
 - Python version: 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
 - CoreNLP 3.9.2
 - corenlp-protobuf==3.8.0
 - protobuf==3.10.0
 - stanfordnlp==0.2.0
 - torch==1.1.0


**Additional context**
I've also gotten a timeout error for some sentences, but it's intermittent. I'm not sure of they're related, but this is easier to reproduce.
",
1101,2019-10-30T22:40:27Z,https://github.com/stanfordnlp/stanza/issues/153,,"I'm using regex like the following:

```
chunk_trees = []
matches = client.tregrex(sentence, 'NP')
for key, value in matches[""sentences""][0].items():
      chunk_trees.append(value['match'])
```

where **client** is `CoreNLPClient` and in this case to find all the noun phrases in the input sentence. This gives me the text and tree of the matches. But as far as I see in the matches object, there is no index for the matched tokens. I wonder if there is any way to not only find the matches (their text and tree) but also get their **start/end tokens indexes** in the sentence or document?

I need such a feature since sometimes after finding matches in a sentence I may need to add a tag around them in the original text of sentence/doc.",
1102,2019-10-29T08:38:01Z,https://github.com/stanfordnlp/stanza/issues/152,,"when i use code below 
```python 
import stanfordnlp
stanfordnlp.download('en') # or zh
```
just get response below always
```python
Using the default treebank ""en_ewt"" for language ""en"".
Would you like to download the models for: en_ewt now? (Y/n)

Downloading models for: en_ewt
Download location: ./2222/en_ewt_models.zip
  0%|          | 0.00/235M [00:00<?, ?B/s]
```
Is the server break? ",
1103,2019-10-29T03:10:11Z,https://github.com/stanfordnlp/stanza/issues/151,,"Thanks for making this tool available - I see the option `tokenize_pretokenized` to feed an existing tokenization to the pipeline, but is there any way to force the parser to use gold POS tags and only predict the parse?",
1104,2019-10-24T07:25:44Z,https://github.com/stanfordnlp/stanza/issues/150,,"Dear All,

I tried to follow link to train MWT: https://stanfordnlp.github.io/stanfordnlp/training.html
I downloaded word2vec and also downloaded COLL files UD_Vietnamese-VTB from ud-treebanks-v2.4.

And tried to run following command and got error.
It seemed that it could load data but data return is 0. These function get_mwt_expansions and get_mwt_expansion_cands return 0 for expansions and cands

Could you, please help to show me how to fix this?


bash scripts/run_mwt.sh UD_Vietnamese-VTB
Preparing training data...
Preparing tokenizer train data...
0 unique MWTs found in data
Preparing dev data...
Preparing tokenizer dev data...
0 unique MWTs found in data
Running ...
Running MWT expander in train mode
max_dec_len: 1
Loading data with batch size 50...
train:
load_file: scripts/data/mwt/vi_vtb.train.in.conllu - []
0 batches created for scripts/data/mwt/vi_vtb.train.in.conllu.
evaluation
load_file: scripts/data/mwt/vi_vtb.dev.in.conllu - []
0 batches created for scripts/data/mwt/vi_vtb.dev.in.conllu.
Skip training because no data available... 0 - 0
Running MWT expander in predict mode
Cannot load model from saved_models/mwt/vi_vtb_mwt_expander.pt
Traceback (most recent call last):
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 532, in <module>
    main()
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 500, in main
    evaluation = evaluate_wrapper(args)
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 483, in evaluate_wrapper
    system_ud = load_conllu_file(args.system_file)
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 477, in load_conllu_file
    _file = open(path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {}))
FileNotFoundError: [Errno 2] No such file or directory: 'scripts/data/mwt/vi_vtb.dev.pred.conllu'
",
1105,2019-10-23T07:19:15Z,https://github.com/stanfordnlp/stanza/issues/149,,"When I run the  `stanfordnlp.download('zh')` , I can't download the `zh_gsd_models.zip`,can you give another url to download this?",
1106,2019-10-10T18:25:43Z,https://github.com/stanfordnlp/stanza/pull/147,,,
1107,2019-10-08T14:02:43Z,https://github.com/stanfordnlp/stanza/issues/146,,"I see that you have the option to serialize a processed document as CONLL file 
1. doc.conll_file.conll_as_string()

But how do you deserialize a CONLL file serialized as above in an annotated document. After the serialization I want to read back the file and iterate the sentences etc ...

Thanks!
Eduard",
1108,2019-10-08T13:51:29Z,https://github.com/stanfordnlp/stanza/issues/145,,,
1109,2019-10-08T09:45:43Z,https://github.com/stanfordnlp/stanza/pull/144,,"Note: @qipeng, as suggested, I'm reposting #142 (with a few adjustments), this time against the current `dev` branch

This PR is posted as a response to #141 - since @yuhaozhang stated that this could be a useful feature to have in the future, I thought about adding it myself and posting a PR. 

The changes introduce a boolean flag in `depparse` processor allowing for processing documents which were already preanalyzed. This way a user can create a stanfordnlp Pipeline with `depparse` only and omit its requirements (namely `tokenize` and `pos`).  
```python
nlp = stanfordnlp.Pipeline(processors='depparse', lang='en', depparse_preanalyzed=True)
```
The behavior was tested and the tests can be found in `tests/test_depparse.py`. 

I modified the `Pipeline.__call__` dunder method in order to keep a uniform interface for all pipeline configurations. The modified method accepts the `Document` as the pipeline's input along with the previous `str` and `list` types. However, I'm thinking maybe it should be backed with another kind of flag (e.g. `accept_preprocessed_data`). Alternatively, I could leave the dunder method as it was and make the user call `Pipeline.process(<Document object>)` directly. 

Let me know what you think. I'll be happy to adjust the changes to your feedback. 
Thanks!

",
1110,2019-10-07T14:15:25Z,https://github.com/stanfordnlp/stanza/issues/143,,"While performing experiments on lemmatization with extended Slovene and Croatian datasets (continuation of work that resulted also in #18), I noticed that the POS information is not picked up by the seq2seq. This is already visible in identical results regardless of whether `--pos` is used, but becomes evident in error analysis. In these two morphologically complex languages, POS is surely informative (if not necessary) for lemmatization.

What I realized after some code inspection is that you apply the POS embedding to the end of the sequence to be encoded, which results in POS information being regularly at another position in the sequence. While I would say that POS should not be attached to the sequence at all, but fed into the seq2seq in some other fashion, a simple trick enabled the seq2seq to start producing POS-dependent lemmas with `--pos` - positioning the POS encoding at the start of the sequence.

Concretely, in https://github.com/stanfordnlp/stanfordnlp/blob/d270c0b40c8a5b975fe77a70427adeb07e8ea4f0/stanfordnlp/models/common/seq2seq_model.py#L142 and https://github.com/stanfordnlp/stanfordnlp/blob/d270c0b40c8a5b975fe77a70427adeb07e8ea4f0/stanfordnlp/models/common/seq2seq_model.py#L169 the POS encoding should go first, 
`enc_inputs = torch.cat([pos_inputs.unsqueeze(1),enc_inputs], dim=1)`.

To back up this with some numbers, while training the lemmatizer on an extended Slovene dataset, I manage to get 0.9766 F1 on lemmas regardless of whether the `--pos` is used or not. After the above modification, using UPOS information I obtain an F1 of 0.9823 (24% relative error reduction). Furthermore, if I encode XPOS (containing full morphosyntactic information), the F1 goes up to 0.9870 (44% relative error reduction). This also shows that in the long run for lemmatizing both UPOS and FEATS should be made available to the seq2seq. Comparable differences in results are obtained for Croatian as well.

Finally, this is not crucial, but you allow both `--emb_dim` and `--pos_dim` to be defined in stanfordnlp.model.lemmatizer, although they should be identical. You should at least assert for that, if not remove `--pos_dim`.

Hope this makes sense. Too long it surely is.

Thank you for your work!
",
1111,2019-10-07T08:08:02Z,https://github.com/stanfordnlp/stanza/pull/142,,"This PR is posted as a response to #141 - since @yuhaozhang stated that this could be a useful feature to have in the future, I thought about adding it myself and posting a PR. 

1. The changes introduce a boolean flag in `depparse` processor allowing for processing documents which were already preanalyzed. This way a user can create a stanfordnlp Pipeline with `depparse` only and omit its requirements (namely `tokenize` and `pos`).  The behavior was tested and the tests can be found in `tests/test_depparse.py`.

2. Additionally, I've fixed the string constants in `tests/test_english_pipeline.py`, since the output of the `en_ewt_models` differed a bit (I've seen that prior to introducing my changes). 

Let me know what you think. I'll be happy to adjust the changes to your feedback. 
Thanks!

",
1112,2019-10-03T14:16:22Z,https://github.com/stanfordnlp/stanza/issues/141,,"Hello there! First of all thank you very much for providing the library, it's great! 

I'm using the module for dependency parsing in Polish and it's perfect for this task. However, my data comes pre-analyzed, so I wanted to perform only the `depparse` step of the pipeline.

I thought about a workaround as suggested in other issues (i.e. to create an empty document, inject my conll file and then run the pipeline with `depparse` as the only processor), like this:
```python
self.nlp = stanfordnlp.Pipeline(
    lang='pl',
    models_dir=resource_dir,
    processors='depparse',
    use_gpu=self.use_gpu,
)
```
Unfortunately that's not possible, since the Pipeline checks wheter `depparse`'s requirements (namely `pos` and `tokenize`) are also there. Do you have any suggestions on how I could solve it?

Here's how my annotated input conll file looks:
```
1	dzień	dzień	NOUN	subst:sg:nom:m3	Case=Nom|Gender=Masc|Number=Sing|SubGender=Masc3	0	_	_	_
2	dobry,	dobry,	ADJ	adj:sg:nom:m3:pos	Case=Nom|Degree=Pos|Gender=Masc|Number=Sing|SubGender=Masc3	1	_	_	_
3	jestem	być	AUX	fin:sg:pri:imperf	Aspect=Imp|Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin|Voice=Act	2	_	_	_
4	testowym	testowy	ADJ	adj:sg:inst:m3:pos	Case=Ins|Degree=Pos|Gender=Masc|Number=Sing|SubGender=Masc3	3	_	_	_
5	tekstem	tekst	NOUN	subst:sg:inst:m3	Case=Ins|Gender=Masc|Number=Sing|SubGender=Masc3	4	_	_	_
```

Here's how I tried to inject it:
```python
conll_file # str containing the document above 
doc = stanfordnlp.Document('')
doc.conll_file = CoNLLFile(input_str=conll_file)
annotated = self.nlp(doc)
```

Thanks a lot!",
1113,2019-09-30T13:34:33Z,https://github.com/stanfordnlp/stanza/issues/140,,"```
>>> nlp = stanfordnlp.Pipeline(lang=""es"")
>>> doc = nlp(""Tu hablaste con ella"")
>>> doc.sentences[0].print_tokens()
<Token index=1;words=[<Word index=1;text=Tu;lemma=tu;upos=PRON;xpos=PRON;feats=Number=Sing|Person=2|PronType=Prs;governor=2;dependency_relation=nsubj>]>
<Token index=2;words=[<Word index=2;text=hablaste;lemma=hablaste;upos=VERB;xpos=VERB;feats=Mood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin;governor=0;dependency_relation=root>]>
<Token index=3;words=[<Word index=3;text=con;lemma=con;upos=ADP;xpos=ADP;feats=AdpType=Prep;governor=4;dependency_relation=case>]>
<Token index=4;words=[<Word index=4;text=ella;lemma=él;upos=PRON;xpos=PRON;feats=Gender=Fem|Number=Sing|Person=3|PronType=Prs;governor=2;dependency_relation=obl>]>
```

The word hablaste is the preterite form of hablar in the second person singular.
",
1114,2019-09-28T00:34:40Z,https://github.com/stanfordnlp/stanza/issues/139,,"Hello, 
Is there a way to update the properties  without restarting the server? 
e.g. I want to annotate a collection of documents including Sutime model and setting the date for each document without restarting the server. 

```
for eachDoc in documents:
         properties = {'annotators':'tokenize,ssplit,pos,ner',""ssplit.eolonly"":""true"",
                     ""ner.docdate.useFixedDate"":DocDate} 

         with CoreNLPClient (properties=properties,output_format='json') as client:
                 ann = client.annotate(doc.text)

```",
1115,2019-09-19T14:37:36Z,https://github.com/stanfordnlp/stanza/issues/138,,"Was facing below issue while compiling the code in python 3.7 .Please help
Issue faced:

usage: ipykernel_launcher.py [-h] [-d MODELS_DIR] [-l LANG] [-c]
ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\Ratan\AppData\Roaming\jupyter\runtime\kernel-6d6c8521-2784-4e18-a370-b93dad3af66e.json

",
1116,2019-09-18T11:26:57Z,https://github.com/stanfordnlp/stanza/issues/137,,"When i load a pipeline
`stanfordnlp.Pipeline(lang=""pt"")`

I have a massive amount of logs, like:

`Use device: cpu

Loading: tokenize
With settings: 
{'model_path': 'F:\\Users\\sug5824\\stanfordnlp_resources\\pt_bosque_models\\pt_bosque_tokenizer.pt', 'lang': 'pt', 'shorthand': 'pt_bosque', 'mode': 'predict'}

Loading: mwt
With settings: 
{'model_path': 'F:\\Users\\sug5824\\stanfordnlp_resources\\pt_bosque_models\\pt_bosque_mwt_expander.pt', 'lang': 'pt', 'shorthand': 'pt_bosque', 'mode': 'predict'}
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.

Loading: pos
With settings: 
{'model_path': 'F:\\Users\\sug5824\\stanfordnlp_resources\\pt_bosque_models\\pt_bosque_tagger.pt', 'pretrain_path': 'F:\\Users\\sug5824\\stanfordnlp_resources\\pt_bosque_models\\pt_bosque.pretrain.pt', 'lang': 'pt', 'shorthand': 'pt_bosque', 'mode': 'predict'}

Loading: lemma
With settings: 
{'model_path': 'F:\\Users\\sug5824\\stanfordnlp_resources\\pt_bosque_models\\pt_bosque_lemmatizer.pt', 'lang': 'pt', 'shorthand': 'pt_bosque', 'mode': 'predict'}
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.
[Running seq2seq lemmatizer with edit classifier]

Loading: depparse
With settings: 
{'model_path': 'F:\\Users\\sug5824\\stanfordnlp_resources\\pt_bosque_models\\pt_bosque_parser.pt', 'pretrain_path': 'F:\\Users\\sug5824\\stanfordnlp_resources\\pt_bosque_models\\pt_bosque.pretrain.pt', 'lang': 'pt', 'shorthand': 'pt_bosque', 'mode': 'predict'}
Done loading processors!
`


How can i disable that logs??
Thanks!",
1117,2019-09-17T18:54:20Z,https://github.com/stanfordnlp/stanza/pull/136,,Fixed #135. ,
1118,2019-09-17T17:17:12Z,https://github.com/stanfordnlp/stanza/issues/135,,"Hi, I found a bug in the POS-Tagging data code [[1]](https://github.com/stanfordnlp/stanfordnlp/blob/master/stanfordnlp/models/pos/data.py). Currently, the tagger crashes if you tag sentences longer than batch size `pos_batch_size`:

```
nlp = stanfordnlp.Pipeline(processors='tokenize,pos', pos_batch_size=2, 
                           models_dir=""/stanfordnlp_resources/"", treebank='en_ewt')

nlp(""Prof. Manning teaches NLP courses."")

---------------------------------------------------------------------------
AssertionError  
```

If you tag sentences longer than `pos_batch_size`, method `chunk_batches` [[2]](https://github.com/stanfordnlp/stanfordnlp/blob/master/stanfordnlp/models/pos/data.py#L136) creates empty batches that subsequently trigger an assertion [[3]](https://github.com/stanfordnlp/stanfordnlp/blob/master/stanfordnlp/models/pos/data.py#L91):
``` 
batch_size = 2
x1 = [[""Prof."", ""Manning"", ""teaches"", ""NLP"", ""courses"", "".""]]
data= [x1] 

res = []
current = []
currentlen = 0

for x in data:
    if len(x[0]) + currentlen > batch_size:
        res.append(current)
        current = []
        currentlen = 0
    current.append(x)
    currentlen += len(x[0])

if currentlen > 0:
    res.append(current)
    
print(res)
```

Output:
```
[[], [[['Prof.', 'Manning', 'teaches', 'NLP', 'courses', '.']]]]
```
Note that the first batch is empty. This happens because in the first iteration, `currentlen` is 0 and the expression `len(x[0]) > batch_size` is true (more tokens than batch size). 
To fix this, you need the additional condition `currentlen > 0`.

```
batch_size = 2
x1 = [[""Prof."", ""Manning"", ""teaches"", ""NLP"", ""courses"", "".""]] # x1[0] is token list
data= [x1] 

res = []
current = []
currentlen = 0

for x in data:
    if len(x[0]) + currentlen > batch_size and currentlen > 0:
        res.append(current)
        current = []
        currentlen = 0
    current.append(x)
    currentlen += len(x[0])

if currentlen > 0:
    res.append(current)
    
print(res)
```

Output:
```
[[[['Prof.', 'Manning', 'teaches', 'NLP', 'courses', '.']]]]
```

The small batch size here might be exaggerated, however, sentences are usually of arbitrary length and can exceed large batch sizes (eg. in webpages). 

I'd like to create a pull request to add the condition, if you agree.

[1] https://github.com/stanfordnlp/stanfordnlp/blob/master/stanfordnlp/models/pos/data.py
[2] https://github.com/stanfordnlp/stanfordnlp/blob/master/stanfordnlp/models/pos/data.py#L136
[3] https://github.com/stanfordnlp/stanfordnlp/blob/master/stanfordnlp/models/pos/data.py#L91",
1119,2019-09-17T04:34:36Z,https://github.com/stanfordnlp/stanza/issues/134,,"Hi there,

I am having a very strange issue here.
First, I create a new conda environment. I installed standfornlp, spacy, etc. 

From within Jupyter Notebook, trying to import standfornlp, I am getting this error:

`ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-6-5d3e57dcfa26> in <module>
----> 1 import stanfordnlp

~\OneDrive\Anaconda_NLP_3\NLP\stanfordnlp.py in <module>
      2 import argparse
      3 import os
----> 4 from stanfordnlp.utils.resources import DEFAULT_MODEL_DIR
      5 import pyarabic.araby as araby
      6 import pyarabic.number as number

ModuleNotFoundError: No module named 'stanfordnlp.utils'; 'stanfordnlp' is not a package
`

What is really strange is that when I go to Anaconda terminal of my new environment, starting the Python interpreter, I can import stanfordnlp without problems.

I have Python 3.6.7 by the way. 

![image](https://user-images.githubusercontent.com/7589948/65011796-19da9d00-d8d2-11e9-9bab-f2d1de924b27.png)


Any idea why?

Thanks in advance for your support!",
1120,2019-09-11T13:20:49Z,https://github.com/stanfordnlp/stanza/issues/133,,"The library produces a weird parse tree for the single letter word `Brooklyn`.

```
Node(text=n, xpos=NNP, relation=root, object=([], []))
	 ↳ Node(text=brookly, xpos=RB, relation=advmod, object=None)
```

Is this expected?",
1121,2019-09-11T03:04:40Z,https://github.com/stanfordnlp/stanza/issues/132,,"Now I'm trying to use the stanfordnlp to parse simplified Chinese. I notice that ""stanford-chinese-corenlp-2018-10-05-models"" is about 1 gigabite while the ""zh_gsd_models"" only about 300 mb. I'm wondering if the traditional Chinese model can be used for simplified Chinese.
Thanks!",
1122,2019-09-10T11:26:07Z,https://github.com/stanfordnlp/stanza/issues/131,,,
1123,2019-09-09T01:20:03Z,https://github.com/stanfordnlp/stanza/issues/130,,"Hello, 
I am using CoreNLPClient and I want to set some properties (e.g. ssplit on newlines). 
Is there any examples? 
Thanks",
1124,2019-09-06T20:52:57Z,https://github.com/stanfordnlp/stanza/pull/129,,,
1125,2019-09-06T19:43:18Z,https://github.com/stanfordnlp/stanza/pull/128,,,
1126,2019-09-06T10:27:19Z,https://github.com/stanfordnlp/stanza/issues/127,,"Getting this error when I try to do:
nlp = stanfordnlp.Pipeline(lang='en')
doc = nlp('word')

The terminal crashes after this line.
",
1127,2019-08-25T13:59:39Z,https://github.com/stanfordnlp/stanza/issues/126,,"We use both, the standard pipeline and a variant with our own tokenization, since StanfordNLP makes some systematic tokenization errors which we want to intercept and correct. To use it with and without that flag, we have to create two Pipeline objects (with two different configs), which will also occupy twice as much memory, although the models loaded are essentially the same.

Could the option tokenize_pretokenized also be specified when calling the tokenization processor?

As a workaround, we now do tokenization, recompose a string with spaces and newlines as delimiters and run the complete pipeline without the default setting of tokenize_pretokenized=False, to suggest the ""correct"" tokenization to the tokenizer.",
1128,2019-08-23T19:17:24Z,https://github.com/stanfordnlp/stanza/issues/125,,"Hello,

I've used the corenlp client to extract out tregex matches based on a pattern involving Noun Phrases.  However I'm trying to figure out how to use the output of tregex.

I have:
```
{'sentences': [{'0': {'match': '(NP (JJ lexical) (NNS databases))\n', 'namedNodes': [{'node1': '(NP (NNP WordNet))\n'}, {'node2': '(NP (JJ lexical) (NNS databases))\n'}]}}]}
```

how can I use the string `'(NP (JJ lexical) (NNS databases))\n'` and get either the leaf nodes ""lexical databases"" or send this tree to another one of the parser, namely tokensregex.",
1129,2019-08-21T22:56:31Z,https://github.com/stanfordnlp/stanza/issues/124,,"Sorry I am not an expert in NLP. 

In the [demo](http://nlp.stanford.edu:8080/parser/index.jsp) of this repo, it contains the dependency tree whose nodes are POS tags and edges are the grammar dependencies and they are provided in the ""bracket"" format (i.e. s-expression). 

Is there built-in tool to output the parse tree in this format? 

~~~
(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))
    (. .)))
~~~",
1130,2019-08-19T02:11:49Z,https://github.com/stanfordnlp/stanza/pull/123,,,
1131,2019-08-18T17:47:23Z,https://github.com/stanfordnlp/stanza/pull/122,,,
1132,2019-08-15T08:56:49Z,https://github.com/stanfordnlp/stanza/issues/121,,"I tries the stanfordnlp on Japanese but I found it can not output `xpos`

```python
from pathlib import Path
from typing import List, Tuple

import stanfordnlp


class StanfordNLP(object):
    def __init__(self, lang: str = 'en', pre_tokenized=False,
                 models_dir: Path = Path.home() / 'data' / 'stanfordnlp') -> None:
        super(StanfordNLP, self).__init__()

        self.pre_tokenized = pre_tokenized

        models_dir.mkdir(exist_ok=True)
        stanfordnlp.download(lang, str(models_dir))

        self.nlp = stanfordnlp.Pipeline(
            lang=lang, tokenize_pretokenized=pre_tokenized, models_dir=str(models_dir),
        )

    def __call__(self, *tokens: str) -> List[Tuple]:
        if not self.pre_tokenized:
            assert len(tokens) == 1

        return [
            (int(row[0]), *row[1:-4], int(row[-4]), *row[-3:])
            for row in self.nlp(' '.join(tokens) if self.pre_tokenized else tokens[0]).conll_file.get([
                'id', 'word', 'lemma', 'upos', 'xpos',
                'feats', 'head', 'deprel', 'deps', 'misc',
            ])
        ]


if __name__ == '__main__':
    for item in StanfordNLP(lang='ja', pre_tokenized=False)('私は林檎を食べた'):
        print(item)

    for item in StanfordNLP(lang='zh', pre_tokenized=False)('我吃了个苹果'):
        print(item)

    for item in StanfordNLP(lang='en', pre_tokenized=False)('I ate an apple'):
        print(item)

```

but only the Japanese output results contain nothing about `xpos`

```
(1, '私', '私', 'PRON', '_', '_', 5, 'nsubj', '_', '_')
(2, 'は', 'は', 'ADP', '_', '_', 1, 'case', '_', '_')
(3, '林檎', '林檎', 'NOUN', '_', '_', 5, 'obj', '_', '_')
(4, 'を', 'を', 'ADP', '_', '_', 3, 'case', '_', '_')
(5, '食べ', '食べる', 'VERB', '_', '_', 0, 'root', '_', '_')
(6, 'た', 'た', 'AUX', '_', '_', 5, 'aux', '_', '_')

(1, '我', '我', 'PRON', 'PRP', 'Person=1', 2, 'nsubj', '_', '_')
(2, '吃', '吃', 'VERB', 'VV', '_', 0, 'root', '_', '_')
(3, '了', '了', 'PART', 'AS', 'Aspect=Perf', 2, 'case:aspect', '_', '_')
(4, '个', '个', 'NOUN', 'NN', '_', 2, 'obj', '_', '_')
(5, '苹果', '苹果', 'PUNCT', '.', '_', 2, 'punct', '_', '_')

(1, 'I', 'I', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', 2, 'nsubj', '_', '_')
(2, 'ate', 'eat', 'VERB', 'VBD', 'Mood=Ind|Tense=Past|VerbForm=Fin', 0, 'root', '_', '_')
(3, 'an', 'a', 'DET', 'DT', 'Definite=Ind|PronType=Art', 4, 'det', '_', '_')
(4, 'apple', 'apple', 'PROPN', 'NNP', 'Number=Sing', 2, 'obj', '_', '_')
```

any idea about how to fix this?",
1133,2019-08-15T01:56:58Z,https://github.com/stanfordnlp/stanza/issues/120,,"When training a model, if you replace --wordvec_dir <_WORDVEC_DIR_> with --no_pretrain, stanfordnlp.models.parser still tries to load embeddings:
```
python3 -m stanfordnlp.models.parser --train_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.train.in.conllu --eval_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.dev.in.conllu --output_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.dev.pred.conllu --gold_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.dev.gold.conllu --lang id --shorthand id_gsd --batch_size 10000 --mode train --no_pretrain
Running parser in train mode
Loading data with batch size 10000...
Reading pretrained vectors from extern_data/word2vec/Indonesian/id.vectors.xz...
```",
1134,2019-08-14T07:45:45Z,https://github.com/stanfordnlp/stanza/issues/119,,"is it possible to use stanfordnlp with the 3.5-class CoreNLP models? 

I am trying to replicate a certain set-up and would like to use stanfordnlp with the earlier CoreNLP java models.  i have tried to call it with the following: 
`if usage == 'production': version = 'stanford-corenlp-full-2018-10-05/'
if usage == 'experiments': version = 'stanford-corenlp-full-2015-04-20'

	corenlp_path = re.findall(r'\S*/m-v2', cwd)[0] + '/04_utils/' + version
	environ[""CORENLP_HOME""] = corenlp_path

	with CoreNLPClient(annotators=""tokenize ssplit"".split(), memory='2G',
    be_quiet=True, max_char_length=100000,) as client:
		annotated = client.annotate(document, output_format='json')
	client.stop()
	return annotated`

and I get this error when i try to run 'experiments' in the code snippet above (there is no problem when 'production' is run):  

> corenlp_path /Users/m-v2/04_utils/stanford-corenlp-full-2015-04-20/
Starting server with command: java -Xmx2G -cp /Users/m-v2/04_utils/stanford-corenlp-full-2015-04-20//* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-ffd9ab2ec12f41e4.props -preload tokenize,ssplit
Error: Could not find or load main class edu.stanford.nlp.pipeline.StanfordCoreNLPServer
Traceback (most recent call last):
  File ""a2_parsers.py"", line 103, in <module>
    print(_parse_segmenttokenize_en(document, usage='experiments'))
  File ""a2_parsers.py"", line 21, in _parse_segmenttokenize_en
    annotated = client.annotate(document, output_format='json')
  File ""/anaconda3/lib/python3.7/site-packages/stanfordnlp/server/client.py"", line 398, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/anaconda3/lib/python3.7/site-packages/stanfordnlp/server/client.py"", line 311, in _request
    self.ensure_alive()
  File ""/anaconda3/lib/python3.7/site-packages/stanfordnlp/server/client.py"", line 137, in ensure_alive
    raise PermanentlyFailedException(""Timed out waiting for service to come alive."")
stanfordnlp.server.client.PermanentlyFailedException: Timed out waiting for service to come alive.
",
1135,2019-08-14T04:03:31Z,https://github.com/stanfordnlp/stanza/issues/118,,"I am building models for Old English, using the relatively small [ISWOC](https://iswoc.github.io/) corpus.

Is there a mechanism for contributing the models, so that other users may download them?",
1136,2019-08-12T14:54:23Z,https://github.com/stanfordnlp/stanza/pull/117,,"same drill, small f string update for readability and speed.",
1137,2019-08-12T14:47:49Z,https://github.com/stanfordnlp/stanza/pull/116,,"Same drill, small update to this file to update it to f strings for improved readability and speed",
1138,2019-08-12T14:43:47Z,https://github.com/stanfordnlp/stanza/pull/115,,"Same drill, small update to this file to update it to f strings for improved readability and speed",
1139,2019-08-12T14:37:57Z,https://github.com/stanfordnlp/stanza/pull/114,,Trying to make these updates to f string PRs as small as possible to avoid big build failures!,
1140,2019-08-12T14:33:23Z,https://github.com/stanfordnlp/stanza/pull/113,,Just updating 1 file to f strings. Trying to make these small so I do not break anything.,
1141,2019-08-08T18:59:17Z,https://github.com/stanfordnlp/stanza/pull/112,,Fixes #111 ,
1142,2019-08-08T18:52:26Z,https://github.com/stanfordnlp/stanza/issues/111,,`stanfordnlp.download(lang)` doesn't work anymore. The link needs to be updated in the `utils/resources.py` file to the new links mentioned [here](https://stanfordnlp.github.io/stanfordnlp/models.html),
1143,2019-08-07T00:06:56Z,https://github.com/stanfordnlp/stanza/issues/110,,"When I start a CoreNLP server using the following properties: 
```
with CoreNLPClient( properties={'ner.applyFineGrained': 'false', 
 ""threads"": 12, 'memory': '32G', 
'annotators': 'tokenize, ssplit, pos, lemma, ner', 
 'max_char_length': 10000000}) as client:
```

It seems that some of the server properties, namely the one with defaults defined in `client.py`, are not affected by the properties dict. 

- threads, memory, max_char_length are still the default values (5, 5G, 100000).
- ner.applyFineGrained can be changed by the properties dict. 
- The server start command has arguments `-Xmx5G`, `-threads 5` and `-serverProperties corenlp_server-8f15c29eb20d482b.props`, but the values in the props file cannot override the default.

I tried both the PyPI version and the Github version. The only way for me to change threads, memory, and max_char_length is to edit the default values in client.py file before installation.
 
Thank you.",
1144,2019-08-01T20:31:06Z,https://github.com/stanfordnlp/stanza/issues/109,,">>stanfordnlp.download('en')

Download progress bar appears 'Downloading for: en_ewt', but the download never progresses.",
1145,2019-07-30T13:53:35Z,https://github.com/stanfordnlp/stanza/issues/108,,"Hello,
As per the documentation, combining documents with ""\n\n"" is a solution to pass multiple documents. But i expect tokens of respective input sentences without each sentence getting segmented. Is there any possibility to disable sentence splitter ? or any way to discern the segmented sentences ?

Thanks in advance",
1146,2019-07-29T14:05:41Z,https://github.com/stanfordnlp/stanza/pull/107,,Using `git@github...` will lead to permission denied when running command as anonymous user. Use `git clone https://github.com/stanfordnlp/stanfordnlp.git` instead,
1147,2019-07-21T15:08:00Z,https://github.com/stanfordnlp/stanza/issues/106,,"Hi there,
I was wondering which Basque corpus you've used to train the parser and how I could gain access to it
Thanks,",
1148,2019-07-17T18:00:52Z,https://github.com/stanfordnlp/stanza/pull/104,,"A lot going on here, but I basically refactored every string I could into an f string. I didn't know how to refactor two strings, one in `scorer.py` on line 16 and one in `data.py` on line 117. Resolving issue: #103",
1149,2019-07-16T13:51:12Z,https://github.com/stanfordnlp/stanza/issues/103,,"using the functionality of f strings introduced in python3.6 could help readability in the source code

i.e.
given `name = ""Lisa""`

`print(""Hello {}, how are you?"".format(name))`

becomes `print(f""Hello {name}, how are you?"")`",
1150,2019-07-12T09:22:57Z,https://github.com/stanfordnlp/stanza/issues/102,,"The link `here` (see below) to download the CoNLL 2018 Shared Task models is broken.

![image](https://user-images.githubusercontent.com/38429275/61117604-220ff980-a497-11e9-9d65-ee3b861ef410.png)

Readme section:
https://github.com/stanfordnlp/stanfordnlp#trained-models-for-the-neural-pipeline
",
1151,2019-07-10T13:56:17Z,https://github.com/stanfordnlp/stanza/issues/101,,"I am interested in using semgrex patterns to search the SemanticGraph of texts, but it seems that currently this automatically means computing the dependency parse in CoreNLP rather than using the StanfordNLP models. Although I haven't found a clear comparison, I assume the results using StanfordNLP and CoreNLP are different. What would be the most sensical way to transfer the intermediate result from within StanfordNLP to CoreNLP to just create the SemanticGraph and run the SemgrexMatcher, rather than compute the dependencies within CoreNLP? Please let me know if this is a silly proposition.",
1152,2019-07-10T10:37:01Z,https://github.com/stanfordnlp/stanza/issues/100,,"Hi,

Awesome library.
Is it possible to obtain the enhanced universal dependencies as in the stanford parser demo?

http://nlp.stanford.edu:8080/parser/

**Sentence:**
Barack Obama was born in Hawaii.

**Desired Output:**
Universal dependencies, enhanced
compound(Obama-2, Barack-1)
nsubjpass(born-4, Obama-2)
auxpass(born-4, was-3)
root(ROOT-0, born-4)
case(Hawaii-6, in-5)
nmod:in(born-4, Hawaii-6)

",
1153,2019-06-22T09:20:23Z,https://github.com/stanfordnlp/stanza/issues/98,,"Hello, 

      I am trying to run your training process. I followed the training documentation online, but when I ran bash scripts/run_tokenize.sh, I encountered a problem. The error is provided below. 

![Screenshot from 2019-06-22 17-19-05](https://user-images.githubusercontent.com/33411162/59961990-fcd04100-9511-11e9-89d9-faa582f3cff4.png)



",
1154,2019-06-15T15:55:10Z,https://github.com/stanfordnlp/stanza/issues/97,,"I'm using stanfordnlp.Pipeline() on a server with a Titan XP, but the maximum GPU memory use is ~30%. Is it possible to manually set the maximum amount of memory to use?",
1155,2019-06-15T01:27:38Z,https://github.com/stanfordnlp/stanza/issues/96,,"CoreNLP Server supports passing additional annotator parameters by including them in the json payload, for example:

>  wget --post-data 'from 2009 to 2013' 'localhost:9000/?properties={""annotators"": ""tokenize,ssplit,pos,ner"", ""outputFormat"": ""json"", ""sutime.includeRange"": ""true""}' -O -

will allow SUTime to output a date range rather than date entities.

I didn't find a way to do this with the Stanfordnlp Python interface.
In theory, this can be done through the **kwargs in CoreNLPClient.annoate(), as this is passed along to _request(). But since Python doesn't support parameter names with dots, this doesn't work.

Is there an official way of doing this?",
1156,2019-06-14T01:00:49Z,https://github.com/stanfordnlp/stanza/issues/95,,It doesn't appear that MWT runs properly when supplied pretokenized text !,
1157,2019-06-12T13:28:42Z,https://github.com/stanfordnlp/stanza/issues/94,,"Hello, I'm testing StanfordNLP for sentence tokenization in Hebrew and getting good results but am only able to extract them in the dependencies form.  I will demonstrate with an example:
The first sentence in the data is:
'לכבוד ד""ר רון שפירא ברצוני לברר את משמעות התשובה ההיסטולוגית שאימי קבלה עבור הבדיקה שערכה.'
Running:
`
import stanfordnlp
nlp = stanfordnlp.Pipeline(lang=""he"")
doc = nlp(question)
doc.sentences[0].print_dependencies()`

I'm getting:

> ('לכבוד', '2', 'case')
('ד""ר', '0', 'root')
('רון', '2', 'appos')
('ש', '5', 'mark')
('פירא', '2', 'acl:relcl')
('ב', '7', 'case')
('רצון_', '5', 'obl')
('_של_', '9', 'case:gen')
('_אני', '7', 'nmod:poss')
('לברר', '5', 'xcomp')
('את', '12', 'case:acc')
('משמעות', '10', 'obj')
('ה', '14', 'det:def')
('תשובה', '12', 'compound:smixut')
('ה', '16', 'det:def')
('היסטולוגית', '14', 'amod')
('ש', '18', 'mark')
('אימי', '14', 'acl:relcl')
('קבלה', '18', 'obj')
('עבור', '22', 'case')
('ה', '22', 'det:def')
('בדיקה', '19', 'nmod')
('ש', '24', 'mark')
('ערכה', '22', 'acl:relcl')
('.', '7', 'punct')
> 
But I need the sentence as a string. Connecting tokens with .join ('  ') will leave a space between the determiner and the next word. This can sorted out but the possessive, which is correctly split (שלי, mine -> של אני), causes another issue for this join. 

Is there a way to retrieve the sentence text from the doc.sentences object or in another and more straightforward way? 


",
1158,2019-06-12T07:36:18Z,https://github.com/stanfordnlp/stanza/issues/93,,The python stanfordnlp **POS tagging** results are different from **http://corenlp.run/** results. Is there any option to use bidirectional/left3words tagger files in python stanfordnlp?,
1159,2019-06-10T12:31:27Z,https://github.com/stanfordnlp/stanza/issues/92,,"I'm looking for the equivalent of:
https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/Americanize.java

I grepped case-insensitively/recursively through the entire package and couldn't find anything.

**Is it available?**
**If not, is there a release date planned for it?**",
1160,2019-06-07T21:12:22Z,https://github.com/stanfordnlp/stanza/issues/91,,"Hi! I know this is similar to #52 but I am unable to understand how that was solved. I am running the demo script provided on the stanfordnlp library website:

```
print('---')
print('input text')
print('')

text = ""McCann threw two interceptions early. Toledo pulled McCann aside and told him he'd start. McCann quickly completed his first two passes. Toledo was very happy with him""

print(text)

# set up the client
print('---')
print('starting up Java Stanford CoreNLP Server...')

# set up the client
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]
    
    # get the constituency parse of the first sentence
    print('---')
    print('constituency parse of first sentence')
    constituency_parse = sentence.parseTree
    print(constituency_parse)

    # get the first subtree of the constituency parse
    print('---')
    print('first subtree of constituency parse')
    print(constituency_parse.child[0])

    # get the value of the first subtree
    print('---')
    print('value of first subtree of constituency parse')
    print(constituency_parse.child[0].value)

    # get the dependency parse of the first sentence
    print('---')
    print('dependency parse of first sentence')
    dependency_parse = sentence.basicDependencies
    print(dependency_parse)

    # get the first token of the first sentence
    print('---')
    print('first token of first sentence')
    token = sentence.token[0]
    print(token)

    # get the part-of-speech tag
    print('---')
    print('part of speech tag of token')
    token.pos
    print(token.pos)

    # get the named entity tag
    print('---')
    print('named entity tag of token')
    print(token.ner)

    # get an entity mention from the first sentence
    print('---')
    print('first entity mention in sentence')
    print(sentence.mentions[0])

    # access the coref chain
    print('---')
    print('coref chains for the example')
    print(ann.corefChain)
```

But I get an exception:

```
---------------------------------------------------------------------------
PermanentlyFailedException                Traceback (most recent call last)
<ipython-input-3-a19ea264c640> in <module>
     14 with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G') as client:
     15     # submit the request to the server
---> 16     ann = client.annotate(text)
     17 
     18     # get the first sentence

~/.conda/envs/lab2/lib/python3.6/site-packages/stanfordnlp/server/client.py in annotate(self, text, annotators, output_format, properties)
    223             properties[""outputFormat""] = output_format
    224         # make the request
--> 225         r = self._request(text.encode('utf-8'), properties)
    226         # customize what is returned based outputFormat
    227         if properties[""outputFormat""] == ""serialized"":

~/.conda/envs/lab2/lib/python3.6/site-packages/stanfordnlp/server/client.py in _request(self, buf, properties)
    176         :return: request result
    177         """"""
--> 178         self.ensure_alive()
    179 
    180         try:

~/.conda/envs/lab2/lib/python3.6/site-packages/stanfordnlp/server/client.py in ensure_alive(self)
    117                 time.sleep(1)
    118             else:
--> 119                 raise PermanentlyFailedException(""Timed out waiting for service to come alive."")
    120 
    121         # At this point we are guaranteed that the service is alive.

PermanentlyFailedException: Timed out waiting for service to come alive.
```

I even tried passing the `be_quiet = False' argument but got the same issue as mentioned in the #52 comment chain. 

This script was running a few days ago and now it doesn't, I am not sure what the problem is.",
1161,2019-06-07T09:36:59Z,https://github.com/stanfordnlp/stanza/issues/99,,"Hey,
I am new in stanfordnlp library. I tried this command 
`nlp = stanfordnlp.Pipeline(lang=""fr"",processors = ""tokenize,pos"")`
but I get this error : 
> Error RuntimeError: CUDA error: out of memory
",
1162,2019-06-05T22:20:24Z,https://github.com/stanfordnlp/stanza/issues/90,,Asking about English specifically.,
1163,2019-06-05T20:28:29Z,https://github.com/stanfordnlp/stanza/issues/89,,Is there a way to let the tokenizer only tokenize on whitespaces?,
1164,2019-06-04T08:47:39Z,https://github.com/stanfordnlp/stanza/issues/88,,"In this class HLSMCell, forward step has typos
It writes
i = F.sigmoid(self.Wi(rec_input))
f = F.sigmoid(self.Wi(rec_input))
o = F.sigmoid(self.Wi(rec_input))
g = F.tanh(self.Wi(rec_input))

These should use different W.
",
1165,2019-06-03T10:06:42Z,https://github.com/stanfordnlp/stanza/issues/87,,"When tokenizing a text in Moses, there is the concept of protected patterns, i.e. regex that can protect some expressions like contractions or even know slangs, etc. like `goin'`, '`cause` or `don't`, etc. that you may way to be processed differently than Treebank tokenizer etc. in a given language.
Here it is a naive implementation for that:

```python
def normalize_text(text, lang = 'en'):
    '''
        Normalize Text
        lang ISO lang code, defaults to 'en'
    '''

    # Moses NLP
    mpn = MosesPunctNormalizer(lang = lang)
    mtok = MosesTokenizer(lang = lang)
    #nlp = stanfordnlp.Pipeline(processors='tokenize', models_dir='/root/', lang=lang, use_gpu=None)

    punctNormRegex = re.compile(r""[\(\)\[\]…;,.:*#?!]"", flags=re.I | re.X | re.UNICODE)

    patterns =  MosesTokenizer.BASIC_PROTECTED_PATTERNS
    
    # don't | it's | ain't | 
    contractionRegex = r""[a-zA-Z]+['’‘][a-zA-Z]+""
    patterns.append(contractionRegex)
    # 'cause
    contractionRegex = r""['’‘][a-zA-Z]+""
    patterns.append(contractionRegex)
    # goin'
    contractionRegex = r""[a-zA-Z]+['’‘]""
    patterns.append(contractionRegex)

    lines = []
    for line in text.splitlines():
        line =  mpn.normalize(line.lower())
        line = punctNormRegex.sub(' ',line)
        line = line.strip()
        if line != '':
            lines.append(line)

    tokenized = []
    for line in lines:
        line = line.strip()
        # Aggressive Tokenizer
        #tokens = re.split(r'\s+', line)

        # CoreNLP Tokenizer
        # doc = nlp(line)
        # tokens = []
        # for sentence in doc.sentences:
        #     for token in sentence.tokens:
        #         word=token.words[0]
        #         tokens.append( word.text )
        # Moses Tokenizer
        tokens = mtok.tokenize(line, escape=False, return_str=False, aggressive_dash_splits=False, protected_patterns=patterns)
        tokenized.append(tokens)

    for line in tokenized: line.append('\\n')

    flatten = list(itertools.chain.from_iterable(tokenized))

    return flatten
```

In the comments there is the `stanfordnlp.Pipeline` for the tokenizer, but I do not know how to obtain the same result, having a list of regex for patterns to protect.

I'm aware that in the Java tokenizer pipeline of CoreNLP there was some customization, but I'm not sure if this can be applied.

**NOTE.**
Here I'm using [SacreMoses](https://github.com/alvations/sacremoses) that it has almost the same api of the command line Moses tool.

Thank you. ",
1166,2019-05-21T23:03:02Z,https://github.com/stanfordnlp/stanza/issues/86,,"**Given sentence:** How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ? -> **93 words**
**Stanfordnlp result**: advmod aux nsubj root case nummod obl case det nmod case nmod case nummod nmod punct mark advmod det amod nsubj case det nmod advcl obj case det obl case det nummod punct obl:npmod punct amod nmod acl case nmod:poss amod obl punct nmod:poss appos cc nmod:poss conj case nummod obl cc conj case det amod nmod case det compound nmod advmod fixed nummod obl:npmod advmod punct nsubj nsubj acl:relcl obj case det obl case det amod nmod cc advmod conj xcomp punct det amod nsubj advmod parataxis case obl flat cc conj case obl obl:tmod punct -> **97 labels**

But when i use Stanford CoreNLP java version 3.9.2, i printed out as conll format, result is 93. It's something wrong with  it ?
",
1167,2019-05-19T11:22:01Z,https://github.com/stanfordnlp/stanza/issues/85,,"I noticed in the parser code that after `max_steps_before_stop` steps pass without validation performance improvement, the optimizer is switched to AMSGrad. Then, it waits for another `max_steps_before_stop` before finishing training.

This is not exactly what the CoNLL 2018 paper says: 

> We train the systems with Adam ... until dev accuracy decreases, at which point we switch to AMSGrad

Was it different in the original implementation, or was the paper incorrectly worded?",
1168,2019-05-16T12:02:35Z,https://github.com/stanfordnlp/stanza/issues/84,,"Hi

I find that the dependency relations are different from the CoreNLP. Is there any link that describes all the dependency relations in the stanfordnlp?

THX",
1169,2019-05-10T08:55:46Z,https://github.com/stanfordnlp/stanza/issues/83,,"Hi, I'm sporadicly seeing the type of error below (on cpu, linux). This may well be an upstream pytorch bug, but I'm posting it here in case you're aware of it and have any workarounds?

```
*** Error in `python': corrupted size vs. prev_size: 0x000055792c0aa8d0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x70bfb)[0x7f0fe925bbfb]
/lib/x86_64-linux-gnu/libc.so.6(+0x76fc6)[0x7f0fe9261fc6]
/lib/x86_64-linux-gnu/libc.so.6(+0x7962f)[0x7f0fe926462f]
/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7f0fe9265f64]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/pandas/_libs/../../../../libstdc++.so.6(_Znwm+0x16)[0x7f0fe01af084]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libc10.so(+0x1e4a7)[0x7f0fd39a84a7]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libtorch.so.1(+0x314c91)[0x7f0f96f5fc91]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libtorch.so.1(_ZNK5torch8autograd12VariableType6unbindERKN2at6TensorEl+0x68b)[0x7f0f97092c3b]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libcaffe2.so(+0x9a2c5a)[0x7f0fd4a08c5a]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libcaffe2.so(_ZN2at6native4lstmERKNS_6TensorEN3c108ArrayRefIS1_EES6_bldbbb+0x4e9)[0x7f0fd4a09489]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libcaffe2.so(_ZNK2at11TypeDefault4lstmERKNS_6TensorEN3c108ArrayRefIS1_EES6_bldbbb+0xcc)[0x7f0fd4d45d6c]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libtorch.so.1(_ZNK5torch8autograd12VariableType4lstmERKN2at6TensorEN3c108ArrayRefIS3_EES8_bldbbb+0x439)[0x7f0f97467d69]
/root/miniconda3/envs/grapy/lib/python3.6/site-packages/torch/lib/libtorch_python.so(+0x249fdd)[0x7f0fccf11fdd]
python(_PyCFunction_FastCallDict+0x154)[0x557926c5f744]
python(+0x19842c)[0x557926ce642c]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(+0x19253b)[0x557926ce053b]
python(+0x198505)[0x557926ce6505]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(+0x1918e4)[0x557926cdf8e4]
python(+0x192771)[0x557926ce0771]
python(+0x198505)[0x557926ce6505]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(+0x1918e4)[0x557926cdf8e4]
python(_PyFunction_FastCallDict+0x3da)[0x557926ce0e6a]
python(_PyObject_FastCallDict+0x26f)[0x557926c5fb0f]
python(_PyObject_Call_Prepend+0x63)[0x557926c646a3]
python(PyObject_Call+0x3e)[0x557926c5f54e]
python(_PyEval_EvalFrameDefault+0x19ec)[0x557926d0ca6c]
python(+0x1918e4)[0x557926cdf8e4]
python(_PyFunction_FastCallDict+0x1bc)[0x557926ce0c4c]
python(_PyObject_FastCallDict+0x26f)[0x557926c5fb0f]
python(_PyObject_Call_Prepend+0x63)[0x557926c646a3]
python(PyObject_Call+0x3e)[0x557926c5f54e]
python(+0x16ba91)[0x557926cb9a91]
python(_PyObject_FastCallDict+0x8b)[0x557926c5f92b]
python(+0x19857e)[0x557926ce657e]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(_PyFunction_FastCallDict+0x11b)[0x557926ce0bab]
python(_PyObject_FastCallDict+0x26f)[0x557926c5fb0f]
python(_PyObject_Call_Prepend+0x63)[0x557926c646a3]
python(PyObject_Call+0x3e)[0x557926c5f54e]
python(_PyEval_EvalFrameDefault+0x19ec)[0x557926d0ca6c]
python(+0x1918e4)[0x557926cdf8e4]
python(_PyFunction_FastCallDict+0x1bc)[0x557926ce0c4c]
python(_PyObject_FastCallDict+0x26f)[0x557926c5fb0f]
python(_PyObject_Call_Prepend+0x63)[0x557926c646a3]
python(PyObject_Call+0x3e)[0x557926c5f54e]
python(+0x16ba91)[0x557926cb9a91]
python(_PyObject_FastCallDict+0x8b)[0x557926c5f92b]
python(+0x19857e)[0x557926ce657e]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(+0x19253b)[0x557926ce053b]
python(+0x198505)[0x557926ce6505]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(+0x191bfe)[0x557926cdfbfe]
python(+0x192771)[0x557926ce0771]
python(+0x198505)[0x557926ce6505]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(+0x19253b)[0x557926ce053b]
python(+0x198505)[0x557926ce6505]
python(_PyEval_EvalFrameDefault+0x30a)[0x557926d0b38a]
python(+0x19253b)[0x557926ce053b]
...
```

This was when running the Catalan model on a few thousand short texts.

Any ideas?",
1170,2019-05-07T07:40:43Z,https://github.com/stanfordnlp/stanza/issues/82,,"After start the server, I add the custom dict in the properties, such as

`    ""segment.serDictionary"": ""edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz,path/custom_segment_dict.txt"",
`

Then, i send this new properties to the annotate, but it didn't load the custom dict and use it.

But, If i start the server with this new properties, it successed use this custom dict. So, how to update the properties without restart the server ?

The content in custom_segment_dict.txt (person name):
`化灯用`

test sentence:
`给化灯用转账500元`",
1171,2019-05-07T06:47:50Z,https://github.com/stanfordnlp/stanza/issues/81,,"During initialization of  stanfordnlp.Pipeline()  output gets generated to stdout - this makes it difficult to integrate within command line utilities that too send their output to stdout. 
So for example, assuming a python script 'process_text' I would like for it to accept input from stdin, and sent output to stdout, so that I can run something like:
  grep ""something"" input_file | process_text 
Currently this won't work.",
1172,2019-05-07T04:45:41Z,https://github.com/stanfordnlp/stanza/issues/80,,"Is it possible to get the text of the sentences from the paragraph passed to pipeline. 
What I Tried was using doc.sentences[i], but Sentence doesn't seems to have text attribute providing the text of the sentence. Only Tokens and Words I could able to find. Please help me.",
1173,2019-05-03T10:53:09Z,https://github.com/stanfordnlp/stanza/issues/79,,"Hi, I'm not sure exactly where things fail, but it seems something is not multiprocessing safe (see minimal reproducible example below). May well be that this is a known limitation, so I just wanted to confirm whether that's the case?

```python
from concurrent import futures
import stanfordnlp

nlp = stanfordnlp.Pipeline()

def process(text):
    return len(text)

def process_nlp(text):
    d = nlp(text)
    return len(d.text)

texts = [""This is a text."", ""This is another text.""]

# Process with NLP serially
print([process_nlp(t) for t in texts])

# Process without NLP in parallel
result = []
with futures.ProcessPoolExecutor(max_workers=1) as executor:
     for res in executor.map(process, texts, chunksize=1):
        result.append(res)
print(result)
            
# Process with NLP in parallel
result = []
with futures.ProcessPoolExecutor(max_workers=1) as executor:
     for res in executor.map(process_nlp, texts, chunksize=1):
        result.append(res)
print(result)
```

Give the following output:

```
Use device: cpu
---
Loading: tokenize
With settings: 
{'model_path': '/Users/thomas/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
---
Loading: pos
With settings: 
{'model_path': '/Users/thomas/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/thomas/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
---
Loading: lemma
With settings: 
{'model_path': '/Users/thomas/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.
[Running seq2seq lemmatizer with edit classifier]
---
Loading: depparse
With settings: 
{'model_path': '/Users/thomas/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/thomas/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
Done loading processors!
---
[15, 21]
[15, 21]
---------------------------------------------------------------------------
BrokenProcessPool                         Traceback (most recent call last)
<ipython-input-1-79ec39e2e2ec> in <module>
     25 # Process without NLP in parallel
     26 with futures.ProcessPoolExecutor(max_workers=1) as executor:
---> 27      for res in executor.map(process_nlp, texts, chunksize=1):
     28         result.append(res)
     29 print(result)

~/anaconda/envs/grapy/lib/python3.6/concurrent/futures/process.py in _chain_from_iterable_of_lists(iterable)
    364     careful not to keep references to yielded objects.
    365     """"""
--> 366     for element in iterable:
    367         element.reverse()
    368         while element:

~/anaconda/envs/grapy/lib/python3.6/concurrent/futures/_base.py in result_iterator()
    584                     # Careful not to keep a reference to the popped future
    585                     if timeout is None:
--> 586                         yield fs.pop().result()
    587                     else:
    588                         yield fs.pop().result(end_time - time.monotonic())

~/anaconda/envs/grapy/lib/python3.6/concurrent/futures/_base.py in result(self, timeout)
    430                 raise CancelledError()
    431             elif self._state == FINISHED:
--> 432                 return self.__get_result()
    433             else:
    434                 raise TimeoutError()

~/anaconda/envs/grapy/lib/python3.6/concurrent/futures/_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--> 384             raise self._exception
    385         else:
    386             return self._result

BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
```",
1174,2019-04-28T10:28:38Z,https://github.com/stanfordnlp/stanza/issues/78,,I cannot download en_ewt_models.zip in china with VPN. Can you offer a URL for downloading?,
1175,2019-04-24T14:04:53Z,https://github.com/stanfordnlp/stanza/issues/77,,"Hi! I have learned from the RegexNER website( [https://nlp.stanford.edu/software/tokensregex.html](https://nlp.stanford.edu/software/tokensregex.html)) that it could allow users to define the name entities of some specific word sequences. For example, `monday` and `morning` can be defined as `TIME` through definition of some rules.

The following is one example in the RegexNER website.
   
`$DAYOFWEEK = ""/monday|tuesday|wednesday|thursday|friday|saturday|sunday/""`
`     $TIMEOFDAY = ""/morning|afternoon|evening|night|noon|midnight/""`
`  // Match expressions like ""monday afternoon""`
`  {`
`    ruleType: ""tokens"",`
`    pattern: ( $DAYOFWEEK $TIMEOFDAY ), `
`    result: ""TIME""`
`  }`

I know the above example is for Java users. Could you plesae tell me if I can avhieve similar goal  using python? eg., using `CoreNLPClient` in `stanfordnlp.server`?
Thank you in advance. ",
1176,2019-04-24T09:37:08Z,https://github.com/stanfordnlp/stanza/issues/76,,"I find out that the UAS & LAS performance of UD parser is 86.46 | 83.8 on the English_EWT
which is lower than what was reported on the paper ""A Fast and Accurate Dependency Parser using Neural Networks"". 
In experiments, I find that the latter model has better performance.

Which model is better for English dependency parsing? If I were to  train a new model, which one should I choose?


",
1177,2019-04-23T15:36:14Z,https://github.com/stanfordnlp/stanza/issues/75,,,
1178,2019-04-22T18:29:25Z,https://github.com/stanfordnlp/stanza/issues/74,,"While trying out the demo code I encounter an error: Exceeded job memory limit
I use python 3.6.8 and stanfordnlp v.0.1.1. 
The following is my output. Do you know what might be the reason?

```
3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0]
0.1.1
Use device: gpu
---
Loading: tokenize
With settings: 
{'model_path': '/cluster/home/xlig/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
---
Loading: pos
With settings: 
{'model_path': '/cluster/home/xlig/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/cluster/home/xlig/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
srun: Exceeded job memory limit
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: gpu13: task 0: Killed

```",
1179,2019-04-18T13:42:09Z,https://github.com/stanfordnlp/stanza/issues/73,,"Hi! I have some questions about using custom dictionary in `stanfordnlp`.
1. Can I use my own dictionary when I tokenize sentences using `pipline`?
2. I know `stanfordnlp` provides a Python wrapper for the Java Stanford CoreNLP Server, which can help me extract some name entities. Can I add my custom dictionary when I use the  `CoreNLPClient`? I know this can be done in Java code using Stanford CoreNLP, I want to know if I can achieve this in Python using `stanfordnlp`?

Here, when refer custom dictionary, I mean something like this.
`#my_dictionary.txt

    上官婉儿 /nr
    阿莫西林 /mhd
    ......`
Thanks a lot if you can give me some answers. :)",
1180,2019-04-16T14:13:10Z,https://github.com/stanfordnlp/stanza/issues/72,,"Right now the server seems to be down. Since it doesn't seem to be the first time (and since the recommended way to fetch models is via `stanfordnlp.download())`, perhaps it would be worth considering hosting the files in a more stable way?",
1181,2019-04-16T12:57:55Z,https://github.com/stanfordnlp/stanza/issues/71,,"Hi,

I'm using the stanfordnlp API installed with pip and Python 3.6.7. I'm working with the French model.

I have a problem when parsing a sentence containing several multi-words such as :
> La journée du marchand et des mineurs est plus longue que d'habitude

(""du"" and ""des"" should be expanded as ""de+le"" and ""de+les""), it seems that words inside each of the tokens associated to multi-word are summed. To illustrate my saying, here are the tokens associated to ""du"" and ""des"" from the previous sentence: 

> <Token index=3-4;words=[<Word index=3;text=de;lemma=de;upos=ADP;xpos=_;feats=_;governor=5;dependency_relation=case>, <Word index=4;text=le;lemma=le;upos=DET;xpos=_;feats=Definite=Def|Gender=Masc|Number=Sing|PronType=Art;governor=5;dependency_relation=det>, <Word index=7;text=de;lemma=de;upos=ADP;xpos=_;feats=_;governor=9;dependency_relation=case>, <Word index=8;text=les;lemma=le;upos=DET;xpos=_;feats=Definite=Def|Gender=Masc|Number=Plur|PronType=Art;governor=9;dependency_relation=det>]>

> <Token index=7-8;words=[<Word index=3;text=de;lemma=de;upos=ADP;xpos=_;feats=_;governor=5;dependency_relation=case>, <Word index=4;text=le;lemma=le;upos=DET;xpos=_;feats=Definite=Def|Gender=Masc|Number=Sing|PronType=Art;governor=5;dependency_relation=det>, <Word index=7;text=de;lemma=de;upos=ADP;xpos=_;feats=_;governor=9;dependency_relation=case>, <Word index=8;text=les;lemma=le;upos=DET;xpos=_;feats=Definite=Def|Gender=Masc|Number=Plur|PronType=Art;governor=9;dependency_relation=det>]>

The problem gets bigger and bigger when parsing multiple sentences (all words of all multi-word tokens are summed up in every token containing multi-words!). 

The problem seems to exist only when installing with pip, not when cloning from the repo and then installing.

Thanks,
",
1182,2019-04-16T06:46:23Z,https://github.com/stanfordnlp/stanza/issues/70,,"Hi,

Is there a better way of doing batching than putting ""\n\n"" between each document ? What does happen if a document already has it in the text ?

Thanks for the answer",
1183,2019-04-10T04:53:45Z,https://github.com/stanfordnlp/stanza/issues/69,,"```
    def update(self, doc, annotators=None, properties=None):
        if properties is None:
            properties = self.default_properties
            properties.update({
```
Is there any example of the update function and the meaning of doc?

If the ""regexner.mapping"" in properties have changed, can i use this update function withou restart server?",
1184,2019-04-09T16:49:48Z,https://github.com/stanfordnlp/stanza/issues/68,,"Hi! I followed the training instructions on the [stanfordnlp website](https://stanfordnlp.github.io/stanfordnlp/training.html) and to learn how to train my own model using Stanfordnlp. After I run the bash scripts `bash scripts/run_tokenize.sh UD_Chinese-GSD --batch_size 32 --dropout 0.33`, I got a file named `zh_gsd_tokenizer.pt` in the `saved_models/tokenize` folder. I have following questions,
1. It seems that this trained `.pt` file can only be used when using `stanfordnlp.Pipeline()`. Stanfordnlp includes an official wrapper for acessing the Java Stanford CoreNLP Server, Can I use/specify this trained `.pt` file when I use Java Stanford CoreNLP Server (the `CoreNLPClient`)?
2. If I want to train my model using my own Chinese document, what should I do? Simply split the document and copy them in the `zh_gsd-ud-train/text/dev.txt` files respectively, and re-run the bash scripts?
3. With the help of `CoreNLPClient` provided in `stanfordnlp.server`, I can get `ner` of each words and use `tokensregex` to do some filtering works. Are there any method for me to do some training works to modify the `ner` of some words? For example, the `ner` of `Wuhan` is `STATE_OR_PROVINCE`, now I want it be `CITY`;  `Wuhan University` is split into `Wuhan` (`ner`=`STATE_OR_PROVINCE`) and `University` (`ner`=`O`), now I want it be `Wuhan University`
with `ner`=`UNIVERSITY`.

I hope I make it clear about my questions. Thank anyone who can give me some explanations :)",
1185,2019-04-08T13:29:40Z,https://github.com/stanfordnlp/stanza/issues/67,,"A few general questions regarding StanfordNLP, CoreNLP and the various English models.

1. I just checked the StanfordNLP. I understand that the quality of the StanfordNLP in dependency parsing is better than CoreNLP. Is this right? Is there an accuracy comparison between the two?

2. Is there a ""CoreNLP Server"" equivalent for StanfordNLP? I mean a server that can return the same results as the StanfordNLP (better results with better accuracy). Are the two projects going to converge, or is the CoreNLP 3.9.2 able to return the same results as the StanfordNLP?

3. I saw that StanfordNLP returns different results when using en_ewt, en_lines, and en_gum. Is the difference due to the fact that the models were trained on a different training data? If so, wouldn't it be beneficial to combine the training data to have a model that is trained on the aggregate data, to achieve better accuracy?

4. When running the StanfordNLP how can I specify if I want to use the GPU or not?

Thanks.

",
1186,2019-04-03T08:08:00Z,https://github.com/stanfordnlp/stanza/issues/66,,"I just installed stanfordnlp and tried to load it using import command. This resulted in the following error.

`import stanfordnlp`

`Traceback (most recent call last):

  File ""/home/Gopal/anaconda/envs/deeplearning_nlp/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  File ""<ipython-input-5-a6012641e6c5>"", line 1, in <module>
    import stanfordnlp

  File ""/home/Gopal/anaconda/envs/deeplearning_nlp/lib/python3.5/site-packages/stanfordnlp/__init__.py"", line 1, in <module>
    from stanfordnlp.pipeline.core import Pipeline

  File ""/home/Gopal/anaconda/envs/deeplearning_nlp/lib/python3.5/site-packages/stanfordnlp/pipeline/core.py"", line 9, in <module>
    from stanfordnlp.pipeline.doc import Document

  File ""/home/Gopal/anaconda/envs/deeplearning_nlp/lib/python3.5/site-packages/stanfordnlp/pipeline/doc.py"", line 175
    return f""<{self.__class__.__name__} index={self.index};words={self.words}>""
                                                                              ^
SyntaxError: invalid syntax`

Do stanfordnlp need any other dependencies. Please address this error.
",
1187,2019-04-01T11:46:14Z,https://github.com/stanfordnlp/stanza/issues/65,,"Hi,
How much memory is actually needed for the software to work. 4GB seems not to be enought whn using CPU only.
Thanks",
1188,2019-03-31T06:52:22Z,https://github.com/stanfordnlp/stanza/issues/64,,"Hi,
I cannot download the model either directly via url or when I use stanfordnlp.download('en').
could you please fix that. Some kind of connection error.
thanks
 ",
1189,2019-03-29T15:38:34Z,https://github.com/stanfordnlp/stanza/issues/63,,...,
1190,2019-03-28T20:29:37Z,https://github.com/stanfordnlp/stanza/issues/62,,"I’m trying to figure something out about the pre-trained, downloadable StanfordNLP models: I see the models correspond to UD repos, but what I’m not sure about is what they are trained on exactly. Is it just the train file, or train+dev, or train+dev+test? Any information you could give me about this would be appreciated!
",
1191,2019-03-28T01:29:56Z,https://github.com/stanfordnlp/stanza/issues/61,,"Hi,
I currently tokenizing a 2gb txt files.  I use ""/n"" to join sentences into a single string and use pipeline(only have the tokenize tool) to process the string. The memory cost grows very fast and leads to Memory Error in the end. I solved it by dividing big files into small files and using multi-process to tokenize the file(Using poll and initial pipelines for each process). However I can't find any document to help me initialize and run pipeline on a given GPU device. I want to know if I can allocate them on different device or there are someway I can accelerate the pipeline using multi-GPU.
",
1192,2019-03-26T17:55:00Z,https://github.com/stanfordnlp/stanza/issues/60,,"Hello,

Thanks for the code. I had a question about evaluation. Was the official evaluation for UAS and LAS calculating the F1 score by calculating it for the each sentence in the dataset and then averaging it by number of sentences (1) or aligning the whole file, counting the number of correct relations/system produced nodes and looking at global F1 score (2)? Thanks!",
1193,2019-03-23T23:37:03Z,https://github.com/stanfordnlp/stanza/issues/59,,"I am trying to obtain the dependencies of the following sentence: `Proficient in Dutch/French & English (both spoken and written)`.

When I check the results in the UI in localhost:9000 (I'm running ""java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000""), I obtain the following:

![image](https://user-images.githubusercontent.com/18284058/54872958-6f6f3a80-4dc4-11e9-908a-259764b74e4d.png)

However, when I do the following (clean_text is a function that removes HTML tags. In this case, it does nothing):

![image](https://user-images.githubusercontent.com/18284058/54872777-c45d8180-4dc1-11e9-8050-b29874736d54.png)

I don't get the same results. Instead I get:

![image](https://user-images.githubusercontent.com/18284058/54872851-e0adee00-4dc2-11e9-9c99-1cf413169a20.png)

Why is there a difference between results? Am I doing something wrong?

",
1194,2019-03-23T09:11:21Z,https://github.com/stanfordnlp/stanza/issues/58,,"I like the `tokenize_pretokenized` option, but I think it's a little awkward to have to convert a pretokenized text (which some other library might have returned to you as a list of tokens, or rather a list of sentences, each of which is a list of tokens) to a string first. In particular for MWEs, this is quite cumbersome, because you have to escape/replace the whitespaces between the units.

So, I'd like to suggest that one could provide a list of lists of tokens to the Pipeline.",
1195,2019-03-17T14:44:28Z,https://github.com/stanfordnlp/stanza/issues/57,,"Hi! I keep having the error constantly. I've tried installing both 3.6.8+ and 3.7.2, but still the error persists. Any ideas what might be the case? 

Traceback (most recent call last):
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 38, in load
    data = torch.load(self.filename, lambda storage, loc: storage)
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/torch/serialization.py"", line 368, in load
    return _load(f, map_location, pickle_module)
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/torch/serialization.py"", line 542, in _load
    result = unpickler.load()
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/pipeline/core.py"", line 93, in __init__
    use_gpu=self.use_gpu)
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/pipeline/depparse_processor.py"", line 14, in __init__
    self.trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/models/depparse/trainer.py"", line 33, in __init__
    self.load(pretrain, model_file)
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/models/depparse/trainer.py"", line 107, in load
    self.model = Parser(self.args, self.vocab, emb_matrix=pretrain.emb)
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 32, in emb
    self._vocab, self._emb = self.load()
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 42, in load
    return self.read_and_save()
  File ""/home/anthony/anaconda3/envs/py368/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 50, in read_and_save
    raise Exception(""Vector file is not provided."")
Exception: Vector file is not provided.

",
1196,2019-03-16T03:14:49Z,https://github.com/stanfordnlp/stanza/issues/56,,"It's the same question in this link [site](https://github.com/stanfordnlp/CoreNLP/issues/264), is there any options for now?

",
1197,2019-03-16T02:36:15Z,https://github.com/stanfordnlp/stanza/issues/55,,"```
from stanfordnlp.server import CoreNLPClient

text = '这是个最好的时代，也是一个最坏的时代！'  

properties = {
        # segment
        ""tokenize.language"": ""zh"",
        ""segment.model"": ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"",
         ...

with CoreNLPClient(properties=properties, annotators=annotators,timeout=60000, threads=5, memory='4G', be_quiet=False) as client: 
    print('---')
    print('first token of first sentence')
    token = sentence.token[0]
    print(token)
    ...
```
The output:
first token of first sentence
word: ""\350\277\231""
pos: ""PN""
value: ""\350\277\231""
originalText: ""\350\277\231""
ner: ""O""
lemma: ""\350\277\231""
beginChar: 0
endChar: 1
",
1198,2019-03-15T03:21:13Z,https://github.com/stanfordnlp/stanza/issues/54,,How to do it?,
1199,2019-03-14T09:25:40Z,https://github.com/stanfordnlp/stanza/issues/53,,"Hello!

I want to use stanfordnlp to extract some Chinese words in a text file. My code is like this.

    with open('F:\\project\\test.txt','r',encoding ='utf8') as f:
        text=f.read()
    with open CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','parse','depparse','coref'], timeout=30000, memory='16G') as client:
        pattern='[{ner:""CITY""}|{ner: ""STATE_OR_PROVINCE""}|{ner:""LOCATION""}]+'
        matches = client.tokensregex(text, pattern, to_words=True)
        print(matches)

But it raised an error:

    UnicodeEncodeError: 'latin-1' codec can't encode character '\u9648' in position 0: Body ('陈') is not valid Latin-1. Use body.encode('utf-8') if you want to send it encoded in UTF-8.

I then changed the encoding way when read the text file

    with open('F:\\project\\test.txt','r',encoding ='latin-1') as f:
         text=f.read()

The error was gone but the extracted words became
  
      {'sentences': [{'length': 0}, {'0': {'text': '±±º£', 'begin': 0, 'end': 1}, 'length': 1}, {'length': 0}, {'length': 0}, {'0': {'text': 'Îäºº', 'begin': 0, 'end': 1}, 'length': 1}, {'length': 0}, {'length': 0}, {'length': 0}, {'length': 0}, {'length': 0}, {'length': 0}, {'0': {'text': 'ÁºÉ½', 'begin': 0, 'end': 1}, 'length': 1}]}

These extracted ones are not Chinese characters. I tried some decode and encode methods on these strange characters and hope they can be transformed into Chinese form, but failed. 
    
Can you give me some suggestions?",
1200,2019-03-12T12:30:44Z,https://github.com/stanfordnlp/stanza/issues/52,,"I am trying out the [demo code](https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/corenlp.py) for using the CoreNLP server.

```
# set up the client
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','parse','depparse','coref'], timeout=30000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)
```
gives me this error:

`PermanentlyFailedException: Timed out waiting for service to come alive.`

I tried increasing the timeout limit, but no success.",
1201,2019-03-12T07:43:00Z,https://github.com/stanfordnlp/stanza/issues/51,,"Error comes up on CentOS7 on GoogleCloud, running Python 3.7.2. >> I had upgraded to Python3.7.2 specifically to fix the issue, as per the existing troubleshooting suggestions, but appearing again.

Works on my home computer under MacOS, gives the above error under CentOS7 on the server",
1202,2019-03-08T15:00:07Z,https://github.com/stanfordnlp/stanza/issues/50,,"Just to note this is really not a massive deal, in most cases the sentence tokenization is very good but I've noticed some failure cases and thought I'd mention them if you're ever looking to improve it.

Like with other sentence tokenizers it doesn't really like seeing multiple capital letter words sequentially, and preemptively splits them apart. This doesn't happen for all instances of `Burger King` but in the corpus it appears to happen definitely more than a handful of times.

```
1       Near    Near    ADP     IN      _       2       case    _       _
2       Burger  Burger  PROPN   NNP     Number=Sing     0       root    _       _

1       King    king    NOUN    NN      Number=Sing     8       nsubj   _       _
2       in      in      ADP     IN      _       4       case    _       _
3       city    city    NOUN    NN      Number=Sing     4       compound        _       _
4       centre  centre  NOUN    NN      Number=Sing     1       nmod    _       _
5       is      be      AUX     VBZ     Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   8       cop     _       _
6       the     the     DET     DT      Definite=Def|PronType=Art       8       det     _       _
7       adult   adult   ADJ     JJ      Degree=Pos      8       amod    _       _
8       establishment   establishment   NOUN    NN      Number=Sing     0       root    _       _
9       Alimentum       Alimentum       PROPN   NNP     Number=Sing     8       appos   _       _
10      .       .       PUNCT   .       _       8       punct   _       _
```

```
1       Alimentum       Alimentum       PROPN   NNP     Number=Sing     4       nsubj   _       _
2       is      be      AUX     VBZ     Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   4       cop     _       _
3       near    near    ADP     IN      _       4       case    _       _
4       Burger  Burger  PROPN   NNP     Number=Sing     0       root    _       _

1       King    king    NOUN    NN      Number=Sing     0       root    _       _
2       in      in      ADP     IN      _       5       case    _       _
3       the     the     DET     DT      Definite=Def|PronType=Art       5       det     _       _
4       city    city    NOUN    NN      Number=Sing     5       compound        _       _
5       center  center  NOUN    NN      Number=Sing     1       nmod    _       _
6       .       .       PUNCT   .       _       1       punct   _       _
```",
1203,2019-03-08T03:12:50Z,https://github.com/stanfordnlp/stanza/issues/49,,"I'm getting some very strange dependency parses on a set of question (interrogative) sentences, and I'm not sure why. As an example, I get the parse below when I use the old Stanford parser web app (http://nlp.stanford.edu:8080/parser/index.jsp) on the question ""Why are people in dense areas more likely to become infected?"" : 

```
advmod(infected-11, Why-1)
auxpass(infected-11, are-2)
nsubjpass(infected-11, people-3)
case(areas-6, in-4)
amod(areas-6, dense-5)
nmod(people-3, areas-6)
advmod(likely-8, more-7)
amod(areas-6, likely-8)
mark(become-10, to-9)
acl(people-3, become-10)
root(ROOT-0, infected-11)
```
Which makes perfect sense to me. But I get the following parse for the same sentence when I use the stanfordnlp.Pipeline() parser (constructed as is with defaults):

```
advmod(to-9, Why-1)
cop(to-9, are-2)
nsubj(to-9, people-3)
case(more-7, in-4)
amod(more-7, dense-5)
nmod(in-4, areas-6)
advmod(to-9, more-7)
root(Why-1, likely-8)
mark(infected-11, to-9)
xcomp(to-9, become-10)
xcomp(infected-11, infected-11)
punct(to-9, ?-12)
```
Obviously there's some slight variations in delivery (inclusion of punct, etc.) but that parse seems just bizarre and wrong. I'm using 0.1.2 on Python 3.7.2 if that's of use. Code I used to get the second parse was:
```
[""{}({}-{}, {}-{})"".format(t.dependency_relation, sent.words[t.governor].text, t.governor+1, t.
    ...: text, t.index) for t in sent.words]
```",
1204,2019-03-06T16:21:39Z,https://github.com/stanfordnlp/stanza/issues/48,,"I tried running the tagger and then the parser with a pretokenized file, calling both from the command line. The parser call was 

```
python -m stanfordnlp.models.parser --eval_file temp.conllu --output_file zh_gsd-pred.conllu --shorthand zh_gsd --mode predict --batch_size 5000
```

And I got the following error:

```
Traceback (most recent call last):
  File ""/home/erick/.pyenv/versions/3.7.2/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/erick/.pyenv/versions/3.7.2/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/erick/dev/stanfordnlp/stanfordnlp/models/parser.py"", line 245, in <module>
    main()
  File ""/home/erick/dev/stanfordnlp/stanfordnlp/models/parser.py"", line 97, in main
    evaluate(args)
  File ""/home/erick/dev/stanfordnlp/stanfordnlp/models/parser.py"", line 223, in evaluate
    batch = DataLoader(args['eval_file'], args['batch_size'], loaded_args, pretrain, vocab=vocab, evaluation=True)
  File ""/home/erick/dev/stanfordnlp/stanfordnlp/models/depparse/data.py"", line 43, in __init__
    data = self.preprocess(data, self.vocab, self.pretrain_vocab, args)
  File ""/home/erick/dev/stanfordnlp/stanfordnlp/models/depparse/data.py"", line 84, in preprocess
    processed_sent += [[int(w[5]) for w in sent]]
  File ""/home/erick/dev/stanfordnlp/stanfordnlp/models/depparse/data.py"", line 84, in <listcomp>
    processed_sent += [[int(w[5]) for w in sent]]
ValueError: invalid literal for int() with base 10: '_'
```

This was because the input file had a ""_"" character in the head position and the reader tried to convert to an int. It's unexpected, though, since the parser was on `predict` mode. Did I miss something?

Anyway, I could get it to work by changing line 84 in `stanfordnlp/models/depparse/data.py` to

```
processed_sent += [[int(w[5]) if w[5] != '_' else 0 for w in sent]]
```

This is related to #34 but I'm doing everything in the CLI instead of calling the API in Python code. Maybe this could help with the other issue?",
1205,2019-03-05T09:40:53Z,https://github.com/stanfordnlp/stanza/issues/47,,"As I am analyzing a large corpus, I concatenated all existing texts as suggested in the description (i.e. by concatenating all texts with two line breaks between them) and setting the parameter `tokenize_protokenized` to `true`.  This approach works with stanfordnlp 0.1.1 but since version 0.1.2 I encounter the following error:
<pre>
`Traceback (most recent call last):
  File ""some_python_file.py"", line 285, in some_function
    doc = nlp(text)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/stanfordnlp/pipeline/core.py"", line 125, in __call__
    self.process(doc)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/stanfordnlp/pipeline/core.py"", line 119, in process
    self.processors[processor_name].process(doc)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/stanfordnlp/pipeline/pos_processor.py"", line 22, in process
    preds += self.trainer.predict(b)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/stanfordnlp/models/pos/trainer.py"", line 71, in predict
    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/stanfordnlp/models/pos/model.py"", line 113, in forward
    char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/stanfordnlp/models/common/char_model.py"", line 28, in forward
    embs = pack_padded_sequence(embs, wordlens, batch_first=True)
  File ""/home/henry/anaconda3/envs/some_env/lib/python3.6/site-packages/torch/nn/utils/rnn.py"", line 148, in pack_padded_sequence
    return PackedSequence(torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first))
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
None`
</pre>
Regarding the requirements, following package versions are installed:

- numpy= 1.15.4
- protobuf= 3.6.1
- requests= 2.21.0
- pytorch= 1.0.1
- tqdm= 4.28.1

and the machine is running Ubuntu 16.04.6.
I thought that maybe there is some empty value between two consecutive blank lines leading to this issue but it was not the case. 
Thank you in advance for looking into this. 

",
1206,2019-03-01T17:40:44Z,https://github.com/stanfordnlp/stanza/issues/46,,"Is it possibile to make the `torch` package (582MB) dependency optional? Not sure if this is necessary for inference on pipelines, but it's a huge package.

```
Collecting torch (from stanfordnlp->-r requirements.txt (line 4))
  Downloading https://files.pythonhosted.org/packages/31/ca/dd2c64f8ab5e7985c4af6e62da933849293906edcdb70dac679c93477733/torch-1.0.1.post2-cp36-cp36m-manylinux1_x86_64.whl (582.5MB)

```",
1207,2019-02-28T19:02:33Z,https://github.com/stanfordnlp/stanza/issues/45,,"Hi, 

I installed the library but I can't even import the library in python.
The error message is as below:

```
File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/stanfordnlp/models/pos/model.py"", line 5, in <module>
    from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence
ImportError: cannot import name 'pack_sequence'
```

I googled this error but there's nothing. 
Could anybody please help me? ",
1208,2019-02-28T18:02:05Z,https://github.com/stanfordnlp/stanza/issues/44,,"I tried to use this library at [here](https://github.com/reza1615/NLP/blob/master/stanfordnlp.ipynb) but I couldn't find any sample codes for other functions of this library. the help pages doen't have any example.
would you please make it?
",
1209,2019-02-28T12:37:46Z,https://github.com/stanfordnlp/stanza/issues/43,,"I am just loading the model, as specified:
```
>>> import stanfordnlp
>>> stanfordnlp.download('en')   # This downloads the English models for the neural pipeline
>>> nlp = stanfordnlp.Pipeline()
```

I am not sure why I have the following error messages:

```Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/lzy/anaconda3/envs/nlp/lib/python3.6/site-packages/stanfordnlp/pipeline/core.py"", line 93, in __init__
    use_gpu=self.use_gpu)
  File ""/home/lzy/anaconda3/envs/nlp/lib/python3.6/site-packages/stanfordnlp/pipeline/depparse_processor.py"", line 14, in __init__
    self.trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
  File ""/home/lzy/anaconda3/envs/nlp/lib/python3.6/site-packages/stanfordnlp/models/depparse/trainer.py"", line 33, in __init__
    self.load(pretrain, model_file)
  File ""/home/lzy/anaconda3/envs/nlp/lib/python3.6/site-packages/stanfordnlp/models/depparse/trainer.py"", line 107, in load
    self.model = Parser(self.args, self.vocab, emb_matrix=pretrain.emb)
  File ""/home/lzy/anaconda3/envs/nlp/lib/python3.6/site-packages/stanfordnlp/models/depparse/model.py"", line 78, in __init__
    self.crit = nn.CrossEntropyLoss(ignore_index=-1, reduction='sum') # ignore padding
TypeError: __init__() got an unexpected keyword argument 'reduction'```",
1210,2019-02-27T02:45:31Z,https://github.com/stanfordnlp/stanza/issues/42,,"I was trying to load just the `depparse` processors, but I kept getting `NoneType` errors. It seams that the `tokenize` processor must be loaded in order to parse documents, otherwise the above error occurs. If this is the case, then `tokenize` should be loaded by default.",
1211,2019-02-26T14:16:33Z,https://github.com/stanfordnlp/stanza/issues/41,,"Dear All
I would like to ask you how to get access to the openie triplets from the python interface.

import stanfordnlp.server as corenlp
text=""This is a text.""
c=corenlp.CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','parse','openie'], timeout=60000)
ann = c.annotate(text,annotators=[""openie""])

how do I retrieve them from ann?

Many thanks
valery",
1212,2019-02-26T10:22:08Z,https://github.com/stanfordnlp/stanza/issues/40,,"Dear All
I would like to ask you how to get access to the constituency parse (the same as the http://corenlp.run/ webpage) from the python interface.

import stanfordnlp.server as corenlp
text=""This is a text.""
c=corenlp.CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','parse'], timeout=60000)
ann = c.annotate(text,annotators=[""parse""] )

how do I retrieve it from ann?

Many thanks
valery
",
1213,2019-02-24T11:50:37Z,https://github.com/stanfordnlp/stanza/issues/39,,"I am getting this error when loading the pretrained models from disk.
I'm working on Ubuntu 18.04 with Python 3.6.8.

Here is the full traceback:

```
Traceback (most recent call last):
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 38, in load
    data = torch.load(self.filename, lambda storage, loc: storage)
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/serialization.py"", line 368, in load
    return _load(f, map_location, pickle_module)
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/serialization.py"", line 542, in _load
    result = unpickler.load()
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/pipeline/core.py"", line 53, in __init__
    use_gpu=self.use_gpu)
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/pipeline/pos_processor.py"", line 14, in __init__
    self.trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/models/pos/trainer.py"", line 31, in __init__
    self.load(pretrain, model_file)
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/models/pos/trainer.py"", line 106, in load
    self.model = Tagger(self.args, self.vocab, emb_matrix=pretrain.emb, share_hid=self.args['share_hid'])
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 32, in emb
    self._vocab, self._emb = self.load()
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 42, in load
    return self.read_and_save()
  File ""/home/mega/anaconda3/envs/allennlp/lib/python3.6/site-packages/stanfordnlp/models/common/pretrain.py"", line 50, in read_and_save
    raise Exception(""Vector file is not provided."")
Exception: Vector file is not provided.
```


",
1214,2019-02-22T11:58:21Z,https://github.com/stanfordnlp/stanza/issues/38,,"I'm running a simple pipeline of tokenization and pos using a 600mb text file in catalan as input. Stanfordnlp automatically runs 24 processes and it's processing about 1mb every 10 minutes or so.

I tried to change pos_batch_size (from 10000 to 100000, then from 200000 to 20000, etc.) and tokenize_batch_size (32, 64, 128, then back), but it seems that I'm hitting a bottle neck, because increasing the batch_size makes the process slower. 

How can I change the number of processes to run?

My system configuration is as follows:

Architecture:          x86_64
Mode(s) opératoire(s) des processeurs :32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) par cœur : 2
Cœur(s) par socket : 12
Socket(s):             2
Nœud(s) NUMA :       2
Identifiant constructeur :GenuineIntel
Famille de processeur :6
Modèle :             79
Model name:            Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz
Révision :           1
Vitesse du processeur en MHz :2500.007
CPU max MHz:           2900,0000
CPU min MHz:           1200,0000
BogoMIPS:              4400.80
Virtualisation :      VT-x
Cache L1d :           32K
Cache L1i :           32K
Cache L2 :            256K
Cache L3 :            30720K
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts

I tried using GPU but it is slower and adjusting the batch_size did not improve the processing time.

NVIDIA-SMI 375.39                 Driver Version: 375.39                    
 GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC 
 Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. 

   0  Tesla M40           Off  | 0000:04:00.0     Off |                    0 
 N/A   40C    P0    63W / 250W |    434MiB / 11443MiB |      0%      Default 
                                                                               
 Processes:                                                       GPU Memory 
  GPU       PID  Type  Process name                               Usage      

   0    172198    C   python                                         107MiB 
   0    172526    C   python                                         107MiB 
   0    186922    C   python                                         107MiB 
   0    187828    C   python                                         107MiB

I'm using Python 3.6.8 over an anaconda environment.",
1215,2019-02-22T10:19:15Z,https://github.com/stanfordnlp/stanza/issues/37,,"```
>>> stanfordnlp.download('en')
Using the default treebank ""en_ewt"" for language ""en"".
Would you like to download the models for: en_ewt now? (Y/n)
y

Default download directory: ...
Hit enter to continue or type an alternate directory.


Downloading models for: en_ewt
Download location: .../en_ewt_models.zip
100%|████████████████████████████████████████████████████████████████████████████| 1.96G/1.96G [1:06:17<00:00, 498kB/s]

Download complete.  Models saved to: .../en_ewt_models.zip
Extracting models file for: en_ewt
Cleaning up...'rm' is not recognized as an internal or external command,
operable program or batch file.
Done.
```

That last thing looks like a Linux remnant which should not happen on Windows.
Python 3.7.2 (64 bit) on Windows 10 (64 bit)",
1216,2019-02-22T03:27:32Z,https://github.com/stanfordnlp/stanza/issues/36,,I've been able to get sentiment analysis working using the `CoreNLPClient` but it looks like the standard output doesn't include the sentiment distribution. Is that correct or am I just missing it? If I switch to the text output format I can find it. ,
1217,2019-02-20T12:00:35Z,https://github.com/stanfordnlp/stanza/issues/35,,"I have tried to run the proposed command ""nlp = stanfordnlp.Pipeline( )""
But the model is too big and it takes a long time to download.
So i tried to download it directly from the website in another folder.
It may sound stupid, but i didn't find out how to install these models as i don't want to move these models to the default folder.

**************************************************************************************************************
>>> import stanfordnlp
>>> nlp = stanfordnlp.Pipeline(model_path=""D:\python\stanfordnlp_resources\en_ewt_models\en_ewt_tokenizer.pt"")
Use device: cpu
Loading: tokenize
With settings:
{'model_path': 'C:\\Users\\84692/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
Cannot load model from C:\Users\84692/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt
**************************************************************************************************************

So is there any solutions to run the models properly?
BTW, can you introduce me about the difference between ""stanfordnlp"", ""stanford corenlp"" and ""stanford parser/Postagger etc""?
Thank you a lot!!!
",
1218,2019-02-19T16:43:01Z,https://github.com/stanfordnlp/stanza/issues/34,,"For my application, I have data in a (very large) CoNNL-U file that has already been tokenized that I would like to parse. How can I (lazily) parse this and write it back to file?

Thank you!",
1219,2019-02-19T16:38:47Z,https://github.com/stanfordnlp/stanza/pull/33,,"* Use `with` statement to ensure files and `StringIO` is closed if necessary as defined in `__exit__`
* Iterate over streams with `for` instead of using `readline`",
1220,2019-02-19T05:38:47Z,https://github.com/stanfordnlp/stanza/pull/32,,,
1221,2019-02-18T19:42:52Z,https://github.com/stanfordnlp/stanza/pull/31,,,
1222,2019-02-18T14:03:23Z,https://github.com/stanfordnlp/stanza/issues/30,,"I followed all the steps provided at https://pypi.org/project/stanfordnlp/ but I get 'vector file not provided' error. 
<img width=""569"" alt=""screen shot 2019-02-18 at 7 44 09 pm"" src=""https://user-images.githubusercontent.com/20519863/52955972-2176a980-33b6-11e9-8e38-719e78f5a453.png"">
",
1223,2019-02-15T13:58:18Z,https://github.com/stanfordnlp/stanza/issues/29,,"Hi there!

Please could you tell me if there is any coref model? Classical (non-neural, jvm-based) CoreNLP includes several ones, but I can't find it here.",
1224,2019-02-14T23:58:54Z,https://github.com/stanfordnlp/stanza/pull/28,,"I added a requirements file to make installation easier. I'm not sure about the versions, I just added what worked for me.",
1225,2019-02-14T16:55:56Z,https://github.com/stanfordnlp/stanza/issues/27,,"First, congrats on the release :tada:. It's great to have these models in such an easy to use form. I'm looking forward to using them in some tri-training experiments, and I was pleased to see @ines get the spaCy wrapper done so quickly: https://github.com/explosion/spacy-stanfordnlp

One thing that's not great in the wrapper at the moment is that we're not minibatching, so inference is super slow. In spaCy batched processing is done via the `.pipe()` method, which expects to take a generator of texts, and yield out a generator of `Doc` objects.

If I understand correctly, StanfordNLP supports batching by concatenating texts together, such as with `\n\n`. The problem is, after concatenating the texts this way, how can I separate out the documents? I want to do something like this:

```python

def pipe(self, texts, batch_size=32):
    for text_batch in itertools.islice(texts, batch_size):
        analyses = stanford_nlp('\n\n'.join(text_batch))
        doc_analyses = unbatch(analyses) # <-- How do I do this?
        for doc_analysis in doc_analyses:
            doc = make_spacy_doc(doc_analysis)
            yield doc
```

It's the `unbatch` part that I don't see an obvious solution for. The texts that are coming in could have newline sequences within them, so I can't rely on just looking for `\n\n` as a control sequence.

Your tokenizer can remove whitespace, but is it guaranteed that non-whitespace characters will be preserved? If so then maybe I can count the non-whitespace characters in the analysis output, and use that to find the document boundaries?",
1226,2019-02-13T12:40:42Z,https://github.com/stanfordnlp/stanza/issues/26,,"I am not able to download the catalan model... I tried using python:

requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))

And directly using wget:

--2019-02-13 13:33:43--  http://nlp.stanford.edu/software/conll_2018/ca_ancora_models.zip
Résolution de nlp.stanford.edu (nlp.stanford.edu)… 171.64.67.140
Connexion à nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80… connecté.
requête HTTP transmise, en attente de la réponse… Erreur de lecture (Connexion ré-initialisée par le correspondant) dans les en-têtes.
Nouvel essai.

--2019-02-13 13:34:48--  (essai :  2) 
http://nlp.stanford.edu/software/conll_2018/ca_ancora_models.zip
Connexion à nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80… connecté.
requête HTTP transmise, en attente de la réponse… Erreur de lecture (Connexion ré-initialisée par le correspondant) dans les en-têtes.
Nouvel essai.

--2019-02-13 13:35:54--  (essai :  3)  http://nlp.stanford.edu/software/conll_2018/ca_ancora_models.zip
Connexion à nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80… connecté.
requête HTTP transmise, en attente de la réponse…",
1227,2019-02-13T09:05:54Z,https://github.com/stanfordnlp/stanza/issues/25,,"Hello,

I am using a real-world dataset with the French treebank ""fr_sequoia"" model and I am facing an issue, with unusual inputs (PUNCT characters) the processing of the sentence ends before finishing.

With a fictional example:
doc = nlp(""La Joconde est un tableau de De Vinci - réalisé entre 1503 et 1506"")
doc.sentences[0].print_tokens()

Return:
<Token index=1;words=[<Word index=1;text=La;lemma=le;upos=DET;xpos=_;feats=Definite=Def|Gender=Fem|Number=Sing|PronType=Art;governor=2;dependency_relation=det>]>
<Token index=2;words=[<Word index=2;text=Joconde;lemma=Joconde;upos=PROPN;xpos=_;feats=Gender=Fem|Number=Sing;governor=5;dependency_relation=nsubj>]>
<Token index=3;words=[<Word index=3;text=est;lemma=être;upos=AUX;xpos=_;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin;governor=5;dependency_relation=cop>]>
<Token index=4;words=[<Word index=4;text=un;lemma=un;upos=DET;xpos=_;feats=Definite=Ind|Gender=Masc|Number=Sing|PronType=Art;governor=5;dependency_relation=det>]>
<Token index=5;words=[<Word index=5;text=tableau;lemma=tableau;upos=NOUN;xpos=_;feats=Gender=Masc|Number=Sing;governor=0;dependency_relation=root>]>
<Token index=6;words=[<Word index=6;text=de;lemma=de;upos=ADP;xpos=_;feats=_;governor=8;dependency_relation=case>]>
<Token index=7;words=[<Word index=7;text=De;lemma=de;upos=ADP;xpos=_;feats=_;governor=8;dependency_relation=case>]>
<Token index=8;words=[<Word index=8;text=Vinci;lemma=vinci;upos=PROPN;xpos=_;feats=Gender=Masc|Number=Sing;governor=5;dependency_relation=nmod>]>

Environment:
Python 3.6.3
Torch version: https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl

Best regards,
",
1228,2019-02-11T14:19:48Z,https://github.com/stanfordnlp/stanza/issues/24,,"I want to re-annotate my corpus, which is already tokenized. So how can I use the POS tagger only? I have tried this.

```
>>> from stanfordnlp import Pipeline
>>> nlp = Pipeline(processors='pos')
Use device: cpu
---
Loading: pos
With settings: 
{'model_path': '/Users/speedcell/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/speedcell/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
Done loading processors!
---
>>> nlp(""this is nice"")
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3267, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-4351b2659f0b>"", line 1, in <module>
    nlp(""this is nice"")
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 74, in __call__
    self.process(doc)
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 68, in process
    self.processors[processor_name].process(doc)
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/pos_processor.py"", line 19, in process
    doc, self.config['batch_size'], self.config, self.pretrain, vocab=self.vocab, evaluation=True)
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/models/pos/data.py"", line 26, in __init__
    self.conll, data = self.load_doc(doc)
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/models/pos/data.py"", line 127, in load_doc
    data = doc.conll_file.get(['word', 'upos', 'xpos', 'feats'], as_sentences=True)
AttributeError: 'NoneType' object has no attribute 'get'
>>> nlp([""this"", ""is"", ""nice""])
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3267, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-4-9346a903a804>"", line 1, in <module>
    nlp([""this"", ""is"", ""nice""])
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 74, in __call__
    self.process(doc)
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py"", line 68, in process
    self.processors[processor_name].process(doc)
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/pipeline/pos_processor.py"", line 19, in process
    doc, self.config['batch_size'], self.config, self.pretrain, vocab=self.vocab, evaluation=True)
  File ""/usr/local/lib/python3.7/site-packages/stanfordnlp/models/pos/data.py"", line 41, in __init__
    data = self.preprocess(data, self.vocab, self.pretrain_vocab, args)
UnboundLocalError: local variable 'data' referenced before assignment
```",
1229,2019-02-11T11:56:34Z,https://github.com/stanfordnlp/stanza/pull/23,,"I have added a flag `should_download` that set to `True` will automatically download models. This is useful when installing the library from scratch and deploying to containers (like Docker, etc.). The behaviors keeps the `confirm_if_exists` method. In this way it acts as `nltk` does when used like

```python
import stanfordnlp

stanfordnlp.download('en', resource_dir='/root', should_download=True, confirm_if_exists=False)
pipeline = stanfordnlp.Pipeline(models_dir='/root',  lang='en', use_gpu=False) # This sets up a default neural pipeline in English
doc = pipeline(""George Washington went to Washington"")

for sentence in doc.sentences:
    print(sentence)
    sentence.print_dependencies()
    sentence.print_tokens()
```",
1230,2019-02-08T18:59:49Z,https://github.com/stanfordnlp/stanza/issues/22,,"Hi,

Thank you so much for introducing a full fledged Python wrapper. 

My issue is:

1. When the models get downloaded, it prompts me to choose a folder path, with the default being home. My home directory in Linux has low space so I chose a different directory path. The models got downloaded there.
2. When I run stanford.Pipeline(), it still looks for the models in the home directory, not where we downloaded the models in the above step. This is because in stanfordnlp -> utils -> resources.py, the default directory is always being assigned Path.home(). Instead it has to be where we download the models. 

Hope you guys take care of this real soon.

Thanks again,
Abhinay. ",
1231,2019-02-08T04:56:08Z,https://github.com/stanfordnlp/stanza/issues/21,,"Hi there,
I am using Python 3.7.2, Anaconda, Windows 10.
Everything works perfectly for me on English. I am trying to use the Arabic model, but it doesn't seem it detect the Arabic model while it's downloaded at this path `C:\Users\mzeid\stanfordnlp_resources\ar_padt_models`

Using ""extract_pos(arabic_doc)"", gives the following results as shown in the screen shot. 
![image](https://user-images.githubusercontent.com/7589948/52459555-23717a80-2b23-11e9-87d5-d393c7b444f1.png)

How can I get this fixed and let the library use Arabic, instead of the default English?

Thanks",
1232,2019-02-07T19:42:17Z,https://github.com/stanfordnlp/stanza/issues/19,,"Hi, thanks for the package. Wondering if sentiment analysis is available using this package?",
1233,2019-02-07T09:27:02Z,https://github.com/stanfordnlp/stanza/issues/18,,"Thank you for the release! I have tried training the Slovenian tagger and the training time is MUCH longer (days) than it is in the tensorflow version (hour or two) due to a very slow model evaluation on dev data.

I think I did isolate the issue, which is the lookup of XPOS and UFeats labels in the corresponding vocabulary after prediction.

https://github.com/stanfordnlp/stanfordnlp/blob/421a8b3427178bde7c02c8c42f5621f351afa007/stanfordnlp/models/pos/trainer.py#L72-L73",
1234,2019-02-07T06:41:39Z,https://github.com/stanfordnlp/stanza/issues/17,,"The java tokenizer was technically reversible because it kept track of spaces, I could not find spaces in the python tokenizer.",
1235,2019-02-06T16:47:16Z,https://github.com/stanfordnlp/stanza/issues/16,,"First of all, thanks a lot for this new Neural NLP pipeline. I'm currently trying to train a new model for French with my data + UD datasets, but before that I would like to be able to properly reproduce the training steps. Now, I'm struggling to train the Tokenizer. Here what I'm doing:

* `mkdir -p ./extern_data/word2vec`
* `scripts/download_vectors.sh ./extern_data/word2vec/`
* `mkdir -p ./data/uddata/`
* `mkdir ./data/tokenize`
* `cd ./data/uddata`
* `git clone https://github.com/UniversalDependencies/UD_French-GSD.git`
* `cd ../..`
* Changing the `UDBASE` env variable to `./data/uddata` in `scripts/config.sh`
* `scripts/run_tokenize.sh UD_French-GSD`

And I get the following output with several errors:
```
Preparing tokenizer train data...
Traceback (most recent call last):
  File ""stanfordnlp/utils/prepare_tokenizer_data.py"", line 14, in <module>
    with open(args.plaintext_file, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: './data/uddata/UD_French-GSD/fr_gsd-ud-train.txt'
cp: cannot stat './data/uddata/UD_French-GSD/fr_gsd-ud-train.txt': No such file or directory
bash: warning: setlocale: LC_ALL: cannot change locale (fr_FR.UTF-8)
bash: warning: setlocale: LC_ALL: cannot change locale (fr_FR.UTF-8)
Preparing tokenizer dev data...
Traceback (most recent call last):
  File ""stanfordnlp/utils/prepare_tokenizer_data.py"", line 14, in <module>
    with open(args.plaintext_file, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: './data/uddata/UD_French-GSD/fr_gsd-ud-dev.txt'
cp: cannot stat './data/uddata/UD_French-GSD/fr_gsd-ud-dev.txt': No such file or directory
Traceback (most recent call last):
  File ""stanfordnlp/utils/avg_sent_len.py"", line 12, in <module>
    with open(toklabels, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: './data/tokenize/fr_gsd-ud-train.toklabels'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
TypeError: ceil() argument after * must be an iterable, not float
Running tokenizer with ...
usage: tokenizer.py [-h] [--txt_file TXT_FILE] [--label_file LABEL_FILE]
                    [--json_file JSON_FILE] [--mwt_json_file MWT_JSON_FILE]
                    [--conll_file CONLL_FILE] [--dev_txt_file DEV_TXT_FILE]
                    [--dev_label_file DEV_LABEL_FILE]
                    [--dev_json_file DEV_JSON_FILE]
                    [--dev_conll_gold DEV_CONLL_GOLD] [--lang LANG]
                    [--shorthand SHORTHAND] [--mode {train,predict}]
                    [--emb_dim EMB_DIM] [--hidden_dim HIDDEN_DIM]
                    [--conv_filters CONV_FILTERS] [--no-residual]
                    [--no-hierarchical] [--hier_invtemp HIER_INVTEMP]
                    [--input_dropout] [--conv_res CONV_RES]
                    [--rnn_layers RNN_LAYERS] [--max_grad_norm MAX_GRAD_NORM]
                    [--anneal ANNEAL] [--anneal_after ANNEAL_AFTER]
                    [--lr0 LR0] [--dropout DROPOUT]
                    [--unit_dropout UNIT_DROPOUT] [--tok_noise TOK_NOISE]
                    [--weight_decay WEIGHT_DECAY] [--max_seqlen MAX_SEQLEN]
                    [--batch_size BATCH_SIZE] [--epochs EPOCHS]
                    [--steps STEPS] [--report_steps REPORT_STEPS]
                    [--shuffle_steps SHUFFLE_STEPS] [--eval_steps EVAL_STEPS]
                    [--save_name SAVE_NAME] [--load_name LOAD_NAME]
                    [--save_dir SAVE_DIR] [--cuda CUDA] [--cpu] [--seed SEED]
tokenizer.py: error: argument --max_seqlen: expected one argument
Running tokenizer in predict mode
Directory saved_models/tokenize do not exist; creating...
Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/jplu/stanfordnlp/stanfordnlp/models/tokenizer.py"", line 182, in <module>
    main()
  File ""/home/jplu/stanfordnlp/stanfordnlp/models/tokenizer.py"", line 93, in main
    evaluate(args)
  File ""/home/jplu/stanfordnlp/stanfordnlp/models/tokenizer.py"", line 159, in evaluate
    mwt_dict = load_mwt_dict(args['mwt_json_file'])
  File ""/home/jplu/stanfordnlp/stanfordnlp/models/tokenize/utils.py"", line 10, in load_mwt_dict
    with open(filename, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: './data/tokenize/fr_gsd-ud-dev-mwt.json'
Traceback (most recent call last):
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 532, in <module>
    main()
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 500, in main
    evaluation = evaluate_wrapper(args)
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 483, in evaluate_wrapper
    system_ud = load_conllu_file(args.system_file)
  File ""stanfordnlp/utils/conll18_ud_eval.py"", line 477, in load_conllu_file
    _file = open(path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {}))
FileNotFoundError: [Errno 2] No such file or directory: './data/tokenize/fr_gsd.dev.pred.conllu'
fr_gsd
```

Apparently by checking the the `stanfordnlp/utils/prepare_tokenizer_data.py` file, indeed there is a need for `txt` files, but they do not exists in the repo of the UD dataset I'm using. Any hint on where can I find them? Or if they are not provided for free, what do they should look like? In order for me to be able to create them from the `conllu` files.

Thanks in advance :)",
1236,2019-02-06T08:13:10Z,https://github.com/stanfordnlp/stanza/issues/15,,I found StanfordNLP only support Chinese (traditional). It would be great to support Chinese (simplified) too. Is there any plan for this feature?,
1237,2019-02-05T19:33:52Z,https://github.com/stanfordnlp/stanza/issues/14,,"Hi,

Thanks for releasing the library. I am wondering would it be possible to release or link to the the language specific XPOS tagsets?  ",
1238,2019-02-05T10:55:09Z,https://github.com/stanfordnlp/stanza/issues/13,,"The [paper](https://nlp.stanford.edu/pubs/qi2018universal.pdf) does go in details for the Tokenizer when dealing with Unicode languages (like Hindi, Marathi or Japanese, etc.). Is the Byte Pair Encoding used for those languages? Recently very good implementations has been released in [C++](https://github.com/glample/fastBPE) used by Facebook LASER or Google's SentencePiece.
Could you release more details about tokenization of Unicode languages (also for unicode points greater than `\uffff` i.e. longer unicode points like `\UXXXXXXXX`, etc.). thanks.",
1239,2019-02-04T18:56:36Z,https://github.com/stanfordnlp/stanza/issues/12,,"I want to access the arabic and chinese models from the Java CoreNLP using the CoreNLP client.
The README says that to use the Java models, I should put the models in the ""distribution folder"". 

1. Is this the same folder as CORENLP_HOME? 

2. How to specify what language model to use when creating the CoreNLP client?",
1240,2019-02-02T07:18:07Z,https://github.com/stanfordnlp/stanza/issues/11,,"I'm testing the Hebrew DP and getting interesting results but I want to store them as a list or dictionary for further analysis. Getting Started shows how to print the dependencies but is there also a way to get them as an iterable that can be stored as a list, dictionary, etc? As print_dependencies is NoneType we can only use try to do so by capturing the printout but I was wondering if there' a direct way to capture the dependencies (with Python...)",
1241,2019-02-01T01:49:24Z,https://github.com/stanfordnlp/stanza/issues/10,,"Thanks for making this available! Please consider adding a feedstock recipe on conda-forge (see [here](https://conda-forge.org/#add_recipe)). I, and many of the folks I know who are/will be excited about this release, use conda and conda environments extensively in our research. It tends to be a lot nicer (and more portable) to use the conda package dependency solver for everything (cf. using `pip` inside of a conda environment).

While someone not on your maintainer team could do it, it's probably a better idea for a maintainer to do it and to add the [update process](https://conda-forge.org/#update_recipe) to your release checklist.

Thanks again for the package, and thanks in advance for considering a conda-forge recipe.",
1242,2019-02-01T00:34:27Z,https://github.com/stanfordnlp/stanza/pull/9,,,
1243,2019-01-31T21:40:42Z,https://github.com/stanfordnlp/stanza/issues/8,,"While trying out the demo code I encounter an error `Vector file is not provided`.

The download of models seems successful and the paths also seem to be correct. 

**I use `python 3.7.0 h4eca856_1 conda-forge` on MacOSX 10.11.6.**

How can I fix this?

Thanks in advance and thanks very much too for sharing Stanford NLP!

```
Downloading models for: en_ewt
Download location: /Users/peter/stanfordnlp_resources/en_ewt_models.zip
100%|██████████| 1.96G/1.96G [10:43<00:00, 4.64MB/s]

Download complete.  Models saved to: /Users/peter/stanfordnlp_resources/en_ewt_models.zip
Extracting models file for: en_ewt
Cleaning up...Done.
```

```
Use device: cpu
---
Loading: tokenize
With settings: 
{'model_path': '/Users/peter/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
---
Loading: pos
With settings: 
{'model_path': '/Users/peter/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/peter/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
Pretrained file exists but cannot be loaded from /Users/peter/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt, due to the following exception:
	[Errno 22] Invalid argument

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/models/common/pretrain.py in load(self)
     37             try:
---> 38                 data = torch.load(self.filename, lambda storage, loc: storage)
     39             except BaseException as e:

~/anaconda/envs/k4/lib/python3.7/site-packages/torch/serialization.py in load(f, map_location, pickle_module)
    366     try:
--> 367         return _load(f, map_location, pickle_module)
    368     finally:

~/anaconda/envs/k4/lib/python3.7/site-packages/torch/serialization.py in _load(f, map_location, pickle_module)
    537     unpickler.persistent_load = persistent_load
--> 538     result = unpickler.load()
    539 

OSError: [Errno 22] Invalid argument

During handling of the above exception, another exception occurred:

Exception                                 Traceback (most recent call last)
<ipython-input-15-22747d34c3ed> in <module>
----> 1 nlp = stanfordnlp.Pipeline()

~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/pipeline/core.py in __init__(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)
     51             print(curr_processor_config)
     52             self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,
---> 53                                                                                       use_gpu=self.use_gpu)
     54         print(""Done loading processors!"")
     55         print('---')

~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/pipeline/pos_processor.py in __init__(self, config, use_gpu)
     12         self.pretrain = Pretrain(config['pretrain_path'])
     13         # set up trainer
---> 14         self.trainer = Trainer(pretrain=self.pretrain, model_file=config['model_path'], use_cuda=use_gpu)
     15         self.build_final_config(config)
     16 

~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/models/pos/trainer.py in __init__(self, args, vocab, pretrain, model_file, use_cuda)
     29         if model_file is not None:
     30             # load everything from file
---> 31             self.load(pretrain, model_file)
     32         else:
     33             assert all(var is not None for var in [args, vocab, pretrain])

~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/models/pos/trainer.py in load(self, pretrain, filename)
    104         self.args = checkpoint['config']
    105         self.vocab = MultiVocab.load_state_dict(checkpoint['vocab'])
--> 106         self.model = Tagger(self.args, self.vocab, emb_matrix=pretrain.emb, share_hid=self.args['share_hid'])
    107         self.model.load_state_dict(checkpoint['model'], strict=False)
    108 

~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/models/common/pretrain.py in emb(self)
     30     def emb(self):
     31         if not hasattr(self, '_emb'):
---> 32             self._vocab, self._emb = self.load()
     33         return self._emb
     34 

~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/models/common/pretrain.py in load(self)
     40                 print(""Pretrained file exists but cannot be loaded from {}, due to the following exception:"".format(self.filename))
     41                 print(""\t{}"".format(e))
---> 42                 return self.read_and_save()
     43             return data['vocab'], data['emb']
     44         else:

~/anaconda/envs/k4/lib/python3.7/site-packages/stanfordnlp/models/common/pretrain.py in read_and_save(self)
     48         # load from pretrained filename
     49         if self.vec_filename is None:
---> 50             raise Exception(""Vector file is not provided."")
     51         print(""Reading pretrained vectors from {}..."".format(self.vec_filename))
     52         first = True

Exception: Vector file is not provided.
```",
1244,2019-01-31T16:15:24Z,https://github.com/stanfordnlp/stanza/issues/7,,"I tested stanfordnlp in a Jupyter notebook. I followed the ""Model Training and Evaluation"" Chapter of your official website.  All things run well until I typed 

`!cd stanfordnlp && ./scripts/prep_tokenize_data.sh`

for the preparation of tokenized data, and it raised like this:

`scripts/treebank_to_shorthand.sh: line 14: lang2lcode: bad array subscript
Preparing tokenizer  data...
Traceback (most recent call last):
  File ""stanfordnlp/utils/prepare_tokenizer_data.py"", line 14, in <module>
    with open(args.plaintext_file, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/u/nlp/data/dependency_treebanks/CoNLL18///_-ud-.txt'
cp: cannot stat '/u/nlp/data/dependency_treebanks/CoNLL18///_-ud-.conllu': No such file or directory
cp: cannot stat '/u/nlp/data/dependency_treebanks/CoNLL18///_-ud-.txt': No such file or directory
./scripts/prep_tokenize_data.sh: line 31: [: ==: unary operator expected`",
1245,2019-01-31T13:39:42Z,https://github.com/stanfordnlp/stanza/issues/6,,First of all congratulations! We have been waiting this for months!!! I'm currently using CoreNLP server an among the other things the NER annotators. Any plans to bring current CRF based Named Entity Recognition in CoreNLP to the new Pytorch Neural Pipeline?,
1246,2019-01-31T13:28:13Z,https://github.com/stanfordnlp/stanza/issues/5,,"`In [1]: import stanfordnlp
  File ""stanfordnlp/pipeline/doc.py"", line 175
    return f""<{self.__class__.__name__} index={self.index};words={self.words}>""
                                                                              ^
SyntaxError: invalid syntax`

getting this error on import. Tried both 'pip install stanfordnlp' and 'install from git' methods. I'm using python 2.7.15 on Mac OS Mojave.",
1247,2019-01-31T09:32:53Z,https://github.com/stanfordnlp/stanza/issues/4,,"Currently, I am getting the following error message if I change the Default download directory:

```
Default download directory: ~/eday/stanfordnlp_resources
Hit enter to continue or type an alternate directory.
~/eday/new_dir/stanfordnlp_resources

Downloading models for: en_ewt
Download location: ~/eday/new_dir/stanfordnlp_resources/en_ewt_models.zip
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.96G/1.96G [04:54<00:00, 11.5MB/s]

Download complete.  Models saved to: ~/eday/new_dir/stanfordnlp_resources/en_ewt_models.zip
Extracting models file for: en_ewt
Cleaning up...Done.
>>> nlp = stanfordnlp.Pipeline()
Use device: gpu
---
Loading: tokenize
With settings:
{'model_path': '~/eday/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}
Cannot load model from ~/eday/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt
```

It is probably because, `DEFAULT_MODEL_DIR` variable in `resources.py` does not get updated after` download_ud_model` function is called.  ",
1248,2019-01-31T08:18:11Z,https://github.com/stanfordnlp/stanza/issues/3,,"Hi,

I've got a question on the license of the models.
The UD treebanks are distributed under different licenses depending on each treebank (e.g. CC-BY-SA / CC-BY-NC-SA / some LGPL / ...) 
Under what license do you distribute the models (which basically allow mimicing the UD databases)? Is that the same license of the UD treebank?",
1249,2019-01-31T02:34:11Z,https://github.com/stanfordnlp/stanza/pull/2,,I added explanation of pipeline_demo.py arguments on README.md.,
1250,2019-01-31T01:12:13Z,https://github.com/stanfordnlp/stanza/issues/1,,"Hi, 
I have one trouble on demo script. After installing stanfordnlp from pip and cloning your repository on my local, I cannot run script demo/pipeline_demo.py to try your code. 

Raised Error is below. Could you tell me solution of this problem.

```
Downloading models for: en_ewt
Download location: /Users/hayata.yamamoto/stanfordnlp_resources/en_ewt_models.zip
Traceback (most recent call last):
  File ""demo/pipeline_demo.py"", line 32, in <module>
    stanfordnlp.download(args.lang, args.models_dir, confirm_if_exists=True)
  File ""/Users/hayata.yamamoto/anaconda3/envs/analytics/lib/python3.6/site-packages/stanfordnlp/utils/resources.py"", line 134, in download
    download_ud_model(default_treebanks[download_label], resource_dir=resource_dir, confirm_if_exists=confirm_if_exists)
  File ""/Users/hayata.yamamoto/anaconda3/envs/analytics/lib/python3.6/site-packages/stanfordnlp/utils/resources.py"", line 101, in download_ud_model
    with open(download_file_path, 'wb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/Users/hayata.yamamoto/stanfordnlp_resources/en_ewt_models.zip'
```
",
