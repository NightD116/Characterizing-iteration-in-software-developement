,created_at,html_url,name,body,published_at
0,2024-03-01T05:10:21Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.8.1,PEFT Integration (with bugfixes),"## Integrating PEFT into several different annotators

We integrate [PEFT](https://github.com/huggingface/peft) into our training pipeline for several different models.  This greatly reduces the size of models with finetuned transformers, letting us make the finetuned versions of those models the `default_accurate` model.  

The biggest gains observed are with the constituency parser and the sentiment classifier.

Previously, the `default_accurate` package used transformers where the head was trained but the transformer itself was not finetuned.

### Model improvements

- POS trained with split optimizer for transformer & non-transformer - unfortunately, did not find settings which consistently improved results https://github.com/stanfordnlp/stanza/pull/1320
- Sentiment trained with peft on the transformer: noticeably improves results for each model.  SST scores go from 68 F1 w/ charlm, to 70 F1 w/ transformer, to 74-75 F1 with finetuned or Peft finetuned transformer.  https://github.com/stanfordnlp/stanza/pull/1335
- NER also trained with peft: unfortunately, no consistent improvements to scores  https://github.com/stanfordnlp/stanza/pull/1336
- depparse includes peft: no consistent improvements yet https://github.com/stanfordnlp/stanza/pull/1337  https://github.com/stanfordnlp/stanza/pull/1344
- Dynamic oracle for top-down constituent parser scheme.  Noticeable improvement in the scores for the topdown parser  https://github.com/stanfordnlp/stanza/pull/1341
- Constituency parser uses peft: this produces significant improvements, close to the full benefit of finetuning the entire transformer when training constituencies.  Example improvement, 87.01 to 88.11 on ID_ICON dataset.  https://github.com/stanfordnlp/stanza/pull/1347
- Scripts to build a silver dataset for the constituency parser with filtering of sentences based on model agreement among the sub-models for the ensembles used.  Preliminary work indicates an improvement in the benefits of the silver trees, with more work needed to find the optimal parameters used to build the silver dataset.  https://github.com/stanfordnlp/stanza/pull/1348
- Lemmatizer ignores goeswith words when training: eliminates words which are a single word, labeled with a single lemma, but split into two words in the UD training data.  Typical example would be split email addresses in the EWT training set.  https://github.com/stanfordnlp/stanza/pull/1346  https://github.com/stanfordnlp/stanza/issues/1345

### Features

- Include SpacesAfter annotations on words in the CoNLL output of documents: https://github.com/stanfordnlp/stanza/issues/1315 https://github.com/stanfordnlp/stanza/pull/1322
- Lemmatizer operates in caseless mode if all of its training data was caseless.  Most relevant to the UD Latin treebanks.  https://github.com/stanfordnlp/stanza/pull/1331  https://github.com/stanfordnlp/stanza/issues/1330
- wandb support for coref   https://github.com/stanfordnlp/stanza/pull/1338
- Coref annotator breaks length ties using POS if available https://github.com/stanfordnlp/stanza/issues/1326  https://github.com/stanfordnlp/stanza/commit/c4c3de5803f27843a5050e10ccae71b3fd9c45e9

### Bugfixes

- Using a proxy with `download_resources_json` was broken: https://github.com/stanfordnlp/stanza/pull/1318  https://github.com/stanfordnlp/stanza/issues/1317  Thank you @ider-zh
- Fix deprecation warnings for escape sequences: https://github.com/stanfordnlp/stanza/pull/1321  https://github.com/stanfordnlp/stanza/issues/1293  Thank you @sterliakov
- Coref training rounding error  https://github.com/stanfordnlp/stanza/pull/1342
- Top-down constituency models were broken for datasets which did not use ROOT as the top level bracket... this was only DA_Arboretum in practice  https://github.com/stanfordnlp/stanza/pull/1354
- V1 of chopping up some longer texts into shorter texts for the transformers to get around length limits.  No idea if this actually produces reasonable results for words after the token limit.  https://github.com/stanfordnlp/stanza/pull/1350  https://github.com/stanfordnlp/stanza/issues/1294
- Coref prediction off-by-one error for short sentences, was falsely throwing an exception at sentence breaks: https://github.com/stanfordnlp/stanza/issues/1333 https://github.com/stanfordnlp/stanza/issues/1339 https://github.com/stanfordnlp/stanza/commit/f1fbaaad983e58dc3fcf318200d685663fb90737
- Clarify error when a language is only partially handled:  https://github.com/stanfordnlp/stanza/commit/da01644b4ba5ba477c36e5d2736012b81bcd00d4  https://github.com/stanfordnlp/stanza/issues/1310

### Additional 1.8.1 Bugfixes

- Older POS models not loaded correctly... need to use `.get()`  https://github.com/stanfordnlp/stanza/commit/13ee3d5cbc2c9174c3e0c67ca75b580e4fe683b1  https://github.com/stanfordnlp/stanza/issues/1357
- Debug logging for the Constituency retag pipeline to better support someone working on Icelandic  https://github.com/stanfordnlp/stanza/commit/6e2520f24d63fa8af4136f10137e57b195fda20a  https://github.com/stanfordnlp/stanza/issues/1356
- `device` arg in `MultilingualPipeline` would crash if `device` was passed for an individual `Pipeline`: https://github.com/stanfordnlp/stanza/commit/44058a0ec296c6da5997bfaf8911a26d425d2cec",2024-03-01T06:47:02Z
1,2024-02-25T06:52:03Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.8.0,PEFT integration,"## Integrating PEFT into several different annotators

We integrate [PEFT](https://github.com/huggingface/peft) into our training pipeline for several different models.  This greatly reduces the size of models with finetuned transformers, letting us make the finetuned versions of those models the `default_accurate` model.  

The biggest gains observed are with the constituency parser and the sentiment classifier.

Previously, the `default_accurate` package used transformers where the head was trained but the transformer itself was not finetuned.

### Model improvements

- POS trained with split optimizer for transformer & non-transformer - unfortunately, did not find settings which consistently improved results https://github.com/stanfordnlp/stanza/pull/1320
- Sentiment trained with peft on the transformer: noticeably improves results for each model.  SST scores go from 68 F1 w/ charlm, to 70 F1 w/ transformer, to 74-75 F1 with finetuned or Peft finetuned transformer.  https://github.com/stanfordnlp/stanza/pull/1335
- NER also trained with peft: unfortunately, no consistent improvements to scores  https://github.com/stanfordnlp/stanza/pull/1336
- depparse includes peft: no consistent improvements yet https://github.com/stanfordnlp/stanza/pull/1337  https://github.com/stanfordnlp/stanza/pull/1344
- Dynamic oracle for top-down constituent parser scheme.  Noticeable improvement in the scores for the topdown parser  https://github.com/stanfordnlp/stanza/pull/1341
- Constituency parser uses peft: this produces significant improvements, close to the full benefit of finetuning the entire transformer when training constituencies.  Example improvement, 87.01 to 88.11 on ID_ICON dataset.  https://github.com/stanfordnlp/stanza/pull/1347
- Scripts to build a silver dataset for the constituency parser with filtering of sentences based on model agreement among the sub-models for the ensembles used.  Preliminary work indicates an improvement in the benefits of the silver trees, with more work needed to find the optimal parameters used to build the silver dataset.  https://github.com/stanfordnlp/stanza/pull/1348
- Lemmatizer ignores goeswith words when training: eliminates words which are a single word, labeled with a single lemma, but split into two words in the UD training data.  Typical example would be split email addresses in the EWT training set.  https://github.com/stanfordnlp/stanza/pull/1346  https://github.com/stanfordnlp/stanza/issues/1345

### Features

- Include SpacesAfter annotations on words in the CoNLL output of documents: https://github.com/stanfordnlp/stanza/issues/1315 https://github.com/stanfordnlp/stanza/pull/1322
- Lemmatizer operates in caseless mode if all of its training data was caseless.  Most relevant to the UD Latin treebanks.  https://github.com/stanfordnlp/stanza/pull/1331  https://github.com/stanfordnlp/stanza/issues/1330
- wandb support for coref   https://github.com/stanfordnlp/stanza/pull/1338
- Coref annotator breaks length ties using POS if available https://github.com/stanfordnlp/stanza/issues/1326  https://github.com/stanfordnlp/stanza/commit/c4c3de5803f27843a5050e10ccae71b3fd9c45e9

### Bugfixes

- Using a proxy with `download_resources_json` was broken: https://github.com/stanfordnlp/stanza/pull/1318  https://github.com/stanfordnlp/stanza/issues/1317  Thank you @ider-zh
- Fix deprecation warnings for escape sequences: https://github.com/stanfordnlp/stanza/pull/1321  https://github.com/stanfordnlp/stanza/issues/1293  Thank you @sterliakov
- Coref training rounding error  https://github.com/stanfordnlp/stanza/pull/1342
- Top-down constituency models were broken for datasets which did not use ROOT as the top level bracket... this was only DA_Arboretum in practice  https://github.com/stanfordnlp/stanza/pull/1354
- V1 of chopping up some longer texts into shorter texts for the transformers to get around length limits.  No idea if this actually produces reasonable results for words after the token limit.  https://github.com/stanfordnlp/stanza/pull/1350  https://github.com/stanfordnlp/stanza/issues/1294
- Coref prediction off-by-one error for short sentences, was falsely throwing an exception at sentence breaks: https://github.com/stanfordnlp/stanza/issues/1333 https://github.com/stanfordnlp/stanza/issues/1339 https://github.com/stanfordnlp/stanza/commit/f1fbaaad983e58dc3fcf318200d685663fb90737
- Clarify error when a language is only partially handled:  https://github.com/stanfordnlp/stanza/commit/da01644b4ba5ba477c36e5d2736012b81bcd00d4  https://github.com/stanfordnlp/stanza/issues/1310
",2024-02-25T07:38:59Z
2,2023-12-02T06:10:46Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.7.0,v1.7.0: Neural coref!,"## Neural coref processor added!

Conjunction-Aware Word-Level Coreference Resolution
https://arxiv.org/abs/2310.06165
original implementation: https://github.com/KarelDO/wl-coref/tree/master

Updated form of Word-Level Coreference Resolution
https://aclanthology.org/2021.emnlp-main.605/
original implementation: https://github.com/vdobrovolskii/wl-coref

If you use Stanza's coref module in your work, please be sure to cite both of the above papers.

Special thanks to [vdobrovolskii](https://github.com/vdobrovolskii), who graciously agreed to allow for integration of his work into Stanza, to @KarelDO for his support of his training enhancement, and to @Jemoka for the LoRA PEFT integration, which makes the finetuning of the transformer based coref annotator much less expensive.

Currently there is one model provided, a transformer based English model trained from OntoNotes.  The provided model is currently based on Electra-Large, as that is more harmonious with the rest of our transformer architecture.  When we have LoRA integration with POS, depparse, and the other processors, we will revisit the question of which transformer is most appropriate for English.

Future work includes ZH and AR models from OntoNotes, additional language support from UD-Coref, and lower cost non-transformer models

https://github.com/stanfordnlp/stanza/pull/1309

## Interface change: English MWT

English now has an MWT model by default.  Text such as `won't` is now marked as a single **token**, split into two **words**, `will` and `not`.  Previously it was expected to be tokenized into two pieces, but the `Sentence` object containing that text would not have a single `Token` object connecting the two pieces.  See https://stanfordnlp.github.io/stanza/mwt.html and https://stanfordnlp.github.io/stanza/data_objects.html#token for more information.

Code that used to operate with `for word in sentence.words` will continue to work as before, but `for token in sentence.tokens` will now produce **one** object for MWT such as `won't`, `cannot`, `Stanza's`, etc.  

Pipeline creation will not change, as MWT is automatically (but not silently) added at `Pipeline` creation time if the language and package includes MWT.

https://github.com/stanfordnlp/stanza/pull/1314/commits/f22dceb93275fc724536b03b31c08a94617880ca  https://github.com/stanfordnlp/stanza/pull/1314/commits/27983aefe191f6abd93dd49915d2515d7c3973d1

## Other updates

- NetworkX representation of enhanced dependencies.  Allows for easier usage of Semgrex on enhanced dependencies - searching over enhanced dependencies requires CoreNLP >= 4.5.6   https://github.com/stanfordnlp/stanza/pull/1295 https://github.com/stanfordnlp/stanza/pull/1298
- Sentence ending punct tags improved for English to avoid labeling non-punct as punct (and POS is switched to using a DataLoader) https://github.com/stanfordnlp/stanza/issues/1000 https://github.com/stanfordnlp/stanza/pull/1303
- Optional rewriting of MWT after the MWT processing step - will give the user more control over fixing common errors.  Although we still encourage posting issues on github so we can fix them for everyone!  https://github.com/stanfordnlp/stanza/pull/1302
- Remove deprecated output methods such as `conll_as_string` and `doc2conll_text`.  Use `""{:C}"".format(doc)` instead  https://github.com/stanfordnlp/stanza/commit/e01650f9c56382495082a9a24fa0310414c46651
- Mixed OntoNotes and WW NER model for English is now the default.  Future versions may include CoNLL 2003 and CoNLL++ data as well.
- Sentences now have a `doc_id` field if the document they are created from has a `doc_id`.  https://github.com/stanfordnlp/stanza/pull/1314/commits/8e2201f42cb99a5a3d8358ce59501c1d88f2585e
- Optional processors added in cases where the user may not want the model we have run by default.  For example, conparse for Turkish (limited training data) or coref for English (the only available model is the transformer model)  https://github.com/stanfordnlp/stanza/pull/1314/commits/3d90d2b8a82048c5cea549b654e52544ed241833

## Updated requirements

- Support dropped for python 3.6 and 3.7.  The `peft` module used for finetuning the transformer used in the coref processor does not support those versions.
- Added `peft` as an optional dependency to transformer based installations
- Added `networkx` as a dependency for reading enhanced dependencies.  Added `toml` as a dependency for reading the coref config.
",2023-12-03T06:47:22Z
3,2023-10-06T05:01:16Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.6.1,Multiple default models and a combined EN NER model,"V1.6.1 is a patch of a bug in the Arabic POS tagger.

We also mark Python 3.11 as supported in the `setup.py` classifiers.  **This will be the last release that supports Python 3.6**

## Multiple model levels

The `package` parameter for building the `Pipeline` now has three default settings:

- `default`, the same as before, where POS, depparse, and NER use the charlm, but lemma does not
- `default-fast`, where POS and depparse are built without the charlm, making them substantially faster on CPU.  Some languages currently have non-charlm NER as well
- `default-accurate`, where the lemmatizer also uses the charlm, and other models use transformers if we have one for that language.  Suggestions for more transformers to use are welcome

Furthermore, package dictionaries are now provided for each UD dataset which encompass the default versions of models for that dataset, although we do not further break that down into `-fast` and `-accurate` versions for each UD dataset.

PR: https://github.com/stanfordnlp/stanza/pull/1287

addresses https://github.com/stanfordnlp/stanza/issues/1259 and https://github.com/stanfordnlp/stanza/issues/1284

## Multiple output heads for one NER model

The NER models now can learn multiple output layers at once.

https://github.com/stanfordnlp/stanza/pull/1289

Theoretically this could be used to save a bit of time on the encoder while tagging multiple classes at once, but the main use case was to crosstrain the OntoNotes model on the WorldWide English newswire data we collected.  The effect is that the model learns to incorporate some named entities from outside the standard OntoNotes vocabulary into the main 18 class tagset, even though the WorldWide training data is only 8 classes.

Results of running the OntoNotes model, with charlm but not transformer, on the OntoNotes and WorldWide test sets:

```
original ontonotes on worldwide:   88.71  69.29
simplify-separate                  88.24  75.75
simplify-connected                 88.32  75.47
```


We also produced combined models for nocharlm and with Electra as the input encoding.  The new English NER models are the packages `ontonotes-combined_nocharlm`, `ontonotes-combined_charlm`, and `ontonotes-combined_electra-large`.

Future plans include using multiple NER datasets for other models as well.

## Other features

- Postprocessing of proposed tokenization possible with dependency injection on the Pipeline (ty @Jemoka).  When creating a `Pipeline`, you can now provide a `callable` via the `tokenize_postprocessor` parameter, and it can adjust the candidate list of tokens to change the tokenization used by the rest of the `Pipeline` https://github.com/stanfordnlp/stanza/pull/1290

- Finetuning for transformers in the NER models: have not yet found helpful settings, though https://github.com/stanfordnlp/stanza/commit/45ef5445f44222df862ed48c1b3743dc09f3d3fd

- SE and SME should both represent Northern Sami, a weird case where UD didn't use the standard 2 letter code https://github.com/stanfordnlp/stanza/issues/1279 https://github.com/stanfordnlp/stanza/commit/88cd0df5da94664cb04453536212812dc97339bb

- charlm for PT (improves accuracy on non-transformer models): https://github.com/stanfordnlp/stanza/commit/c10763d0218ce87f8f257114a201cc608dbd7b3a

- build models  with transformers for a few additional languages: MR, AR, PT, JA https://github.com/stanfordnlp/stanza/commit/45b387531c67bafa9bc41ee4d37ba0948daa9742 https://github.com/stanfordnlp/stanza/commit/0f3761ee63c57f66630a8e94ba6276900c190a74 https://github.com/stanfordnlp/stanza/commit/c55472acbd32aa0e55d923612589d6c45dc569cc https://github.com/stanfordnlp/stanza/commit/c10763d0218ce87f8f257114a201cc608dbd7b3a


## Bugfixes

- V1.6.1 fixes a bug in the Arabic POS model which was an unfortunate side effect of the NER change to allow multiple tag sets at once: https://github.com/stanfordnlp/stanza/commit/b56f442d4d179c07411a44a342c224408eb6a6a9

- Scenegraph CoreNLP connection needed to be checked before sending messages: https://github.com/stanfordnlp/CoreNLP/issues/1346#issuecomment-1713267522 https://github.com/stanfordnlp/stanza/commit/c71bf3fdac8b782a61454c090763e8885d0e3824

- `run_ete.py` was not correctly processing the charlm, meaning the whole thing wouldn't actually run  https://github.com/stanfordnlp/stanza/commit/16f29f3dcf160f0d10a47fec501ab717adf0d4d7

- Chinese NER model was pointing to the wrong pretrain https://github.com/stanfordnlp/stanza/issues/1285 https://github.com/stanfordnlp/stanza/commit/82a02151da17630eb515792a508a967ef70a6cef",2023-10-06T05:16:12Z
4,2023-10-03T04:56:10Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.6.0,Multiple default models and a combined EN NER model,"## Multiple model levels

The `package` parameter for building the `Pipeline` now has three default settings:

- `default`, the same as before, where POS, depparse, and NER use the charlm, but lemma does not
- `default-fast`, where POS and depparse are built without the charlm, making them substantially faster on CPU.  Some languages currently have non-charlm NER as well
- `default-accurate`, where the lemmatizer also uses the charlm, and other models use transformers if we have one for that language.  Suggestions for more transformers to use are welcome

Furthermore, package dictionaries are now provided for each UD dataset which encompass the default versions of models for that dataset, although we do not further break that down into `-fast` and `-accurate` versions for each UD dataset.

PR: https://github.com/stanfordnlp/stanza/pull/1287

addresses https://github.com/stanfordnlp/stanza/issues/1259 and https://github.com/stanfordnlp/stanza/issues/1284

## Multiple output heads for one NER model

The NER models now can learn multiple output layers at once.

https://github.com/stanfordnlp/stanza/pull/1289

Theoretically this could be used to save a bit of time on the encoder while tagging multiple classes at once, but the main use case was to crosstrain the OntoNotes model on the WorldWide English newswire data we collected.  The effect is that the model learns to incorporate some named entities from outside the standard OntoNotes vocabulary into the main 18 class tagset, even though the WorldWide training data is only 8 classes.

Results of running the OntoNotes model, with charlm but not transformer, on the OntoNotes and WorldWide test sets:

```
original ontonotes on worldwide:   88.71  69.29
simplify-separate                  88.24  75.75
simplify-connected                 88.32  75.47
```


We also produced combined models for nocharlm and with Electra as the input encoding.  The new English NER models are the packages `ontonotes-combined_nocharlm`, `ontonotes-combined_charlm`, and `ontonotes-combined_electra-large`.

Future plans include using multiple NER datasets for other models as well.

## Other features

- Postprocessing of proposed tokenization possible with dependency injection on the Pipeline (ty @Jemoka).  When creating a `Pipeline`, you can now provide a `callable` via the `tokenize_postprocessor` parameter, and it can adjust the candidate list of tokens to change the tokenization used by the rest of the `Pipeline` https://github.com/stanfordnlp/stanza/pull/1290

- Finetuning for transformers in the NER models: have not yet found helpful settings, though https://github.com/stanfordnlp/stanza/commit/45ef5445f44222df862ed48c1b3743dc09f3d3fd

- SE and SME should both represent Northern Sami, a weird case where UD didn't use the standard 2 letter code https://github.com/stanfordnlp/stanza/issues/1279 https://github.com/stanfordnlp/stanza/commit/88cd0df5da94664cb04453536212812dc97339bb

- charlm for PT (improves accuracy on non-transformer models): https://github.com/stanfordnlp/stanza/commit/c10763d0218ce87f8f257114a201cc608dbd7b3a

- build models  with transformers for a few additional languages: MR, AR, PT, JA https://github.com/stanfordnlp/stanza/commit/45b387531c67bafa9bc41ee4d37ba0948daa9742 https://github.com/stanfordnlp/stanza/commit/0f3761ee63c57f66630a8e94ba6276900c190a74 https://github.com/stanfordnlp/stanza/commit/c55472acbd32aa0e55d923612589d6c45dc569cc https://github.com/stanfordnlp/stanza/commit/c10763d0218ce87f8f257114a201cc608dbd7b3a


## Bugfixes

- Scenegraph CoreNLP connection needed to be checked before sending messages: https://github.com/stanfordnlp/CoreNLP/issues/1346#issuecomment-1713267522 https://github.com/stanfordnlp/stanza/commit/c71bf3fdac8b782a61454c090763e8885d0e3824

- `run_ete.py` was not correctly processing the charlm, meaning the whole thing wouldn't actually run  https://github.com/stanfordnlp/stanza/commit/16f29f3dcf160f0d10a47fec501ab717adf0d4d7

- Chinese NER model was pointing to the wrong pretrain https://github.com/stanfordnlp/stanza/issues/1285 https://github.com/stanfordnlp/stanza/commit/82a02151da17630eb515792a508a967ef70a6cef",2023-10-03T05:11:06Z
5,2023-09-07T19:38:08Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.5.1,v1.5.1: charlm & transformer integration in depparse,"## Features

depparse can have transformer as an embedding https://github.com/stanfordnlp/stanza/pull/1282/commits/ee171cd167900fbaac16ff4b1f2fbd1a6e97de0a

Lemmatizer can remember word,pos it has seen before with a flag https://github.com/stanfordnlp/stanza/issues/1263 https://github.com/stanfordnlp/stanza/commit/a87ffd0a4f43262457cf7eecf5555a621c6dc24e

Scoring scripts for Flair and spAcy NER models (requires the appropriate packages, of course)  https://github.com/stanfordnlp/stanza/pull/1282/commits/63dc212b467cd549039392743a0be493cc9bc9d8  https://github.com/stanfordnlp/stanza/pull/1282/commits/c42aed569f9d376e71708b28b0fe5b478697ba05  https://github.com/stanfordnlp/stanza/pull/1282/commits/eab062341480e055f93787d490ff31d923a68398

SceneGraph connection for the CoreNLP client   https://github.com/stanfordnlp/stanza/pull/1282/commits/d21a95cc90443ec4737de6d7ba68a106d12fb285

Update constituency parser to reduce the learning rate on plateau.  Fiddling with the learning rates significantly improves performance https://github.com/stanfordnlp/stanza/pull/1282/commits/f753a4f35b7c2cf7e8e6b01da3a60f73493178e1

Tokenize [] based on () rules if the original dataset doesn't have [] in it  https://github.com/stanfordnlp/stanza/pull/1282/commits/063b4ba3c6ce2075655a70e54c434af4ce7ac3a9

Attempt to finetune the charlm when building models (have not found effective settings for this yet)  https://github.com/stanfordnlp/stanza/pull/1282/commits/048fdc9c9947a154d4426007301d63d920e60db0

Add the charlm to the lemmatizer - this will not be the default, since it is slower, but it is more accurate  https://github.com/stanfordnlp/stanza/pull/1282/commits/e811f52b4cf88d985e7dbbd499fe30dbf2e76d8d   https://github.com/stanfordnlp/stanza/pull/1282/commits/66add6d519deb54ca9be5fe3148023a5d7d815e4  https://github.com/stanfordnlp/stanza/pull/1282/commits/f086de2359cce16ef2718c0e6e3b5deef1345c74

## Bugfixes

Forgot to include the lemmatizer in CoreNLP 4.5.3, now in 4.5.4 https://github.com/stanfordnlp/stanza/commit/4dda14bd585893044708c70e30c1c3efec509863 https://github.com/bjascob/LemmInflect/issues/14#issuecomment-1470954013

prepare_ner_dataset was always creating an Armenian pipeline, even for non-Armenian langauges https://github.com/stanfordnlp/stanza/commit/78ff85ce7eed596ad195a3f26474065717ad63b3

Fix an empty `bulk_process` throwing an exception  https://github.com/stanfordnlp/stanza/pull/1282/commits/5e2d15d1aa59e4a1fee8bba1de60c09ba21bf53e  https://github.com/stanfordnlp/stanza/issues/1278

Unroll the recursion in the Tarjan part of the Chuliu-Edmonds algorithm - should remove stack overflow errors  https://github.com/stanfordnlp/stanza/pull/1282/commits/e0917b0967ba9752fdf489b86f9bfd19186c38eb

## Minor updates

Put NER and POS scores on one line to make it easier to grep for: https://github.com/stanfordnlp/stanza/commit/da2ae33e8ef9e48842685dfed88896b646dba8c4 https://github.com/stanfordnlp/stanza/commit/8c4cb04d38c1101318755270f3aa75c54236e3fe

Switch all pretrains to use a name which indicates their source, rather than the dataset they are used for: https://github.com/stanfordnlp/stanza/pull/1282/commits/d1c68ed01276b3cf1455d497057fbc0b82da49e5 and many others

Pipeline uses `torch.no_grad()` for a slight speed boost  https://github.com/stanfordnlp/stanza/pull/1282/commits/36ab82edfc574d46698c5352e07d2fcb0d68d3b3

Generalize save names, which eventually allows for putting `transformer`, `charlm` or `nocharlm` in the save name - this lets us distinguish different complexities of model https://github.com/stanfordnlp/stanza/pull/1282/commits/cc0845826973576d8d8ed279274e6509250c9ad5 for constituency, and others for the other models

Add the model's flags to the `--help` for the `run` scripts, such as https://github.com/stanfordnlp/stanza/pull/1282/commits/83c0901c6ca2827224e156477e42e403d330a16e  https://github.com/stanfordnlp/stanza/pull/1282/commits/7c171dd8d066c6973a8ee18a016b65f62376ea4c   https://github.com/stanfordnlp/stanza/pull/1282/commits/8e1d112bee42f2211f5153fcc89083b97e3d2600

Remove the dependency on `six`  https://github.com/stanfordnlp/stanza/pull/1282/commits/6daf97142ebc94cca7114a8cda5a20bf66f7f707  (thank you @BLKSerene )

## New Models

VLSP constituency https://github.com/stanfordnlp/stanza/commit/500435d3ec1b484b0f1152a613716565022257f2

VLSP constituency -> tagging https://github.com/stanfordnlp/stanza/commit/cb0f22d7be25af0b3b2790e3ce1b9dbc277c13a7

CTB 5.1 constituency   https://github.com/stanfordnlp/stanza/pull/1282/commits/f2ef62b96c79fcaf0b8aa70e4662d33b26dadf31

Add support for CTB 9.0, although those models are not distributed yet https://github.com/stanfordnlp/stanza/pull/1282/commits/1e3ea8a10b2e485bc7c79c6ab41d1f1dd8c2022f

Added an Indonesian charlm

Indonesian constituency from ICON treebank https://github.com/stanfordnlp/stanza/pull/1218

All languages with pretrained charlms now have an option to use that charlm for dependency parsing

French combined models out of `GSD`, `ParisStories`, `Rhapsodie`, and `Sequoia` https://github.com/stanfordnlp/stanza/pull/1282/commits/ba64d37d3bf21af34373152e92c9f01241e27d8b

UD 2.12 support  https://github.com/stanfordnlp/stanza/pull/1282/commits/4f987d2cd708ce4ca27935d347bb5b5d28a78058
",2023-09-08T22:22:33Z
6,2023-03-14T05:27:12Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.5.0,Stanza v1.5.0,"# Ssurgeon interface

Headlining this release is the initial release of Ssurgeon, a rule-based dependency graph editing tool.  Along with the existing Semgrex integration with CoreNLP, Ssurgeon allows for rewriting of dependencies such as in the UD datasets.  More information is in the GURT 2023 paper, https://aclanthology.org/2023.tlt-1.7/

In addition to this addition, there are two other CoreNLP integrations, a long list of bugfixes, a few other minor features, and a long list of constituency parser experiments which were somewhere between ""ineffective"" and ""small improvements"" and are available for people to experiment with.

## CoreNLP integration:
- Ssurgeon interface!  New interface allows for editing of dependency graphs using Semgrex patterns and Ssurgeon rules.    https://github.com/stanfordnlp/stanza/pull/1205  https://aclanthology.org/2023.tlt-1.7/
- English Morphology class (deterministic English lemmatizer) https://github.com/stanfordnlp/stanza/commit/6aed177731e883ce92057be7e78abdce3141a862
- English constituency -> dependency converter https://github.com/stanfordnlp/stanza/commit/0987794c9e960b32ed75d5804dd5c586466ae061

## Bugfixes:
- Bugfix for older versions of torch: https://github.com/stanfordnlp/stanza/commit/376d7ea76248131a96d23e236ab165e7d5a544bb
- Bugfix for training (integration with new scoring script) https://github.com/stanfordnlp/stanza/issues/1167 https://github.com/stanfordnlp/stanza/commit/9c39636c438cbeb00ab7a7e8d9caa0bcd31ccc44
- Demo was showing constituency parser along with dependency parsing, even with conparse off: https://github.com/stanfordnlp/stanza/commit/cbc13b0219281f2c27e89ccf2914e13f8aa2bb1b
- Replace absurdly long characters with UNK (thank you @khughitt) https://github.com/stanfordnlp/stanza/issues/1137 https://github.com/stanfordnlp/stanza/pull/1140 
- Package all relevant pretrains into default.zip - otherwise pretrains used by NER models which are not the default pretrain were being missed.  https://github.com/stanfordnlp/stanza/commit/435685f875766e0b9b2b9b1d4792db1c452f9722
- stanza-train NER training bugfix (wrong pretrain): https://github.com/stanfordnlp/stanza/commit/2757cb40edf7a4bf9f62e31eec4b3632ac5ebcb9
- Pass around device everywhere instead of calling cuda().  this should fix models occasionally being split over multiple devices.  would also allow for use of MPS, but the current torch implementation for MPS is buggy https://github.com/stanfordnlp/stanza/issues/1209  https://github.com/stanfordnlp/stanza/pull/1159
- Fix error in preparing tokenizer datasets (thanks @dvzubarev): https://github.com/stanfordnlp/stanza/pull/1161
- Fix unnecessary slowness in preparing tokenizer datasets (again, thanks @dvzubarev): https://github.com/stanfordnlp/stanza/pull/1162
- Fix using the correct pretrain when rebuilding POS tags for a Depparse dataset (again, thanks @dvzubarev): https://github.com/stanfordnlp/stanza/pull/1170
- When using the tregex interface to corenlp, add parse if it isn't already there (again, depparse was being confused with parse): https://github.com/stanfordnlp/stanza/commit/b118473604d50d678c2857c0f39f59ba0cd9c2a3
- Update use of emoji to match latest releases: https://github.com/stanfordnlp/stanza/issues/1195 https://github.com/stanfordnlp/stanza/commit/ea345a88f8916c2ab2cd2e6260caa7831dfe2f23

## Features:
- Mechanism for resplitting tokens into MWT https://github.com/stanfordnlp/stanza/issues/95 https://github.com/stanfordnlp/stanza/commit/8fac17f625173b2c2bf1cecf611deecb37399322
- CLI for tokenizing text into one paragraph per line, whitespace separated (useful for Glove, for example) https://github.com/stanfordnlp/stanza/commit/cfd44d17f806703b7ed6719993501366a52afbb1
- `detach().cpu()` speeds things up significantly in some cases https://github.com/stanfordnlp/stanza/commit/ccfbc56b3b312fdde1350104a0d0d5645c9c80cc
- Potentially use a constituency model as a classifier - WIP research project https://github.com/stanfordnlp/stanza/pull/1190
- add an output format `""{:C}""` for document objects which prints out documents as CoNLL: https://github.com/stanfordnlp/stanza/pull/1169
- If a constituency tree is available, include it when outputting conll format for documents: https://github.com/stanfordnlp/stanza/pull/1171
- Same with sentiment: https://github.com/stanfordnlp/stanza/commit/abb581945a70fec335dbfadd71bf8c457fa908eb
- Additional language code coverage (thank you @juanro49) https://github.com/stanfordnlp/stanza/commit/5802b10882026c4694a4d966e4200c48c5469b1b https://github.com/stanfordnlp/stanza/commit/f06bf86b566772ea6551c663835ddb9a6f5584ff https://github.com/stanfordnlp/stanza/commit/32f83fa2f2333f42925323c4ac9da059dffdf1dc https://github.com/stanfordnlp/stanza/commit/34505758c9d8de4ca70bfbe5418448ad54af088f
- Allow loading a pipeline for new languages (useful when developing a new suite of models) https://github.com/stanfordnlp/stanza/commit/e7fcd262a6c5f3f71b339fe989bcaa177fb378f1
- Script to count the work done by annotators on aws sagemaker private workforce: https://github.com/stanfordnlp/stanza/pull/1186
- Streaming interface which batch processes items in the stream: https://github.com/stanfordnlp/stanza/commit/2c9fe3dad434b271fa23c20a9cf8ccaf63991f16 https://github.com/stanfordnlp/stanza/issues/550
- Can pass a defaultdict to MultilingualPipeline, useful for specifying the processors for each language at once: https://github.com/stanfordnlp/stanza/commit/70fd2fdc94575dec79c4994ea2dc66a719768ab0 https://github.com/stanfordnlp/stanza/issues/1199
- Transformer at bottom layer of POS - currently only available in English as the `en_combined_bert` model, others to come https://github.com/stanfordnlp/stanza/pull/1132

## New models:
- Armenian NER model using an NER labeling of armtdp (thanks to @ShakeHakobyan): https://github.com/myavrum/ArmTDP-NER https://github.com/stanfordnlp/stanza/issues/1206 https://github.com/stanfordnlp/stanza/pull/1212
- Sindhi tokenization from ISRA https://github.com/stanfordnlp/stanza/pull/1117
- Sindhi NER from SiNER: https://github.com/stanfordnlp/stanza/commit/2a8ded4b0c327761b047caf433128f13b1ad14bf
- Erzya from UD 2.11 https://github.com/stanfordnlp/stanza/commit/0344ac34b5df602a49da25d58655a24a0ffcd208

## Conparser experiments:
- Transformer stack (initial implementation did not help) https://arxiv.org/abs/2010.10669 https://github.com/stanfordnlp/stanza/commit/110031e29259b34be6f958fd6d67d4774d6b084a
- TREE_LSTM constituent composition method (didn't beat MAX) https://github.com/stanfordnlp/stanza/commit/2f722c828fa1364131b670da5b925082e9aa336a
- Learned weighting between bert layers (this did help a little) https://github.com/stanfordnlp/stanza/commit/2d0c69ee449501155225efc2afb53b4ba6eeefe7
- Silver trees: train 10 models, use those models to vote on good trees, use those trees to then train new models.  helps smaller treebanks such as IT and VI, but no effect on EN  https://github.com/stanfordnlp/stanza/pull/1148
- New in_order_compound transition scheme: no improvement https://github.com/stanfordnlp/stanza/commit/f560b08902cf9f9e20656697c367500389115057
- Multistage training with madgrad or adamw: definite improvement. madgrad included as optional dependency https://github.com/stanfordnlp/stanza/commit/2706c4b100285e50f3d9a69e51ca5955e15ba41d https://github.com/stanfordnlp/stanza/commit/f500936b5ca4ba2305a028241996e5d198afd94b
- Report the scores of tags when retagging (does not affect the conparser training) https://github.com/stanfordnlp/stanza/commit/766341942962e5a5a0aa0cda3dd170ac098ac6f9
- FocalLoss on the transitions using optional dependency: didn't help https://arxiv.org/abs/1708.02002 https://github.com/stanfordnlp/stanza/commit/90a8337083f0dc057ea2a9ee794595a6b292850f
- LargeMarginSoftmax: didn't help https://github.com/tk1980/LargeMarginInSoftmax https://github.com/stanfordnlp/stanza/commit/5edd7242073720aff94f07904009ce0cad47b7ff
- Maxout layer: didn't help https://arxiv.org/abs/1302.4389 https://github.com/stanfordnlp/stanza/commit/c708ce7736ffb021f9a0065f2bedaa8b73de52ba
- Reverse parsing: not expected to help, potentially can be useful when building silver treebanks.  May also be useful as a two step parser in the future.  https://github.com/stanfordnlp/stanza/commit/4954845ba4b16240e6acf8d45d83161a0dec8d33",2023-03-14T05:09:33Z
7,2022-09-15T06:04:09Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.4.2,Stanza v1.4.2,"# Stanza v1.4.2: Minor version bump to improve (python) dependencies

- Pipeline cache in Multilingual is a single OrderedDict
https://github.com/stanfordnlp/stanza/issues/1115#issuecomment-1239759362
https://github.com/stanfordnlp/stanza/commit/ba3f64d5f571b1dc70121551364fc89d103ca1cd

- Don't require `pytest` for all installations unless needed for testing
https://github.com/stanfordnlp/stanza/issues/1120
https://github.com/stanfordnlp/stanza/commit/8c1d9d80e2e12729f60f05b81e88e113fbdd3482

- hide SiLU and Minh imports if the version of torch installed doesn't have those nonlinearities
https://github.com/stanfordnlp/stanza/issues/1120
https://github.com/stanfordnlp/stanza/commit/6a90ad4bacf923c88438da53219c48355b847ed3

- Reorder & normalize installations in setup.py
https://github.com/stanfordnlp/stanza/pull/1124
",2022-09-15T05:47:38Z
8,2022-09-14T02:52:32Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.4.1,Stanza v1.4.1,"# Stanza v1.4.1: Improvements to pos, conparse, and sentiment, jupyter visualization, and wider language coverage

## Overview

We improve the quality of the POS, constituency, and sentiment models, add an integration to displaCy, and add new models for a variety of languages.

## New NER models

- New Polish NER model based on NKJP from Karol Saputa and ryszardtuora
https://github.com/stanfordnlp/stanza/issues/1070
https://github.com/stanfordnlp/stanza/pull/1110

- Make GermEval2014 the default German NER model, including an optional Bert version
https://github.com/stanfordnlp/stanza/issues/1018
https://github.com/stanfordnlp/stanza/pull/1022

- Japanese conversion of GSD by Megagon
https://github.com/stanfordnlp/stanza/pull/1038

- Marathi NER dataset from L3Cube.  Includes a Sentiment model as well
https://github.com/stanfordnlp/stanza/pull/1043

- Thai conversion of LST20
https://github.com/stanfordnlp/stanza/commit/555fc0342decad70f36f501a7ea1e29fa0c5b317

- Kazakh conversion of KazNERD
https://github.com/stanfordnlp/stanza/pull/1091/commits/de6cd25c2e5b936bc4ad2764b7b67751d0b862d7

## Other new models

- Sentiment conversion of Tass2020 for Spanish
https://github.com/stanfordnlp/stanza/pull/1104

- VIT constituency dataset for Italian
https://github.com/stanfordnlp/stanza/pull/1091/commits/149f1440dc32d47fbabcc498cfcd316e53aca0c6
... and many subsequent updates

- Combined UD models for Hebrew
https://github.com/stanfordnlp/stanza/issues/1109
https://github.com/stanfordnlp/stanza/commit/e4fcf003feb984f535371fb91c9e380dd187fd12

- For UD models with small train dataset & larger test dataset, flip the datasets
UD_Buryat-BDT UD_Kazakh-KTB UD_Kurmanji-MG UD_Ligurian-GLT UD_Upper_Sorbian-UFAL
https://github.com/stanfordnlp/stanza/issues/1030
https://github.com/stanfordnlp/stanza/commit/9618d60d63c49ec1bfff7416e3f1ad87300c7073

- Spanish conparse model from multiple sources - AnCora, LDC-NW, LDC-DF
https://github.com/stanfordnlp/stanza/commit/47740c6252a6717f12ef1fde875cf19fa1cd67cc

## Model improvements

- Pretrained charlm integrated into POS.  Gives a small to decent gain for most languages without much additional cost
https://github.com/stanfordnlp/stanza/pull/1086

- Pretrained charlm integrated into Sentiment.  Improves English, others not so much
https://github.com/stanfordnlp/stanza/pull/1025

- LSTM, 2d maxpool as optional items in the Sentiment
from the paper `Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling`
https://github.com/stanfordnlp/stanza/pull/1098

- First learn with AdaDelta, then with another optimizer in conparse training.  Very helpful
https://github.com/stanfordnlp/stanza/commit/b1d10d3bdd892c7f68d2da7f4ba68a6ae3087f52

- Grad clipping in conparse training
https://github.com/stanfordnlp/stanza/commit/365066add019096332bcba0da4a626f68b70d303

## Pipeline interface improvements

- GPU memory savings: charlm reused between different processors in the same pipeline
https://github.com/stanfordnlp/stanza/pull/1028

- Word vectors not saved in the NER models.  Saves bandwidth & disk space
https://github.com/stanfordnlp/stanza/pull/1033

- Functions to return tagsets for NER and conparse models
https://github.com/stanfordnlp/stanza/issues/1066
https://github.com/stanfordnlp/stanza/pull/1073
https://github.com/stanfordnlp/stanza/commit/36b84db71f19e37b36119e2ec63f89d1e509acb0
https://github.com/stanfordnlp/stanza/commit/2db43c834bc8adbb8b096cf135f0fab8b8d886cb

- displaCy integration with NER and dependency trees
https://github.com/stanfordnlp/stanza/commit/20714137d81e5e63d2bcee420b22c4fd2a871306

## Bugfixes

- Fix that it takes forever to tokenize a single long token (catastrophic backtracking in regex)
TY to Sk Adnan Hassan (VT) and Zainab Aamir (Stony Brook)
https://github.com/stanfordnlp/stanza/pull/1056

- Starting a new corenlp client w/o server shouldn't wait for the server to be available
TY to Mariano Crosetti
https://github.com/stanfordnlp/stanza/issues/1059
https://github.com/stanfordnlp/stanza/pull/1061

- Read raw glove word vectors (they have no header information)
https://github.com/stanfordnlp/stanza/pull/1074

- Ensure that illegal languages are not chosen by the LangID model
https://github.com/stanfordnlp/stanza/issues/1076
https://github.com/stanfordnlp/stanza/pull/1077

- Fix cache in Multilingual pipeline
https://github.com/stanfordnlp/stanza/issues/1115
https://github.com/stanfordnlp/stanza/commit/cdf18d8b19c92b0cfbbf987e82b0080ea7b4db32

- Fix loading of previously unseen languages in Multilingual pipeline
https://github.com/stanfordnlp/stanza/issues/1101
https://github.com/stanfordnlp/stanza/commit/e551ebe60a4d818bc5ba8880dda741cc8bd1aed7

- Fix that conparse would occasionally train to NaN early in the training
https://github.com/stanfordnlp/stanza/commit/c4d785729e42ac90f298e0ef4ab487d14fa35591

## Improved training tools

- W&B integration for all models: can be activated with --wandb flag in the training scripts
https://github.com/stanfordnlp/stanza/pull/1040

- New webpages for building charlm, NER, and Sentiment
https://stanfordnlp.github.io/stanza/new_language_charlm.html
https://stanfordnlp.github.io/stanza/new_language_ner.html
https://stanfordnlp.github.io/stanza/new_language_sentiment.html

- Script to download Oscar 2019 data for charlm from HF (requires `datasets` module)
https://github.com/stanfordnlp/stanza/pull/1014

- Unify sentiment training into a Python script, replacing the old shell script
https://github.com/stanfordnlp/stanza/pull/1021
https://github.com/stanfordnlp/stanza/pull/1023

- Convert sentiment to use .json inputs.  In particular, this helps with languages with spaces in words such as Vietnamese
https://github.com/stanfordnlp/stanza/pull/1024

- Slightly faster charlm training
https://github.com/stanfordnlp/stanza/pull/1026

- Data conversion of WikiNER generalized for retraining / add new WikiNER models
https://github.com/stanfordnlp/stanza/pull/1039

- XPOS factory now determined at start of POS training.  Makes addition of new languages easier
https://github.com/stanfordnlp/stanza/pull/1082

- Checkpointing and continued training for charlm, conparse, sentiment
https://github.com/stanfordnlp/stanza/pull/1090
https://github.com/stanfordnlp/stanza/commit/0e6de808eacf14cd64622415eeaeeac2d60faab2
https://github.com/stanfordnlp/stanza/commit/e5793c9dd5359f7e8f4fe82bf318a2f8fd190f54

- Option to write the results of a NER model to a file
https://github.com/stanfordnlp/stanza/pull/1108

- Add fake dependencies to a conllu formatted dataset for better integration with evaluation tools
https://github.com/stanfordnlp/stanza/commit/6544ef3fa5e4f1b7f06dbcc5521fbf9b1264197a

- Convert an AMT NER result to Stanza .json
https://github.com/stanfordnlp/stanza/commit/cfa7e496ca7c7662478e03c5565e1b2b2c026fad

- Add a ton of language codes, including 3 letter codes for languages we generally treat as 2 letters
https://github.com/stanfordnlp/stanza/commit/5a5e9187f81bd76fcd84ad713b51215b64234986
https://github.com/stanfordnlp/stanza/commit/b32a98e477e9972737ad64deea0bda8d6cebb4ec and others
",2022-09-14T16:41:36Z
9,2022-04-23T04:36:46Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.4.0,Stanza v1.4.0,"# Stanza v1.4.0: Transformer integration to NER and conparse

## Overview

As part of the new Stanza release, we integrate transformer inputs to the NER and conparse modules.  In addition, we now support several additional languages for NER and conparse.

## Pipeline interface improvements

- Download resources.json and models into temp dirs first to avoid race conditions between multiple processors
https://github.com/stanfordnlp/stanza/issues/213
https://github.com/stanfordnlp/stanza/pull/1001

- Download models for Pipelines automatically, without needing to call `stanza.download(...)`
https://github.com/stanfordnlp/stanza/issues/486
https://github.com/stanfordnlp/stanza/pull/943

- Add ability to turn off downloads
https://github.com/stanfordnlp/stanza/commit/68455d895986357a2c1f496e52c4e59ee0feb165

- Add a new interface where both processors and package can be set
https://github.com/stanfordnlp/stanza/issues/917
https://github.com/stanfordnlp/stanza/commit/f37042924b7665bbaf006b02dcbf8904d71931a1

- When using pretokenized tokens, get character offsets from text if available
https://github.com/stanfordnlp/stanza/issues/967
https://github.com/stanfordnlp/stanza/pull/975

- If Bert or other transformers are used, cache the models rather than loading multiple times
https://github.com/stanfordnlp/stanza/pull/980

- Allow for disabling processors on individual runs of a pipeline
https://github.com/stanfordnlp/stanza/issues/945
https://github.com/stanfordnlp/stanza/pull/947

## Other general improvements

- Add # text and # sent_id to conll output
https://github.com/stanfordnlp/stanza/discussions/918
https://github.com/stanfordnlp/stanza/pull/983
https://github.com/stanfordnlp/stanza/pull/995

- Add ner to the token conll output
https://github.com/stanfordnlp/stanza/discussions/993
https://github.com/stanfordnlp/stanza/pull/996

- Fix missing Slovak MWT model
https://github.com/stanfordnlp/stanza/issues/971
https://github.com/stanfordnlp/stanza/commit/5aa19ec2e6bc610576bc12d226d6f247a21dbd75

- Upgrades to EN, IT, and Indonesian models
https://github.com/stanfordnlp/stanza/issues/1003
https://github.com/stanfordnlp/stanza/pull/1008
IT improvements with the help of @attardi and @msimi

- Fix improper tokenization of Chinese text with leading whitespace
https://github.com/stanfordnlp/stanza/issues/920
https://github.com/stanfordnlp/stanza/pull/924

- Check if a CoreNLP model exists before downloading it (thank you @interNULL)
https://github.com/stanfordnlp/stanza/pull/965

- Convert the run_charlm script to python
https://github.com/stanfordnlp/stanza/pull/942

- Typing and lint fixes (thank you @asears)
https://github.com/stanfordnlp/stanza/pull/833
https://github.com/stanfordnlp/stanza/pull/856

- stanza-train examples now compatible with the python training scripts
https://github.com/stanfordnlp/stanza/issues/896

## NER features

- Bert integration (not by default, thank you @vythaihn)
https://github.com/stanfordnlp/stanza/pull/976

- Swedish model (thank you @EmilStenstrom)
https://github.com/stanfordnlp/stanza/issues/912
https://github.com/stanfordnlp/stanza/pull/857

- Persian model
https://github.com/stanfordnlp/stanza/issues/797

- Danish model
https://github.com/stanfordnlp/stanza/pull/910/commits/3783cc494ee8c6b6d062c4d652a428a04a4ee839

- Norwegian model (both NB and NN)
https://github.com/stanfordnlp/stanza/pull/910/commits/31fa23e5239b10edca8ecea46e2114f9cc7b031d

- Use updated Ukrainian data (thank you @gawy)
https://github.com/stanfordnlp/stanza/pull/873

- Myanmar model (thank you UCSY)
https://github.com/stanfordnlp/stanza/pull/845

- Training improvements for finetuning models
https://github.com/stanfordnlp/stanza/issues/788
https://github.com/stanfordnlp/stanza/pull/791

- Fix inconsistencies in B/S/I/E tags
https://github.com/stanfordnlp/stanza/issues/928#issuecomment-1027987531
https://github.com/stanfordnlp/stanza/pull/961

- Add an option for multiple NER models at the same time, merging the results together
https://github.com/stanfordnlp/stanza/issues/928
https://github.com/stanfordnlp/stanza/pull/955

## Constituency parser

- Dynamic oracle (improves accuracy a bit)
https://github.com/stanfordnlp/stanza/pull/866

- Missing tags now okay in the parser
https://github.com/stanfordnlp/stanza/issues/862
https://github.com/stanfordnlp/stanza/commit/04dbf4f65e417a2ceb19897ab62c4cf293187c0b

- bugfix of () not being escaped when output in a tree
https://github.com/stanfordnlp/stanza/commit/eaf134ca699aca158dc6e706878037a20bc8cbd4

- charlm integration by default
https://github.com/stanfordnlp/stanza/pull/799

- Bert integration (not the default model) (thank you @vythaihn and @hungbui0411)
https://github.com/stanfordnlp/stanza/commit/05a0b04ee6dd701ca1c7c60197be62d4c13b17b6
https://github.com/stanfordnlp/stanza/commit/0bbe8d10f895560a2bf16f542d2e3586d5d45b7e

- Preemptive bugfix for incompatible devices from @zhaochaocs
https://github.com/stanfordnlp/stanza/issues/989
https://github.com/stanfordnlp/stanza/pull/1002

- New models:
DA, based on [Arboretum](http://catalog.elra.info/en-us/repository/browse/ELRA-W0084/)
 IT, based on the [Turin treebank](http://www.di.unito.it/~tutreeb/treebanks.html)
JA, based on [ALT](https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/)
PT, based on [Cintil](https://catalogue.elra.info/en-us/repository/browse/ELRA-W0055/)
TR, based on [Starlang](https://www.researchgate.net/publication/344829282_Creating_A_Syntactically_Felicitous_Constituency_Treebank_For_Turkish)
ZH, based on CTB7

",2022-04-23T06:01:01Z
10,2021-10-05T02:15:34Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.3.0,Stanza 1.3.0: LangID and Constituency Parser,"# Overview

Stanza 1.3.0 introduces a language id model, a constituency parser, a dictionary in the tokenizer, and some additional features and bugfixes.

## New features

- **Langid model and multilingual pipeline**
Based on ""A reproduction of Apple's bi-directional LSTM models for language identification in short strings."" by Toftrup et al 2021
(https://github.com/stanfordnlp/stanza/commit/154b0e8e59d3276744ae0c8ea56dc226f777fba8)

- **Constituency parser**
Based on ""In-Order Transition-based Constituent Parsing"" by Jiangming Liu and Yue Zhang.  Currently an `en_wsj` model available, with more to come.
(https://github.com/stanfordnlp/stanza/commit/90318023432d584c62986123ef414a1fa93683ca)

- **Evalb interface to CoreNLP**
Useful for evaluating the parser - requires CoreNLP 4.3.0 or later

- **Dictonary tokenizer feature**
Noticeably improved performance for ZH, VI, TH
(https://github.com/stanfordnlp/stanza/pull/776)

## Bugfixes / Reliability

- **HuggingFace integration**
No more git issues complaining about unavailable models!  (Hopefully)
(https://github.com/stanfordnlp/stanza/commit/f7af5049568f81a716106fee5403d339ca246f38)

- **Sentiment processor crashes on certain inputs**
(issue https://github.com/stanfordnlp/stanza/issues/804, fixed by https://github.com/stanfordnlp/stanza/commit/e232f67f3850a32a1b4f3a99e9eb4f5c5580c019)
",2021-10-06T06:28:19Z
11,2021-08-09T23:11:56Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.2.3,Stanza v1.2.3: Two new NER models and some minor bugfixes,"# Overview

In anticipation of a larger release with some new features, we make a small update to fix some existing bugs and add two more NER models.

## Bugfixes

- **Sentiment models would crash on no text** (issue https://github.com/stanfordnlp/stanza/issues/769, fixed by https://github.com/stanfordnlp/stanza/pull/781/commits/47889e3043c27f9c5abd9913016929f1857de7bf)

- **Java processes as a context were not properly closed** (https://github.com/stanfordnlp/stanza/pull/781/commits/a39d2ff6801a23aa73add1f710d809a9c0a793b1)

## Interface improvements

- **Downloading tokenize now downloads mwt for languages which require it** (issue https://github.com/stanfordnlp/stanza/issues/774, fixed by https://github.com/stanfordnlp/stanza/pull/777, from davidrft)

- **NER model can finetune and save to/from different filenames** (https://github.com/stanfordnlp/stanza/pull/781/commits/0714a0134f0af6ef486b49ce934f894536e31d43)

- **NER model now displays a confusion matrix at the end of training** (https://github.com/stanfordnlp/stanza/pull/781/commits/9bbd3f712f97cb2702a0852e1c353d4d54b4b33b)

## NER models

- **Afrikaans, trained in NCHLT** (https://github.com/stanfordnlp/stanza/pull/781/commits/6f1f04b6d674691cf9932d780da436063ebd3381)

- **Italian, trained on a model from FBK** (https://github.com/stanfordnlp/stanza/pull/781/commits/d9a361fd7f13105b68569fddeab650ea9bd04b7f)

",2021-08-09T23:12:42Z
12,2021-07-13T04:49:50Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.2.2,Stanza v1.2.2,"# Overview

A regression in NER results occurred in 1.2.1 when fixing a bug in VI models based around spaces.

## Bugfixes

- **Fix Sentiment not loading correctly on Windows because of pickling issue** (https://github.com/stanfordnlp/stanza/pull/742) (thanks to @BramVanroy)

- **Fix NER bulk process not filling out data structures as expected** (https://github.com/stanfordnlp/stanza/issues/721) (https://github.com/stanfordnlp/stanza/pull/722)

- **Fix NER space issue causing a performance regression** (https://github.com/stanfordnlp/stanza/issues/739) (https://github.com/stanfordnlp/stanza/pull/732)

## Interface improvements

- **Add an NER run script** (https://github.com/stanfordnlp/stanza/pull/738)
",2021-07-15T18:49:01Z
13,2021-06-08T22:32:56Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.2.1,Stanza v1.2.1,"# Overview

All models other than NER and Sentiment were retrained with the new UD 2.8 release.  All of the updates include the data augmentation fixes applied in 1.2.0, along with new augmentations tokenization issues and end-of-sentence issues.  This release also features various enhancements, bug fixes, and performance improvements, along with 4 new NER models.

## Model improvements

- **Add Bulgarian, Finnish, Hungarian, Vietnamese NER models**
  - The Bulgarian model is trained on BSNLP 2019 data.
  - The Finnish model is trained on the Turku NER data.
  - The Hungarian model is trained on a combination of the NYTK dataset and earlier business and criminal NER datasets.
  - The Vietnamese model is trained on the VLSP 2018 data.
  - Furthermore, the script for preparing the lang-uk NER data has been integrated (https://github.com/stanfordnlp/stanza/commit/c1f0bee1074997d9376adaec45dc00f813d00b38)

- **Use new word vectors for Armenian, including better coverage for the new Western Armenian dataset**(https://github.com/stanfordnlp/stanza/pull/718/commits/d9e8301addc93450dc880b06cb665ad10d869242)

- **Add copy mechanism in the seq2seq model**.  This fixes some unusual Spanish multi-word token expansion errors and potentially improves lemmatization performance. (https://github.com/stanfordnlp/stanza/pull/692 https://github.com/stanfordnlp/stanza/issues/684)

- **Fix Spanish POS and depparse mishandling a leading `¿` missing** (https://github.com/stanfordnlp/stanza/pull/699 https://github.com/stanfordnlp/stanza/issues/698)

- **Fix tokenization breaking when a newline splits a Chinese token**(https://github.com/stanfordnlp/stanza/pull/632 https://github.com/stanfordnlp/stanza/issues/531)

- **Fix tokenization of parentheses in Chinese**(https://github.com/stanfordnlp/stanza/commit/452d842ed596bb7807e604eeb2295fd4742b7e89)

- **Fix various issues with characters not present in UD training data** such as ellipses characters or unicode apostrophe
(https://github.com/stanfordnlp/stanza/pull/719/commits/db0555253f0a68c76cf50209387dd2ff37794197 https://github.com/stanfordnlp/stanza/pull/719/commits/f01a1420755e3e0d9f4d7c2895e0261e581f7413 https://github.com/stanfordnlp/stanza/pull/719/commits/85898c50f14daed75b96eed9cd6e9d6f86e2d197)

- **Fix a variety of issues with Vietnamese tokenization** - remove language specific model improvement which got roughly 1% F1 but caused numerous hard-to-track issues (https://github.com/stanfordnlp/stanza/pull/719/commits/3ccb132e03ce28a9061ec17d2c0ae84cc2000548)

- **Fix spaces in the Vietnamese words not being found in the embedding used for POS and depparse**(https://github.com/stanfordnlp/stanza/pull/719/commits/197212269bc33b66759855a5addb99d1f465e4f4)

- **Include UD_English-GUMReddit in the GUM models**(https://github.com/stanfordnlp/stanza/pull/719/commits/9e6367cb9bdd635d579fd8d389cb4d5fa121c413)

- **Add Pronouns & PUD to the mixed English models** (various data improvements made this more appealing)(https://github.com/stanfordnlp/stanza/pull/719/commits/f74bef7b2ed171bf9c027ae4dfd3a10272040a46)

## Interface enhancements

- **Add ability to pass a Document to the pipeline in pretokenized mode**(https://github.com/stanfordnlp/stanza/commit/f88cd8c2f84aedeaec34a11b4bc27573657a66e2 https://github.com/stanfordnlp/stanza/issues/696)

- **Track comments when reading and writing conll files** (https://github.com/stanfordnlp/stanza/pull/676 originally from @danielhers in https://github.com/stanfordnlp/stanza/pull/155)

- **Add a proxy parameter for downloads to pass through to the requests module** (https://github.com/stanfordnlp/stanza/pull/638)

- **add sent_idx to tokens** (https://github.com/stanfordnlp/stanza/commit/ee6135c538e24ff37d08b86f34668ccb223c49e1)

## Bugfixes

- **Fix Windows encoding issues when reading conll documents** from @yanirmr (b40379eaf229e7ffc7580def57ee1fad46080261 https://github.com/stanfordnlp/stanza/pull/695)

- **Fix tokenization breaking when second batch is exactly eval_length**(https://github.com/stanfordnlp/stanza/commit/726368644d7b1019825f915fabcfe1e4528e068e https://github.com/stanfordnlp/stanza/issues/634 https://github.com/stanfordnlp/stanza/issues/631)

## Efficiency improvements

- **Bulk process for tokenization** - greatly speeds up the use case of many small docs (https://github.com/stanfordnlp/stanza/pull/719/commits/5d2d39ec822c65cb5f60d547357ad8b821683e3c)

- **Optimize MWT usage in pipeline & fix MWT bulk_process** (https://github.com/stanfordnlp/stanza/pull/642 https://github.com/stanfordnlp/stanza/pull/643 https://github.com/stanfordnlp/stanza/pull/644)

## CoreNLP integration

- **Add a UD Enhancer tool which interfaces with CoreNLP's generic enhancer** (https://github.com/stanfordnlp/stanza/pull/675)

- **Add an interface to CoreNLP tokensregex using stanza tokenization** (https://github.com/stanfordnlp/stanza/pull/659)

",2021-06-17T17:12:31Z
14,2021-01-27T23:00:47Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.2.0,Stanza v1.2.0,"# Overview

All models other than NER and Sentiment were retrained with the new UD 2.7 release.  Quite a few of them have data augmentation fixes for problems which arise in common use rather than when running an evaluation task. This release also features various enhancements, bug fixes, and performance improvements.

## New features and enhancements

- **Models trained on combined datasets in English and Italian** The default models for English are now a combination of EWT and GUM.  The default models for Italian now combine ISDT, VIT, Twittiro, PosTWITA, and a custom dataset including MWT tokens.

- **NER Transfer Learning** Allows users to fine-tune all or part of the parameters of trained NER models on a new dataset for transfer learning (#351, thanks to @gawy for the contribution)

- **Multi-document support** The Stanza `Pipeline` now supports multi-`Document` input! To process multiple documents without having to worry about document boundaries, simply pass a list of Stanza `Document` objects into the `Pipeline`. (https://github.com/stanfordnlp/stanza/issues/70 https://github.com/stanfordnlp/stanza/pull/577)

- **Added API links from token to sentence** It's easier to access Stanza data objects from related ones. To access the sentence object  a token or a word, simply use `token.sent` or `word.sent`. (https://github.com/stanfordnlp/stanza/issues/533 https://github.com/stanfordnlp/stanza/pull/554)

- **New external tokenizer for Thai with PyThaiNLP** Try it out with, for example, `stanza.Pipeline(lang='th', processors={'tokenize': 'pythainlp'}, package=None)`. (https://github.com/stanfordnlp/stanza/pull/567)

- **Faster tokenization** We have improved how the data pipeline works internally to reduce redundant data wrangling, and significantly sped up the tokenization of long texts. If you have a really long line of text, you could experience up to 10x speedup or more without changing anything. (#522)

- **Added a method for getting all the supported languages from the resources file** Wondering what languages Stanza supports and want to determine it programmatically? Wonder no more! Try `stanza.resources.common.list_available_languages()`. (https://github.com/stanfordnlp/stanza/issues/511 https://github.com/stanfordnlp/stanza/commit/fa52f8562f20ab56807b35ba204d6f9ca60b47ab)

- **Load mwt automagically if a model needs it** Multi-word token expansion is one of the most common things to miss from your `Pipeline` instantiation, and remembering to include it is a pain -- until now. (https://github.com/stanfordnlp/stanza/pull/516 https://github.com/stanfordnlp/stanza/issues/515  and many others)

- **Vietnamese sentiment model based on VSFC** This is now part of the default language package for Vietnamese that you get from `stanza.download(""vi"")`. Enjoy!

- **More informative errors for missing models** Stanza now throws more helpful exceptions with informative exception messages when you are missing models (https://github.com/stanfordnlp/stanza/pull/437 https://github.com/stanfordnlp/stanza/issues/430 ... https://github.com/stanfordnlp/stanza/issues/324 https://github.com/stanfordnlp/stanza/pull/438 ... https://github.com/stanfordnlp/stanza/issues/529 https://github.com/stanfordnlp/stanza/commit/953966539c955951d01e3d6b4561fab02a1f546c ... https://github.com/stanfordnlp/stanza/issues/575 https://github.com/stanfordnlp/stanza/pull/578)

## Bugfixes

- **Fixed NER documentation for German** to correctly point to the GermEval 2014 model for download. (https://github.com/stanfordnlp/stanza/commit/4ee9f12be5911bb600d2f162b1684cb4686c391e https://github.com/stanfordnlp/stanza/issues/559)

- **External tokenization library integration respects `no_ssplit`** so you can enjoy using them without messing up your preferred sentence segmentation just like Stanza tokenizers. (https://github.com/stanfordnlp/stanza/issues/523 https://github.com/stanfordnlp/stanza/pull/556)

- **Telugu lemmatizer and tokenizer improvements** Telugu models set to use identity lemmatizer by default, and the tokenizer is retrained to separate sentence final punctuation (https://github.com/stanfordnlp/stanza/issues/524 https://github.com/stanfordnlp/stanza/commit/ba0aec30e6e691155bc0226e4cdbb829cb3489df)

- **Spanish model would not tokenize foo,bar** Now fixed (https://github.com/stanfordnlp/stanza/issues/528 https://github.com/stanfordnlp/stanza/commit/123d5029303a04185c5574b76fbed27cb992cadd)

- **Arabic model would not tokenize `asdf .`** Now fixed (https://github.com/stanfordnlp/stanza/issues/545 https://github.com/stanfordnlp/stanza/commit/03b7ceacf73870b2a15b46479677f4914ea48745)

- **Various tokenization models would split URLs and/or emails** Now URLs and emails are robustly handled with regexes. (https://github.com/stanfordnlp/stanza/issues/539 https://github.com/stanfordnlp/stanza/pull/588)

- **Various parser and pos models would deterministically label ""punct"" for the final word** Resolved via data augmentation (https://github.com/stanfordnlp/stanza/issues/471 https://github.com/stanfordnlp/stanza/issues/488 https://github.com/stanfordnlp/stanza/pull/491)

- **Norwegian tokenizers retrained to separate final punct** The fix is an upstream data fix (https://github.com/stanfordnlp/stanza/issues/305 https://github.com/UniversalDependencies/UD_Norwegian-Bokmaal/pull/5)

- **Bugfix for conll eval** Fix the error in data conversion from python object of Document to CoNLL format. (https://github.com/stanfordnlp/stanza/pull/484 https://github.com/stanfordnlp/stanza/issues/483, thanks @m0re4u )

- **Less randomness in sentiment results** Fixes prediction fluctuation in sentiment prediction. (https://github.com/stanfordnlp/stanza/issues/458 https://github.com/stanfordnlp/stanza/commit/274474c3b0e4155ab6e221146ac347ca433f81a6)

- **Bugfix which should make it easier to use in jupyter / colab** This fixes the issue where jupyter notebooks (and by extension colab) don't like it when you use sys.stderr as the stderr of popen (https://github.com/stanfordnlp/stanza/pull/434 https://github.com/stanfordnlp/stanza/issues/431)

- **Misc fixes for training, concurrency, and edge cases in basic Pipeline usage**
  - **Fix for mwt training** (https://github.com/stanfordnlp/stanza/pull/446)
  - **Fix for race condition in seq2seq models** (https://github.com/stanfordnlp/stanza/pull/463 https://github.com/stanfordnlp/stanza/issues/462)
  - **Fix for race condition in CRF** (https://github.com/stanfordnlp/stanza/pull/566 https://github.com/stanfordnlp/stanza/issues/561)
  - **Fix for empty text in pipeline** (https://github.com/stanfordnlp/stanza/pull/475 https://github.com/stanfordnlp/stanza/issues/474)
  - **Fix for resources not freed when downloading** (https://github.com/stanfordnlp/stanza/issues/502 https://github.com/stanfordnlp/stanza/pull/503)
  - **Fix for vietnamese pipeline not working** (https://github.com/stanfordnlp/stanza/issues/531 https://github.com/stanfordnlp/stanza/pull/535)

## BREAKING CHANGES

- **Renamed `stanza.models.tokenize` -> `stanza.models.tokenization`** https://github.com/stanfordnlp/stanza/pull/452  This stops the tokenize directory shadowing a built in library",2021-01-29T20:05:25Z
15,2020-08-13T06:05:37Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.1.1,Stanza v1.1.1,"## Overview

This release features support for extending the capability of the Stanza pipeline with customized processors, a new sentiment analysis tool, improvements to the `CoreNLPClient` functionality, new models for a few languages (including Thai, which is supported for the first time in Stanza), new biomedical and clinical English packages, alternative servers for downloading resource files, and various improvements and bugfixes.

## New Features and Enhancements

- **New Sentiment Analysis Models for English, German, Chinese**: The default Stanza pipelines for English, German and Chinese now include sentiment analysis models. The released models are based on a convolutional neural network architecture, and predict three-way sentiment labels (negative/neutral/positive). For more information and details on the datasets used to train these models and their performance, please visit the Stanza website.

- **New Biomedical and Clinical English Model Packages**: Stanza now features syntactic analysis and named entity recognition functionality for English biomedical literature text and clinical notes. These newly introduced packages include: 2 individual biomedical syntactic analysis pipelines, 8 biomedical NER models, 1 clinical syntactic pipelines and 2 clinical NER models. For detailed information on how to download and use these pipelines, please visit [Stanza's biomedical models page](https://stanfordnlp.github.io/stanza/biomed.html).

- **Support for Adding User Customized Processors via Python Decorators**: Stanza now supports adding customized processors or processor variants (i.e., an alternative of existing processors) into existing pipelines. The name and implementation of the added customized processors or processor variants can be specified via `@register_processor` or `@register_processor_variant` decorators. See Stanza website for more information and examples (see [custom Processors](https://stanfordnlp.github.io/stanza/pipeline.html#building-your-own-processors-and-using-them-in-the-neural-pipeline) and [Processor variants](https://stanfordnlp.github.io/stanza/pipeline.html#processor-variants)). (PR https://github.com/stanfordnlp/stanza/pull/322)

- **Support for Editable Properties For Data Objects**: We have made it easier to extend the functionality of the Stanza neural pipeline by adding new annotations to Stanza's data objects (e.g., `Document`, `Sentence`, `Token`, etc). Aside from the annotation they already support, additional annotation can be easily attached through `data_object.add_property()`. See [our documentation](https://stanfordnlp.github.io/stanza/data_objects.html#adding-new-properties-to-stanza-data-objects) for more information and examples. (PR https://github.com/stanfordnlp/stanza/pull/323)

- **Support for Automated CoreNLP Installation and CoreNLP Model Download**: CoreNLP can now be easily downloaded in Stanza with `stanza.install_corenlp(dir='path/to/corenlp/installation')`; CoreNLP models can now be downloaded with `stanza.download_corenlp_models(model='english', version='4.1.0', dir='path/to/corenlp/installation')`. For more details please see the Stanza website. (PR https://github.com/stanfordnlp/stanza/pull/363)

- **Japanese Pipeline Supports SudachiPy as External Tokenizer**: You can now use the [SudachiPy library](https://github.com/WorksApplications/SudachiPy) as tokenizer in a Stanza Japanese pipeline. Turn on this when building a pipeline with `nlp = stanza.Pipeline('ja', processors={'tokenize': 'sudachipy'}`. Note that this will require a separate installation of the SudachiPy library via pip. (PR https://github.com/stanfordnlp/stanza/pull/365)

- **New Alternative Server for Stable Download of Resource Files**: Users in certain areas of the world that do not have stable access to GitHub servers can now download models from alternative Stanford server by specifying a new `resources_url` argument. For example, `stanza.download(lang='en', resources_url='stanford')` will now download the resource file and English pipeline from Stanford servers. (Issue https://github.com/stanfordnlp/stanza/issues/331, PR https://github.com/stanfordnlp/stanza/pull/356)

- **`CoreNLPClient` Supports New Multiprocessing-friendly Mechanism to Start the CoreNLP Server**: The `CoreNLPClient` now supports a new `Enum` values with better semantics for its `start_server` argument for finer-grained control over how the server is launched, including a new option called `StartServer.TRY_START` that launches the CoreNLP Server if one isn't running already, but doesn't fail if one has already been launched. This option makes it easier for `CoreNLPClient` to be used in a multiprocessing environment. Boolean values are still supported for backward compatibility, but we recommend `StartServer.FORCE_START` and `StartSerer.DONT_START` for better readability. (PR https://github.com/stanfordnlp/stanza/pull/302)

- **New Semgrex Interface in CoreNLP Client for Dependency Parses of Arbitrary Languages**: Stanford CoreNLP has a module which allows searches over dependency graphs using a regex-like language.  Previously, this was only usable for languages which CoreNLP already supported dependency trees.  This release expands it to dependency graphs for any language.  (Issue https://github.com/stanfordnlp/stanza/issues/399, PR https://github.com/stanfordnlp/stanza/pull/392)

- **New Tokenizer for Thai Language**: The available UD data for Thai is quite small.  The authors of [pythainlp](https://github.com/PyThaiNLP/pythainlp) helped provide us two tokenization datasets, Orchid and Inter-BEST.  Future work will include POS, NER, and Sentiment.  (Issue https://github.com/stanfordnlp/stanza/issues/148)

- **Support for Serialization of Document Objects**: Now you can serialize and deserialize the entire document by running `serialized_string = doc.to_serialized()` and `doc = Document.from_serialized(serialized_string)`. The serialized string can be decoded into Python objects by running `objs = pickle.loads(serialized_string)`. (Issue https://github.com/stanfordnlp/stanza/issues/361, PR https://github.com/stanfordnlp/stanza/pull/366)

- **Improved Tokenization Speed**: Previously, the tokenizer was the slowest member of the neural pipeline, several times slower than any of the other processors.  This release brings it in line with the others.  The speedup is from improving the text processing before the data is passed to the GPU.  (Relevant commits: https://github.com/stanfordnlp/stanza/commit/546ed13563c3530b414d64b5a815c0919ab0513a, https://github.com/stanfordnlp/stanza/commit/8e2076c6a0bc8890a54d9ed6931817b1536ae33c, https://github.com/stanfordnlp/stanza/commit/7f5be823a587c6d1bec63d47cd22818c838901e7, etc.)

- **User provided Ukrainian NER model**: We now have a [model](https://github.com/gawy/stanza-lang-uk/releases/tag/v0.9) built from the [lang-uk NER dataset](https://github.com/lang-uk/ner-uk), provided by a user for redistribution.

## Breaking Interface Changes

- **Token.id is Tuple and Word.id is Integer**: The `id` attribute for a token will now return a tuple of integers to represent the indices of the token (or a singleton tuple in the case of a single-word token), and the `id` for a word will now return an integer to represent the word index. Previously both attributes are encoded as strings and requires manual conversion for downstream processing. This change brings more convenient handling of these attributes. (Issue: https://github.com/stanfordnlp/stanza/issues/211, PR: https://github.com/stanfordnlp/stanza/pull/357)

- **Changed Default Pipeline Packages for Several Languages for Improved Robustness**: Languages that have changed default packages include: Polish (default is now `PDB` model, from previous `LFG`, https://github.com/stanfordnlp/stanza/issues/220), Korean (default is now `GSD`, from previous `Kaist`, https://github.com/stanfordnlp/stanza/issues/276), Lithuanian (default is now `ALKSNIS`, from previous `HSE`, https://github.com/stanfordnlp/stanza/issues/415).

- **CoreNLP 4.1.0 is required**: `CoreNLPClient` requires CoreNLP 4.1.0 or a later version. The client expects recent modifications that were made to the CoreNLP server.

- **Properties Cache removed from CoreNLP client**: The properties_cache has been removed from `CoreNLPClient` and the `CoreNLPClient's` `annotate()` method no longer has a `properties_key` argument. Python dictionaries with custom request properties should be directly supplied to `annotate()` via the `properties` argument.

## Bugfixes and Other Improvements

- **Fixed Logging Behavior**: This is mainly for fixing the issue that Stanza will override the global logging setting in Python and influence downstream logging behaviors. (Issue https://github.com/stanfordnlp/stanza/issues/278, PR https://github.com/stanfordnlp/stanza/pull/290)

- **Compatibility Fix for PyTorch v1.6.0**: We've updated several processors to adapt to new API changes in PyTorch v1.6.0. (Issues https://github.com/stanfordnlp/stanza/issues/412 https://github.com/stanfordnlp/stanza/issues/417, PR https://github.com/stanfordnlp/stanza/pull/406)

- **Improved Batching for Long Sentences in Dependency Parser**: This is mainly for fixing an issue where long sentences will cause an out of GPU memory issue in the dependency parser. (Issue https://github.com/stanfordnlp/stanza/issues/387)

- **Improved neural tokenizer robustness to whitespaces**: the neural tokenizer is now more robust to the presence of multiple consecutive whitespace characters (PR https://github.com/stanfordnlp/stanza/pull/380)

- **Resolved properties issue when switching languages with requests to CoreNLP server**: An issue with default properties has been resolved. Users can now switch between CoreNLP supported languages with and get expected properties for each language by default.",2020-08-13T06:26:20Z
16,2020-04-27T06:22:36Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.0.1,Stanza v1.0.1,"## Overview

This is a maintenance release of Stanza. It features new support for `jieba` as Chinese tokenizer, faster lemmatizer implementation, improved compatibility with CoreNLP v4.0.0, and many more!

## Enhancements

- **Supporting `jieba` library as Chinese tokenizer**. The Stanza (simplified and traditional) Chinese pipelines now support using the `jieba` Chinese word segmentation library as tokenizer. Turn on this feature in a pipeline with: `nlp = stanza.Pipeline('zh', processors={'tokenize': 'jieba'}`, or by specifying argument `tokenize_with_jieba=True`.

- **Setting resource directory with environment variable**. You can now override the default model location `$HOME/stanza_resources` by setting an environmental variable `STANZA_RESOURCES_DIR` (https://github.com/stanfordnlp/stanza/issues/227). The new directory will then be used to store and look up model files. Thanks to @dhpollack for implementing this feature.

- **Faster lemmatizer implementation.** The lemmatizer implementation has been improved to be about 3x faster on CPU and 5x faster on GPU (https://github.com/stanfordnlp/stanza/issues/249). Thanks to @mahdiman for identifying the original issue.

- **Improved compatibility with CoreNLP 4.0.0**. The client is now fully compatible with the latest [v4.0.0 release of the CoreNLP package](https://stanfordnlp.github.io/CoreNLP/).

## Bugfixes

- **Correct character offsets in NER outputs from pre-tokenized text**. We fixed an issue where the NER outputs from pre-tokenized text may be off-by-one (https://github.com/stanfordnlp/stanza/issues/229). Thanks to @RyanElliott10 for reporting the issue.

- **Correct Vietnamese tokenization on sentences beginning with punctuation**. We fixed an issue where the Vietnamese tokenizer may throw an `AssertionError` on sentences that begin with a punctuation (https://github.com/stanfordnlp/stanza/issues/217). Thanks to @aryamccarthy for reporting this issue.

- **Correct pytorch version requirement**. Stanza is now asking for `pytorch>=1.3.0` to avoid a runtime error raised by pytorch ((https://github.com/stanfordnlp/stanza/issues/231)). Thanks to @Vodkazy for reporting this.

## Known Model Issues & Solutions

- **Default Korean Kaist tokenizer failing on punctuation.** The default Korean Kaist model is reported to have issues with separating punctuations during tokenization (https://github.com/stanfordnlp/stanza/issues/276). Switching to the Korean `GSD` model may solve this issue.

- **Default Polish LFG POS tagger incorrectly labeling last word in sentence as `PUNCT`**. The default Polish model trained on the `LFG` treebank may incorrectly tag the last word in a sentence as `PUNCT` (https://github.com/stanfordnlp/stanza/issues/220). This issue may be solved by switching to the Polish `PDB` model.

",2020-04-27T07:21:58Z
17,2020-04-27T06:22:36Z,https://github.com/stanfordnlp/stanza/releases/tag/1.0.1,Stanza v1.0.1,"## Overview

This is a maintenance release of Stanza. It features new support for `jieba` as Chinese tokenizer, faster lemmatizer implementation, improved compatibility with CoreNLP v4.0.0, and many more!

## Enhancements

- **Supporting `jieba` library as Chinese tokenizer**. The Stanza (simplified and traditional) Chinese pipelines now support using the `jieba` Chinese word segmentation library as tokenizer. Turn on this feature in a pipeline with: `nlp = stanza.Pipeline('zh', processors={'tokenize': 'jieba'}`, or by specifying argument `tokenize_with_jieba=True`.

- **Setting resource directory with environment variable**. You can now override the default model location `$HOME/stanza_resources` by setting an environmental variable `STANZA_RESOURCES_DIR` (https://github.com/stanfordnlp/stanza/issues/227). The new directory will then be used to store and look up model files. Thanks to @dhpollack for implementing this feature.

- **Faster lemmatizer implementation.** The lemmatizer implementation has been improved to be about 3x faster on CPU and 5x faster on GPU (https://github.com/stanfordnlp/stanza/issues/249). Thanks to @mahdiman for identifying the original issue.

- **Improved compatibility with CoreNLP 4.0.0**. The client is now fully compatible with the latest [v4.0.0 release of the CoreNLP package](https://stanfordnlp.github.io/CoreNLP/).

## Bugfixes

- **Correct character offsets in NER outputs from pre-tokenized text**. We fixed an issue where the NER outputs from pre-tokenized text may be off-by-one (https://github.com/stanfordnlp/stanza/issues/229). Thanks to @RyanElliott10 for reporting the issue.

- **Correct Vietnamese tokenization on sentences beginning with punctuation**. We fixed an issue where the Vietnamese tokenizer may throw an `AssertionError` on sentences that begin with a punctuation (https://github.com/stanfordnlp/stanza/issues/217). Thanks to @aryamccarthy for reporting this issue.

- **Correct pytorch version requirement**. Stanza is now asking for `pytorch>=1.3.0` to avoid a runtime error raised by pytorch ((https://github.com/stanfordnlp/stanza/issues/231)). Thanks to @Vodkazy for reporting this.

## Known Model Issues & Solutions

- **Default Korean Kaist tokenizer failing on punctuation.** The default Korean Kaist model is reported to have issues with separating punctuations during tokenization (https://github.com/stanfordnlp/stanza/issues/276). Switching to the Korean `GSD` model may solve this issue.

- **Default Polish LFG POS tagger incorrectly labeling last word in sentence as `PUNCT`**. The default Polish model trained on the `LFG` treebank may incorrectly tag the last word in a sentence as `PUNCT` (https://github.com/stanfordnlp/stanza/issues/220). This issue may be solved by switching to the Polish `PDB` model.

",2020-04-27T21:43:25Z
18,2020-03-17T02:05:43Z,https://github.com/stanfordnlp/stanza/releases/tag/v1.0.0,Stanza v1.0.0,"## Overview
This is the first major release of Stanza (previously known as [StanfordNLP](https://github.com/stanfordnlp/stanfordnlp/)), a software package to process many human languages. The main features of this release are
* **Multi-lingual named entity recognition support**. Stanza supports named entity recognition in 8 languages (and 12 datasets): Arabic, Chinese, Dutch, English, French, German, Russian, and Spanish. The most comprehensive NER models in each language is now part of the default model download of that model, along with other models trained on the largest dataset available.
* **Accurate neural network models**. Stanza features highly accurate data-driven neural network models for a wide collection of natural language processing tasks, including tokenization, sentence segmentation, part-of-speech tagging, morphological feature tagging, dependency parsing, and named entity recognition.
* **State-of-the-art pretrained models freely available**. Stanza features a few hundred pretrained models for 60+ languages, all freely availble and easily downloadable from native Python code. Most of these models achieve state-of-the-art (or competitive) performance on these tasks.
* **Expanded language support**. Stanza now supports more than 60 human languages, representing a wide-range of language families.
* **Easy-to-use native Python interface**. We've improved the usability of the interface to maximize transparency. Now intermediate processing results are more easily viewed and accessed as native Python objects.
* **Anaconda support**. Stanza now officially supports installation from Anaconda. You can install Stanza through Stanford NLP Group's Anaconda channel `conda install -c stanfordnlp stanza`.
* **Improved documentation**. We have improved [our documentation](https://stanfordnlp.github.io/stanza/) to include a comprehensive coverage of the basic and advanced functionalities supported by Stanza.
* **Improved CoreNLP support in Python**. We have improved the robustness and efficiency of the `CoreNLPClient` to access the Java CoreNLP software from Python code. It is also forward compatible with the next major release of CoreNLP.
## Enhancements and Bugfixes
This release also contains many enhancements and bugfixes:
* [Enhancement] Improved lemmatization support with proper conditioning on POS tags (#143). Thanks to @nljubesi for the report!
* [Enhancement] Get the text corresponding to sentences in the document. Access it through `sentence.text`. (#80)
* [Enhancement] Improved logging. Stanza now uses Python's `logging` for all procedual logging, which can be controlled globally either through `logging_level` or a `verbose` shortcut. See [this page](https://stanfordnlp.github.io/stanza/pipeline.html#pipeline) for more information. (#81)
* [Enhancement] Allow the user to use the Stanza tokenizer with their own sentence split, which might be useful for applications like machine translation. Simply set `tokenize_no_ssplit` to `True` at pipeline instantiation. (#108)
* [Enhancement] Support running the dependency parser only given tokenized,  sentence segmented, and POS/morphological feature tagged data. Simply set `depparse_pretagged` to `True` at pipeline instantiation. (#141) Thanks @mrapacz for the contribution!
* [Enhancement] Added spaCy as an option for tokenizing (and sentence segmenting) English text for efficiency. See this [documentation page](https://stanfordnlp.github.io/stanza/tokenize.html#use-spacy-for-fast-tokenization-and-sentence-segmentation) for a quick example.
* [Enhancement] Add character offsets to tokens, sentences, and spans.
* [Bugfix] Correctly decide whether to load pretrained embedding files given training flags. (#120)
* [Bugfix] Google proto buffers reporting errors for long input when using the `CoreNLPClient`. (#154)
* [Bugfix] Remove deprecation warnings from newer versions of PyTorch. (#162)
## Breaking Changes
Note that if your code was developed on a previous version of the package, there are potentially many breaking changes in this release. The most notable changes are in the `Document` objects, which contain all the annotations for the raw text or document fed into the Stanza pipeline. The underlying implementation of `Document` and all related data objects have broken away from using the CoNLL-U format as its internal representation for more flexibility and efficiency accessing their attributes, although it is still compatible with CoNLL-U to maintain ease of conversion between the two. Moreover, many properties have been renamed for clarity and sometimes aliased for ease of access. Please see our documentation page about these [data objects](https://stanfordnlp.github.io/stanza/data_objects.html) for more information.",2020-03-17T17:13:53Z
19,2019-05-16T07:35:44Z,https://github.com/stanfordnlp/stanza/releases/tag/v0.2.0,v0.2.0,"This release features major improvements on memory efficiency and speed of the neural network pipeline in stanfordnlp and various bugfixes. These features include:

- The downloadable pretrained neural network models are now substantially smaller in size (due to the use of smaller pretrained vocabularies) with comparable performance. Notably, the default English model is now ~9x smaller in size, German ~11x, French ~6x and  Chinese ~4x. As a result, memory efficiency of the neural pipelines for most languages are substantially improved.

- Substantial speedup of the neural lemmatizer via reduced neural sequence-to-sequence operations.

- The neural network pipeline can now take in a Python list of strings representing pre-tokenized text. (https://github.com/stanfordnlp/stanfordnlp/issues/58)

- A requirements checking framework is now added in the neural pipeline, ensuring the proper processors are specified for a given pipeline configuration. The pipeline will now raise an exception when a requirement is not satisfied. (https://github.com/stanfordnlp/stanfordnlp/issues/42)

- Bugfix related to alignment between tokens and words post the multi-word expansion processor. (https://github.com/stanfordnlp/stanfordnlp/issues/71)

- More options are added for customizing the Stanford CoreNLP server at start time, including specifying properties for the default pipeline, and setting all server options such as username/password. For more details on different options, please checkout the [client documentation page](https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html#customizing-properties-for-server-start-and-requests).

- `CoreNLPClient` instance can now be created with CoreNLP default language properties as:
```python
client = CoreNLPClient(properties='chinese')
```

- Alternatively, a properties file can now be used during the creation of a `CoreNLPClient`:
```python
client = CoreNLPClient(properties='/path/to/corenlp.props')
```

- All specified CoreNLP annotators are now preloaded by default when a `CoreNLPClient` instance is created. (https://github.com/stanfordnlp/stanfordnlp/issues/56)
",2019-05-16T17:31:43Z
20,2019-02-26T22:39:54Z,https://github.com/stanfordnlp/stanza/releases/tag/v0.1.2,v0.1.2,"This is a maintenance release of stanfordnlp. This release features:

* Allowing the tokenizer to treat the incoming document as pretokenized with space separated words in newline separated sentences. Set `tokenize_pretokenized` to `True` when building the pipeline to skip the neural tokenizer, and run all downstream components with your own tokenized text. (#24, #34)
* Speedup in the POS/Feats tagger in evaluation (up to 2 orders of magnitude). (#18)
* Various minor fixes and documentation improvements

We would also like to thank the following community members for their contribution:
Code improvements: @lwolfsonkin 
Documentation improvements: @0xflotus 
And thanks to everyone that raised issues and helped improve stanfordnlp!",2019-02-26T23:34:52Z
21,2019-01-30T00:55:00Z,https://github.com/stanfordnlp/stanza/releases/tag/v0.1.0,v0.1.0,"The initial release of StanfordNLP. StanfordNLP is the combination of the software package used by the Stanford team in the CoNLL 2018 Shared Task on Universal Dependency Parsing, and the group’s official Python interface to the [Stanford CoreNLP software](https://stanfordnlp.github.io/CoreNLP). This package is built with highly accurate neural network components that enables efficient training and evaluation with your own annotated data. The modules are built on top of [PyTorch](https://pytorch.org/) (v1.0.0).

StanfordNLP features:

- Native Python implementation requiring minimal efforts to set up;
- Full neural network pipeline for robust text analytics, including tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging and dependency parsing;
- Pretrained neural models supporting 53 (human) languages featured in 73 treebanks;
- A stable, officially maintained Python interface to CoreNLP.",2019-01-30T22:10:15Z
