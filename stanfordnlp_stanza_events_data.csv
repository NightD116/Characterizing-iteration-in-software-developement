,type,payload
0,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1263', 'id': 1806139068, 'node_id': 'I_kwDOBj_0V85rp368', 'number': 1263, 'title': 'Possibility of speeding up Stanza lemmmatizer by excluding reduntant words', 'user': {'login': 'mrgransky', 'id': 11946010, 'node_id': 'MDQ6VXNlcjExOTQ2MDEw', 'avatar_url': 'https://avatars.githubusercontent.com/u/11946010?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mrgransky', 'html_url': 'https://github.com/mrgransky', 'followers_url': 'https://api.github.com/users/mrgransky/followers', 'following_url': 'https://api.github.com/users/mrgransky/following{/other_user}', 'gists_url': 'https://api.github.com/users/mrgransky/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mrgransky/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mrgransky/subscriptions', 'organizations_url': 'https://api.github.com/users/mrgransky/orgs', 'repos_url': 'https://api.github.com/users/mrgransky/repos', 'events_url': 'https://api.github.com/users/mrgransky/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mrgransky/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059612, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTI=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 10, 'created_at': '2023-07-15T14:39:41Z', 'updated_at': '2024-04-05T13:45:11Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""**Given**:\r\n\r\nI have a small sample document with limited number of words as follows:\r\n\r\n    d ='''\r\n    I go to school by the school bus everyday with all of my best friends. \r\n    There are several students who also take the buses to school. Buses are quite cheap in my city.\r\n    The city which I live in has an enormous number of brilliant schools with smart students.\r\n    We have a nice math teacher in my school whose name is Jane Doe.\r\n    She also teaches several other topics in our school, including physics, chemistry and sometimes literature as a substitute teacher.\r\n    Other classes don't appreciate her efforts as much as my class. She must be nominated as the best school's teacher.\r\n    My school is located far from my apartment. This is why, I am taking the bus to school everyday.\r\n    '''\r\n**Goal**:\r\n\r\nConsidering my real-world large document with more words (`4000 ~ 8000 words`), I would like to speed up my Stanza lemmatizer by *probably* excluding lemmatizing repeated words, *e.g.*, words which has occurred more than once.\r\nI do not intend to use `set()` method to obtain the unique lemmas in my result list, rather I intend to ignore lemmatizing words which have already been lemmatized.\r\n\r\nFor instance, for the given sample raw document `d`, there are several redundant words which could be ignored in the process:\r\n\r\n    Word                 Lemma\r\n    --------------------------------------------------\r\n    school               school\r\n    school               school <<<<< Redundant\r\n    bus                  bus\r\n    everyday             everyday\r\n    friends              friend\r\n    students             student\r\n    buses                bus\r\n    school               school <<<<< Redundant\r\n    Buses                bus <<<<< Redundant\r\n    cheap                cheap\r\n    city                 city\r\n    city                 city <<<<< Redundant\r\n    live                 live\r\n    enormous             enormous\r\n    number               number\r\n    brilliant            brilliant\r\n    schools              school\r\n    smart                smart\r\n    students             student\r\n    nice                 nice\r\n    math                 math\r\n    teacher              teacher\r\n    school               school <<<<< Redundant\r\n    Jane                 jane\r\n    Doe                  doe\r\n    teaches              teach\r\n    topics               topic\r\n    school               school <<<<< Redundant\r\n    including            include\r\n    physics              physics\r\n    chemistry            chemistry\r\n    literature           literature\r\n    substitute           substitute\r\n    teacher              teacher <<<<< Redundant\r\n    classes              class\r\n    appreciate           appreciate\r\n    efforts              effort\r\n    class                class\r\n    nominated            nominate\r\n    school               school <<<<< Redundant\r\n    teacher              teacher <<<<< Redundant\r\n    school               school <<<<< Redundant\r\n    located              locate\r\n    apartment            apartment\r\n    bus                  bus <<<<< Redundant\r\n    school               school <<<<< Redundant\r\n    everyday             everyday <<<<< Redundant\r\n\r\nMy [*inefficient*] solution:\r\n\r\n    import stanza\r\n    import nltk\r\n    nltk_modules = ['punkt', 'averaged_perceptron_tagger', 'stopwords', 'wordnet', 'omw-1.4',]\r\n    nltk.download(nltk_modules, quiet=True, raise_on_error=True,)\r\n    STOPWORDS = nltk.corpus.stopwords.words(nltk.corpus.stopwords.fileids())\r\n    \r\n    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos', tokenize_no_ssplit=True,download_method=DownloadMethod.REUSE_RESOURCES)\r\n    doc = nlp(d)\r\n    %timeit -n 10000 [ wlm.lower() for _, s in enumerate(doc.sentences) for _, w in enumerate(s.words) if (wlm:=w.lemma) and len(wlm)>2 and wlm not in STOPWORDS]\r\n    10.5 ms ± 112 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\r\n\r\nMy [*alternative*] solution, a little faster but still **NOT** efficient for (`4000 ~ 8000 words`):\r\n\r\n    def get_lm():\r\n      words_list = list()\r\n      lemmas_list = list()\r\n      for _, vsnt in enumerate(doc.sentences):\r\n        for _, vw in enumerate(vsnt.words):\r\n          wlm = vw.lemma.lower()\r\n          wtxt = vw.text.lower()\r\n          if wtxt in words_list and wlm in lemmas_list:\r\n            lemmas_list.append(wlm)\r\n          elif ( wtxt not in words_list and wlm and len(wlm) > 2 and wlm not in STOPWORDS ):\r\n            lemmas_list.append(wlm)\r\n          words_list.append(wtxt)\r\n      return lemmas_list\r\n    %timeit -n 10000 get_lm()\r\n    7.85 ms ± 66.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\r\n\r\nMy ideal result for this sample document, from either solution, should contain even repeated lemmas, as follows:\r\n\r\n    lm = [ wlm.lower() for _, s in enumerate(doc.sentences) for _, w in enumerate(s.words) if (wlm:=w.lemma) and len(wlm)>2 and wlm not in STOPWORDS] # solution 1\r\n    # lm = get_lm() # solution 2\r\n    print(len(lm), lm)\r\n    47 ['school', 'school', 'bus', 'everyday', 'friend', 'student', 'bus', 'school', 'bus', 'cheap', 'city', 'city', 'live', 'enormous', 'number', 'brilliant', 'school', 'smart', 'student', 'nice', 'math', 'teacher', 'school', 'jane', 'doe', 'teach', 'topic', 'school', 'include', 'physics', 'chemistry', 'literature', 'substitute', 'teacher', 'class', 'appreciate', 'effort', 'class', 'nominate', 'school', 'teacher', 'school', 'locate', 'apartment', 'bus', 'school', 'everyday']\r\n\r\nIs there any better or more efficient approach for this problem when considering large corpus or documents?\r\n\r\nCheers,"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2039845046', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1263#issuecomment-2039845046', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263', 'id': 2039845046, 'node_id': 'IC_kwDOBj_0V855lZC2', 'user': {'login': 'mrgransky', 'id': 11946010, 'node_id': 'MDQ6VXNlcjExOTQ2MDEw', 'avatar_url': 'https://avatars.githubusercontent.com/u/11946010?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mrgransky', 'html_url': 'https://github.com/mrgransky', 'followers_url': 'https://api.github.com/users/mrgransky/followers', 'following_url': 'https://api.github.com/users/mrgransky/following{/other_user}', 'gists_url': 'https://api.github.com/users/mrgransky/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mrgransky/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mrgransky/subscriptions', 'organizations_url': 'https://api.github.com/users/mrgransky/orgs', 'repos_url': 'https://api.github.com/users/mrgransky/repos', 'events_url': 'https://api.github.com/users/mrgransky/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mrgransky/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-05T13:45:10Z', 'updated_at': '2024-04-05T13:45:10Z', 'author_association': 'NONE', 'body': 'Thanks dude, I can confirm that by adding an option: `""lemma_store_results"":True` to my lang_configs dictionary, it works slightly faster. Here is the modified version:\r\n\r\n```\r\nlang_configs = {\r\n\t""en"": {""processors"":""tokenize,lemma,pos"", ""package"":\'lines\',""tokenize_no_ssplit"":True, ""lemma_store_results"":True},\r\n\t""fi"": {""processors"":""tokenize,lemma,pos,mwt"", ""package"":\'ftb\',""tokenize_no_ssplit"":True, ""lemma_store_results"":True}, # FTB \r\n}\r\n# and the rest of the code...\r\n```\r\n\r\nIt would be nice to have this update on the stanza webpage which discusses the instructions for **Lemmatization** [here](https://stanfordnlp.github.io/stanza/lemma.html#options), it is currently missing.\r\n\r\nCheers,', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2039845046/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
1,DeleteEvent,"{'ref': 'english_mwt', 'ref_type': 'branch', 'pusher_type': 'user'}"
2,PushEvent,"{'repository_id': 104854615, 'push_id': 17855670672, 'size': 9, 'distinct_size': 9, 'ref': 'refs/heads/dev', 'head': 'c27b5b9b7aaccb33b293eae6d465f103b938aa2e', 'before': 'ec66b10b1835806e048fced3a2dce1650db6ccaa', 'commits': [{'sha': '5591bb2ef00f262e3ca4d1016dbe90def6604782', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Pass around the original text when decoding MWT.  This will make it possible to use the original text directly, even in the face of unknown characters (which currently trips up the copy mechanism)', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/5591bb2ef00f262e3ca4d1016dbe90def6604782'}, {'sha': '2b1a7ed66e33f3fb1b22c9b3c3c3f51f1da9e139', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Indent the json output for MWTs - makes it a bit readable compared to the original', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/2b1a7ed66e33f3fb1b22c9b3c3c3f51f1da9e139'}, {'sha': '4394a1f0303dde44fd3d61faec40e311b08e804d', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Check that the MWTs in English datasets always add up to the pieces of the MWTs.  Could potentially check this for other languages', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4394a1f0303dde44fd3d61faec40e311b08e804d'}, {'sha': '0fc972c2e9595402800b2e3a4022bf8948c76e02', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'If all the MWTs in a training set are composed exactly of their subwords, then whenever possible, replace all the characters with characters from the original text.  Should greatly help unknown characters or previously unseen words in general.', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/0fc972c2e9595402800b2e3a4022bf8948c76e02'}, {'sha': 'd31473a37042b0030dfb313d01712b478c89fc9e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Is it necessary to mark this as travis to get it run there?', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/d31473a37042b0030dfb313d01712b478c89fc9e'}, {'sha': '4dd6fc8dff14c9242f57a6750669ee93ee796bdc', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""Add a test of the new MWT model's capability to build the pieces' text from the original text of the split token"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4dd6fc8dff14c9242f57a6750669ee93ee796bdc'}, {'sha': '509adbf6c2f1c11d4d917cbe6f7433229e088477', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Rename the MWT test for the unknown words to leave room to add more tests', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/509adbf6c2f1c11d4d917cbe6f7433229e088477'}, {'sha': 'd390ad410f9012aa445d7317523c912fa5167dba', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""Change the MWT dictionary lookup to only look for lowercasing if the original word matches one of a couple expected casing formats, in which case we can recreate those formats after using the dictionary lookup.  Otherwise, you get unexpected tokenizations such as She's -> she 's.  https://github.com/stanfordnlp/stanza/issues/1371"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/d390ad410f9012aa445d7317523c912fa5167dba'}, {'sha': 'c27b5b9b7aaccb33b293eae6d465f103b938aa2e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Simplify an awkward looking loop by using np operations on strings', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/c27b5b9b7aaccb33b293eae6d465f103b938aa2e'}]}"
3,PullRequestEvent,"{'action': 'closed', 'number': 1378, 'pull_request': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378', 'id': 1803995374, 'node_id': 'PR_kwDOBj_0V85rhsju', 'html_url': 'https://github.com/stanfordnlp/stanza/pull/1378', 'diff_url': 'https://github.com/stanfordnlp/stanza/pull/1378.diff', 'patch_url': 'https://github.com/stanfordnlp/stanza/pull/1378.patch', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378', 'number': 1378, 'state': 'closed', 'locked': False, 'title': 'English mwt', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'body': 'For languages where the MWT words exactly make up the text of the token, build the pieces of the MWT using the text from the original token we are splitting.  Should fix a bunch of the errors observed in https://github.com/stanfordnlp/stanza/issues/1371', 'created_at': '2024-04-03T07:52:38Z', 'updated_at': '2024-04-04T22:04:53Z', 'closed_at': '2024-04-04T22:04:52Z', 'merged_at': '2024-04-04T22:04:52Z', 'merge_commit_sha': 'c27b5b9b7aaccb33b293eae6d465f103b938aa2e', 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/commits', 'review_comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/comments', 'review_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378/comments', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/9de0495e5a9a9232bcf36f1e941d0bea67ef97c0', 'head': {'label': 'stanfordnlp:english_mwt', 'ref': 'english_mwt', 'sha': '9de0495e5a9a9232bcf36f1e941d0bea67ef97c0', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-04-04T17:29:01Z', 'pushed_at': '2024-04-04T08:20:52Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85299, 'stargazers_count': 7021, 'watchers_count': 7021, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 96, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 96, 'watchers': 7021, 'default_branch': 'main'}}, 'base': {'label': 'stanfordnlp:dev', 'ref': 'dev', 'sha': '7a4f48c738f0db8923aa5da88d0a9743eaee4c6a', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-04-04T17:29:01Z', 'pushed_at': '2024-04-04T08:20:52Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85299, 'stargazers_count': 7021, 'watchers_count': 7021, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 96, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 96, 'watchers': 7021, 'default_branch': 'main'}}, '_links': {'self': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378'}, 'html': {'href': 'https://github.com/stanfordnlp/stanza/pull/1378'}, 'issue': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378'}, 'comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/commits'}, 'statuses': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/9de0495e5a9a9232bcf36f1e941d0bea67ef97c0'}}, 'author_association': 'COLLABORATOR', 'auto_merge': None, 'active_lock_reason': None, 'merged': True, 'mergeable': None, 'rebaseable': None, 'mergeable_state': 'unknown', 'merged_by': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'comments': 0, 'review_comments': 0, 'maintainer_can_modify': False, 'commits': 9, 'additions': 255, 'deletions': 20, 'changed_files': 9}}"
4,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1373', 'id': 2203742630, 'node_id': 'I_kwDOBj_0V86DWnGm', 'number': 1373, 'title': 'when i use depprase for denpendency analysis with chinese in client, the results:root:3;rootnode:2?it is correct? can rootnode be a numer over 0?', 'user': {'login': 'xy1137030414', 'id': 73565715, 'node_id': 'MDQ6VXNlcjczNTY1NzE1', 'avatar_url': 'https://avatars.githubusercontent.com/u/73565715?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/xy1137030414', 'html_url': 'https://github.com/xy1137030414', 'followers_url': 'https://api.github.com/users/xy1137030414/followers', 'following_url': 'https://api.github.com/users/xy1137030414/following{/other_user}', 'gists_url': 'https://api.github.com/users/xy1137030414/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/xy1137030414/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/xy1137030414/subscriptions', 'organizations_url': 'https://api.github.com/users/xy1137030414/orgs', 'repos_url': 'https://api.github.com/users/xy1137030414/repos', 'events_url': 'https://api.github.com/users/xy1137030414/events{/privacy}', 'received_events_url': 'https://api.github.com/users/xy1137030414/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-23T07:10:53Z', 'updated_at': '2024-04-04T19:46:01Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Before you start, make sure to check out:\r\n* Our documentation: https://stanfordnlp.github.io/stanza/\r\n* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html\r\n* Github issues (especially closed ones)\r\nYour question might have an answer in these places!\r\n\r\nIf you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!\r\n\r\nIf you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2038079182', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1373#issuecomment-2038079182', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373', 'id': 2038079182, 'node_id': 'IC_kwDOBj_0V855ep7O', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-04T19:45:59Z', 'updated_at': '2024-04-04T19:45:59Z', 'author_association': 'COLLABORATOR', 'body': ""I would need more information to diagnose this.  What text did you give to which tool that resulted in weird dependencies?  In general though, root nodes are always 0\r\n\r\nAs a heads up, the alert email is sent as soon as the issue is created, and edits don't send followup emails, so next time I recommend deleting an empty issue and recreating it."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2038079182/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
5,WatchEvent,{'action': 'started'}
6,PushEvent,"{'repository_id': 104854615, 'push_id': 17843096908, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'ec66b10b1835806e048fced3a2dce1650db6ccaa', 'before': '7a4f48c738f0db8923aa5da88d0a9743eaee4c6a', 'commits': [{'sha': 'ec66b10b1835806e048fced3a2dce1650db6ccaa', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'When converting de_spmrl, wrap a few trees which were just singleton nodes in their most common constituents', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/ec66b10b1835806e048fced3a2dce1650db6ccaa'}]}"
7,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724', 'id': 923140812, 'node_id': 'MDU6SXNzdWU5MjMxNDA4MTI=', 'number': 724, 'title': 'Fine-tuning a model', 'user': {'login': 'sarves', 'id': 1219595, 'node_id': 'MDQ6VXNlcjEyMTk1OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1219595?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sarves', 'html_url': 'https://github.com/sarves', 'followers_url': 'https://api.github.com/users/sarves/followers', 'following_url': 'https://api.github.com/users/sarves/following{/other_user}', 'gists_url': 'https://api.github.com/users/sarves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sarves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sarves/subscriptions', 'organizations_url': 'https://api.github.com/users/sarves/orgs', 'repos_url': 'https://api.github.com/users/sarves/repos', 'events_url': 'https://api.github.com/users/sarves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sarves/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618829254, 'node_id': 'MDU6TGFiZWwyNjE4ODI5MjU0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/pinned', 'name': 'pinned', 'color': '6E6224', 'default': False, 'description': ''}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 24, 'created_at': '2021-06-16T21:49:09Z', 'updated_at': '2024-04-04T07:20:29Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi \r\n\r\n**1. Is there a way that I can fine-tune an existing model?**\r\n\r\nFor instance, already there is a model for Tamil (ttb.pt) in Stanza. Can I use that train or fine-tune with more Tamil data, instead of training from the scratch? \r\nCurrently (with Stanza 1.2), if a model.pt exist in saved_model directory, the training process is skipped. \r\n\r\n**2. Where can I find the value of parameters like drop_out, --batch_size etc used to train the current Stanza model for Tamil?** (are they same as the default values provided in respective <model>.py, for instance https://github.com/stanfordnlp/stanza/blob/main/stanza/models/tokenizer.py)\r\n\r\nThank you', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2036378216', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724#issuecomment-2036378216', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'id': 2036378216, 'node_id': 'IC_kwDOBj_0V855YKpo', 'user': {'login': 'umutarioz', 'id': 77281065, 'node_id': 'MDQ6VXNlcjc3MjgxMDY1', 'avatar_url': 'https://avatars.githubusercontent.com/u/77281065?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/umutarioz', 'html_url': 'https://github.com/umutarioz', 'followers_url': 'https://api.github.com/users/umutarioz/followers', 'following_url': 'https://api.github.com/users/umutarioz/following{/other_user}', 'gists_url': 'https://api.github.com/users/umutarioz/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/umutarioz/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/umutarioz/subscriptions', 'organizations_url': 'https://api.github.com/users/umutarioz/orgs', 'repos_url': 'https://api.github.com/users/umutarioz/repos', 'events_url': 'https://api.github.com/users/umutarioz/events{/privacy}', 'received_events_url': 'https://api.github.com/users/umutarioz/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-04T07:20:28Z', 'updated_at': '2024-04-04T07:20:28Z', 'author_association': 'NONE', 'body': 'Thanks I found. ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2036378216/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
8,DeleteEvent,"{'ref': 'shallow-deppase-broken', 'ref_type': 'branch', 'pusher_type': 'user'}"
9,DeleteEvent,"{'ref': 'peft-pos', 'ref_type': 'branch', 'pusher_type': 'user'}"
10,DeleteEvent,"{'ref': 'con_self_gan', 'ref_type': 'branch', 'pusher_type': 'user'}"
11,DeleteEvent,"{'ref': 'tagger_mha', 'ref_type': 'branch', 'pusher_type': 'user'}"
12,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 14, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T23:42:57Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035814454', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2035814454', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2035814454, 'node_id': 'IC_kwDOBj_0V855WBA2', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T23:42:56Z', 'updated_at': '2024-04-03T23:42:56Z', 'author_association': 'COLLABORATOR', 'body': ""Just to be clear, it's not merged yet - but probably soon, since it is passing the current unit tests and is a lot better on the test cases you gave us.  Especially if you report back saying you like this branch more than the current release :)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035814454/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
13,PushEvent,"{'repository_id': 104854615, 'push_id': 17837829502, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': '9de0495e5a9a9232bcf36f1e941d0bea67ef97c0', 'before': '5800e02a8df1b4856cdd6ec12cb48f8f4bfbe705', 'commits': [{'sha': '9de0495e5a9a9232bcf36f1e941d0bea67ef97c0', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Simplify an awkward looking loop by using np operations on strings', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/9de0495e5a9a9232bcf36f1e941d0bea67ef97c0'}]}"
14,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 13, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T23:20:35Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035797245', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2035797245', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2035797245, 'node_id': 'IC_kwDOBj_0V855V8z9', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T23:20:34Z', 'updated_at': '2024-04-03T23:20:34Z', 'author_association': 'NONE', 'body': 'Thanks for looking into this @AngledLuffa!!  I will update my stanza to the latest on the dev branch to pick up these changes. ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035797245/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
15,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 12, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T22:46:31Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035734223', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2035734223', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2035734223, 'node_id': 'IC_kwDOBj_0V855VtbP', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T22:46:30Z', 'updated_at': '2024-04-03T22:46:30Z', 'author_association': 'COLLABORATOR', 'body': 'Eh, well, the non-100% appears to be entirely typos which were annotated in the test set and not handled in the expected manner by the model.  Situations where the tokenizer isn\'t actually splitting an MWT, but if you force the MWT processor to make a decision, it doesn\'t really know how to process ""Mens room"" instead of ""Men\'s room"" etc\r\n\r\n```\r\n11944,11945c11943,11944\r\n< 9     Cox\r\n< 10    \'\r\n---\r\n> 9     Co\r\n> 10    x\'\r\n15578,15579c15577,15578\r\n< 16    sheep\r\n< 17    s\r\n---\r\n> 16    shee\r\n> 17    ps\r\n26469,26470c26467,26468\r\n< 1     Men\r\n< 2     s\r\n---\r\n> 1     Me\r\n> 2     ns\r\n```\r\n\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035734223/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
16,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 11, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T22:28:27Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035715813', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2035715813', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2035715813, 'node_id': 'IC_kwDOBj_0V855Vo7l', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T22:28:27Z', 'updated_at': '2024-04-03T22:28:27Z', 'author_association': 'COLLABORATOR', 'body': ""This looks much better to me, but I will say there's an annoying 99.98% accuracy on the test set, whereas I would have expected 100% now that the casing is fixed and the model is forced to copy the input whenever possible.  Hopefully it's just a random word which doesn't show up in the training data and isn't being correctly split, rather than one of the hallucinations or re-casings we're trying to fix"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035715813/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
17,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 10, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T22:26:24Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035713040', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2035713040', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2035713040, 'node_id': 'IC_kwDOBj_0V855VoQQ', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T22:26:23Z', 'updated_at': '2024-04-03T22:26:23Z', 'author_association': 'COLLABORATOR', 'body': 'With these changes, here\'s what I get in the linked branch (which I\'ll merge into `dev` once I start hearing back from stakeholders in this issue) for the text you mentioned above\r\n\r\n```\r\n>>> text = """"""\r\n... In God’s name let him be so.\r\n... Didn’t I say so?\r\n... They began to be frightened at last at Pulcheria Alexandrovna\'s strange silence.\r\n... Pulcheria Alexandrovna\'s illness was a strange nervous one.\r\n... Couldn\'t he stop and retract it all?\r\n... She\'ll be my nurse.\r\n... It was quite an accident Lebeziatnikov\'s turning up.\r\n... Wasn\'t I right in saying that we were birds of a feather?\r\n... Couldn\'t he come?\r\n... She\'ll get it at the shop, my dear.\r\n... Who married the Marquis of Saint-Méran\'s daughter?\r\n... But to Dantès\' eye there was no darkness.\r\n... Couldn’t he have waited for the good season to increase his chances?\r\n... She\'d never noticed if it hadn\'t been for Sid.\r\n... Wasn\'t that a happy thought?\r\n... """"""\r\n>>> text = text.strip().split(""\\n"")\r\n>>> for line in text:\r\n...   print([x.text for x in pipe(line).sentences[0].words])\r\n\r\n\r\n[\'In\', \'God\', \'’s\', \'name\', \'let\', \'him\', \'be\', \'so\', \'.\']\r\n[\'Did\', \'n’t\', \'I\', \'say\', \'so\', \'?\']\r\n[\'They\', \'began\', \'to\', \'be\', \'frightened\', \'at\', \'last\', \'at\', \'Pulcheria\', \'Alexandrovna\', ""\'s"", \'strange\', \'silence\', \'.\']\r\n[\'Pulcheria\', \'Alexandrovna\', ""\'s"", \'illness\', \'was\', \'a\', \'strange\', \'nervous\', \'one\', \'.\']\r\n[\'Could\', ""n\'t"", \'he\', \'stop\', \'and\', \'retract\', \'it\', \'all\', \'?\']\r\n[\'She\', ""\'ll"", \'be\', \'my\', \'nurse\', \'.\']\r\n[\'It\', \'was\', \'quite\', \'an\', \'accident\', \'Lebeziatnikov\', ""\'s"", \'turning\', \'up\', \'.\']\r\n[\'Was\', ""n\'t"", \'I\', \'right\', \'in\', \'saying\', \'that\', \'we\', \'were\', \'birds\', \'of\', \'a\', \'feather\', \'?\']\r\n[\'Could\', ""n\'t"", \'he\', \'come\', \'?\']\r\n[\'She\', ""\'ll"", \'get\', \'it\', \'at\', \'the\', \'shop\', \',\', \'my\', \'dear\', \'.\']\r\n[\'Who\', \'married\', \'the\', \'Marquis\', \'of\', \'Saint\', \'-\', \'Méran\', ""\'s"", \'daughter\', \'?\']\r\n[\'But\', \'to\', \'Dantès\', ""\'"", \'eye\', \'there\', \'was\', \'no\', \'darkness\', \'.\']\r\n[\'Could\', \'n’t\', \'he\', \'have\', \'waited\', \'for\', \'the\', \'good\', \'season\', \'to\', \'increase\', \'his\', \'chances\', \'?\']\r\n[\'She\', ""\'d"", \'never\', \'noticed\', \'if\', \'it\', \'had\', ""n\'t"", \'been\', \'for\', \'Sid\', \'.\']\r\n[\'Was\', ""n\'t"", \'that\', \'a\', \'happy\', \'thought\', \'?\']\r\n```\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035713040/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
18,PushEvent,"{'repository_id': 104854615, 'push_id': 17837148237, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': '5800e02a8df1b4856cdd6ec12cb48f8f4bfbe705', 'before': 'a15b981fafeed5110145ab7a2519b32ad324731f', 'commits': [{'sha': '5800e02a8df1b4856cdd6ec12cb48f8f4bfbe705', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""Change the MWT dictionary lookup to only look for lowercasing if the original word matches one of a couple expected casing formats, in which case we can recreate those formats after using the dictionary lookup.  Otherwise, you get unexpected tokenizations such as She's -> she 's.  https://github.com/stanfordnlp/stanza/issues/1371"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/5800e02a8df1b4856cdd6ec12cb48f8f4bfbe705'}]}"
19,PushEvent,"{'repository_id': 104854615, 'push_id': 17836951152, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': 'a15b981fafeed5110145ab7a2519b32ad324731f', 'before': '9d851a40f54933e4bed94ccdd4fb0542d8ef841a', 'commits': [{'sha': 'a15b981fafeed5110145ab7a2519b32ad324731f', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Rename the MWT test for the unknown words to leave room to add more tests', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/a15b981fafeed5110145ab7a2519b32ad324731f'}]}"
20,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 9, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T21:52:36Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035664372', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2035664372', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2035664372, 'node_id': 'IC_kwDOBj_0V855VcX0', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T21:52:34Z', 'updated_at': '2024-04-03T21:52:34Z', 'author_association': 'COLLABORATOR', 'body': 'I think the use of lowercasing in the MWT is just a straight up logic bug:\r\n\r\nhttps://github.com/stanfordnlp/stanza/blob/c2d72bd14cf8cc28bd4e41a620692bbce5f43835/stanza/models/mwt/trainer.py#L99\r\n\r\nhttps://github.com/stanfordnlp/stanza/blob/c2d72bd14cf8cc28bd4e41a620692bbce5f43835/stanza/models/mwt/trainer.py#L112\r\n\r\nMy own expectation with tokenization is that it doesn\'t change the characters used unnecessarily, but consider:\r\n\r\n```\r\n>>> [x.text for x in pipe(""JENNIFER HAS NICE ANTENNAE"").sentences[0].words]\r\n[\'JENNIFER\', \'HAS\', \'NICE\', \'ANTENNAE\']\r\n>>> [x.text for x in pipe(""JENNIFER\'S GOT NICE ANTENNAE"").sentences[0].words]\r\n[\'JENNIFER\', ""\'S"", \'GOT\', \'NICE\', \'ANTENNAE\']\r\n>>> [x.text for x in pipe(""SHE\'S GOT NICE ANTENNAE"").sentences[0].words]   # oops, this shows up in the dictionary\r\n[\'she\', ""\'s"", \'GOT\', \'NICE\', \'ANTENNAE\']\r\n```\r\n\r\nMaybe a reasonable fix would be to only look up all lowercase, leading uppercase, and all uppercase, and weird mixed cases just have to go through the seq2seq model', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2035664372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
21,WatchEvent,{'action': 'started'}
22,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724', 'id': 923140812, 'node_id': 'MDU6SXNzdWU5MjMxNDA4MTI=', 'number': 724, 'title': 'Fine-tuning a model', 'user': {'login': 'sarves', 'id': 1219595, 'node_id': 'MDQ6VXNlcjEyMTk1OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1219595?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sarves', 'html_url': 'https://github.com/sarves', 'followers_url': 'https://api.github.com/users/sarves/followers', 'following_url': 'https://api.github.com/users/sarves/following{/other_user}', 'gists_url': 'https://api.github.com/users/sarves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sarves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sarves/subscriptions', 'organizations_url': 'https://api.github.com/users/sarves/orgs', 'repos_url': 'https://api.github.com/users/sarves/repos', 'events_url': 'https://api.github.com/users/sarves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sarves/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618829254, 'node_id': 'MDU6TGFiZWwyNjE4ODI5MjU0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/pinned', 'name': 'pinned', 'color': '6E6224', 'default': False, 'description': ''}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 23, 'created_at': '2021-06-16T21:49:09Z', 'updated_at': '2024-04-03T14:43:17Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi \r\n\r\n**1. Is there a way that I can fine-tune an existing model?**\r\n\r\nFor instance, already there is a model for Tamil (ttb.pt) in Stanza. Can I use that train or fine-tune with more Tamil data, instead of training from the scratch? \r\nCurrently (with Stanza 1.2), if a model.pt exist in saved_model directory, the training process is skipped. \r\n\r\n**2. Where can I find the value of parameters like drop_out, --batch_size etc used to train the current Stanza model for Tamil?** (are they same as the default values provided in respective <model>.py, for instance https://github.com/stanfordnlp/stanza/blob/main/stanza/models/tokenizer.py)\r\n\r\nThank you', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2034821072', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724#issuecomment-2034821072', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'id': 2034821072, 'node_id': 'IC_kwDOBj_0V855SOfQ', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T14:43:16Z', 'updated_at': '2024-04-03T14:43:16Z', 'author_association': 'COLLABORATOR', 'body': ""> I downloaded with that code but ner_tagger gives still an error: FileNotFoundError: [Errno 2] No such file or directory: 'i2b2'.\r\n\r\nWhat did you do to run `ner_tagger`?\r\n\r\nYou want to give it whatever path for the NER model download, possibly `~/stanza_resources/en/ner/i2b2.pt`, possibly somewhere else if you're running on Windows or if you've changed the resources download directory"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2034821072/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
23,WatchEvent,{'action': 'started'}
24,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724', 'id': 923140812, 'node_id': 'MDU6SXNzdWU5MjMxNDA4MTI=', 'number': 724, 'title': 'Fine-tuning a model', 'user': {'login': 'sarves', 'id': 1219595, 'node_id': 'MDQ6VXNlcjEyMTk1OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1219595?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sarves', 'html_url': 'https://github.com/sarves', 'followers_url': 'https://api.github.com/users/sarves/followers', 'following_url': 'https://api.github.com/users/sarves/following{/other_user}', 'gists_url': 'https://api.github.com/users/sarves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sarves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sarves/subscriptions', 'organizations_url': 'https://api.github.com/users/sarves/orgs', 'repos_url': 'https://api.github.com/users/sarves/repos', 'events_url': 'https://api.github.com/users/sarves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sarves/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618829254, 'node_id': 'MDU6TGFiZWwyNjE4ODI5MjU0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/pinned', 'name': 'pinned', 'color': '6E6224', 'default': False, 'description': ''}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 22, 'created_at': '2021-06-16T21:49:09Z', 'updated_at': '2024-04-03T11:36:04Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi \r\n\r\n**1. Is there a way that I can fine-tune an existing model?**\r\n\r\nFor instance, already there is a model for Tamil (ttb.pt) in Stanza. Can I use that train or fine-tune with more Tamil data, instead of training from the scratch? \r\nCurrently (with Stanza 1.2), if a model.pt exist in saved_model directory, the training process is skipped. \r\n\r\n**2. Where can I find the value of parameters like drop_out, --batch_size etc used to train the current Stanza model for Tamil?** (are they same as the default values provided in respective <model>.py, for instance https://github.com/stanfordnlp/stanza/blob/main/stanza/models/tokenizer.py)\r\n\r\nThank you', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2034338909', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724#issuecomment-2034338909', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'id': 2034338909, 'node_id': 'IC_kwDOBj_0V855QYxd', 'user': {'login': 'umutarioz', 'id': 77281065, 'node_id': 'MDQ6VXNlcjc3MjgxMDY1', 'avatar_url': 'https://avatars.githubusercontent.com/u/77281065?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/umutarioz', 'html_url': 'https://github.com/umutarioz', 'followers_url': 'https://api.github.com/users/umutarioz/followers', 'following_url': 'https://api.github.com/users/umutarioz/following{/other_user}', 'gists_url': 'https://api.github.com/users/umutarioz/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/umutarioz/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/umutarioz/subscriptions', 'organizations_url': 'https://api.github.com/users/umutarioz/orgs', 'repos_url': 'https://api.github.com/users/umutarioz/repos', 'events_url': 'https://api.github.com/users/umutarioz/events{/privacy}', 'received_events_url': 'https://api.github.com/users/umutarioz/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T11:36:03Z', 'updated_at': '2024-04-03T11:36:03Z', 'author_association': 'NONE', 'body': ""Thanks\r\nI downloaded with that code but ner_tagger gives still an error: FileNotFoundError: [Errno 2] No such file or directory: 'i2b2'. \r\nWhat should I do to use this model to fine tune with my dataset via ner_tagger?\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2034338909/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
25,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 8, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T07:56:42Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2033814594', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2033814594', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2033814594, 'node_id': 'IC_kwDOBj_0V855OYxC', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T07:56:41Z', 'updated_at': '2024-04-03T07:56:41Z', 'author_association': 'COLLABORATOR', 'body': ""I should note that although this helps with the OOV characters, there is another issue with words at the start of a sentence being split with the dictionary lookup and then lowercased... I don't think that's correct behavior of the model, and I can probably fix that a lot faster than it took to fix this."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2033814594/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
26,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 7, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T07:54:18Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2033809587', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2033809587', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2033809587, 'node_id': 'IC_kwDOBj_0V855OXiz', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T07:54:17Z', 'updated_at': '2024-04-03T07:54:17Z', 'author_association': 'COLLABORATOR', 'body': ""The `english_mwt` branch should now have a fix for most of these issues, although I wouldn't be surprised if the new model still occasionally hallucinates text which isn't the right length (in which case the new splitting mechanism won't work).  Please LMK if this helps, and feel free to report whichever words still aren't split correctly.\r\n\r\nThe constituency parser issue is just because the parser as part of the pipeline is using the MWT as input, so it's getting the weird splits just like the other models."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2033809587/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
27,PullRequestEvent,"{'action': 'opened', 'number': 1378, 'pull_request': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378', 'id': 1803995374, 'node_id': 'PR_kwDOBj_0V85rhsju', 'html_url': 'https://github.com/stanfordnlp/stanza/pull/1378', 'diff_url': 'https://github.com/stanfordnlp/stanza/pull/1378.diff', 'patch_url': 'https://github.com/stanfordnlp/stanza/pull/1378.patch', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378', 'number': 1378, 'state': 'open', 'locked': False, 'title': 'English mwt', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'body': 'For languages where the MWT words exactly make up the text of the token, build the pieces of the MWT using the text from the original token we are splitting.  Should fix a bunch of the errors observed in https://github.com/stanfordnlp/stanza/issues/1371', 'created_at': '2024-04-03T07:52:38Z', 'updated_at': '2024-04-03T07:52:38Z', 'closed_at': None, 'merged_at': None, 'merge_commit_sha': None, 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/commits', 'review_comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/comments', 'review_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378/comments', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/9d851a40f54933e4bed94ccdd4fb0542d8ef841a', 'head': {'label': 'stanfordnlp:english_mwt', 'ref': 'english_mwt', 'sha': '9d851a40f54933e4bed94ccdd4fb0542d8ef841a', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-04-02T17:40:22Z', 'pushed_at': '2024-04-03T07:52:38Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85341, 'stargazers_count': 7018, 'watchers_count': 7018, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 97, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 97, 'watchers': 7018, 'default_branch': 'main'}}, 'base': {'label': 'stanfordnlp:dev', 'ref': 'dev', 'sha': '7a4f48c738f0db8923aa5da88d0a9743eaee4c6a', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-04-02T17:40:22Z', 'pushed_at': '2024-04-03T07:52:38Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85341, 'stargazers_count': 7018, 'watchers_count': 7018, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 97, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 97, 'watchers': 7018, 'default_branch': 'main'}}, '_links': {'self': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378'}, 'html': {'href': 'https://github.com/stanfordnlp/stanza/pull/1378'}, 'issue': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378'}, 'comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1378/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1378/commits'}, 'statuses': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/9d851a40f54933e4bed94ccdd4fb0542d8ef841a'}}, 'author_association': 'COLLABORATOR', 'auto_merge': None, 'active_lock_reason': None, 'merged': False, 'mergeable': None, 'rebaseable': None, 'mergeable_state': 'unknown', 'merged_by': None, 'comments': 0, 'review_comments': 0, 'maintainer_can_modify': False, 'commits': 6, 'additions': 183, 'deletions': 10, 'changed_files': 9}}"
28,PushEvent,"{'repository_id': 104854615, 'push_id': 17823414060, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': '9d851a40f54933e4bed94ccdd4fb0542d8ef841a', 'before': 'b7a4d4ee1b5f4ff3b41427dc16473bfb08166bed', 'commits': [{'sha': '9d851a40f54933e4bed94ccdd4fb0542d8ef841a', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""Add a test of the new MWT model's capability to build the pieces' text from the original text of the split token"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/9d851a40f54933e4bed94ccdd4fb0542d8ef841a'}]}"
29,PushEvent,"{'repository_id': 104854615, 'push_id': 17823236225, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': 'b7a4d4ee1b5f4ff3b41427dc16473bfb08166bed', 'before': '08edaa1f8e61bbeae207348d79a657cb69c1f5ec', 'commits': [{'sha': 'b7a4d4ee1b5f4ff3b41427dc16473bfb08166bed', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Is it necessary to mark this as travis to get it run there?', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/b7a4d4ee1b5f4ff3b41427dc16473bfb08166bed'}]}"
30,PushEvent,"{'repository_id': 104854615, 'push_id': 17822609714, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': '08edaa1f8e61bbeae207348d79a657cb69c1f5ec', 'before': 'abfea89d2fceb005544c231e7e08f29b71f01458', 'commits': [{'sha': '08edaa1f8e61bbeae207348d79a657cb69c1f5ec', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'If all the MWTs in a training set are composed exactly of their subwords, then whenever possible, replace all the characters with characters from the original text.  Should greatly help unknown characters or previously unseen words in general.', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/08edaa1f8e61bbeae207348d79a657cb69c1f5ec'}]}"
31,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-04-03T06:52:41Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2033691138', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2033691138', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2033691138, 'node_id': 'IC_kwDOBj_0V855N6oC', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-03T06:52:39Z', 'updated_at': '2024-04-03T06:52:39Z', 'author_association': 'COLLABORATOR', 'body': 'Thought about it some.  Realized I was probably overthinking, and the easiest thing to do was continue using the current model and make it replace the prediction characters with the original text if the length added up to the original text.  Only for languages where this property happens, of course.\r\n\r\n@qipeng ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2033691138/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
32,PushEvent,"{'repository_id': 104854615, 'push_id': 17822328202, 'size': 3, 'distinct_size': 3, 'ref': 'refs/heads/english_mwt', 'head': 'abfea89d2fceb005544c231e7e08f29b71f01458', 'before': 'a968f799fdaf6f86e086516bbf5b075bb958b0a5', 'commits': [{'sha': 'f4cff3becd6b4e8f239b5f5e1c6e9f66862b89a5', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Pass around the original text when decoding MWT.  This will make it possible to use the original text directly, even in the face of unknown characters (which currently trips up the copy mechanism)', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/f4cff3becd6b4e8f239b5f5e1c6e9f66862b89a5'}, {'sha': '5ead35e0d31654b0ed5430f5f66b2c88d9b63794', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Indent the json output for MWTs - makes it a bit readable compared to the original', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/5ead35e0d31654b0ed5430f5f66b2c88d9b63794'}, {'sha': 'abfea89d2fceb005544c231e7e08f29b71f01458', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Check that the MWTs in English datasets always add up to the pieces of the MWTs.  Could potentially check this for other languages', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/abfea89d2fceb005544c231e7e08f29b71f01458'}]}"
33,PushEvent,"{'repository_id': 104854615, 'push_id': 17821666137, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': 'a968f799fdaf6f86e086516bbf5b075bb958b0a5', 'before': '04ea22ca37ff4942f57c9dd681da30df4d1b99c7', 'commits': [{'sha': 'a968f799fdaf6f86e086516bbf5b075bb958b0a5', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Check that the MWTs in English datasets always add up to the pieces of the MWTs.  Could potentially check this for other languages', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/a968f799fdaf6f86e086516bbf5b075bb958b0a5'}]}"
34,PushEvent,"{'repository_id': 104854615, 'push_id': 17821423890, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/english_mwt', 'head': '04ea22ca37ff4942f57c9dd681da30df4d1b99c7', 'before': 'dbe3124a3475f96d7b928226406daca6b04d7fd4', 'commits': [{'sha': '04ea22ca37ff4942f57c9dd681da30df4d1b99c7', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Indent the json output for MWTs - makes it a bit readable compared to the original', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/04ea22ca37ff4942f57c9dd681da30df4d1b99c7'}]}"
35,CreateEvent,"{'ref': 'english_mwt', 'ref_type': 'branch', 'master_branch': 'main', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'pusher_type': 'user'}"
36,WatchEvent,{'action': 'started'}
37,WatchEvent,{'action': 'started'}
38,WatchEvent,{'action': 'started'}
39,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724', 'id': 923140812, 'node_id': 'MDU6SXNzdWU5MjMxNDA4MTI=', 'number': 724, 'title': 'Fine-tuning a model', 'user': {'login': 'sarves', 'id': 1219595, 'node_id': 'MDQ6VXNlcjEyMTk1OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1219595?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sarves', 'html_url': 'https://github.com/sarves', 'followers_url': 'https://api.github.com/users/sarves/followers', 'following_url': 'https://api.github.com/users/sarves/following{/other_user}', 'gists_url': 'https://api.github.com/users/sarves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sarves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sarves/subscriptions', 'organizations_url': 'https://api.github.com/users/sarves/orgs', 'repos_url': 'https://api.github.com/users/sarves/repos', 'events_url': 'https://api.github.com/users/sarves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sarves/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618829254, 'node_id': 'MDU6TGFiZWwyNjE4ODI5MjU0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/pinned', 'name': 'pinned', 'color': '6E6224', 'default': False, 'description': ''}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 21, 'created_at': '2021-06-16T21:49:09Z', 'updated_at': '2024-04-02T08:51:06Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi \r\n\r\n**1. Is there a way that I can fine-tune an existing model?**\r\n\r\nFor instance, already there is a model for Tamil (ttb.pt) in Stanza. Can I use that train or fine-tune with more Tamil data, instead of training from the scratch? \r\nCurrently (with Stanza 1.2), if a model.pt exist in saved_model directory, the training process is skipped. \r\n\r\n**2. Where can I find the value of parameters like drop_out, --batch_size etc used to train the current Stanza model for Tamil?** (are they same as the default values provided in respective <model>.py, for instance https://github.com/stanfordnlp/stanza/blob/main/stanza/models/tokenizer.py)\r\n\r\nThank you', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031431355', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724#issuecomment-2031431355', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'id': 2031431355, 'node_id': 'IC_kwDOBj_0V855FS67', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-02T08:51:05Z', 'updated_at': '2024-04-02T08:51:05Z', 'author_association': 'COLLABORATOR', 'body': 'Oh, if what you needed was the `i2b2` model itself, you can do\r\n\r\n```\r\npipe = stanza.download(""en"", processors=""ner"", package=""i2b2"")\r\n```\r\n\r\nIt should download both the NER model and its pretrain & charlms', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031431355/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
40,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724', 'id': 923140812, 'node_id': 'MDU6SXNzdWU5MjMxNDA4MTI=', 'number': 724, 'title': 'Fine-tuning a model', 'user': {'login': 'sarves', 'id': 1219595, 'node_id': 'MDQ6VXNlcjEyMTk1OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1219595?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sarves', 'html_url': 'https://github.com/sarves', 'followers_url': 'https://api.github.com/users/sarves/followers', 'following_url': 'https://api.github.com/users/sarves/following{/other_user}', 'gists_url': 'https://api.github.com/users/sarves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sarves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sarves/subscriptions', 'organizations_url': 'https://api.github.com/users/sarves/orgs', 'repos_url': 'https://api.github.com/users/sarves/repos', 'events_url': 'https://api.github.com/users/sarves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sarves/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618829254, 'node_id': 'MDU6TGFiZWwyNjE4ODI5MjU0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/pinned', 'name': 'pinned', 'color': '6E6224', 'default': False, 'description': ''}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 20, 'created_at': '2021-06-16T21:49:09Z', 'updated_at': '2024-04-02T08:45:30Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi \r\n\r\n**1. Is there a way that I can fine-tune an existing model?**\r\n\r\nFor instance, already there is a model for Tamil (ttb.pt) in Stanza. Can I use that train or fine-tune with more Tamil data, instead of training from the scratch? \r\nCurrently (with Stanza 1.2), if a model.pt exist in saved_model directory, the training process is skipped. \r\n\r\n**2. Where can I find the value of parameters like drop_out, --batch_size etc used to train the current Stanza model for Tamil?** (are they same as the default values provided in respective <model>.py, for instance https://github.com/stanfordnlp/stanza/blob/main/stanza/models/tokenizer.py)\r\n\r\nThank you', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031420050', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724#issuecomment-2031420050', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'id': 2031420050, 'node_id': 'IC_kwDOBj_0V855FQKS', 'user': {'login': 'umutarioz', 'id': 77281065, 'node_id': 'MDQ6VXNlcjc3MjgxMDY1', 'avatar_url': 'https://avatars.githubusercontent.com/u/77281065?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/umutarioz', 'html_url': 'https://github.com/umutarioz', 'followers_url': 'https://api.github.com/users/umutarioz/followers', 'following_url': 'https://api.github.com/users/umutarioz/following{/other_user}', 'gists_url': 'https://api.github.com/users/umutarioz/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/umutarioz/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/umutarioz/subscriptions', 'organizations_url': 'https://api.github.com/users/umutarioz/orgs', 'repos_url': 'https://api.github.com/users/umutarioz/repos', 'events_url': 'https://api.github.com/users/umutarioz/events{/privacy}', 'received_events_url': 'https://api.github.com/users/umutarioz/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-02T08:45:29Z', 'updated_at': '2024-04-02T08:45:29Z', 'author_association': 'NONE', 'body': 'Thanks for quick reply and info.\r\nLet me explain myself in detail.\r\nMy first aim is to extract the symptoms/problems from the text. Thus we prepared datasets fit into training requirements of stanza models (tarin, dev, test, BIO tagged in json format).  Now we want to retrain the Stanza biomedical NER models (or transformers) with those datasets to increase the extraction accuracy.\r\nIn the second step of our project, we will need different languages. Thus we need datasets to be translated into those languages and then retrain the existing models (Stanza biomedical NER models or transformers) again to extract the symptoms/problems from the different languages.\r\n\r\nI prepared those flags for ner_tagger:\r\n\r\npython3 -m stanza.models.ner_tagger \r\n--wordvec_dir /....../word2vec \r\n--train_file /........train.json \r\n--eval_file /.....dev.json \r\n--eval_output_file /......./evaluation_results \r\n--mode train \r\n--finetune \r\n--finetune_load_name i2b2 \r\n--train_classifier_only \r\n--shorthand en_medicalner \r\n--scheme bio \r\n--train_scheme bio \r\n--gradient_checkpointing\r\n\r\nBut it could not find the i2b2 model. Is those correct or how can I correct? or Is this possible? or Should I use the transformer model to retrain instead of Stanza Biomedical NER model?\r\n\r\nBest\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031420050/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
41,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724', 'id': 923140812, 'node_id': 'MDU6SXNzdWU5MjMxNDA4MTI=', 'number': 724, 'title': 'Fine-tuning a model', 'user': {'login': 'sarves', 'id': 1219595, 'node_id': 'MDQ6VXNlcjEyMTk1OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1219595?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sarves', 'html_url': 'https://github.com/sarves', 'followers_url': 'https://api.github.com/users/sarves/followers', 'following_url': 'https://api.github.com/users/sarves/following{/other_user}', 'gists_url': 'https://api.github.com/users/sarves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sarves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sarves/subscriptions', 'organizations_url': 'https://api.github.com/users/sarves/orgs', 'repos_url': 'https://api.github.com/users/sarves/repos', 'events_url': 'https://api.github.com/users/sarves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sarves/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618829254, 'node_id': 'MDU6TGFiZWwyNjE4ODI5MjU0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/pinned', 'name': 'pinned', 'color': '6E6224', 'default': False, 'description': ''}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 19, 'created_at': '2021-06-16T21:49:09Z', 'updated_at': '2024-04-02T07:16:13Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi \r\n\r\n**1. Is there a way that I can fine-tune an existing model?**\r\n\r\nFor instance, already there is a model for Tamil (ttb.pt) in Stanza. Can I use that train or fine-tune with more Tamil data, instead of training from the scratch? \r\nCurrently (with Stanza 1.2), if a model.pt exist in saved_model directory, the training process is skipped. \r\n\r\n**2. Where can I find the value of parameters like drop_out, --batch_size etc used to train the current Stanza model for Tamil?** (are they same as the default values provided in respective <model>.py, for instance https://github.com/stanfordnlp/stanza/blob/main/stanza/models/tokenizer.py)\r\n\r\nThank you', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031243197', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724#issuecomment-2031243197', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'id': 2031243197, 'node_id': 'IC_kwDOBj_0V855Ek-9', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-02T07:16:12Z', 'updated_at': '2024-04-02T07:16:12Z', 'author_association': 'COLLABORATOR', 'body': ""There's certainly no way to extend the English i2b2 model to another\nlanguage.\n\nFrankly the best way to do this would be to start from the i2b2 dataset\n(you can start your search here https://www.i2b2.org/NLP/DataSets/) and\neither 1) retrain with a more effective base embedding, probably a\ntransformer, possibly a transformer specifically built for the field and/or\n2) augment the dataset with more annotations that cover the cases you are\nconcerned about.\n\nThere is a flag in ner_tagger.py which lets you start training from an\nexisting model (--finetune) but unless your finetuning data includes enough\ndata for each of the existing classes, you'll just wipe out the model's\nknowledge of those classes by doing this, so you're going to need to track\ndown the i2b2 dataset to make any actual progress on this problem.\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031243197/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
42,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724', 'id': 923140812, 'node_id': 'MDU6SXNzdWU5MjMxNDA4MTI=', 'number': 724, 'title': 'Fine-tuning a model', 'user': {'login': 'sarves', 'id': 1219595, 'node_id': 'MDQ6VXNlcjEyMTk1OTU=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1219595?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sarves', 'html_url': 'https://github.com/sarves', 'followers_url': 'https://api.github.com/users/sarves/followers', 'following_url': 'https://api.github.com/users/sarves/following{/other_user}', 'gists_url': 'https://api.github.com/users/sarves/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sarves/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sarves/subscriptions', 'organizations_url': 'https://api.github.com/users/sarves/orgs', 'repos_url': 'https://api.github.com/users/sarves/repos', 'events_url': 'https://api.github.com/users/sarves/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sarves/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618829254, 'node_id': 'MDU6TGFiZWwyNjE4ODI5MjU0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/pinned', 'name': 'pinned', 'color': '6E6224', 'default': False, 'description': ''}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 18, 'created_at': '2021-06-16T21:49:09Z', 'updated_at': '2024-04-02T06:56:53Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi \r\n\r\n**1. Is there a way that I can fine-tune an existing model?**\r\n\r\nFor instance, already there is a model for Tamil (ttb.pt) in Stanza. Can I use that train or fine-tune with more Tamil data, instead of training from the scratch? \r\nCurrently (with Stanza 1.2), if a model.pt exist in saved_model directory, the training process is skipped. \r\n\r\n**2. Where can I find the value of parameters like drop_out, --batch_size etc used to train the current Stanza model for Tamil?** (are they same as the default values provided in respective <model>.py, for instance https://github.com/stanfordnlp/stanza/blob/main/stanza/models/tokenizer.py)\r\n\r\nThank you', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031213439', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/724#issuecomment-2031213439', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/724', 'id': 2031213439, 'node_id': 'IC_kwDOBj_0V855Edt_', 'user': {'login': 'umutarioz', 'id': 77281065, 'node_id': 'MDQ6VXNlcjc3MjgxMDY1', 'avatar_url': 'https://avatars.githubusercontent.com/u/77281065?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/umutarioz', 'html_url': 'https://github.com/umutarioz', 'followers_url': 'https://api.github.com/users/umutarioz/followers', 'following_url': 'https://api.github.com/users/umutarioz/following{/other_user}', 'gists_url': 'https://api.github.com/users/umutarioz/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/umutarioz/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/umutarioz/subscriptions', 'organizations_url': 'https://api.github.com/users/umutarioz/orgs', 'repos_url': 'https://api.github.com/users/umutarioz/repos', 'events_url': 'https://api.github.com/users/umutarioz/events{/privacy}', 'received_events_url': 'https://api.github.com/users/umutarioz/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-02T06:56:52Z', 'updated_at': '2024-04-02T06:56:52Z', 'author_association': 'NONE', 'body': 'Hi, just to be clear. For example, I want to fine tune and train the i2b2 model with the similar datasets to improve its capabilities especially for the extraction of problem tag and then for the other languages. Could you show me the way for this?\r\nBest', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2031213439/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
43,PushEvent,"{'repository_id': 104854615, 'push_id': 17803721516, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '1139a637b5f9fee854daa90365f42c607b0dda3c', 'before': '79344efc0ca88c3aa0d0d520013fe0a0a8edfe8f', 'commits': [{'sha': '1139a637b5f9fee854daa90365f42c607b0dda3c', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'adding bakeoff scores', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/1139a637b5f9fee854daa90365f42c607b0dda3c'}]}"
44,PushEvent,"{'repository_id': 104854615, 'push_id': 17803697828, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '79344efc0ca88c3aa0d0d520013fe0a0a8edfe8f', 'before': '9ea14ea8382209d699529474fdebf507321f5ad9', 'commits': [{'sha': '79344efc0ca88c3aa0d0d520013fe0a0a8edfe8f', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'fix linear sum implementation', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/79344efc0ca88c3aa0d0d520013fe0a0a8edfe8f'}]}"
45,PushEvent,"{'repository_id': 104854615, 'push_id': 17803668377, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '9ea14ea8382209d699529474fdebf507321f5ad9', 'before': '7d82f8241fcc45e0c6a463d087dc7393428c5919', 'commits': [{'sha': '9ea14ea8382209d699529474fdebf507321f5ad9', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'fix another typo', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/9ea14ea8382209d699529474fdebf507321f5ad9'}]}"
46,PushEvent,"{'repository_id': 104854615, 'push_id': 17803646773, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '7d82f8241fcc45e0c6a463d087dc7393428c5919', 'before': '44a2982a1bb92375abfb64514e2300205669d6f4', 'commits': [{'sha': '7d82f8241fcc45e0c6a463d087dc7393428c5919', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'minor bug fix', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/7d82f8241fcc45e0c6a463d087dc7393428c5919'}]}"
47,PushEvent,"{'repository_id': 104854615, 'push_id': 17803620494, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '44a2982a1bb92375abfb64514e2300205669d6f4', 'before': 'fce8b847abae8ae88fc3010619582d0304116422', 'commits': [{'sha': '44a2982a1bb92375abfb64514e2300205669d6f4', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'scoring used for bakeoff', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/44a2982a1bb92375abfb64514e2300205669d6f4'}]}"
48,PushEvent,"{'repository_id': 104854615, 'push_id': 17803029861, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': 'fce8b847abae8ae88fc3010619582d0304116422', 'before': 'cd0a8d214aa73fc2b333ba1153767f7b8e414989', 'commits': [{'sha': 'fce8b847abae8ae88fc3010619582d0304116422', 'author': {'email': 'houjun@stanford.edu', 'name': 'Houjun Liu'}, 'message': 'updated for some input', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/fce8b847abae8ae88fc3010619582d0304116422'}]}"
49,PushEvent,"{'repository_id': 104854615, 'push_id': 17801927144, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': 'cd0a8d214aa73fc2b333ba1153767f7b8e414989', 'before': '3380238fb45022b6fdc2d71ec11b683d178e9d24', 'commits': [{'sha': 'cd0a8d214aa73fc2b333ba1153767f7b8e414989', 'author': {'email': 'houjun@stanford.edu', 'name': 'Houjun Liu'}, 'message': 'updating scoring, etc.', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/cd0a8d214aa73fc2b333ba1153767f7b8e414989'}]}"
50,PushEvent,"{'repository_id': 104854615, 'push_id': 17800111178, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '7a4f48c738f0db8923aa5da88d0a9743eaee4c6a', 'before': '40aa836c340d24f4a071e389f5d9c6a1a03f1ed6', 'commits': [{'sha': '7a4f48c738f0db8923aa5da88d0a9743eaee4c6a', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a conversion of de_spmrl for constituencies', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/7a4f48c738f0db8923aa5da88d0a9743eaee4c6a'}]}"
51,PushEvent,"{'repository_id': 104854615, 'push_id': 17799937864, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '40aa836c340d24f4a071e389f5d9c6a1a03f1ed6', 'before': '42718135e2ab4b145bbb5861d55bb9424ca3549f', 'commits': [{'sha': '40aa836c340d24f4a071e389f5d9c6a1a03f1ed6', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a log line showing which files are missing a training column', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/40aa836c340d24f4a071e389f5d9c6a1a03f1ed6'}]}"
52,DeleteEvent,"{'ref': 'pos_dataloader_maxlen', 'ref_type': 'branch', 'pusher_type': 'user'}"
53,PushEvent,"{'repository_id': 104854615, 'push_id': 17799919066, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '42718135e2ab4b145bbb5861d55bb9424ca3549f', 'before': 'e680e5a1f3e8afd07a73e8d8bd2a85a3d820081b', 'commits': [{'sha': '42718135e2ab4b145bbb5861d55bb9424ca3549f', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Make a variant of the dataloader which limits a batch to 5000 words or less (by default) for the Pipeline.  Should help avoid OOM for things such as a few very long sentence soaking up too many resources.  https://github.com/stanfordnlp/stanza/issues/1372', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/42718135e2ab4b145bbb5861d55bb9424ca3549f'}]}"
54,PullRequestEvent,"{'action': 'closed', 'number': 1375, 'pull_request': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375', 'id': 1790881730, 'node_id': 'PR_kwDOBj_0V85qvq_C', 'html_url': 'https://github.com/stanfordnlp/stanza/pull/1375', 'diff_url': 'https://github.com/stanfordnlp/stanza/pull/1375.diff', 'patch_url': 'https://github.com/stanfordnlp/stanza/pull/1375.patch', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375', 'number': 1375, 'state': 'closed', 'locked': False, 'title': 'Make a variant of the dataloader which limits a batch to 5000 words o…', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'body': 'Make a variant of the dataloader which limits a batch to 5000 words or less (by default) for the Pipeline.  Should help avoid OOM for things such as a few very long sentence soaking up too many resources.  https://github.com/stanfordnlp/stanza/issues/1372\r\n', 'created_at': '2024-03-26T06:40:55Z', 'updated_at': '2024-04-01T23:54:58Z', 'closed_at': '2024-04-01T23:54:58Z', 'merged_at': '2024-04-01T23:54:58Z', 'merge_commit_sha': '42718135e2ab4b145bbb5861d55bb9424ca3549f', 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/commits', 'review_comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/comments', 'review_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375/comments', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/2d6d1faad22484ac77674e40f90df3c2a67ea9f1', 'head': {'label': 'stanfordnlp:pos_dataloader_maxlen', 'ref': 'pos_dataloader_maxlen', 'sha': '2d6d1faad22484ac77674e40f90df3c2a67ea9f1', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-04-01T10:23:24Z', 'pushed_at': '2024-04-01T23:54:45Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85282, 'stargazers_count': 7017, 'watchers_count': 7017, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 96, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 96, 'watchers': 7017, 'default_branch': 'main'}}, 'base': {'label': 'stanfordnlp:dev', 'ref': 'dev', 'sha': 'e680e5a1f3e8afd07a73e8d8bd2a85a3d820081b', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-04-01T10:23:24Z', 'pushed_at': '2024-04-01T23:54:45Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85282, 'stargazers_count': 7017, 'watchers_count': 7017, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 96, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 96, 'watchers': 7017, 'default_branch': 'main'}}, '_links': {'self': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375'}, 'html': {'href': 'https://github.com/stanfordnlp/stanza/pull/1375'}, 'issue': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375'}, 'comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/commits'}, 'statuses': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/2d6d1faad22484ac77674e40f90df3c2a67ea9f1'}}, 'author_association': 'COLLABORATOR', 'auto_merge': None, 'active_lock_reason': None, 'merged': True, 'mergeable': None, 'rebaseable': None, 'mergeable_state': 'unknown', 'merged_by': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'comments': 0, 'review_comments': 0, 'maintainer_can_modify': False, 'commits': 1, 'additions': 177, 'deletions': 1, 'changed_files': 4}}"
55,PushEvent,"{'repository_id': 104854615, 'push_id': 17799917157, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/pos_dataloader_maxlen', 'head': '2d6d1faad22484ac77674e40f90df3c2a67ea9f1', 'before': '199987855d5b148fc6cb9934f94f9e894edfd427', 'commits': [{'sha': '2d6d1faad22484ac77674e40f90df3c2a67ea9f1', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Make a variant of the dataloader which limits a batch to 5000 words or less (by default) for the Pipeline.  Should help avoid OOM for things such as a few very long sentence soaking up too many resources.  https://github.com/stanfordnlp/stanza/issues/1372', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/2d6d1faad22484ac77674e40f90df3c2a67ea9f1'}]}"
56,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1377', 'id': 2219175128, 'node_id': 'I_kwDOBj_0V86ERezY', 'number': 1377, 'title': 'Stanza misses multi-word token id links for ""dunno""', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-04-01T22:18:26Z', 'updated_at': '2024-04-01T23:31:00Z', 'closed_at': '2024-04-01T23:30:59Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWith the introduction of multi-word tokens (MWT) for english, we came across a test case where the tokens of a multi-word token are not linked correctly to associated token ids.  \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nI dunno.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `dunno` reveal that one of the tokens for that word is not linked to by its multi-word token:\r\n\r\n```json\r\n\r\n[\r\n\r\n// multi-word token here for ""dunno""\r\n// id only links to 2 and 4, missing 3  \r\n  {\r\n    ""end_char"": 7,\r\n    ""id"": [\r\n      2,\r\n      4\r\n    ],\r\n    ""misc"": ""SpaceAfter=No"",\r\n    ""start_char"": 2,\r\n    ""text"": ""dunno""\r\n  },\r\n\r\n// ""du"" is linked by this multi-word token\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 4,\r\n    ""feats"": ""Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin"",\r\n    ""head"": 0,\r\n    ""id"": 2,\r\n    ""lemma"": ""do"",\r\n    ""start_char"": 2,\r\n    ""text"": ""du"",\r\n    ""upos"": ""AUX"",\r\n    ""xpos"": ""VBP""\r\n  },\r\n  \r\n  / ""n"" not linked by multi-word token\r\n  {\r\n    ""deprel"": ""advmod"",\r\n    ""end_char"": 5,\r\n    ""head"": 2,\r\n    ""id"": 3,\r\n    ""lemma"": ""not"",\r\n    ""start_char"": 4,\r\n    ""text"": ""n"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""RB""\r\n  },\r\n  \r\n  // ""no"" is linked by multi-word token\r\n  {\r\n    ""deprel"": ""discourse"",\r\n    ""end_char"": 7,\r\n    ""head"": 2,\r\n    ""id"": 4,\r\n    ""lemma"": ""no"",\r\n    ""start_char"": 5,\r\n    ""text"": ""no"",\r\n    ""upos"": ""INTJ"",\r\n    ""xpos"": ""UH""\r\n  },\r\n]\r\n\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe MWT token links to all of the children tokens it encompasses. id: `[2, 3, 4]`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI\'m not sure if this behaviour is intended or not.  Are the IDs of the MWT token intended to act as a tuple, i.e. a range, or should they include every token that\'s a member of the multi-word token?  If it\'s the latter then I believe this is a bug.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2030787928', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1377#issuecomment-2030787928', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377', 'id': 2030787928, 'node_id': 'IC_kwDOBj_0V855C11Y', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-01T23:30:59Z', 'updated_at': '2024-04-01T23:30:59Z', 'author_association': 'NONE', 'body': ""Thanks for clarifying.  It's fine if it works that way.  I had coded my implementation to assume every id would be linked to by the multi-word token, but it appears that this assumption is wrong.  I've updated my implementation to treat the `id`s of the multiword token as a range.\r\n\r\nThanks for the prompt response!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2030787928/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
57,IssuesEvent,"{'action': 'closed', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1377', 'id': 2219175128, 'node_id': 'I_kwDOBj_0V86ERezY', 'number': 1377, 'title': 'Stanza misses multi-word token id links for ""dunno""', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-04-01T22:18:26Z', 'updated_at': '2024-04-01T23:31:00Z', 'closed_at': '2024-04-01T23:30:59Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWith the introduction of multi-word tokens (MWT) for english, we came across a test case where the tokens of a multi-word token are not linked correctly to associated token ids.  \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nI dunno.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `dunno` reveal that one of the tokens for that word is not linked to by its multi-word token:\r\n\r\n```json\r\n\r\n[\r\n\r\n// multi-word token here for ""dunno""\r\n// id only links to 2 and 4, missing 3  \r\n  {\r\n    ""end_char"": 7,\r\n    ""id"": [\r\n      2,\r\n      4\r\n    ],\r\n    ""misc"": ""SpaceAfter=No"",\r\n    ""start_char"": 2,\r\n    ""text"": ""dunno""\r\n  },\r\n\r\n// ""du"" is linked by this multi-word token\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 4,\r\n    ""feats"": ""Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin"",\r\n    ""head"": 0,\r\n    ""id"": 2,\r\n    ""lemma"": ""do"",\r\n    ""start_char"": 2,\r\n    ""text"": ""du"",\r\n    ""upos"": ""AUX"",\r\n    ""xpos"": ""VBP""\r\n  },\r\n  \r\n  / ""n"" not linked by multi-word token\r\n  {\r\n    ""deprel"": ""advmod"",\r\n    ""end_char"": 5,\r\n    ""head"": 2,\r\n    ""id"": 3,\r\n    ""lemma"": ""not"",\r\n    ""start_char"": 4,\r\n    ""text"": ""n"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""RB""\r\n  },\r\n  \r\n  // ""no"" is linked by multi-word token\r\n  {\r\n    ""deprel"": ""discourse"",\r\n    ""end_char"": 7,\r\n    ""head"": 2,\r\n    ""id"": 4,\r\n    ""lemma"": ""no"",\r\n    ""start_char"": 5,\r\n    ""text"": ""no"",\r\n    ""upos"": ""INTJ"",\r\n    ""xpos"": ""UH""\r\n  },\r\n]\r\n\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe MWT token links to all of the children tokens it encompasses. id: `[2, 3, 4]`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI\'m not sure if this behaviour is intended or not.  Are the IDs of the MWT token intended to act as a tuple, i.e. a range, or should they include every token that\'s a member of the multi-word token?  If it\'s the latter then I believe this is a bug.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}}"
58,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1377', 'id': 2219175128, 'node_id': 'I_kwDOBj_0V86ERezY', 'number': 1377, 'title': 'Stanza misses multi-word token id links for ""dunno""', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-04-01T22:18:26Z', 'updated_at': '2024-04-01T23:27:49Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWith the introduction of multi-word tokens (MWT) for english, we came across a test case where the tokens of a multi-word token are not linked correctly to associated token ids.  \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nI dunno.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `dunno` reveal that one of the tokens for that word is not linked to by its multi-word token:\r\n\r\n```json\r\n\r\n[\r\n\r\n// multi-word token here for ""dunno""\r\n// id only links to 2 and 4, missing 3  \r\n  {\r\n    ""end_char"": 7,\r\n    ""id"": [\r\n      2,\r\n      4\r\n    ],\r\n    ""misc"": ""SpaceAfter=No"",\r\n    ""start_char"": 2,\r\n    ""text"": ""dunno""\r\n  },\r\n\r\n// ""du"" is linked by this multi-word token\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 4,\r\n    ""feats"": ""Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin"",\r\n    ""head"": 0,\r\n    ""id"": 2,\r\n    ""lemma"": ""do"",\r\n    ""start_char"": 2,\r\n    ""text"": ""du"",\r\n    ""upos"": ""AUX"",\r\n    ""xpos"": ""VBP""\r\n  },\r\n  \r\n  / ""n"" not linked by multi-word token\r\n  {\r\n    ""deprel"": ""advmod"",\r\n    ""end_char"": 5,\r\n    ""head"": 2,\r\n    ""id"": 3,\r\n    ""lemma"": ""not"",\r\n    ""start_char"": 4,\r\n    ""text"": ""n"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""RB""\r\n  },\r\n  \r\n  // ""no"" is linked by multi-word token\r\n  {\r\n    ""deprel"": ""discourse"",\r\n    ""end_char"": 7,\r\n    ""head"": 2,\r\n    ""id"": 4,\r\n    ""lemma"": ""no"",\r\n    ""start_char"": 5,\r\n    ""text"": ""no"",\r\n    ""upos"": ""INTJ"",\r\n    ""xpos"": ""UH""\r\n  },\r\n]\r\n\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe MWT token links to all of the children tokens it encompasses. id: `[2, 3, 4]`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI\'m not sure if this behaviour is intended or not.  Are the IDs of the MWT token intended to act as a tuple, i.e. a range, or should they include every token that\'s a member of the multi-word token?  If it\'s the latter then I believe this is a bug.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2030784275', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1377#issuecomment-2030784275', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377', 'id': 2030784275, 'node_id': 'IC_kwDOBj_0V855C08T', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-04-01T23:27:48Z', 'updated_at': '2024-04-01T23:27:48Z', 'author_association': 'COLLABORATOR', 'body': 'If I do this, I get all three `Word`s, so I think it is behaving as expected.  The UD annotation standard is to mark the start and end points (inclusive).  Is there something else you observed that needs fixed?\r\n\r\n```\r\npipe(""I dunno where it went"").sentences[0].tokens[1]\r\n\r\n[\r\n  {\r\n    ""id"": [\r\n      2,\r\n      4\r\n    ],\r\n    ""text"": ""dunno"",\r\n    ""start_char"": 2,\r\n    ""end_char"": 7,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 2,\r\n    ""text"": ""du"",\r\n    ""lemma"": ""do"",\r\n    ""upos"": ""AUX"",\r\n    ""xpos"": ""VBP"",\r\n    ""feats"": ""Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin"",\r\n    ""head"": 0,\r\n    ""deprel"": ""root"",\r\n    ""start_char"": 2,\r\n    ""end_char"": 4\r\n  },\r\n  {\r\n    ""id"": 3,\r\n    ""text"": ""n"",\r\n    ""lemma"": ""not"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""RB"",\r\n    ""head"": 2,\r\n    ""deprel"": ""advmod"",\r\n    ""start_char"": 4,\r\n    ""end_char"": 5\r\n  },\r\n  {\r\n    ""id"": 4,\r\n    ""text"": ""no"",\r\n    ""lemma"": ""no"",\r\n    ""upos"": ""INTJ"",\r\n    ""xpos"": ""UH"",\r\n    ""head"": 2,\r\n    ""deprel"": ""discourse"",\r\n    ""start_char"": 5,\r\n    ""end_char"": 7\r\n  }\r\n]\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2030784275/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
59,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1377', 'id': 2219175128, 'node_id': 'I_kwDOBj_0V86ERezY', 'number': 1377, 'title': 'Stanza misses multi-word token id links for ""dunno""', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-04-01T22:18:26Z', 'updated_at': '2024-04-01T22:18:26Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWith the introduction of multi-word tokens (MWT) for english, we came across a test case where the tokens of a multi-word token are not linked correctly to associated token ids.  \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nI dunno.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `dunno` reveal that one of the tokens for that word is not linked to by its multi-word token:\r\n\r\n```json\r\n\r\n[\r\n\r\n// multi-word token here for ""dunno""\r\n// id only links to 2 and 4, missing 3  \r\n  {\r\n    ""end_char"": 7,\r\n    ""id"": [\r\n      2,\r\n      4\r\n    ],\r\n    ""misc"": ""SpaceAfter=No"",\r\n    ""start_char"": 2,\r\n    ""text"": ""dunno""\r\n  },\r\n\r\n// ""du"" is linked by this multi-word token\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 4,\r\n    ""feats"": ""Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin"",\r\n    ""head"": 0,\r\n    ""id"": 2,\r\n    ""lemma"": ""do"",\r\n    ""start_char"": 2,\r\n    ""text"": ""du"",\r\n    ""upos"": ""AUX"",\r\n    ""xpos"": ""VBP""\r\n  },\r\n  \r\n  / ""n"" not linked by multi-word token\r\n  {\r\n    ""deprel"": ""advmod"",\r\n    ""end_char"": 5,\r\n    ""head"": 2,\r\n    ""id"": 3,\r\n    ""lemma"": ""not"",\r\n    ""start_char"": 4,\r\n    ""text"": ""n"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""RB""\r\n  },\r\n  \r\n  // ""no"" is linked by multi-word token\r\n  {\r\n    ""deprel"": ""discourse"",\r\n    ""end_char"": 7,\r\n    ""head"": 2,\r\n    ""id"": 4,\r\n    ""lemma"": ""no"",\r\n    ""start_char"": 5,\r\n    ""text"": ""no"",\r\n    ""upos"": ""INTJ"",\r\n    ""xpos"": ""UH""\r\n  },\r\n]\r\n\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe MWT token links to all of the children tokens it encompasses. id: `[2, 3, 4]`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI\'m not sure if this behaviour is intended or not.  Are the IDs of the MWT token intended to act as a tuple, i.e. a range, or should they include every token that\'s a member of the multi-word token?  If it\'s the latter then I believe this is a bug.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1377/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
60,WatchEvent,{'action': 'started'}
61,WatchEvent,{'action': 'started'}
62,WatchEvent,{'action': 'started'}
63,PushEvent,"{'repository_id': 104854615, 'push_id': 17773879273, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'e680e5a1f3e8afd07a73e8d8bd2a85a3d820081b', 'before': '309dcc4ef1ab478f30630ed658dc93fe64c2c372', 'commits': [{'sha': 'e680e5a1f3e8afd07a73e8d8bd2a85a3d820081b', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'muril-large also has no length limit', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/e680e5a1f3e8afd07a73e8d8bd2a85a3d820081b'}]}"
64,WatchEvent,{'action': 'started'}
65,WatchEvent,{'action': 'started'}
66,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1376', 'id': 2213807986, 'node_id': 'I_kwDOBj_0V86D9Ady', 'number': 1376, 'title': '[QUESTION] : Named Entity Recognition (NER) on lemmatas / lemmatized words', 'user': {'login': 'sambaPython24', 'id': 60874076, 'node_id': 'MDQ6VXNlcjYwODc0MDc2', 'avatar_url': 'https://avatars.githubusercontent.com/u/60874076?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sambaPython24', 'html_url': 'https://github.com/sambaPython24', 'followers_url': 'https://api.github.com/users/sambaPython24/followers', 'following_url': 'https://api.github.com/users/sambaPython24/following{/other_user}', 'gists_url': 'https://api.github.com/users/sambaPython24/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sambaPython24/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sambaPython24/subscriptions', 'organizations_url': 'https://api.github.com/users/sambaPython24/orgs', 'repos_url': 'https://api.github.com/users/sambaPython24/repos', 'events_url': 'https://api.github.com/users/sambaPython24/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sambaPython24/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-28T17:52:36Z', 'updated_at': '2024-03-28T18:58:48Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hey, thank you for your outstanding work. \r\n\r\nAfter having read both the documentation on [Named Entity Recognition](https://stanfordnlp.github.io/stanza/ner.html) and [Lemmatization](https://stanfordnlp.github.io/stanza/lemma.html), I was wondering wether you \r\ncould do Named Entity Recognition on the lemmatized words. \r\n\r\nAs an alternative, is that possible with the [Client Regex Usage](https://stanfordnlp.github.io/stanza/client_regex.html) ?\r\nThank you\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2025906307', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1376#issuecomment-2025906307', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376', 'id': 2025906307, 'node_id': 'IC_kwDOBj_0V854wOCD', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-28T18:58:46Z', 'updated_at': '2024-03-28T18:58:46Z', 'author_association': 'COLLABORATOR', 'body': ""We don't have a mechanism built in to do that, but you can certainly try passing the lemmas instead of the tokens.  You would just need to copy the mechanism in ner_processor.py"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2025906307/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
67,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1376', 'id': 2213807986, 'node_id': 'I_kwDOBj_0V86D9Ady', 'number': 1376, 'title': '[QUESTION] : Named Entity Recognition (NER) on lemmatas / lemmatized words', 'user': {'login': 'sambaPython24', 'id': 60874076, 'node_id': 'MDQ6VXNlcjYwODc0MDc2', 'avatar_url': 'https://avatars.githubusercontent.com/u/60874076?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sambaPython24', 'html_url': 'https://github.com/sambaPython24', 'followers_url': 'https://api.github.com/users/sambaPython24/followers', 'following_url': 'https://api.github.com/users/sambaPython24/following{/other_user}', 'gists_url': 'https://api.github.com/users/sambaPython24/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sambaPython24/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sambaPython24/subscriptions', 'organizations_url': 'https://api.github.com/users/sambaPython24/orgs', 'repos_url': 'https://api.github.com/users/sambaPython24/repos', 'events_url': 'https://api.github.com/users/sambaPython24/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sambaPython24/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-28T17:52:36Z', 'updated_at': '2024-03-28T17:52:36Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hey, thank you for your outstanding work. \r\n\r\nAfter having read both the documentation on [Named Entity Recognition](https://stanfordnlp.github.io/stanza/ner.html) and [Lemmatization](https://stanfordnlp.github.io/stanza/lemma.html), I was wondering wether you \r\ncould do Named Entity Recognition on the lemmatized words. \r\n\r\nAs an alternative, is that possible with the [Client Regex Usage](https://stanfordnlp.github.io/stanza/client_regex.html) ?\r\nThank you\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1376/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
68,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 9, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-28T10:19:28Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024849610', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2024849610', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2024849610, 'node_id': 'IC_kwDOBj_0V854sMDK', 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-28T10:19:27Z', 'updated_at': '2024-03-28T10:19:27Z', 'author_association': 'NONE', 'body': ""> As for how I would combine them, I'd just add the train sections together, then dev & test on one of the datasets, possible Perseus. For POS, the XPOS tags are clearly incompatible, so I'd train with just the UPOS and features from the other dataset. Not sure how compatible the dependencies would be.\r\n\r\nTechnically, from PROIEL you should only use Sphranztes' _Chronicles_ if you already have Perseus. Or, if you are using this version (https://perseusdl.github.io/treebank_data/), which is a subset of the full Perseus Greek collection, you can also add the _New Testament_. By adding texts that are already in Perseus (Herodotus) you do not only increase the weight of some authors/works in the training data (I am not sure about the concrete effect this has and whether it is positive or negative, I guess it depends on how the trained model is used). But you could end up using as test some data already used for training--not necessarily of course, it depends on how you divide the works between train and test for the two corpora. If that happens, the performance you obtain could be artificially inflated, at least on some tasks. Btw, on which task are you testing? POS-tagging I guess?\r\n\r\nIf you search for more training data, I found some refs to existing treebanks in this paper: https://aclanthology.org/W19-7812.pdf . Moreover, there are the GLAUx trees (https://perseids-publications.github.io/glaux-trees/ , files here: https://github.com/perseids-publications/glaux-trees/tree/master/public/xml) and the Aphtonius trees: https://github.com/polinayordanova/Treebank-of-Aphtonius-Progymnasmata . But I don't know about the compatibility of annotation between different collections (the last two should be AGDT-like), andyou can expect overlap in texts between at least some collections."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024849610/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
69,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 8, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-28T09:49:02Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024794970', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2024794970', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2024794970, 'node_id': 'IC_kwDOBj_0V854r-ta', 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-28T09:49:01Z', 'updated_at': '2024-03-28T09:49:01Z', 'author_association': 'NONE', 'body': ""> Well, don't tell my PI about the few hours I just spent on this :P Hope the demo works as you want... do let us know if there's anything we can do\r\n\r\nYes, after your modification everything works as expected!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024794970/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
70,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 7, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-28T08:56:19Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024701093', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2024701093', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2024701093, 'node_id': 'IC_kwDOBj_0V854rnyl', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-28T08:56:17Z', 'updated_at': '2024-03-28T08:56:17Z', 'author_association': 'COLLABORATOR', 'body': ""As for how I would combine them, I'd just add the train sections together, then dev & test on one of the datasets, possible Perseus.  For POS, the XPOS tags are clearly incompatible, so I'd train with just the UPOS and features from the other dataset.  Not sure how compatible the dependencies would be.\r\n\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024701093/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
71,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-28T07:33:47Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024580440', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2024580440', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2024580440, 'node_id': 'IC_kwDOBj_0V854rKVY', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-28T07:33:46Z', 'updated_at': '2024-03-28T07:33:46Z', 'author_association': 'COLLABORATOR', 'body': ""Well, don't tell my PI about the few hours I just spent on this :P  Hope the demo works as you want... do let us know if there's anything we can do"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024580440/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
72,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-28T07:14:47Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024553826', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2024553826', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2024553826, 'node_id': 'IC_kwDOBj_0V854rD1i', 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-28T07:14:46Z', 'updated_at': '2024-03-28T07:14:46Z', 'author_association': 'NONE', 'body': 'Our model in any case is a lemmatizer for inscriptions, a different kind of data (https://github.com/agile-gronlp/agile). I am not sure abut the improvement we could have with a model with more Herodotus and more New Testament...', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024553826/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
73,WatchEvent,{'action': 'started'}
74,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-28T07:10:28Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024548324', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2024548324', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2024548324, 'node_id': 'IC_kwDOBj_0V854rCfk', 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-28T07:10:27Z', 'updated_at': '2024-03-28T07:10:27Z', 'author_association': 'NONE', 'body': ""Interesting, but were duplicated texts removed? The PROIEL treebank contains three Greek texts: _The Greek New Testament_, Herodotus' _Histories_, and Sphrantzes' _Chronicles_ (see at https://dev.syntacticus.org/proiel.html#contents). Note that the first two works are also contained in Perseus (based on the list of works in the Perseus Digital Library website: https://www.perseus.tufts.edu/hopper/collection%3Fcollection%3DPerseus:collection:Greco-Roman).\r\nSo combining the two training sets means potentially repeating a portion of data twice (and even testing twice? it depends on how the split between train, dev and test is made). I am not sure about the implications of this (overfitting?)..."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2024548324/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
75,WatchEvent,{'action': 'started'}
76,WatchEvent,{'action': 'started'}
77,WatchEvent,{'action': 'started'}
78,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-03-27T19:38:13Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2023822124', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2023822124', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2023822124, 'node_id': 'IC_kwDOBj_0V854oRMs', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-27T19:38:12Z', 'updated_at': '2024-03-27T19:38:12Z', 'author_association': 'NONE', 'body': ""@AngledLuffa  I should note also, this bug is occurring with the constituency parse also.  We get `schoolmaterr` and `'s` as the leaf nodes for `schoolmaster's`"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2023822124/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
79,PushEvent,"{'repository_id': 104854615, 'push_id': 17723899023, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/gh-pages', 'head': '0ebad61a9d9254373d34b98dc1e2558b94f36a23', 'before': 'b7106b71ad17beaf622040ec2f9738de0ba27281', 'commits': [{'sha': '0ebad61a9d9254373d34b98dc1e2558b94f36a23', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'We got rid of the --lang flag as being redundant', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/0ebad61a9d9254373d34b98dc1e2558b94f36a23'}]}"
80,PushEvent,"{'repository_id': 104854615, 'push_id': 17723869977, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '309dcc4ef1ab478f30630ed658dc93fe64c2c372', 'before': '1173416c4081bf380b688ec57813d856a6884ea1', 'commits': [{'sha': '309dcc4ef1ab478f30630ed658dc93fe64c2c372', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Need to use 0000 and 0001 for dev and test now that charlm files have 4 digits', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/309dcc4ef1ab478f30630ed658dc93fe64c2c372'}]}"
81,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-27T05:51:06Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2022005604', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2022005604', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2022005604, 'node_id': 'IC_kwDOBj_0V854hVtk', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-27T05:51:05Z', 'updated_at': '2024-03-27T05:51:05Z', 'author_association': 'COLLABORATOR', 'body': ""Ah, neat.\r\n\r\nI found that the Perseus and Proiel datasets have very similar annotation schemes, to the point that if I combine the two training sets, the combined model does quite well on the two test sets:\r\n\r\n```\r\n            PR dev  PR test    PE dev   PE test\r\nPR           97.34   97.51      75.85    71.31\r\nPE           87.34   87.73      92.31    88.45\r\nCombined     97.01   97.33      92.23    88.58\r\n```\r\n\r\nSo that seems like a simple way to get a noticeable win for your demonstration.  Want that model?\r\n\r\nI can't speak to whether or not the tag & dependency schemes are also the same yet, but they might be"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2022005604/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
82,PushEvent,"{'repository_id': 104854615, 'push_id': 17721461917, 'size': 2, 'distinct_size': 2, 'ref': 'refs/heads/multilingual-coref', 'head': '3380238fb45022b6fdc2d71ec11b683d178e9d24', 'before': '4c8128093113d8156346ac1a306f8be2efdd8058', 'commits': [{'sha': 'a99d83dd26f93539d5a0aa280a0b8e753743236a', 'author': {'email': 'houjun@stanford.edu', 'name': 'Houjun Liu'}, 'message': 'metrics for rough predictor and dummy confusion', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/a99d83dd26f93539d5a0aa280a0b8e753743236a'}, {'sha': '3380238fb45022b6fdc2d71ec11b683d178e9d24', 'author': {'email': 'houjun@stanford.edu', 'name': 'Houjun Liu'}, 'message': ""Merge branch 'multilingual-coref' of github.com:stanfordnlp/stanza into multilingual-coref"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/3380238fb45022b6fdc2d71ec11b683d178e9d24'}]}"
83,WatchEvent,{'action': 'started'}
84,WatchEvent,{'action': 'started'}
85,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1270', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1270/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1270/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1270/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1270', 'id': 1849821337, 'node_id': 'I_kwDOBj_0V85uQgiZ', 'number': 1270, 'title': '[QUESTION] Small Dockerfile', 'user': {'login': 'FSchmidt-FUNKE', 'id': 132065692, 'node_id': 'U_kgDOB98pnA', 'avatar_url': 'https://avatars.githubusercontent.com/u/132065692?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/FSchmidt-FUNKE', 'html_url': 'https://github.com/FSchmidt-FUNKE', 'followers_url': 'https://api.github.com/users/FSchmidt-FUNKE/followers', 'following_url': 'https://api.github.com/users/FSchmidt-FUNKE/following{/other_user}', 'gists_url': 'https://api.github.com/users/FSchmidt-FUNKE/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/FSchmidt-FUNKE/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/FSchmidt-FUNKE/subscriptions', 'organizations_url': 'https://api.github.com/users/FSchmidt-FUNKE/orgs', 'repos_url': 'https://api.github.com/users/FSchmidt-FUNKE/repos', 'events_url': 'https://api.github.com/users/FSchmidt-FUNKE/events{/privacy}', 'received_events_url': 'https://api.github.com/users/FSchmidt-FUNKE/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}, {'id': 2618823935, 'node_id': 'MDU6TGFiZWwyNjE4ODIzOTM1', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/stale', 'name': 'stale', 'color': 'ffffff', 'default': False, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2023-08-14T13:53:02Z', 'updated_at': '2024-03-26T09:02:50Z', 'closed_at': '2023-12-15T08:10:13Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi everyone,\r\n\r\nI want to build a service that uses stanza, but our Cloud Foundry provider only allows a maximum Docker container size of 4GB. Because torch is a huge package, which installs many cuda libaries etc. the container with the german model is over 6GB.\r\n\r\n```\r\n# syntax=docker/dockerfile:1\r\nFROM python:3.11-slim-bullseye\r\n\r\nRUN apt-get update && \\\r\n    apt-get upgrade -y && \\\r\n    apt install sudo -y\r\n\r\nRUN useradd -ms /bin/bash worker\r\nUSER worker\r\nWORKDIR /home/worker\r\nENV PATH=""/home/worker/.local/bin:${PATH}""\r\n\r\nRUN python -m pip install --no-cache-dir --user --disable-pip-version-check --upgrade pip\r\nCOPY requirements.txt requirements.txt\r\nRUN pip install --no-cache-dir --user -r requirements.txt\r\n\r\nCOPY download_model.py download_model.py\r\nRUN python3 download_model.py\r\n\r\nCOPY app.py app.py\r\nCOPY app_logic.py app_logic.py\r\nCOPY utils.py utils.py\r\n\r\nEXPOSE 8080\r\nCMD [""gunicorn"", ""--bind"", "":8080"", ""--workers"", ""1"", ""--worker-class"", ""uvicorn.workers.UvicornWorker"", ""--threads"", ""10"", ""--timeout"", ""3600"", ""app:app""]\r\n```\r\n\r\nI only need a CPU version and I only want to do inference. Stanza provides better results for my use case than spaCy, but currently I\'m forced to use spaCy, because the container is only 1.5GB with the de_core_news_lg model. Is there a smaller CPU inference version or a DockerFile that builds a smaller image for inference on CPU?', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1270/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1270/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2019873295', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1270#issuecomment-2019873295', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1270', 'id': 2019873295, 'node_id': 'IC_kwDOBj_0V854ZNIP', 'user': {'login': 'msehnout', 'id': 9369632, 'node_id': 'MDQ6VXNlcjkzNjk2MzI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/9369632?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/msehnout', 'html_url': 'https://github.com/msehnout', 'followers_url': 'https://api.github.com/users/msehnout/followers', 'following_url': 'https://api.github.com/users/msehnout/following{/other_user}', 'gists_url': 'https://api.github.com/users/msehnout/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/msehnout/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/msehnout/subscriptions', 'organizations_url': 'https://api.github.com/users/msehnout/orgs', 'repos_url': 'https://api.github.com/users/msehnout/repos', 'events_url': 'https://api.github.com/users/msehnout/events{/privacy}', 'received_events_url': 'https://api.github.com/users/msehnout/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-26T09:02:50Z', 'updated_at': '2024-03-26T09:02:50Z', 'author_association': 'NONE', 'body': 'Hi,\r\n\r\nI also stumbled upon this issue. My project uses https://python-poetry.org/ as a package manager and I found solution that worked for me here: https://github.com/python-poetry/poetry/issues/7685#issuecomment-1632693935\r\n\r\n```\r\npoetry add --source pytorch-cpu torch\r\npoetry source add --priority=explicit pytorch-cpu https://download.pytorch.org/whl/cpu\r\n```\r\n\r\nMaybe this helps someone else as well :) ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2019873295/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
86,PullRequestEvent,"{'action': 'opened', 'number': 1375, 'pull_request': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375', 'id': 1790881730, 'node_id': 'PR_kwDOBj_0V85qvq_C', 'html_url': 'https://github.com/stanfordnlp/stanza/pull/1375', 'diff_url': 'https://github.com/stanfordnlp/stanza/pull/1375.diff', 'patch_url': 'https://github.com/stanfordnlp/stanza/pull/1375.patch', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375', 'number': 1375, 'state': 'open', 'locked': False, 'title': 'Make a variant of the dataloader which limits a batch to 5000 words o…', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'body': 'Make a variant of the dataloader which limits a batch to 5000 words or less (by default) for the Pipeline.  Should help avoid OOM for things such as a few very long sentence soaking up too many resources.  https://github.com/stanfordnlp/stanza/issues/1372\r\n', 'created_at': '2024-03-26T06:40:55Z', 'updated_at': '2024-03-26T06:40:55Z', 'closed_at': None, 'merged_at': None, 'merge_commit_sha': None, 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/commits', 'review_comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/comments', 'review_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375/comments', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/199987855d5b148fc6cb9934f94f9e894edfd427', 'head': {'label': 'stanfordnlp:pos_dataloader_maxlen', 'ref': 'pos_dataloader_maxlen', 'sha': '199987855d5b148fc6cb9934f94f9e894edfd427', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-03-25T22:36:46Z', 'pushed_at': '2024-03-26T06:40:55Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85262, 'stargazers_count': 7006, 'watchers_count': 7006, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 96, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 96, 'watchers': 7006, 'default_branch': 'main'}}, 'base': {'label': 'stanfordnlp:dev', 'ref': 'dev', 'sha': '1173416c4081bf380b688ec57813d856a6884ea1', 'user': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 104854615, 'node_id': 'MDEwOlJlcG9zaXRvcnkxMDQ4NTQ2MTU=', 'name': 'stanza', 'full_name': 'stanfordnlp/stanza', 'private': False, 'owner': {'login': 'stanfordnlp', 'id': 3046006, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjMwNDYwMDY=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3046006?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stanfordnlp', 'html_url': 'https://github.com/stanfordnlp', 'followers_url': 'https://api.github.com/users/stanfordnlp/followers', 'following_url': 'https://api.github.com/users/stanfordnlp/following{/other_user}', 'gists_url': 'https://api.github.com/users/stanfordnlp/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stanfordnlp/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stanfordnlp/subscriptions', 'organizations_url': 'https://api.github.com/users/stanfordnlp/orgs', 'repos_url': 'https://api.github.com/users/stanfordnlp/repos', 'events_url': 'https://api.github.com/users/stanfordnlp/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stanfordnlp/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/stanfordnlp/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza', 'forks_url': 'https://api.github.com/repos/stanfordnlp/stanza/forks', 'keys_url': 'https://api.github.com/repos/stanfordnlp/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/stanfordnlp/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/stanfordnlp/stanza/teams', 'hooks_url': 'https://api.github.com/repos/stanfordnlp/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/events', 'assignees_url': 'https://api.github.com/repos/stanfordnlp/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/stanfordnlp/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/tags', 'blobs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/stanfordnlp/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/stanfordnlp/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/stanfordnlp/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/stanfordnlp/stanza/subscription', 'commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/stanfordnlp/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/stanfordnlp/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/stanfordnlp/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/stanfordnlp/stanza/merges', 'archive_url': 'https://api.github.com/repos/stanfordnlp/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/stanfordnlp/stanza/downloads', 'issues_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/stanfordnlp/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/stanfordnlp/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/stanfordnlp/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/stanfordnlp/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/stanfordnlp/stanza/deployments', 'created_at': '2017-09-26T08:00:56Z', 'updated_at': '2024-03-25T22:36:46Z', 'pushed_at': '2024-03-26T06:40:55Z', 'git_url': 'git://github.com/stanfordnlp/stanza.git', 'ssh_url': 'git@github.com:stanfordnlp/stanza.git', 'clone_url': 'https://github.com/stanfordnlp/stanza.git', 'svn_url': 'https://github.com/stanfordnlp/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85262, 'stargazers_count': 7006, 'watchers_count': 7006, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': True, 'has_discussions': True, 'forks_count': 872, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 96, 'license': {'key': 'other', 'name': 'Other', 'spdx_id': 'NOASSERTION', 'url': None, 'node_id': 'MDc6TGljZW5zZTA='}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': ['artificial-intelligence', 'corenlp', 'deep-learning', 'machine-learning', 'named-entity-recognition', 'natural-language-processing', 'nlp', 'python', 'pytorch', 'universal-dependencies'], 'visibility': 'public', 'forks': 872, 'open_issues': 96, 'watchers': 7006, 'default_branch': 'main'}}, '_links': {'self': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375'}, 'html': {'href': 'https://github.com/stanfordnlp/stanza/pull/1375'}, 'issue': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375'}, 'comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1375/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/pulls/1375/commits'}, 'statuses': {'href': 'https://api.github.com/repos/stanfordnlp/stanza/statuses/199987855d5b148fc6cb9934f94f9e894edfd427'}}, 'author_association': 'COLLABORATOR', 'auto_merge': None, 'active_lock_reason': None, 'merged': False, 'mergeable': None, 'rebaseable': None, 'mergeable_state': 'unknown', 'merged_by': None, 'comments': 0, 'review_comments': 0, 'maintainer_can_modify': False, 'commits': 1, 'additions': 166, 'deletions': 1, 'changed_files': 4}}"
87,CreateEvent,"{'ref': 'pos_dataloader_maxlen', 'ref_type': 'branch', 'master_branch': 'main', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'pusher_type': 'user'}"
88,WatchEvent,{'action': 'started'}
89,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-25T20:49:58Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018890073', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2018890073', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2018890073, 'node_id': 'IC_kwDOBj_0V854VdFZ', 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T20:49:58Z', 'updated_at': '2024-03-25T20:49:58Z', 'author_association': 'NONE', 'body': 'Great, thanks, it works now! With a colleague we are presenting a Stanza-based lemmatizer for Ancient Greek next week, so we are relieved to see it working again :)', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018890073/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
90,IssuesEvent,"{'action': 'closed', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-25T20:49:58Z', 'closed_at': '2024-03-25T20:49:57Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}}"
91,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372', 'id': 2200346640, 'node_id': 'I_kwDOBj_0V86DJqAQ', 'number': 1372, 'title': 'CUDA OutOfMemory Error with bulk_process()', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-21T14:27:41Z', 'updated_at': '2024-03-25T17:58:15Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m receiving a CUDA OutOfMemory error when running relatively small batches of IMDB reviews. Curiously, the torch CUDA OOM error seems to be trying to allocate very small (e.g., 216MB) reservations when this happens. I\'m running this on my university\'s HPC: [Quartz with a V100](https://kb.iu.edu/d/avjk) reserved to just my program, so it should be able to reserve up to 32GB of GPU memory.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Obtain dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n2. Sample 3,000 texts from dataset\r\n3. Spin up preprocessing pipeline \r\n```python\r\nnlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=True, use_gpu=True, pos_batch_size=1000)\r\n```\r\n4. Attempt to preprocess text in batches: \r\n\r\n```python\r\n    # Update the pandas dataframe with the tokenized reviews\r\n    preprocessed = []\r\n    chunks = np.array_split(test_data[\'review\'], len(test_data)//10)\r\n    print(""Preprocessing texts - this may take a while"")\r\n    for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n        torch.cuda.empty_cache()\r\n        results = nlp.bulk_process(chunk)\r\n        for doc in results:\r\n            preprocessed.append(\r\n                [word.text.lower()\r\n                 for sentence in doc.sentences\r\n                 for word in sentence.words\r\n                 if word.upos not in [""PUNCT"", ""SYM"", ""NUM"", \'X\']\r\n                 and word.text.lower() not in stops\r\n                ]\r\n            )\r\n    test_data[\'review_tokens\'] = preprocessed\r\n```\r\n5. See error\r\n```\r\nPreprocessing texts - this may take a while\r\nChunks of 10 texts:  20%|███████████████████████████████████▏                                                                                                                                               | 59/300 [00:40<02:45,  1.46it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 181, in forward\r\n    all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 31.74 GiB total capacity; 672.50 MiB already allocated; 81.12 MiB free; 948.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\n**Expected behavior**\r\nI expected for the code to complete and the preprocessed text to be in a list for reincorporation into a Pandas DataFrame\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux \r\n - Python version: 3.10\r\n - Stanza version: 1.7.0\r\n\r\n**Additional context**\r\n* I\'m not running the CoreNLP Java server - just out-of-the-box Stanza\r\n* I\'ve tried with other batch sizes and get the same outcome.\r\n* I\'ve been working with the HPC team at my university - some other things we have tried\r\n    * The `torch.cuda.empty_cache()` was added - it didn\'t change the outcome, this error occurs with or without that line\r\n    * Added `os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:734""`\r\n    * This originally occured in a Jupyter Lab notebook - I got the error there and thinking that it may be because I was using it in an interactive session tried it again in a .py file - the error replicates in both contexts\r\n    * Examining `nvidia-smi` before/after running in Jupyter:\r\n        * Before: Tesla V100-PCIE-32GB - GPU Memory Usage: 0\r\n        * After: Tesla V100-PCIE-32GB - GPU Memory Usage: 32412MiB / 32768 MiB\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018586864', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372#issuecomment-2018586864', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'id': 2018586864, 'node_id': 'IC_kwDOBj_0V854UTDw', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T17:58:15Z', 'updated_at': '2024-03-25T17:58:15Z', 'author_association': 'NONE', 'body': ""Thanks @AngledLuffa!\r\n\r\nOne of our HPC folks actually figured out what was going on.  I was using Tensorflow earlier in the program and apparently the garbage collector wasn't clearing up the GPU memory before Torch was trying to grab it.\r\n\r\nAdding this code before the `bulk_process` method fixed the issue:\r\n```python\r\nimport gc\r\nfrom numba import cuda\r\ngc.collect()\r\ncuda.select_device(0)\r\ndevice = cuda.get_current_device()\r\ndevice.reset()\r\n```\r\n\r\nFrustrating that TF and Torch can't learn to coexist - but hey - found a workaround!\r\n\r\nI'm OK with closing this issue - but don't know if you want me to leave it open to attach the feature you're working on to."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018586864/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
92,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372', 'id': 2200346640, 'node_id': 'I_kwDOBj_0V86DJqAQ', 'number': 1372, 'title': 'CUDA OutOfMemory Error with bulk_process()', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-21T14:27:41Z', 'updated_at': '2024-03-25T15:23:12Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m receiving a CUDA OutOfMemory error when running relatively small batches of IMDB reviews. Curiously, the torch CUDA OOM error seems to be trying to allocate very small (e.g., 216MB) reservations when this happens. I\'m running this on my university\'s HPC: [Quartz with a V100](https://kb.iu.edu/d/avjk) reserved to just my program, so it should be able to reserve up to 32GB of GPU memory.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Obtain dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n2. Sample 3,000 texts from dataset\r\n3. Spin up preprocessing pipeline \r\n```python\r\nnlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=True, use_gpu=True, pos_batch_size=1000)\r\n```\r\n4. Attempt to preprocess text in batches: \r\n\r\n```python\r\n    # Update the pandas dataframe with the tokenized reviews\r\n    preprocessed = []\r\n    chunks = np.array_split(test_data[\'review\'], len(test_data)//10)\r\n    print(""Preprocessing texts - this may take a while"")\r\n    for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n        torch.cuda.empty_cache()\r\n        results = nlp.bulk_process(chunk)\r\n        for doc in results:\r\n            preprocessed.append(\r\n                [word.text.lower()\r\n                 for sentence in doc.sentences\r\n                 for word in sentence.words\r\n                 if word.upos not in [""PUNCT"", ""SYM"", ""NUM"", \'X\']\r\n                 and word.text.lower() not in stops\r\n                ]\r\n            )\r\n    test_data[\'review_tokens\'] = preprocessed\r\n```\r\n5. See error\r\n```\r\nPreprocessing texts - this may take a while\r\nChunks of 10 texts:  20%|███████████████████████████████████▏                                                                                                                                               | 59/300 [00:40<02:45,  1.46it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 181, in forward\r\n    all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 31.74 GiB total capacity; 672.50 MiB already allocated; 81.12 MiB free; 948.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\n**Expected behavior**\r\nI expected for the code to complete and the preprocessed text to be in a list for reincorporation into a Pandas DataFrame\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux \r\n - Python version: 3.10\r\n - Stanza version: 1.7.0\r\n\r\n**Additional context**\r\n* I\'m not running the CoreNLP Java server - just out-of-the-box Stanza\r\n* I\'ve tried with other batch sizes and get the same outcome.\r\n* I\'ve been working with the HPC team at my university - some other things we have tried\r\n    * The `torch.cuda.empty_cache()` was added - it didn\'t change the outcome, this error occurs with or without that line\r\n    * Added `os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:734""`\r\n    * This originally occured in a Jupyter Lab notebook - I got the error there and thinking that it may be because I was using it in an interactive session tried it again in a .py file - the error replicates in both contexts\r\n    * Examining `nvidia-smi` before/after running in Jupyter:\r\n        * Before: Tesla V100-PCIE-32GB - GPU Memory Usage: 0\r\n        * After: Tesla V100-PCIE-32GB - GPU Memory Usage: 32412MiB / 32768 MiB\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018265643', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372#issuecomment-2018265643', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'id': 2018265643, 'node_id': 'IC_kwDOBj_0V854TEor', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T15:23:12Z', 'updated_at': '2024-03-25T15:23:12Z', 'author_association': 'COLLABORATOR', 'body': ""My experience has been that with the smaller batch size, it is able to free up the memory used by the previous batch(es) in time to allocate the newer batch.  It's possible there are hardwares or drivers where that doesn't work, though.  When I read these files, I sorted them to make it easier to identify the file in question with the very long sentence... perhaps you could do something similar and see if there's another underlying issue?  Meanwhile, I hope that today I'll be able to add a feature where it is a little more cautious about processing overly long sentences together in giant batches."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018265643/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
93,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-25T14:57:11Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/silvia/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/silvia/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\nIt does not happen if I install stanza 1.2 instead.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018198454', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2018198454', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2018198454, 'node_id': 'IC_kwDOBj_0V854S0O2', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T14:57:09Z', 'updated_at': '2024-03-25T14:57:09Z', 'author_association': 'COLLABORATOR', 'body': 'Ah, sorry for the inconvenience.  I had updated the POS models to use a\nsmaller batch size to better reflect some code changes we made in the\nprevious version, and I had forgotten to push the updated manifest with the\nmd5 sums for the new models.  It should be fixed now.\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2018198454/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
94,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-25T13:18:39Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/my_name/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/my_name/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/my_name/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017992530', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374#issuecomment-2017992530', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'id': 2017992530, 'node_id': 'IC_kwDOBj_0V854SB9S', 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T13:18:38Z', 'updated_at': '2024-03-25T13:18:38Z', 'author_association': 'NONE', 'body': 'It does not happen if I install stanza 1.2 instead', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017992530/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
95,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1374', 'id': 2205741052, 'node_id': 'I_kwDOBj_0V86DeO_8', 'number': 1374, 'title': ""ValueError: mismatching md5 value when downloading 'grc' model"", 'user': {'login': 'silvia-stopponi', 'id': 75065892, 'node_id': 'MDQ6VXNlcjc1MDY1ODky', 'avatar_url': 'https://avatars.githubusercontent.com/u/75065892?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/silvia-stopponi', 'html_url': 'https://github.com/silvia-stopponi', 'followers_url': 'https://api.github.com/users/silvia-stopponi/followers', 'following_url': 'https://api.github.com/users/silvia-stopponi/following{/other_user}', 'gists_url': 'https://api.github.com/users/silvia-stopponi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/silvia-stopponi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/silvia-stopponi/subscriptions', 'organizations_url': 'https://api.github.com/users/silvia-stopponi/orgs', 'repos_url': 'https://api.github.com/users/silvia-stopponi/repos', 'events_url': 'https://api.github.com/users/silvia-stopponi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/silvia-stopponi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-25T13:13:58Z', 'updated_at': '2024-03-25T13:13:58Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWhen I download the \'grc\' model the download reaches 100%, but I get the following error:\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Open the Python interpreter\r\n```\r\nimport stanza\r\nstanza.download(\'grc\')\r\n\r\nDownloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 16.8MB/s]\r\n2024-03-25 14:00:37 INFO: Downloaded file to /home/silvia/stanza_resources/resources.json\r\n2024-03-25 14:00:37 INFO: Downloading default packages for language: grc (Ancient_Greek) ...\r\nDownloading https://huggingface.co/stanfordnlp/stanza-grc/resolve/v1.8.0/models/default.zip: 100%|█| 121M/121M [00:14<0\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/my_name/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 594, in download\r\n    request_file(\r\n  File ""/home/my_name/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 154, in request_file\r\n    assert_file_exists(path, md5, alternate_md5)\r\n  File ""/home/my_name/stanza-testenv/lib/python3.8/site-packages/stanza/resources/common.py"", line 107, in assert_file_exists\r\n    raise ValueError(""md5 for %s is %s, expected %s"" % (path, file_md5, md5))\r\nValueError: md5 for /home/my_name/stanza_resources/grc/default.zip is 7c3562a76f82045c92e8216c68ee00a0, expected 9855292e615b94b30581504c2941a96a\r\n```\r\n\r\n**Expected behavior**\r\nNo error at the end of download.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu for Windows\r\n - Python version: 3.8.10\r\n - Stanza version: 1.8.1\r\n\r\n**Additional context**\r\nI installed everything in a virtual environment. But I get the same problem in a Google Colab notebook.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1374/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
96,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372', 'id': 2200346640, 'node_id': 'I_kwDOBj_0V86DJqAQ', 'number': 1372, 'title': 'CUDA OutOfMemory Error with bulk_process()', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-21T14:27:41Z', 'updated_at': '2024-03-25T13:05:15Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m receiving a CUDA OutOfMemory error when running relatively small batches of IMDB reviews. Curiously, the torch CUDA OOM error seems to be trying to allocate very small (e.g., 216MB) reservations when this happens. I\'m running this on my university\'s HPC: [Quartz with a V100](https://kb.iu.edu/d/avjk) reserved to just my program, so it should be able to reserve up to 32GB of GPU memory.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Obtain dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n2. Sample 3,000 texts from dataset\r\n3. Spin up preprocessing pipeline \r\n```python\r\nnlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=True, use_gpu=True, pos_batch_size=1000)\r\n```\r\n4. Attempt to preprocess text in batches: \r\n\r\n```python\r\n    # Update the pandas dataframe with the tokenized reviews\r\n    preprocessed = []\r\n    chunks = np.array_split(test_data[\'review\'], len(test_data)//10)\r\n    print(""Preprocessing texts - this may take a while"")\r\n    for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n        torch.cuda.empty_cache()\r\n        results = nlp.bulk_process(chunk)\r\n        for doc in results:\r\n            preprocessed.append(\r\n                [word.text.lower()\r\n                 for sentence in doc.sentences\r\n                 for word in sentence.words\r\n                 if word.upos not in [""PUNCT"", ""SYM"", ""NUM"", \'X\']\r\n                 and word.text.lower() not in stops\r\n                ]\r\n            )\r\n    test_data[\'review_tokens\'] = preprocessed\r\n```\r\n5. See error\r\n```\r\nPreprocessing texts - this may take a while\r\nChunks of 10 texts:  20%|███████████████████████████████████▏                                                                                                                                               | 59/300 [00:40<02:45,  1.46it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 181, in forward\r\n    all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 31.74 GiB total capacity; 672.50 MiB already allocated; 81.12 MiB free; 948.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\n**Expected behavior**\r\nI expected for the code to complete and the preprocessed text to be in a list for reincorporation into a Pandas DataFrame\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux \r\n - Python version: 3.10\r\n - Stanza version: 1.7.0\r\n\r\n**Additional context**\r\n* I\'m not running the CoreNLP Java server - just out-of-the-box Stanza\r\n* I\'ve tried with other batch sizes and get the same outcome.\r\n* I\'ve been working with the HPC team at my university - some other things we have tried\r\n    * The `torch.cuda.empty_cache()` was added - it didn\'t change the outcome, this error occurs with or without that line\r\n    * Added `os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:734""`\r\n    * This originally occured in a Jupyter Lab notebook - I got the error there and thinking that it may be because I was using it in an interactive session tried it again in a .py file - the error replicates in both contexts\r\n    * Examining `nvidia-smi` before/after running in Jupyter:\r\n        * Before: Tesla V100-PCIE-32GB - GPU Memory Usage: 0\r\n        * After: Tesla V100-PCIE-32GB - GPU Memory Usage: 32412MiB / 32768 MiB\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017967832', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372#issuecomment-2017967832', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'id': 2017967832, 'node_id': 'IC_kwDOBj_0V854R77Y', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T13:05:15Z', 'updated_at': '2024-03-25T13:05:15Z', 'author_association': 'NONE', 'body': 'Good catch - I was actually playing around with that as well - when I run it with `tokenize_no_ssplit=False` I get the same error.\r\n\r\nI just tried again with `nlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=False, use_gpu=True, pos_batch_size=100)` and get the following error:\r\n\r\n```\r\nChunks of 10 texts:   1%|▎                      | 4/300 [00:03<03:50,  1.28it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 175, in forward\r\n    all_forward_chars = self.charmodel_forward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 31.74 GiB total capacity; 787.23 MiB already allocated; 153.12 MiB free; 874.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017967832/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
97,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370', 'id': 2194529214, 'node_id': 'I_kwDOBj_0V86Czdu-', 'number': 1370, 'title': 'Italian model runs out of memory on COLAB A100 gpu', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-19T10:04:36Z', 'updated_at': '2024-03-25T09:10:47Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""\r\n\r\nTrying to process small texts  (300-500kb) on a 40gb GPU on colab raises an OutOfMemoryError. Here is log. The English model, on the same text, does not. It happens even with processors='tokenize,lemma,pos'.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n<ipython-input-4-e215e2dbfcd3> in <cell line: 2>()\r\n      1 s = open('KINGSLEY_TIASPETTOACENTRALPARL.txt').read()\r\n----> 2 doc = nlp(s)\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\r\n    114 \r\n    115     def forward(self, input: Tensor) -> Tensor:\r\n--> 116         return F.linear(input, self.weight, self.bias)\r\n    117 \r\n    118     def extra_repr(self) -> str:\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free. Process 3295 has 10.35 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 37.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017525365', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370#issuecomment-2017525365', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'id': 2017525365, 'node_id': 'IC_kwDOBj_0V854QP51', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T09:10:47Z', 'updated_at': '2024-03-25T09:10:47Z', 'author_association': 'NONE', 'body': ""Yes, that solved the issue. I feel stupid for not thinking about it, but that's a lesson learned. Thanks a lot."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017525365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
98,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372', 'id': 2200346640, 'node_id': 'I_kwDOBj_0V86DJqAQ', 'number': 1372, 'title': 'CUDA OutOfMemory Error with bulk_process()', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-21T14:27:41Z', 'updated_at': '2024-03-25T08:22:17Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m receiving a CUDA OutOfMemory error when running relatively small batches of IMDB reviews. Curiously, the torch CUDA OOM error seems to be trying to allocate very small (e.g., 216MB) reservations when this happens. I\'m running this on my university\'s HPC: [Quartz with a V100](https://kb.iu.edu/d/avjk) reserved to just my program, so it should be able to reserve up to 32GB of GPU memory.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Obtain dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n2. Sample 3,000 texts from dataset\r\n3. Spin up preprocessing pipeline \r\n```python\r\nnlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=True, use_gpu=True, pos_batch_size=1000)\r\n```\r\n4. Attempt to preprocess text in batches: \r\n\r\n```python\r\n    # Update the pandas dataframe with the tokenized reviews\r\n    preprocessed = []\r\n    chunks = np.array_split(test_data[\'review\'], len(test_data)//10)\r\n    print(""Preprocessing texts - this may take a while"")\r\n    for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n        torch.cuda.empty_cache()\r\n        results = nlp.bulk_process(chunk)\r\n        for doc in results:\r\n            preprocessed.append(\r\n                [word.text.lower()\r\n                 for sentence in doc.sentences\r\n                 for word in sentence.words\r\n                 if word.upos not in [""PUNCT"", ""SYM"", ""NUM"", \'X\']\r\n                 and word.text.lower() not in stops\r\n                ]\r\n            )\r\n    test_data[\'review_tokens\'] = preprocessed\r\n```\r\n5. See error\r\n```\r\nPreprocessing texts - this may take a while\r\nChunks of 10 texts:  20%|███████████████████████████████████▏                                                                                                                                               | 59/300 [00:40<02:45,  1.46it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 181, in forward\r\n    all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 31.74 GiB total capacity; 672.50 MiB already allocated; 81.12 MiB free; 948.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\n**Expected behavior**\r\nI expected for the code to complete and the preprocessed text to be in a list for reincorporation into a Pandas DataFrame\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux \r\n - Python version: 3.10\r\n - Stanza version: 1.7.0\r\n\r\n**Additional context**\r\n* I\'m not running the CoreNLP Java server - just out-of-the-box Stanza\r\n* I\'ve tried with other batch sizes and get the same outcome.\r\n* I\'ve been working with the HPC team at my university - some other things we have tried\r\n    * The `torch.cuda.empty_cache()` was added - it didn\'t change the outcome, this error occurs with or without that line\r\n    * Added `os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:734""`\r\n    * This originally occured in a Jupyter Lab notebook - I got the error there and thinking that it may be because I was using it in an interactive session tried it again in a .py file - the error replicates in both contexts\r\n    * Examining `nvidia-smi` before/after running in Jupyter:\r\n        * Before: Tesla V100-PCIE-32GB - GPU Memory Usage: 0\r\n        * After: Tesla V100-PCIE-32GB - GPU Memory Usage: 32412MiB / 32768 MiB\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017449459', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372#issuecomment-2017449459', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'id': 2017449459, 'node_id': 'IC_kwDOBj_0V854P9Xz', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-25T08:22:16Z', 'updated_at': '2024-03-25T08:22:16Z', 'author_association': 'COLLABORATOR', 'body': ""I spent some time verifying these things, and I think the POS only puts things on the GPU when it's about to process it.  Indeed, the problem seems to be somewhere in a specific document range, rather than running out of GPU over a long run.  If I run with a batch size of 250, it dies when processing a specific file from the training set (`aclImdb/train/pos/10044_9.txt`), even if I only process that chunk.  It works with a smaller batch size, though.\r\n\r\nI don't think the `tokenize_no_ssplit` is correct.  It definitely looks like that file has a bunch of separate sentences in it.  What happens is the file gets tokenized into a single sentence of ~2000 words, then everything batched with it at once becomes too memory intensive.\r\n\r\nArguably the POS should be able to handle that gracefully...  there are a few options I can think of which might help"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2017449459/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
99,WatchEvent,{'action': 'started'}
100,WatchEvent,{'action': 'started'}
101,PushEvent,"{'repository_id': 104854615, 'push_id': 17684722485, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '4c8128093113d8156346ac1a306f8be2efdd8058', 'before': '1edd34af4f4246361e0ad886f09159780d973114', 'commits': [{'sha': '4c8128093113d8156346ac1a306f8be2efdd8058', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': ""don't call main by default"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4c8128093113d8156346ac1a306f8be2efdd8058'}]}"
102,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 12, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-24T18:59:08Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016906295', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2016906295', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2016906295, 'node_id': 'IC_kwDOBj_0V854N4w3', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-24T18:59:08Z', 'updated_at': '2024-03-24T18:59:08Z', 'author_association': 'NONE', 'body': 'What I have learned over the last few weeks, one may need to go deeper into the source and how the training is done.  Each approach seems to have perhaps more success with one case, while another is better with another case. I see in many ways the merits of how Stanza is approaching the subject.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016906295/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
103,PushEvent,"{'repository_id': 104854615, 'push_id': 17679149286, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '1edd34af4f4246361e0ad886f09159780d973114', 'before': '2cf7084d44888db24f01e96be3a079541d751537', 'commits': [{'sha': '1edd34af4f4246361e0ad886f09159780d973114', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'concatenating converter for multilingual', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/1edd34af4f4246361e0ad886f09159780d973114'}]}"
104,PushEvent,"{'repository_id': 104854615, 'push_id': 17678190082, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '1173416c4081bf380b688ec57813d856a6884ea1', 'before': 'b22ad91871a16e3fcea09bbe6db5603a34c5775f', 'commits': [{'sha': '1173416c4081bf380b688ec57813d856a6884ea1', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a note on roberta-base', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/1173416c4081bf380b688ec57813d856a6884ea1'}]}"
105,WatchEvent,{'action': 'started'}
106,PushEvent,"{'repository_id': 104854615, 'push_id': 17675798481, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '2cf7084d44888db24f01e96be3a079541d751537', 'before': 'bef78a9077f4178f1477c174ea2bb0f71d2a62fd', 'commits': [{'sha': '2cf7084d44888db24f01e96be3a079541d751537', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'make rough scorer return all scores', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/2cf7084d44888db24f01e96be3a079541d751537'}]}"
107,WatchEvent,{'action': 'started'}
108,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1373', 'id': 2203742630, 'node_id': 'I_kwDOBj_0V86DWnGm', 'number': 1373, 'title': 'when i', 'user': {'login': 'xy1137030414', 'id': 73565715, 'node_id': 'MDQ6VXNlcjczNTY1NzE1', 'avatar_url': 'https://avatars.githubusercontent.com/u/73565715?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/xy1137030414', 'html_url': 'https://github.com/xy1137030414', 'followers_url': 'https://api.github.com/users/xy1137030414/followers', 'following_url': 'https://api.github.com/users/xy1137030414/following{/other_user}', 'gists_url': 'https://api.github.com/users/xy1137030414/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/xy1137030414/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/xy1137030414/subscriptions', 'organizations_url': 'https://api.github.com/users/xy1137030414/orgs', 'repos_url': 'https://api.github.com/users/xy1137030414/repos', 'events_url': 'https://api.github.com/users/xy1137030414/events{/privacy}', 'received_events_url': 'https://api.github.com/users/xy1137030414/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-23T07:10:53Z', 'updated_at': '2024-03-23T07:10:53Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Before you start, make sure to check out:\r\n* Our documentation: https://stanfordnlp.github.io/stanza/\r\n* Our FAQ: https://stanfordnlp.github.io/stanza/faq.html\r\n* Github issues (especially closed ones)\r\nYour question might have an answer in these places!\r\n\r\nIf you still couldn't find the answer to your question, feel free to delete this text and write down your question. The more information you provide with your question, the faster we will be able to help you!\r\n\r\nIf you have a question about an issue you're facing when using Stanza, please try to provide a detailed step-by-step guide to reproduce the issue you're facing. Try to at least provide a minimal code sample to reproduce the problem you are facing, instead of just describing it. That would greatly help us in locating the issue faster and help you resolve it!\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1373/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
109,PushEvent,"{'repository_id': 104854615, 'push_id': 17671484823, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'b22ad91871a16e3fcea09bbe6db5603a34c5775f', 'before': '14989cb20949f46e74e2874e045542968d90648c', 'commits': [{'sha': 'b22ad91871a16e3fcea09bbe6db5603a34c5775f', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""Don't print an error that makes no sense"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/b22ad91871a16e3fcea09bbe6db5603a34c5775f'}]}"
110,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372', 'id': 2200346640, 'node_id': 'I_kwDOBj_0V86DJqAQ', 'number': 1372, 'title': 'CUDA OutOfMemory Error with bulk_process()', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-21T14:27:41Z', 'updated_at': '2024-03-23T02:15:27Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m receiving a CUDA OutOfMemory error when running relatively small batches of IMDB reviews. Curiously, the torch CUDA OOM error seems to be trying to allocate very small (e.g., 216MB) reservations when this happens. I\'m running this on my university\'s HPC: [Quartz with a V100](https://kb.iu.edu/d/avjk) reserved to just my program, so it should be able to reserve up to 32GB of GPU memory.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Obtain dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n2. Sample 3,000 texts from dataset\r\n3. Spin up preprocessing pipeline \r\n```python\r\nnlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=True, use_gpu=True, pos_batch_size=1000)\r\n```\r\n4. Attempt to preprocess text in batches: \r\n\r\n```python\r\n    # Update the pandas dataframe with the tokenized reviews\r\n    preprocessed = []\r\n    chunks = np.array_split(test_data[\'review\'], len(test_data)//10)\r\n    print(""Preprocessing texts - this may take a while"")\r\n    for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n        torch.cuda.empty_cache()\r\n        results = nlp.bulk_process(chunk)\r\n        for doc in results:\r\n            preprocessed.append(\r\n                [word.text.lower()\r\n                 for sentence in doc.sentences\r\n                 for word in sentence.words\r\n                 if word.upos not in [""PUNCT"", ""SYM"", ""NUM"", \'X\']\r\n                 and word.text.lower() not in stops\r\n                ]\r\n            )\r\n    test_data[\'review_tokens\'] = preprocessed\r\n```\r\n5. See error\r\n```\r\nPreprocessing texts - this may take a while\r\nChunks of 10 texts:  20%|███████████████████████████████████▏                                                                                                                                               | 59/300 [00:40<02:45,  1.46it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 181, in forward\r\n    all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 31.74 GiB total capacity; 672.50 MiB already allocated; 81.12 MiB free; 948.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\n**Expected behavior**\r\nI expected for the code to complete and the preprocessed text to be in a list for reincorporation into a Pandas DataFrame\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux \r\n - Python version: 3.10\r\n - Stanza version: 1.7.0\r\n\r\n**Additional context**\r\n* I\'m not running the CoreNLP Java server - just out-of-the-box Stanza\r\n* I\'ve tried with other batch sizes and get the same outcome.\r\n* I\'ve been working with the HPC team at my university - some other things we have tried\r\n    * The `torch.cuda.empty_cache()` was added - it didn\'t change the outcome, this error occurs with or without that line\r\n    * Added `os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:734""`\r\n    * This originally occured in a Jupyter Lab notebook - I got the error there and thinking that it may be because I was using it in an interactive session tried it again in a .py file - the error replicates in both contexts\r\n    * Examining `nvidia-smi` before/after running in Jupyter:\r\n        * Before: Tesla V100-PCIE-32GB - GPU Memory Usage: 0\r\n        * After: Tesla V100-PCIE-32GB - GPU Memory Usage: 32412MiB / 32768 MiB\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016309209', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372#issuecomment-2016309209', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'id': 2016309209, 'node_id': 'IC_kwDOBj_0V854Lm_Z', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-23T02:15:27Z', 'updated_at': '2024-03-23T02:15:27Z', 'author_association': 'NONE', 'body': 'Replicated with `pos_batch_size=100` - same outcome. The interesting thing is that nvidia-smi before running the program is confirmed at 0% memory usage and nearly 100% post. Is it possible that in bulk_process it doesn\'t release the GPU memory as it iterates such that the memory usage accumulates and never releases, resulting in death by a thousand cuts?\r\n\r\n```\r\nChunks\u2007of\u200710\u2007texts:\u2007\u200723%\r\n\u200769/300\u2007[00:51<02:58,\u2007\u20071.29it/s]\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\nCell In[5], line 86\r\n     84 for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n     85     torch.cuda.empty_cache()\r\n---> 86     results = nlp.bulk_process(chunk)\r\n     87     for doc in results:\r\n     88         preprocessed.append(\r\n     89             [word.text.lower()\r\n     90              for sentence in doc.sentences\r\n   (...)\r\n     94             ]\r\n     95         )\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/pipeline/core.py:438, in Pipeline.bulk_process(self, docs, *args, **kwargs)\r\n    436 # Wrap each text as a Document unless it is already such a document\r\n    437 docs = [doc if isinstance(doc, Document) else Document([], text=doc) for doc in docs]\r\n--> 438 return self.process(docs, *args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/pipeline/core.py:427, in Pipeline.process(self, doc, processors)\r\n    425     if self.processors.get(processor_name):\r\n    426         process = self.processors[processor_name].bulk_process if bulk else self.processors[processor_name].process\r\n--> 427         doc = process(doc)\r\n    428 return doc\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py:258, in UDProcessor.bulk_process(self, docs)\r\n    255 combined_doc.num_tokens = sum(doc.num_tokens for doc in docs)\r\n    256 combined_doc.num_words = sum(doc.num_words for doc in docs)\r\n--> 258 self.process(combined_doc) # annotations are attached to sentence objects\r\n    260 return docs\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py:85, in POSProcessor.process(self, document)\r\n     83         for i, b in enumerate(batch):\r\n     84             idx.extend(b[-1])\r\n---> 85             preds += self.trainer.predict(b)\r\n     87 preds = unsort(preds, idx)\r\n     88 dataset.doc.set([doc.UPOS, doc.XPOS, doc.FEATS], [y for x in preds for y in x])\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py:72, in Trainer.predict(self, batch, unsort)\r\n     70 self.model.eval()\r\n     71 batch_size = word.size(0)\r\n---> 72 _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n     73 upos_seqs = [self.vocab[\'upos\'].unmap(sent) for sent in preds[0].tolist()]\r\n     74 xpos_seqs = [self.vocab[\'xpos\'].unmap(sent) for sent in preds[1].tolist()]\r\n\r\nFile /N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don\'t have any hooks, we want to skip the rest of the logic in\r\n   1497 # this function, and just call forward.\r\n   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1499         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1501     return forward_call(*args, **kwargs)\r\n   1502 # Do not call functions when jit is used\r\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/models/pos/model.py:181, in Tagger.forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n    178     all_forward_chars = [self.charmodel_forward_transform(x) for x in all_forward_chars]\r\n    179 all_forward_chars = pack(pad_sequence(all_forward_chars, batch_first=True))\r\n--> 181 all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n    182 if self.charmodel_backward_transform is not None:\r\n    183     all_backward_chars = [self.charmodel_backward_transform(x) for x in all_backward_chars]\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py:213, in CharacterLanguageModel.build_char_representation(self, sentences)\r\n    210 chars = get_long_tensor(chars, len(all_data), pad_id=vocab.unit2id(CHARLM_END)).to(device=device)\r\n    212 with torch.no_grad():\r\n--> 213     output, _, _ = self.forward(chars, char_lens)\r\n    214     res = [output[i, offsets] for i, offsets in enumerate(char_offsets)]\r\n    215     res = unsort(res, orig_idx)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py:155, in CharacterLanguageModel.forward(self, chars, charlens, hidden)\r\n    153 output, hidden = self.charlstm(embs, charlens, hx=hidden)\r\n    154 output = self.dropout(pad_packed_sequence(output, batch_first=True)[0])\r\n--> 155 decoded = self.decoder(output)\r\n    156 return output, hidden, decoded\r\n\r\nFile /N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don\'t have any hooks, we want to skip the rest of the logic in\r\n   1497 # this function, and just call forward.\r\n   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1499         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1501     return forward_call(*args, **kwargs)\r\n   1502 # Do not call functions when jit is used\r\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nFile /N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\r\n    113 def forward(self, input: Tensor) -> Tensor:\r\n--> 114     return F.linear(input, self.weight, self.bias)\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 0; 31.74 GiB total capacity; 696.91 MiB already allocated; 33.12 MiB free; 994.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016309209/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
111,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 11, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-23T01:03:13Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016248813', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2016248813', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2016248813, 'node_id': 'IC_kwDOBj_0V854LYPt', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-23T01:03:12Z', 'updated_at': '2024-03-23T01:03:12Z', 'author_association': 'COLLABORATOR', 'body': ""It is a `verb` if you use the `default_accurate` models.  That has the more accurate constituency parser, anyway, so I would suggest doing that if accurate constituency parses are desired\r\n\r\nAs for the root cause, that word doesn't show up in the training data, so all it has to go on are the embeddings and the context of the sentence.  Sometimes it will get such a thing wrong"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016248813/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
112,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372', 'id': 2200346640, 'node_id': 'I_kwDOBj_0V86DJqAQ', 'number': 1372, 'title': 'CUDA OutOfMemory Error with bulk_process()', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-21T14:27:41Z', 'updated_at': '2024-03-23T00:30:54Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m receiving a CUDA OutOfMemory error when running relatively small batches of IMDB reviews. Curiously, the torch CUDA OOM error seems to be trying to allocate very small (e.g., 216MB) reservations when this happens. I\'m running this on my university\'s HPC: [Quartz with a V100](https://kb.iu.edu/d/avjk) reserved to just my program, so it should be able to reserve up to 32GB of GPU memory.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Obtain dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n2. Sample 3,000 texts from dataset\r\n3. Spin up preprocessing pipeline \r\n```python\r\nnlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=True, use_gpu=True, pos_batch_size=1000)\r\n```\r\n4. Attempt to preprocess text in batches: \r\n\r\n```python\r\n    # Update the pandas dataframe with the tokenized reviews\r\n    preprocessed = []\r\n    chunks = np.array_split(test_data[\'review\'], len(test_data)//10)\r\n    print(""Preprocessing texts - this may take a while"")\r\n    for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n        torch.cuda.empty_cache()\r\n        results = nlp.bulk_process(chunk)\r\n        for doc in results:\r\n            preprocessed.append(\r\n                [word.text.lower()\r\n                 for sentence in doc.sentences\r\n                 for word in sentence.words\r\n                 if word.upos not in [""PUNCT"", ""SYM"", ""NUM"", \'X\']\r\n                 and word.text.lower() not in stops\r\n                ]\r\n            )\r\n    test_data[\'review_tokens\'] = preprocessed\r\n```\r\n5. See error\r\n```\r\nPreprocessing texts - this may take a while\r\nChunks of 10 texts:  20%|███████████████████████████████████▏                                                                                                                                               | 59/300 [00:40<02:45,  1.46it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 181, in forward\r\n    all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 31.74 GiB total capacity; 672.50 MiB already allocated; 81.12 MiB free; 948.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\n**Expected behavior**\r\nI expected for the code to complete and the preprocessed text to be in a list for reincorporation into a Pandas DataFrame\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux \r\n - Python version: 3.10\r\n - Stanza version: 1.7.0\r\n\r\n**Additional context**\r\n* I\'m not running the CoreNLP Java server - just out-of-the-box Stanza\r\n* I\'ve tried with other batch sizes and get the same outcome.\r\n* I\'ve been working with the HPC team at my university - some other things we have tried\r\n    * The `torch.cuda.empty_cache()` was added - it didn\'t change the outcome, this error occurs with or without that line\r\n    * Added `os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:734""`\r\n    * This originally occured in a Jupyter Lab notebook - I got the error there and thinking that it may be because I was using it in an interactive session tried it again in a .py file - the error replicates in both contexts\r\n    * Examining `nvidia-smi` before/after running in Jupyter:\r\n        * Before: Tesla V100-PCIE-32GB - GPU Memory Usage: 0\r\n        * After: Tesla V100-PCIE-32GB - GPU Memory Usage: 32412MiB / 32768 MiB\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016225753', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372#issuecomment-2016225753', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'id': 2016225753, 'node_id': 'IC_kwDOBj_0V854LSnZ', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-23T00:30:54Z', 'updated_at': '2024-03-23T00:30:54Z', 'author_association': 'COLLABORATOR', 'body': ""It's very strange that it is only allocating small amounts, but it's failing to go through the dataset.  I did just find out that some of the models have default batch sizes which are not reasonable.  What happens if you try the flag `pos_batch_size=100` when creating the `Pipeline`?\r\n\r\npossible duplicate:\r\nhttps://github.com/stanfordnlp/stanza/issues/1370"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016225753/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
113,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370', 'id': 2194529214, 'node_id': 'I_kwDOBj_0V86Czdu-', 'number': 1370, 'title': 'Italian model runs out of memory on COLAB A100 gpu', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-19T10:04:36Z', 'updated_at': '2024-03-23T00:29:05Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""\r\n\r\nTrying to process small texts  (300-500kb) on a 40gb GPU on colab raises an OutOfMemoryError. Here is log. The English model, on the same text, does not. It happens even with processors='tokenize,lemma,pos'.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n<ipython-input-4-e215e2dbfcd3> in <cell line: 2>()\r\n      1 s = open('KINGSLEY_TIASPETTOACENTRALPARL.txt').read()\r\n----> 2 doc = nlp(s)\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\r\n    114 \r\n    115     def forward(self, input: Tensor) -> Tensor:\r\n--> 116         return F.linear(input, self.weight, self.bias)\r\n    117 \r\n    118     def extra_repr(self) -> str:\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free. Process 3295 has 10.35 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 37.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016223010', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370#issuecomment-2016223010', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'id': 2016223010, 'node_id': 'IC_kwDOBj_0V854LR8i', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-23T00:29:04Z', 'updated_at': '2024-03-23T00:29:04Z', 'author_association': 'COLLABORATOR', 'body': ""It would seem the default batch size for some of the POS models was not set\r\nto something reasonable when we changed the batching scheme.  I'll get to\r\nwork updating the models to reflect something that actually fits in a GPU,\r\nand in the meantime, you can use the parameter `pos_batch_size=100` when\r\ncreating the Pipeline to avoid this happening\r\n\r\nOn Fri, Mar 22, 2024 at 9:46\u202fAM Luca ***@***.***> wrote:\r\n\r\n> Different text, same result (but this book is not copyrighted, so I can\r\n> attach it). There is only one active pipeline. Colab has been acting up\r\n> lately, so it could also be that there is a problem on their end (even if I\r\n> don't understand how this should affect a specific model).\r\n>\r\n> pirandello_il_fu_mattia_pascal.txt\r\n> <https://github.com/stanfordnlp/stanza/files/14725863/pirandello_il_fu_mattia_pascal.txt>\r\n> Screenshot.2024-03-22.at.17.44.28.png (view on web)\r\n> <https://github.com/stanfordnlp/stanza/assets/33907162/95cfb283-c5f8-48b8-86c9-357d7a6f7855>\r\n>\r\n> Here is the error message:\r\n> ------------------------------\r\n>\r\n> OutOfMemoryError Traceback (most recent call last)\r\n> <https://localhost:8080/#> in <cell line: 4>()\r\n> 2 s = open('archive/pirandello_il_fu_mattia_pascal.txt').read()\r\n> 3\r\n> ----> 4 doc = nlp(s)\r\n>\r\n> 9 frames\r\n> /usr/local/lib/python3.10/dist-packages/torch/nn/utils/rnn.py\r\n> <https://localhost:8080/#> in pad_packed_sequence(sequence, batch_first,\r\n> padding_value, total_length)\r\n> 331 )\r\n> 332 max_seq_length = total_length\r\n> --> 333 padded_output, lengths = _VF._pad_packed_sequence(\r\n> 334 sequence.data, sequence.batch_sizes, batch_first, padding_value,\r\n> max_seq_length)\r\n> 335 unsorted_indices = sequence.unsorted_indices\r\n>\r\n> OutOfMemoryError: CUDA out of memory. Tried to allocate 18.10 GiB. GPU 0\r\n> has a total capacity of 15.77 GiB of which 10.72 GiB is free. Process 15062\r\n> has 5.05 GiB memory in use. Of the allocated memory 4.52 GiB is allocated\r\n> by PyTorch, and 154.43 MiB is reserved by PyTorch but unallocated. If\r\n> reserved but unallocated memory is large try setting\r\n> PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.\r\n> See documentation for Memory Management (\r\n> https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/stanfordnlp/stanza/issues/1370#issuecomment-2015491322>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AA2AYWK4QNZWVAUFET2N7DTYZRN5XAVCNFSM6AAAAABE5HBBLOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJVGQ4TCMZSGI>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016223010/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
114,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 10, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-22T22:45:08Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016043417', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2016043417', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2016043417, 'node_id': 'IC_kwDOBj_0V854KmGZ', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-22T22:45:08Z', 'updated_at': '2024-03-22T22:45:08Z', 'author_association': 'NONE', 'body': 'What could cause the wrong POS of ""miaut"" in ""Der Hund bellt, die Katze miaut.""?\r\n\r\n ""miaut"" is not a verb in stanza.  I am curious how this could happen.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2016043417/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
115,PushEvent,"{'repository_id': 104854615, 'push_id': 17669156081, 'size': 0, 'distinct_size': 0, 'ref': 'refs/heads/multilingual-coref', 'head': 'bef78a9077f4178f1477c174ea2bb0f71d2a62fd', 'before': 'bfbe0e5aea0684ff546c22a9ff4e62c3d2385a52', 'commits': []}"
116,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-03-22T20:01:57Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2015817841', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2015817841', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2015817841, 'node_id': 'IC_kwDOBj_0V854JvBx', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-22T20:01:57Z', 'updated_at': '2024-03-22T20:01:57Z', 'author_association': 'NONE', 'body': 'Yeah that would be very helpful.  The apostrophe splitting used to be very stable for the english model before the addition of MWT tokens.  ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2015817841/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
117,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370', 'id': 2194529214, 'node_id': 'I_kwDOBj_0V86Czdu-', 'number': 1370, 'title': 'Italian model runs out of memory on COLAB A100 gpu', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-19T10:04:36Z', 'updated_at': '2024-03-22T16:45:57Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""\r\n\r\nTrying to process small texts  (300-500kb) on a 40gb GPU on colab raises an OutOfMemoryError. Here is log. The English model, on the same text, does not. It happens even with processors='tokenize,lemma,pos'.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n<ipython-input-4-e215e2dbfcd3> in <cell line: 2>()\r\n      1 s = open('KINGSLEY_TIASPETTOACENTRALPARL.txt').read()\r\n----> 2 doc = nlp(s)\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\r\n    114 \r\n    115     def forward(self, input: Tensor) -> Tensor:\r\n--> 116         return F.linear(input, self.weight, self.bias)\r\n    117 \r\n    118     def extra_repr(self) -> str:\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free. Process 3295 has 10.35 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 37.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2015491322', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370#issuecomment-2015491322', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'id': 2015491322, 'node_id': 'IC_kwDOBj_0V854IfT6', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-22T16:45:57Z', 'updated_at': '2024-03-22T16:45:57Z', 'author_association': 'NONE', 'body': 'Different text, same result (but this book is not copyrighted, so I can attach it). There is only one active pipeline. Colab has been acting up lately, so it could also be that there is a problem on their end (even if I don\'t understand how this should affect a specific model).\r\n\r\n[pirandello_il_fu_mattia_pascal.txt](https://github.com/stanfordnlp/stanza/files/14725863/pirandello_il_fu_mattia_pascal.txt)\r\n\r\n<img width=""459"" alt=""Screenshot 2024-03-22 at 17 44 28"" src=""https://github.com/stanfordnlp/stanza/assets/33907162/95cfb283-c5f8-48b8-86c9-357d7a6f7855"">\r\n\r\nHere is the error message:\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n[<ipython-input-11-2fe37f150748>](https://localhost:8080/#) in <cell line: 4>()\r\n      2 s = open(\'archive/pirandello_il_fu_mattia_pascal.txt\').read()\r\n      3 \r\n----> 4 doc = nlp(s)\r\n\r\n9 frames\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/utils/rnn.py](https://localhost:8080/#) in pad_packed_sequence(sequence, batch_first, padding_value, total_length)\r\n    331                              )\r\n    332         max_seq_length = total_length\r\n--> 333     padded_output, lengths = _VF._pad_packed_sequence(\r\n    334         sequence.data, sequence.batch_sizes, batch_first, padding_value, max_seq_length)\r\n    335     unsorted_indices = sequence.unsorted_indices\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 18.10 GiB. GPU 0 has a total capacity of 15.77 GiB of which 10.72 GiB is free. Process 15062 has 5.05 GiB memory in use. Of the allocated memory 4.52 GiB is allocated by PyTorch, and 154.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2015491322/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
118,PushEvent,"{'repository_id': 104854615, 'push_id': 17663759873, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '14989cb20949f46e74e2874e045542968d90648c', 'before': '51cdd17b3cd4a12024bc9c90a09c0c7ec031adce', 'commits': [{'sha': '14989cb20949f46e74e2874e045542968d90648c', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'fill in xlm-roberta-large numbers for various German tasks', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/14989cb20949f46e74e2874e045542968d90648c'}]}"
119,PushEvent,"{'repository_id': 104854615, 'push_id': 17657595606, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '51cdd17b3cd4a12024bc9c90a09c0c7ec031adce', 'before': '86ddaab31c73a7d0a389d0557f3696c29d441657', 'commits': [{'sha': '51cdd17b3cd4a12024bc9c90a09c0c7ec031adce', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Change transformer nickname for xlm-roberta-base to make it more distinctive from other Roberta', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/51cdd17b3cd4a12024bc9c90a09c0c7ec031adce'}]}"
120,WatchEvent,{'action': 'started'}
121,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368', 'id': 2190218719, 'node_id': 'I_kwDOBj_0V86CjBXf', 'number': 1368, 'title': '[QUESTION] Is constituency available in Stanza German Model?', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-16T19:45:57Z', 'updated_at': '2024-03-21T20:42:09Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Not sure where to look for this information, which processors are available for the German model\r\n\r\nhttps://stanfordnlp.github.io/stanza/pipeline.html', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2013692649', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368#issuecomment-2013692649', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'id': 2013692649, 'node_id': 'IC_kwDOBj_0V854BoLp', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T20:42:08Z', 'updated_at': '2024-03-21T20:42:08Z', 'author_association': 'COLLABORATOR', 'body': ""The current release will download the models as well.  However, the dev\nbranch will load them faster.  You can look up installing python modules\nfrom git repos, or I'll put something together after making a little more\nprogress on the next release\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2013692649/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
122,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368', 'id': 2190218719, 'node_id': 'I_kwDOBj_0V86CjBXf', 'number': 1368, 'title': '[QUESTION] Is constituency available in Stanza German Model?', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-16T19:45:57Z', 'updated_at': '2024-03-21T19:41:09Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Not sure where to look for this information, which processors are available for the German model\r\n\r\nhttps://stanfordnlp.github.io/stanza/pipeline.html', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2013506499', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368#issuecomment-2013506499', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'id': 2013506499, 'node_id': 'IC_kwDOBj_0V854A6vD', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T19:41:09Z', 'updated_at': '2024-03-21T19:41:09Z', 'author_association': 'NONE', 'body': '@AngledLuffa \r\nStill new to Stanza, not sure to work on dev branch having the model from huggingface, am I right?\r\n\r\nOr the dev branch would automatically download the right compatible models?', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2013506499/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
123,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1372', 'id': 2200346640, 'node_id': 'I_kwDOBj_0V86DJqAQ', 'number': 1372, 'title': 'CUDA OutOfMemory Error with bulk_process()', 'user': {'login': 'amckenny', 'id': 34607079, 'node_id': 'MDQ6VXNlcjM0NjA3MDc5', 'avatar_url': 'https://avatars.githubusercontent.com/u/34607079?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/amckenny', 'html_url': 'https://github.com/amckenny', 'followers_url': 'https://api.github.com/users/amckenny/followers', 'following_url': 'https://api.github.com/users/amckenny/following{/other_user}', 'gists_url': 'https://api.github.com/users/amckenny/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/amckenny/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/amckenny/subscriptions', 'organizations_url': 'https://api.github.com/users/amckenny/orgs', 'repos_url': 'https://api.github.com/users/amckenny/repos', 'events_url': 'https://api.github.com/users/amckenny/events{/privacy}', 'received_events_url': 'https://api.github.com/users/amckenny/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-21T14:27:41Z', 'updated_at': '2024-03-21T14:27:41Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m receiving a CUDA OutOfMemory error when running relatively small batches of IMDB reviews. Curiously, the torch CUDA OOM error seems to be trying to allocate very small (e.g., 216MB) reservations when this happens. I\'m running this on my university\'s HPC: [Quartz with a V100](https://kb.iu.edu/d/avjk) reserved to just my program, so it should be able to reserve up to 32GB of GPU memory.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Obtain dataset from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n2. Sample 3,000 texts from dataset\r\n3. Spin up preprocessing pipeline \r\n```python\r\nnlp = stanza.Pipeline(\'en\', processors=\'tokenize,mwt,pos\', tokenize_no_ssplit=True, use_gpu=True, pos_batch_size=1000)\r\n```\r\n4. Attempt to preprocess text in batches: \r\n\r\n```python\r\n    # Update the pandas dataframe with the tokenized reviews\r\n    preprocessed = []\r\n    chunks = np.array_split(test_data[\'review\'], len(test_data)//10)\r\n    print(""Preprocessing texts - this may take a while"")\r\n    for chunk in tqdm(chunks, desc=""Chunks of 10 texts""):\r\n        torch.cuda.empty_cache()\r\n        results = nlp.bulk_process(chunk)\r\n        for doc in results:\r\n            preprocessed.append(\r\n                [word.text.lower()\r\n                 for sentence in doc.sentences\r\n                 for word in sentence.words\r\n                 if word.upos not in [""PUNCT"", ""SYM"", ""NUM"", \'X\']\r\n                 and word.text.lower() not in stops\r\n                ]\r\n            )\r\n    test_data[\'review_tokens\'] = preprocessed\r\n```\r\n5. See error\r\n```\r\nPreprocessing texts - this may take a while\r\nChunks of 10 texts:  20%|███████████████████████████████████▏                                                                                                                                               | 59/300 [00:40<02:45,  1.46it/s]\r\nTraceback (most recent call last):\r\n  File ""/N/slate/amckenny/class/ml.py"", line 200, in <module>\r\n    main()\r\n  File ""/N/slate/amckenny/class/ml.py"", line 187, in main\r\n    results = nlp.bulk_process(chunk)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 438, in bulk_process\r\n    return self.process(docs, *args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/core.py"", line 427, in process\r\n    doc = process(doc)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py"", line 258, in bulk_process\r\n    self.process(combined_doc) # annotations are attached to sentence objects\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py"", line 85, in process\r\n    preds += self.trainer.predict(b)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py"", line 72, in predict\r\n    _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/pos/model.py"", line 181, in forward\r\n    all_backward_chars = self.charmodel_backward.build_char_representation(text)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 213, in build_char_representation\r\n    output, _, _ = self.forward(chars, char_lens)\r\n  File ""/N/u/amckenny/Quartz/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py"", line 155, in forward\r\n    decoded = self.decoder(output)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File ""/N/soft/rhel8/deeplearning/Python-3.10.10/lib/python3.10/site-packages/torch/nn/modules/linear.py"", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 31.74 GiB total capacity; 672.50 MiB already allocated; 81.12 MiB free; 948.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\n**Expected behavior**\r\nI expected for the code to complete and the \r\n\r\n**Environment (please complete the following information):**\r\n - OS: Linux \r\n - Python version: 3.10\r\n - Stanza version: 1.7.0\r\n\r\n**Additional context**\r\n* I\'m not running the CoreNLP Java server - just out-of-the-box Stanza\r\n* I\'ve tried with other batch sizes and get the same outcome.\r\n* I\'ve been working with the HPC team at my university - some other things we have tried\r\n    * The `torch.cuda.empty_cache()` was added to see if I could resolve this on my own (in consultation with the university HPC staff) - it didn\'t change the outcome, this error occurs with or without that line\r\n    * This originally occured in a Jupyter Lab notebook - I got the error there and thinking that it may be because I was using it in an interactive session tried it again in a .py file - the error replicates in both contexts\r\n    * Examining `nvidia-smi` before/after running in Jupyter:\r\n        * Before: Tesla V100-PCIE-32GB - GPU Memory Usage: 0\r\n        * After: Tesla V100-PCIE-32GB - GPU Memory Usage: 32412MiB / 32768 MiB\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1372/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
124,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 33, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-21T09:03:01Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011683395', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2011683395', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2011683395, 'node_id': 'IC_kwDOBj_0V85359pD', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T09:03:00Z', 'updated_at': '2024-03-21T09:03:00Z', 'author_association': 'NONE', 'body': ""I still want to do some trials with the model, so it makes sense that the lemmatizer won't work fine with unknown words, but let's see if that's the case.\r\n\r\nAbout leaving the thread open until the next release, that sounds good to me.\r\n\r\nThanks!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011683395/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
125,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 32, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-21T09:00:02Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011678397', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2011678397', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2011678397, 'node_id': 'IC_kwDOBj_0V85358a9', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T09:00:01Z', 'updated_at': '2024-03-21T09:00:01Z', 'author_association': 'COLLABORATOR', 'body': ""> PS. I've tried the lemmatizer with a sentence, to see if it works, and it does annotate the lemma, I haven't checked if that's correct, but it looks that what is annotated is correct. Still I'll suggest changing that.\r\n\r\nThis is probably because the model memorizes known lemmas.  If you know of any ANG words which aren't in the training data, I wouldn't expect it to work too well.\r\n\r\n> When will it be visible on the list of available models? Just out of curiosity.\r\n\r\nGood question.  How about with the next release?  There are a couple things to update between now and then, so probably in a couple weeks.\r\n\r\nLeave this open until then?"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011678397/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
126,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 31, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-21T08:53:24Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011666829', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2011666829', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2011666829, 'node_id': 'IC_kwDOBj_0V85355mN', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T08:53:23Z', 'updated_at': '2024-03-21T08:53:23Z', 'author_association': 'NONE', 'body': ""Thanks, I have tried and it works! When will it be visible on the list of available models? Just out of curiosity.\r\n\r\nThanks for spotting the issue with the lemmas. I wasn't too happy with that when I saw it, but I didn't work on the data. I'll let my collaborators know about this, so that they can sort that out before publishing the treebank, or adding it to UD. Should we take into consideration the guidelines in universaldependencies.org? My collaborator followed an article to annotate the data, but it wasn't from there, it was from somewhere else (not entirely sure about this).\r\n\r\nPS. I've tried the lemmatizer with a sentence, to see if it works, and it does annotate the lemma, I haven't checked if that's correct, but it looks that what is annotated is correct. Still I'll suggest changing that.\r\n\r\nThanks!\r\n\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011666829/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
127,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-03-21T08:30:14Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011627435', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2011627435', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2011627435, 'node_id': 'IC_kwDOBj_0V8535v-r', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T08:30:13Z', 'updated_at': '2024-03-21T08:30:13Z', 'author_association': 'COLLABORATOR', 'body': ""This is probably fixable in one of a couple different ways.  The English datasets are generally built so that the MWT are exactly composed of the subwords, so there's no reason to do anything other than split the original text.  Alternatively, there may be some generalization of that to other languages where the model first checks if it's supposed to use an exact split of the words, and then it uses the seq2seq model only for words that aren't an exact split."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011627435/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
128,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 30, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-21T08:27:20Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011622824', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2011622824', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2011622824, 'node_id': 'IC_kwDOBj_0V8535u2o', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T08:27:19Z', 'updated_at': '2024-03-21T08:27:19Z', 'author_association': 'COLLABORATOR', 'body': ""It's up, and it should be downloadable with the current release (and of course the dev branch).  LMK if it's not working in any way\r\n\r\nAlthough flipping through the data, I think people will likely be not happy about the lemma annotations \r\n\r\n```\r\n1       Swa     swā ‘so as, consequently’       ADV     adverb  Uninflected=Yes 2       advmod  _       _\r\n2       awriten āwrītan VERB    main-verb       Tense=Past|Uninflected=Yes|VerbForm=Part        0       root    _       _\r\n3       is      bēon/wesan/sēon ‘to be’ AUX     auxiliary-verb  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   2       aux:pass        _       _\r\n4       on      on ‘on, upon’ (PREP)    ADP     adposition      Uninflected=Yes 7       case    _       _\r\n5       ðæs     se-sēo-ðæt (DEM)        DET     demonstrative-article   Case=Gen|Gender=Masc|Number=Sing|PronType=DemArt        6       det     _       _\r\n```\r\n\r\nThe biggest downside is this type of annotation won't work at all for unknown text.  The lemmatizer is a seq2seq model and isn't really appropriate for figuring out the tags as part of the lemma."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011622824/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
129,PushEvent,"{'repository_id': 104854615, 'push_id': 17640895510, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '86ddaab31c73a7d0a389d0557f3696c29d441657', 'before': '4b87510a392596a26e94965ba86439331ef492da', 'commits': [{'sha': '86ddaab31c73a7d0a389d0557f3696c29d441657', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a constituency model for German.  https://github.com/stanfordnlp/stanza/issues/1368', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/86ddaab31c73a7d0a389d0557f3696c29d441657'}]}"
130,PushEvent,"{'repository_id': 104854615, 'push_id': 17640888372, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '4b87510a392596a26e94965ba86439331ef492da', 'before': '78eedfa9c1ec430a06b806a3bf5561315d8cb1ef', 'commits': [{'sha': '4b87510a392596a26e94965ba86439331ef492da', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a pipeline for ANG: https://github.com/stanfordnlp/stanza/issues/1365', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4b87510a392596a26e94965ba86439331ef492da'}]}"
131,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368', 'id': 2190218719, 'node_id': 'I_kwDOBj_0V86CjBXf', 'number': 1368, 'title': '[QUESTION] Is constituency available in Stanza German Model?', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-16T19:45:57Z', 'updated_at': '2024-03-21T07:55:35Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Not sure where to look for this information, which processors are available for the German model\r\n\r\nhttps://stanfordnlp.github.io/stanza/pipeline.html', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011431612', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368#issuecomment-2011431612', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'id': 2011431612, 'node_id': 'IC_kwDOBj_0V8535AK8', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-21T07:55:34Z', 'updated_at': '2024-03-21T07:55:34Z', 'author_association': 'COLLABORATOR', 'body': ""I updated the models to use a German model from the SPMRL Tiger treebank using Electra.  It gets 94.08 on the test set, but there's a little grade inflation so to speak if you compare it to certain other popular constituency parsers since there is an extra node in the top layer of most of the test trees.\r\n\r\nhttps://huggingface.co/german-nlp-group/electra-base-german-uncased\r\n\r\nIt's compatible with the existing release, but I suggest installing the dev branch to at least save time when loading the entire pipeline - the current release would load 3 copies of the transformer, whereas the dev branch only loads from disk once and then copies on the GPU.  The plan is to make a new release soon once we make it also only have one copy of the transformer on the GPU at a time"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2011431612/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
132,PushEvent,"{'repository_id': 104854615, 'push_id': 17640523440, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '78eedfa9c1ec430a06b806a3bf5561315d8cb1ef', 'before': '7d2434873a6043aed3cead5ce23d42a3413a5b23', 'commits': [{'sha': '78eedfa9c1ec430a06b806a3bf5561315d8cb1ef', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Use load_bert_copy in the conparser to avoid an expensive reload of the transformer when loading the Pipeline.  Really, we should rearchitect the whole thing to not even have the deepcopy', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/78eedfa9c1ec430a06b806a3bf5561315d8cb1ef'}]}"
133,PushEvent,"{'repository_id': 104854615, 'push_id': 17639030000, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '7d2434873a6043aed3cead5ce23d42a3413a5b23', 'before': '4532da2ead5dd493b7b1fcdaef1c815d4ff94342', 'commits': [{'sha': '7d2434873a6043aed3cead5ce23d42a3413a5b23', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'It seems the German Electra is performing better in general than the bert models', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/7d2434873a6043aed3cead5ce23d42a3413a5b23'}]}"
134,PushEvent,"{'repository_id': 104854615, 'push_id': 17636180849, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '4532da2ead5dd493b7b1fcdaef1c815d4ff94342', 'before': 'f15386b4e860685e2d307058aeb0213e98f04442', 'commits': [{'sha': '4532da2ead5dd493b7b1fcdaef1c815d4ff94342', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add blocks of scores for a couple other German transformer experiments', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4532da2ead5dd493b7b1fcdaef1c815d4ff94342'}]}"
135,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-03-20T17:30:29Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2010179871', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2010179871', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2010179871, 'node_id': 'IC_kwDOBj_0V8530Okf', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-20T17:30:27Z', 'updated_at': '2024-03-20T17:30:27Z', 'author_association': 'NONE', 'body': ""@AngledLuffa Yes it appears the splitting mechanism is actually quite broken.  We run stanza against gigantic amounts of text, with no problem retaining the correct text values when did so against version of Stanza before the introduction of MWT tokens.  In our latest runs we encountered thousand's of errors involving them, and here are a sample of sentences where the token resolves the wrong capitalization or munges the text of the original word (sometimes inexplicably) for word's with an apostrophe / contraction / possessive:\r\n\r\n```\r\nIn God’s name let him be so.\r\nDidn’t I say so?\r\nThey began to be frightened at last at Pulcheria Alexandrovna's strange silence.\r\nPulcheria Alexandrovna's illness was a strange nervous one.\r\nCouldn't he stop and retract it all?\r\nShe'll be my nurse.\r\nIt was quite an accident Lebeziatnikov's turning up.\r\nWasn't I right in saying that we were birds of a feather?\r\nCouldn't he come?\r\nShe'll get it at the shop, my dear.\r\nWho married the Marquis of Saint-Méran's daughter?\r\nBut to Dantès' eye there was no darkness.\r\nCouldn’t he have waited for the good season to increase his chances?\r\nShe'd never noticed if it hadn't been for Sid.\r\nWasn't that a happy thought?\r\n```\r\n\r\nE.g. digging into the types of failures being seen, by showing the token split for the `text` value:\r\n\r\n`In god’s name let him be so.` -> God's / god / 's (loss of capitalization)\r\n\r\n`Didn't I say so.` -> Didn't / did / not (loss of capitalization)\r\n\r\n`They began to be frightened at last at Pulcheria Alexandrovna's strange silence.` -> Alexandrovna's / Alexandronan / 's (flat out wrong)\r\n\r\n`Couldn't he stop and retract it all?` -> Couldn't / could / n't (loss of capitalization)\r\n\r\n`It was quite an accident Lebeziatnikov's turning up.` -> Lebeziatnikov's / Lebeziatikovv / 's (flat out wrong)\r\n\r\n`Who married the Marquis of Saint-Méran's daughter?` -> Méran's / M<UNK>ran / 's (wild!!)\r\n\r\n`But to Dantès' eye there was no darkness.` -> Dantès' / Dant<UNK>s /  ' (clear bug here)\r\n\r\n-----\r\n\r\nSadly I think we might have to revert back to a version of Stanza without MWT-based tokens as they don't appear to be stable enough for our purposes to rely on. :( "", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2010179871/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
136,WatchEvent,{'action': 'started'}
137,ForkEvent,"{'forkee': {'id': 774709596, 'node_id': 'R_kgDOLi0hXA', 'name': 'stanza', 'full_name': 'wzsxb233/stanza', 'private': False, 'owner': {'login': 'wzsxb233', 'id': 110449943, 'node_id': 'U_kgDOBpVVFw', 'avatar_url': 'https://avatars.githubusercontent.com/u/110449943?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/wzsxb233', 'html_url': 'https://github.com/wzsxb233', 'followers_url': 'https://api.github.com/users/wzsxb233/followers', 'following_url': 'https://api.github.com/users/wzsxb233/following{/other_user}', 'gists_url': 'https://api.github.com/users/wzsxb233/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/wzsxb233/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/wzsxb233/subscriptions', 'organizations_url': 'https://api.github.com/users/wzsxb233/orgs', 'repos_url': 'https://api.github.com/users/wzsxb233/repos', 'events_url': 'https://api.github.com/users/wzsxb233/events{/privacy}', 'received_events_url': 'https://api.github.com/users/wzsxb233/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/wzsxb233/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': True, 'url': 'https://api.github.com/repos/wzsxb233/stanza', 'forks_url': 'https://api.github.com/repos/wzsxb233/stanza/forks', 'keys_url': 'https://api.github.com/repos/wzsxb233/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/wzsxb233/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/wzsxb233/stanza/teams', 'hooks_url': 'https://api.github.com/repos/wzsxb233/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/wzsxb233/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/wzsxb233/stanza/events', 'assignees_url': 'https://api.github.com/repos/wzsxb233/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/wzsxb233/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/wzsxb233/stanza/tags', 'blobs_url': 'https://api.github.com/repos/wzsxb233/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/wzsxb233/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/wzsxb233/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/wzsxb233/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/wzsxb233/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/wzsxb233/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/wzsxb233/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/wzsxb233/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/wzsxb233/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/wzsxb233/stanza/subscription', 'commits_url': 'https://api.github.com/repos/wzsxb233/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/wzsxb233/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/wzsxb233/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/wzsxb233/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/wzsxb233/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/wzsxb233/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/wzsxb233/stanza/merges', 'archive_url': 'https://api.github.com/repos/wzsxb233/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/wzsxb233/stanza/downloads', 'issues_url': 'https://api.github.com/repos/wzsxb233/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/wzsxb233/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/wzsxb233/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/wzsxb233/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/wzsxb233/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/wzsxb233/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/wzsxb233/stanza/deployments', 'created_at': '2024-03-20T03:07:58Z', 'updated_at': '2024-03-20T03:07:58Z', 'pushed_at': '2024-03-19T20:39:45Z', 'git_url': 'git://github.com/wzsxb233/stanza.git', 'ssh_url': 'git@github.com:wzsxb233/stanza.git', 'clone_url': 'https://github.com/wzsxb233/stanza.git', 'svn_url': 'https://github.com/wzsxb233/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85432, 'stargazers_count': 0, 'watchers_count': 0, 'language': None, 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'has_discussions': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'main', 'public': True}}"
138,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-03-19T23:12:23Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n\r\n// MWT token is correct:\r\n\r\n    {\r\n      ""end_char"": 18,\r\n      ""id"": [\r\n        2,\r\n        3\r\n      ],\r\n      ""start_char"": 4,\r\n      ""text"": ""schoolmaster\'s""\r\n    },\r\n\r\n// non-MWT text resolves incorrectly to ""schoolmaterr"":\r\n\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 4,\r\n      ""id"": 2,\r\n      ""lemma"": ""schoolmaterr"",\r\n      ""text"": ""schoolmaterr"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n\r\n // correct: \r\n \r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 2,\r\n      ""id"": 3,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n   \r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2008324372', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371#issuecomment-2008324372', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'id': 2008324372, 'node_id': 'IC_kwDOBj_0V853tJkU', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T23:12:22Z', 'updated_at': '2024-03-19T23:12:22Z', 'author_association': 'COLLABORATOR', 'body': 'Ultimately we need a major upgrade to the tokenizer, including the MWT\r\nsplitting mechanism.  There is a copy mechanism which tries to use the\r\noriginal text, but clearly it misses some cases.  If you collect a few of\r\nthese weird outliers, we\'ll rebuild the training data using them, and it\r\nshould improve the results.\r\n\r\nOn Tue, Mar 19, 2024 at 3:58\u202fPM Kelsey Hannan ***@***.***>\r\nwrote:\r\n\r\n> *Describe the bug*\r\n> In response to the changes of multi-word tokens in #1361\r\n> <https://github.com/stanfordnlp/stanza/issues/1361>, I encountered an\r\n> error with how Stanza generates tokens for words with apostrophes,\r\n> particularly contractions.\r\n>\r\n> *To Reproduce*\r\n> Steps to reproduce the behavior:\r\n>\r\n>    1. Run the sentence:\r\n>\r\n> The schoolmaster\'s wife started a sewing class.\r\n>\r\n>\r\n>    1. Check the Universal Dependencies, in particular the tokens for\r\n>    schoolmaster\'s reveal the incorrect base word of schoolmaterr:\r\n>\r\n> // MWT token is correct:\r\n>    {""end_char""=>18, ""id""=>[2, 3], ""start_char""=>4, ""text""=>""schoolmaster\'s""},\r\n>    // non-MWT text resolves incorrectly to ""schoolmaterr""\r\n>    {""deprel""=>""nmod:poss"",\r\n>     ""feats""=>""Number=Sing"",\r\n>     ""head""=>4,\r\n>     ""id""=>2,\r\n>     ""lemma""=>""schoolmaterr"",\r\n>     ""text""=>""schoolmaterr"",\r\n>     ""upos""=>""NOUN"",\r\n>     ""xpos""=>""NN""},\r\n>\r\n>  // correct\r\n>    {""deprel""=>""case"", ""head""=>2, ""id""=>3, ""lemma""=>""\'s"", ""text""=>""\'s"", ""upos""=>""PART"", ""xpos""=>""POS""},\r\n>\r\n> *Expected behavior*\r\n> The non-MWT part of ""schoolmaster\'s"" resolves the tokens as schoolmaster\r\n> / \'s\r\n>\r\n> *Environment (please complete the following information):*\r\n>\r\n>    - OS: Mac OS Ventura\r\n>    - Python version: Python 3.12.2 using Poetry 1.8.2\r\n>    - Stanza version: Stanza from the dev branch up to commit b62c1e7\r\n>    <https://github.com/stanfordnlp/stanza/commit/b62c1e7f8e0e17ea38d49683aea8a0cfb3db8b90>\r\n>\r\n> *Additional context*\r\n> I believe we found more errors like this, I will report them when I come\r\n> across them.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/stanfordnlp/stanza/issues/1371>, or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AA2AYWM2TEXVFYQL2NZANR3YZC7IPAVCNFSM6AAAAABE6OQT6KVHI2DSMVQWIX3LMV43ASLTON2WKOZSGE4TMMJZGM4DSOI>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2008324372/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
139,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1371', 'id': 2196193899, 'node_id': 'I_kwDOBj_0V86C50Jr', 'number': 1371, 'title': 'Stanza resolves wrong text for tokens in a multi-word token', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-19T22:57:51Z', 'updated_at': '2024-03-19T22:57:51Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nIn response to the changes of multi-word tokens in #1361, I encountered an error with how Stanza generates tokens for words with apostrophes, particularly contractions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the sentence:\r\n```\r\nThe schoolmaster\'s wife started a sewing class.\r\n```\r\n2. Check the Universal Dependencies, in particular the tokens for `schoolmaster\'s` reveal the incorrect base word of `schoolmaterr`:\r\n\r\n```json\r\n// MWT token is correct:\r\n   {""end_char""=>18, ""id""=>[2, 3], ""start_char""=>4, ""text""=>""schoolmaster\'s""},\r\n   \r\n// non-MWT text resolves incorrectly to ""schoolmaterr""\r\n   {""deprel""=>""nmod:poss"",\r\n    ""feats""=>""Number=Sing"",\r\n    ""head""=>4,\r\n    ""id""=>2,\r\n    ""lemma""=>""schoolmaterr"",\r\n    ""text""=>""schoolmaterr"",\r\n    ""upos""=>""NOUN"",\r\n    ""xpos""=>""NN""},\r\n    \r\n // correct\r\n   {""deprel""=>""case"", ""head""=>2, ""id""=>3, ""lemma""=>""\'s"", ""text""=>""\'s"", ""upos""=>""PART"", ""xpos""=>""POS""},\r\n```\r\n\r\n**Expected behavior**\r\nThe non-MWT part of ""schoolmaster\'s"" resolves the tokens as `schoolmaster` / `\'s`\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Mac OS Ventura \r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: Stanza from the `dev` branch up to commit b62c1e7f8e0e17e\r\n\r\n**Additional context**\r\nI believe we found more errors like this, I will report them when I come across them. \r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1371/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
140,PushEvent,"{'repository_id': 104854615, 'push_id': 17616756161, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'f15386b4e860685e2d307058aeb0213e98f04442', 'before': '0044c3b20f5b5b9fac2fd30af792654fa45658a8', 'commits': [{'sha': 'f15386b4e860685e2d307058aeb0213e98f04442', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add some notes on constituency scores', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/f15386b4e860685e2d307058aeb0213e98f04442'}]}"
141,PushEvent,"{'repository_id': 104854615, 'push_id': 17616665542, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '0044c3b20f5b5b9fac2fd30af792654fa45658a8', 'before': '3dc1665cf3d0b20e126cabe71dea2a9573c002aa', 'commits': [{'sha': '0044c3b20f5b5b9fac2fd30af792654fa45658a8', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Current DE_GSD is missing some xpos tags (future 2.14 UD will have that fixed)', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/0044c3b20f5b5b9fac2fd30af792654fa45658a8'}]}"
142,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 29, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-19T18:14:50Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007840879', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2007840879', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2007840879, 'node_id': 'IC_kwDOBj_0V853rThv', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T18:14:50Z', 'updated_at': '2024-03-19T18:14:50Z', 'author_association': 'NONE', 'body': ""Good to hear that it's possible to post the models!\r\n\r\nBy raw data you mean the non-annotated one? I think I sent the annotated train, dev, and test sets previously in my first comment in this thread, but in any case I'll send the conllu files, as well as the raw one, in the following link:\r\n\r\nhttps://we.tl/t-uyd9zNQU1r\r\n\r\nI'll let you know when the UD treebank is published. My guess is that we would need to include that article in the bibliography when referring to the ANG model? In any case I'll take a look at the article itself.\r\n\r\nThanks!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007840879/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
143,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 28, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-19T16:53:07Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007677876', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2007677876', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2007677876, 'node_id': 'IC_kwDOBj_0V853qru0', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T16:53:06Z', 'updated_at': '2024-03-19T16:53:06Z', 'author_association': 'COLLABORATOR', 'body': 'Glad to hear it helped!  If you include that in your writeup, the model used is the Flair character model\r\n\r\nhttps://aclanthology.org/C18-1139/\r\n\r\nWe would need the raw data for recreating the models, obv.  I think I should be able to post the ANG models without the data, though, today or tomorrow.  Let me double check with my PI that trained models w/o the data works @manning ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007677876/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
144,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370', 'id': 2194529214, 'node_id': 'I_kwDOBj_0V86Czdu-', 'number': 1370, 'title': 'Italian model runs out of memory on COLAB A100 gpu', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-19T10:04:36Z', 'updated_at': '2024-03-19T16:49:31Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""\r\n\r\nTrying to process small texts  (300-500kb) on a 40gb GPU on colab raises an OutOfMemoryError. Here is log. The English model, on the same text, does not. It happens even with processors='tokenize,lemma,pos'.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n<ipython-input-4-e215e2dbfcd3> in <cell line: 2>()\r\n      1 s = open('KINGSLEY_TIASPETTOACENTRALPARL.txt').read()\r\n----> 2 doc = nlp(s)\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\r\n    114 \r\n    115     def forward(self, input: Tensor) -> Tensor:\r\n--> 116         return F.linear(input, self.weight, self.bias)\r\n    117 \r\n    118     def extra_repr(self) -> str:\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free. Process 3295 has 10.35 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 37.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007671108', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370#issuecomment-2007671108', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'id': 2007671108, 'node_id': 'IC_kwDOBj_0V853qqFE', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T16:49:30Z', 'updated_at': '2024-03-19T16:49:30Z', 'author_association': 'COLLABORATOR', 'body': ""Sure, the data would be quite helpful.  I'll take a look and see if there's something weird going on.  Do you have multiple pipelines loaded at once or some other possible unexpected use case?  I would think that a sentence of only 631 characters would work just fine on a 16GB GPU."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007671108/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
145,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370', 'id': 2194529214, 'node_id': 'I_kwDOBj_0V86Czdu-', 'number': 1370, 'title': 'Italian model runs out of memory on COLAB A100 gpu', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-19T10:04:36Z', 'updated_at': '2024-03-19T15:48:43Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""\r\n\r\nTrying to process small texts  (300-500kb) on a 40gb GPU on colab raises an OutOfMemoryError. Here is log. The English model, on the same text, does not. It happens even with processors='tokenize,lemma,pos'.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n<ipython-input-4-e215e2dbfcd3> in <cell line: 2>()\r\n      1 s = open('KINGSLEY_TIASPETTOACENTRALPARL.txt').read()\r\n----> 2 doc = nlp(s)\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\r\n    114 \r\n    115     def forward(self, input: Tensor) -> Tensor:\r\n--> 116         return F.linear(input, self.weight, self.bias)\r\n    117 \r\n    118     def extra_repr(self) -> str:\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free. Process 3295 has 10.35 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 37.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007543746', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370#issuecomment-2007543746', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'id': 2007543746, 'node_id': 'IC_kwDOBj_0V853qK_C', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T15:48:42Z', 'updated_at': '2024-03-19T15:48:42Z', 'author_association': 'NONE', 'body': ""Hi, thanks for the repoly. \r\n\r\nMy bad: I tried with a 40gb instance, but Colab switched to a 16gb version do to availability reasons. In any case, the model should run with 16gb as well. As for the length of sentences, the longest is 631 characters, while the longer token is 23 characters. It shouldn't be an issue, I believe.  Also, I get the same result with other similar texts.\r\n\r\nThe tokenizer and lemmatizer work, but if I try to run the pos-tagger I get an error. Is there any other info that I can provide? The notebook? The data?\r\n\r\n\r\nThanks again\r\n\r\n\r\n\r\n\r\n\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007543746/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
146,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 27, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-19T14:56:31Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007405563', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2007405563', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2007405563, 'node_id': 'IC_kwDOBj_0V853ppP7', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T14:56:30Z', 'updated_at': '2024-03-19T14:56:30Z', 'author_association': 'NONE', 'body': ""The charlm helped for the lemmatizer, tagger and dependency parser. Not too much, but it helped. \r\n\r\nThese are the results of the depparser with charlm:\r\n\r\nUD_Old_English-TEST\r\nUAS     LAS     CLAS     MLAS    BLEX\r\n77.06   64.60  58.84    53.80    58.84\r\n\r\nAnd these are the results without charlm:\r\n\r\nUD_Old_English-TEST\r\nUAS     LAS      CLAS     MLAS    BLEX\r\n73.75   62.68   55.70    51.60     55.70\r\n\r\nWe will aim at having the UD data released, but cannot say when, since that doesn't depend on me. I have already contacted Dan for that, so waiting for his response and then we'll start preparing everything.\r\n\r\nDo you need the UD data to be released for the language to be added to the pipeline? Or that's independent?\r\n\r\nThanks!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007405563/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
147,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 26, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-19T14:48:56Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007384631', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2007384631', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2007384631, 'node_id': 'IC_kwDOBj_0V853pkI3', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T14:48:55Z', 'updated_at': '2024-03-19T14:48:55Z', 'author_association': 'COLLABORATOR', 'body': 'Do you have final numbers on the dependency parser?  Just wondering if the charlm eventually helped it or not.\r\n\r\nIf & when you have the UD data released, I can try a couple of the smaller models on my side to see if it helps - I was thinking the charlm itself might overfit given the current amount of raw text training data.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007384631/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
148,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370', 'id': 2194529214, 'node_id': 'I_kwDOBj_0V86Czdu-', 'number': 1370, 'title': 'Italian model runs out of memory on COLAB A100 gpu', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-19T10:04:36Z', 'updated_at': '2024-03-19T14:37:27Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""\r\n\r\nTrying to process small texts  (300-500kb) on a 40gb GPU on colab raises an OutOfMemoryError. Here is log. The English model, on the same text, does not. It happens even with processors='tokenize,lemma,pos'.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n<ipython-input-4-e215e2dbfcd3> in <cell line: 2>()\r\n      1 s = open('KINGSLEY_TIASPETTOACENTRALPARL.txt').read()\r\n----> 2 doc = nlp(s)\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\r\n    114 \r\n    115     def forward(self, input: Tensor) -> Tensor:\r\n--> 116         return F.linear(input, self.weight, self.bias)\r\n    117 \r\n    118     def extra_repr(self) -> str:\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free. Process 3295 has 10.35 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 37.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007356670', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370#issuecomment-2007356670', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'id': 2007356670, 'node_id': 'IC_kwDOBj_0V853pdT-', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T14:37:26Z', 'updated_at': '2024-03-19T14:37:26Z', 'author_association': 'COLLABORATOR', 'body': ""Hmm, the frames elided would tell me which particular module you were in at the time of OOM.  There's also the issue that it seems to be 16G instead of 40G:\r\n\r\n> Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free\r\n\r\nbut Stanza should definitely fit on that anyway.  Is there a particularly long sentence or token?"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2007356670/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
149,WatchEvent,{'action': 'started'}
150,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1370', 'id': 2194529214, 'node_id': 'I_kwDOBj_0V86Czdu-', 'number': 1370, 'title': 'Italian model runs out of memory on COLAB A100 gpu', 'user': {'login': 'lucaducceschi', 'id': 33907162, 'node_id': 'MDQ6VXNlcjMzOTA3MTYy', 'avatar_url': 'https://avatars.githubusercontent.com/u/33907162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lucaducceschi', 'html_url': 'https://github.com/lucaducceschi', 'followers_url': 'https://api.github.com/users/lucaducceschi/followers', 'following_url': 'https://api.github.com/users/lucaducceschi/following{/other_user}', 'gists_url': 'https://api.github.com/users/lucaducceschi/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lucaducceschi/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lucaducceschi/subscriptions', 'organizations_url': 'https://api.github.com/users/lucaducceschi/orgs', 'repos_url': 'https://api.github.com/users/lucaducceschi/repos', 'events_url': 'https://api.github.com/users/lucaducceschi/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lucaducceschi/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-19T10:04:36Z', 'updated_at': '2024-03-19T10:04:36Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""---------------------------------------------------------------------------\r\nOutOfMemoryError                          Traceback (most recent call last)\r\n<ipython-input-4-e215e2dbfcd3> in <cell line: 2>()\r\n      1 s = open('KINGSLEY_TIASPETTOACENTRALPARL.txt').read()\r\n----> 2 doc = nlp(s)\r\n\r\n11 frames\r\n\r\nTrying to process small texts  (300-500kb) on a 40gb GPU on colab raises an OutOfMemoryError. Here is log. The English model, on the same text, does not. It happens even with processors='tokenize,lemma,pos'.\r\n\r\n\r\n\r\n\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\r\n    114 \r\n    115     def forward(self, input: Tensor) -> Tensor:\r\n--> 116         return F.linear(input, self.weight, self.bias)\r\n    117 \r\n    118     def extra_repr(self) -> str:\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 9.50 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.42 GiB is free. Process 3295 has 10.35 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 37.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1370/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
151,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 25, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-19T08:24:44Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2006271836', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2006271836', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2006271836, 'node_id': 'IC_kwDOBj_0V853lUdc', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-19T08:24:42Z', 'updated_at': '2024-03-19T08:24:42Z', 'author_association': 'NONE', 'body': ""Hi,\r\n\r\nI'm attaching the final versions of the models. I have included the lemma, pos, and depparser models trained with the charlm models, so these are the latest versions available, with the data I have at hand. \r\n\r\nhttps://we.tl/t-YWaO3WQXm5\r\n\r\nIf you need anything else from me, please let me know. In the meantime, I'll contact UD for the next steps, regarding the treebank.\r\n\r\nThanks for your help and suggestions!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2006271836/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
152,PushEvent,"{'repository_id': 104854615, 'push_id': 17604082203, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/gh-pages', 'head': 'b7106b71ad17beaf622040ec2f9738de0ba27281', 'before': 'bc7270c446604bb02101ae69191967d3e85fe3ae', 'commits': [{'sha': 'b7106b71ad17beaf622040ec2f9738de0ba27281', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Update new language documentation to match how you actually create a Pipeline', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/b7106b71ad17beaf622040ec2f9738de0ba27281'}]}"
153,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1343', 'id': 2134866480, 'node_id': 'I_kwDOBj_0V85_P3ow', 'number': 1343, 'title': 'Issues Training Models for New Language Support', 'user': {'login': 'LilitKharatyan', 'id': 107880162, 'node_id': 'U_kgDOBm4e4g', 'avatar_url': 'https://avatars.githubusercontent.com/u/107880162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/LilitKharatyan', 'html_url': 'https://github.com/LilitKharatyan', 'followers_url': 'https://api.github.com/users/LilitKharatyan/followers', 'following_url': 'https://api.github.com/users/LilitKharatyan/following{/other_user}', 'gists_url': 'https://api.github.com/users/LilitKharatyan/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/LilitKharatyan/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/LilitKharatyan/subscriptions', 'organizations_url': 'https://api.github.com/users/LilitKharatyan/orgs', 'repos_url': 'https://api.github.com/users/LilitKharatyan/repos', 'events_url': 'https://api.github.com/users/LilitKharatyan/events{/privacy}', 'received_events_url': 'https://api.github.com/users/LilitKharatyan/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-02-14T17:50:53Z', 'updated_at': '2024-03-18T19:13:59Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""I am trying to train a pipeline for a new language (xcl). My goal is to train the full pipeline (tokenizer, tagger, parser, lemmatizer, and morphological parser) for this language, starting with the tokenizer. I've followed the instructions provided in the official documentation and GitHub repository closely but have encountered several issues that hinder my progress.\r\n\r\nHere are the steps I've taken and the issues encountered:\r\n\r\n1. Setting Up Environment and Data: After organizing my .conllu files for training and validation as per the guidelines, I set the environment variables in config.sh and sourced it. My data is for the language code xcl, which is not recognized by Stanza, so I used HY (Armenian) as a temporary workaround.\r\n\r\n2. Training the Tokenizer: When attempting to train the tokenizer using the command \r\n`python3 -m stanza.utils.training.run_tokenizer HY_Classical_Armenian`\r\n\r\nI encountered a FileNotFoundError related to missing .toklabels files, which should have been generated during the data preparation step. The exact error message was:\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'data/tokenize/hy_classical_armenian-ud-train.toklabels'\r\n\r\nThis indicates that either the preparation step was missed or did not complete successfully, or there's a mismatch in the expected directory structure or naming convention. However, following the instructions from the documentation and GitHub, it wasn't clear how to proceed with the preparation step for a language not yet recognized by Stanza.\r\n\r\n- Could you provide more detailed instructions or clarification on how to correctly prepare the data for model training, especially for new languages not currently supported by Stanza? This includes generating the necessary .toklabels files.\r\n\r\n- Is there a recommended approach to adding support for entirely new languages, ensuring that all necessary preprocessing and setup steps are covered?\r\n\r\n- Any advice on troubleshooting or steps I might have overlooked would be greatly appreciated. I am especially interested in any scripts or commands specific to preparing data for new languages."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2004729267', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1343#issuecomment-2004729267', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343', 'id': 2004729267, 'node_id': 'IC_kwDOBj_0V853fb2z', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-18T19:13:57Z', 'updated_at': '2024-03-18T19:13:57Z', 'author_association': 'COLLABORATOR', 'body': 'Are you able to sync the dev branch and try that?  I just ran into this\nissue with someone looking to add their models for Albanian:\n\nhttps://github.com/stanfordnlp/stanza/issues/1360\n\nHere is the format for the Pipeline you need to use:\n\nhttps://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1974916658\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2004729267/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
154,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1343', 'id': 2134866480, 'node_id': 'I_kwDOBj_0V85_P3ow', 'number': 1343, 'title': 'Issues Training Models for New Language Support', 'user': {'login': 'LilitKharatyan', 'id': 107880162, 'node_id': 'U_kgDOBm4e4g', 'avatar_url': 'https://avatars.githubusercontent.com/u/107880162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/LilitKharatyan', 'html_url': 'https://github.com/LilitKharatyan', 'followers_url': 'https://api.github.com/users/LilitKharatyan/followers', 'following_url': 'https://api.github.com/users/LilitKharatyan/following{/other_user}', 'gists_url': 'https://api.github.com/users/LilitKharatyan/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/LilitKharatyan/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/LilitKharatyan/subscriptions', 'organizations_url': 'https://api.github.com/users/LilitKharatyan/orgs', 'repos_url': 'https://api.github.com/users/LilitKharatyan/repos', 'events_url': 'https://api.github.com/users/LilitKharatyan/events{/privacy}', 'received_events_url': 'https://api.github.com/users/LilitKharatyan/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-02-14T17:50:53Z', 'updated_at': '2024-03-18T11:06:52Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""I am trying to train a pipeline for a new language (xcl). My goal is to train the full pipeline (tokenizer, tagger, parser, lemmatizer, and morphological parser) for this language, starting with the tokenizer. I've followed the instructions provided in the official documentation and GitHub repository closely but have encountered several issues that hinder my progress.\r\n\r\nHere are the steps I've taken and the issues encountered:\r\n\r\n1. Setting Up Environment and Data: After organizing my .conllu files for training and validation as per the guidelines, I set the environment variables in config.sh and sourced it. My data is for the language code xcl, which is not recognized by Stanza, so I used HY (Armenian) as a temporary workaround.\r\n\r\n2. Training the Tokenizer: When attempting to train the tokenizer using the command \r\n`python3 -m stanza.utils.training.run_tokenizer HY_Classical_Armenian`\r\n\r\nI encountered a FileNotFoundError related to missing .toklabels files, which should have been generated during the data preparation step. The exact error message was:\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'data/tokenize/hy_classical_armenian-ud-train.toklabels'\r\n\r\nThis indicates that either the preparation step was missed or did not complete successfully, or there's a mismatch in the expected directory structure or naming convention. However, following the instructions from the documentation and GitHub, it wasn't clear how to proceed with the preparation step for a language not yet recognized by Stanza.\r\n\r\n- Could you provide more detailed instructions or clarification on how to correctly prepare the data for model training, especially for new languages not currently supported by Stanza? This includes generating the necessary .toklabels files.\r\n\r\n- Is there a recommended approach to adding support for entirely new languages, ensuring that all necessary preprocessing and setup steps are covered?\r\n\r\n- Any advice on troubleshooting or steps I might have overlooked would be greatly appreciated. I am especially interested in any scripts or commands specific to preparing data for new languages."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2003620092', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1343#issuecomment-2003620092', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1343', 'id': 2003620092, 'node_id': 'IC_kwDOBj_0V853bND8', 'user': {'login': 'LilitKharatyan', 'id': 107880162, 'node_id': 'U_kgDOBm4e4g', 'avatar_url': 'https://avatars.githubusercontent.com/u/107880162?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/LilitKharatyan', 'html_url': 'https://github.com/LilitKharatyan', 'followers_url': 'https://api.github.com/users/LilitKharatyan/followers', 'following_url': 'https://api.github.com/users/LilitKharatyan/following{/other_user}', 'gists_url': 'https://api.github.com/users/LilitKharatyan/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/LilitKharatyan/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/LilitKharatyan/subscriptions', 'organizations_url': 'https://api.github.com/users/LilitKharatyan/orgs', 'repos_url': 'https://api.github.com/users/LilitKharatyan/repos', 'events_url': 'https://api.github.com/users/LilitKharatyan/events{/privacy}', 'received_events_url': 'https://api.github.com/users/LilitKharatyan/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-18T11:06:51Z', 'updated_at': '2024-03-18T11:06:51Z', 'author_association': 'NONE', 'body': 'So I have my models uploaded in the resources folder and when I want to deploy the model like this I run into the error below:\r\nnlp = stanza.Pipeline(lang=\'xcl\',\r\n                      dir=stanza_resources_path,\r\n                      processors=\'tokenize,pos,lemma,depparse\',\r\n                      tokenize_model_path=\'/content/stanza/stanza/resources/xcl/classicalarmenian/tokenizers/xcl_classicalarmenian_tokenizer.pt\',\r\n                      pos_model_path=\'/content/stanza/stanza/resources/xcl/classicalarmenian/pos/xcl_classicalarmenian_nocharlm_tagger.pt\',\r\n                      lemma_model_path=\'/content/stanza/stanza/resources/xcl/classicalarmenian/lemmatizer/xcl_classicalarmenian_nocharlm_lemmatizer.pt\',\r\n                      depparse_model_path=\'/content/stanza/stanza/resources/xcl/classicalarmenian/parsers/xcl_classicalarmenian_nocharlm_parser.pt\')\r\n\r\n`WARNING:stanza:Unsupported language: xcl.\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n[<ipython-input-10-c4fb6b6e0aaf>](https://localhost:8080/#) in <cell line: 5>()\r\n      3 \r\n      4 # Initialize the pipeline with your custom models\r\n----> 5 nlp = stanza.Pipeline(lang=\'xcl\',\r\n      6                       dir=stanza_resources_path,\r\n      7                       processors=\'tokenize,pos,lemma,depparse\',\r\n\r\n[/content/stanza/stanza/pipeline/core.py](https://localhost:8080/#) in __init__(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\r\n    260             raise ValueError(\'No processors to load for language {}.  Please check if your language or package is correctly set.\'.format(lang))\r\n    261         load_table = make_table([\'Processor\', \'Package\'], [(row[0], "";"".join(model_spec.package for model_spec in row[1])) for row in self.load_list])\r\n--> 262         logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n    263 \r\n    264         self.config = build_default_config(resources, lang, self.dir, self.load_list)\r\n\r\nUnboundLocalError: local variable \'lang_name\' referenced before assignment`\r\n\r\nHow can we overcome this issue and deploy the model? Thanks\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2003620092/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
155,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 9, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T20:26:41Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002606611', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002606611', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002606611, 'node_id': 'IC_kwDOBj_0V853XVoT', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T20:26:40Z', 'updated_at': '2024-03-17T20:26:40Z', 'author_association': 'NONE', 'body': '# Thank you\r\nvaluable tip!!!', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002606611/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
156,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 8, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T20:25:26Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002606310', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002606310', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002606310, 'node_id': 'IC_kwDOBj_0V853XVjm', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T20:25:25Z', 'updated_at': '2024-03-17T20:25:25Z', 'author_association': 'COLLABORATOR', 'body': 'True true.  But what you can do is\r\n\r\n```\r\n>>> doc = pipe(""Der Firma liegt genau am Ortseingang."")\r\n>>> doc.sentences[0].words[4]\r\n{\r\n  ""id"": 5,\r\n  ""text"": ""an""\r\n}\r\n>>> doc.sentences[0].words[4].parent\r\n[\r\n  {\r\n    ""id"": [\r\n      5,\r\n      6\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 22,\r\n    ""end_char"": 24\r\n  },\r\n  {\r\n    ""id"": 5,\r\n    ""text"": ""an""\r\n  },\r\n  {\r\n    ""id"": 6,\r\n    ""text"": ""dem""\r\n  }\r\n]\r\n>>> doc.sentences[0].words[4].parent.start_char\r\n22\r\n>>> doc.sentences[0].words[4].parent.end_char\r\n24\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002606310/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
157,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 7, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T20:21:33Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002605269', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002605269', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002605269, 'node_id': 'IC_kwDOBj_0V853XVTV', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T20:21:30Z', 'updated_at': '2024-03-17T20:21:30Z', 'author_association': 'NONE', 'body': '#### Stanza\r\n\r\nI could be doing it wrong. \r\n\r\nI doubt I get start_char and end_char for ""an"" and ""dem""\r\n```\r\nforeach word in sentence.words\r\n\r\n doc.sentences[idx].words()\r\n```\r\n\r\n\r\n\r\n```\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002605269/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
158,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T20:18:16Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002604388', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002604388', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002604388, 'node_id': 'IC_kwDOBj_0V853XVFk', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T20:18:15Z', 'updated_at': '2024-03-17T20:18:15Z', 'author_association': 'NONE', 'body': '> a multi-word token\r\nI have yet to appreciate the benefits of treating it as a multi-word token.\r\n\r\nSo far, I  only know very limited langauges.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002604388/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
159,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T20:15:57Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002603631', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002603631', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002603631, 'node_id': 'IC_kwDOBj_0V853XU5v', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T20:15:55Z', 'updated_at': '2024-03-17T20:15:55Z', 'author_association': 'COLLABORATOR', 'body': ""A ha ha I accidentally used the English CoreNLP instead of German.  Let me revise...\r\n\r\n```\r\nNLP> Der Firma liegt genau am Ortseingang.\r\n\r\nSentence #1 (8 tokens):\r\nDer Firma liegt genau am Ortseingang.\r\n\r\nTokens:\r\n[Text=Der CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DET NamedEntityTag=O]\r\n[Text=Firma CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NOUN NamedEntityTag=O]\r\n[Text=liegt CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VERB NamedEntityTag=O]\r\n[Text=genau CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=ADV NamedEntityTag=O]\r\n[Text=an CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=ADP NamedEntityTag=O]\r\n[Text=dem CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=DET NamedEntityTag=O]\r\n[Text=Ortseingang CharacterOffsetBegin=25 CharacterOffsetEnd=36 PartOfSpeech=NOUN NamedEntityTag=O]\r\n[Text=. CharacterOffsetBegin=36 CharacterOffsetEnd=37 PartOfSpeech=PUNCT NamedEntityTag=O]\r\n\r\nDependency Parse (enhanced plus plus dependencies):\r\nroot(ROOT-0, liegt-3)\r\ndet(Firma-2, Der-1)\r\nnsubj(liegt-3, Firma-2)\r\nadvmod(Ortseingang-7, genau-4)\r\ncase(Ortseingang-7, an-5)\r\ndet(Ortseingang-7, dem-6)\r\nobl:an(liegt-3, Ortseingang-7)\r\npunct(liegt-3, .-8)\r\n```\r\n\r\nOkay, that's much better.  It also splits `am`, then labels the start and end characters as the same (overlapping) text positions as the original word.  So effectively it's the same design choice as made in Stanza, but without an explicit marker that it was a multi-word token."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002603631/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
160,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T19:59:45Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002599191', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002599191', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002599191, 'node_id': 'IC_kwDOBj_0V853XT0X', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T19:59:43Z', 'updated_at': '2024-03-17T19:59:43Z', 'author_association': 'NONE', 'body': 'thx for tips how to parse.  really helpful.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002599191/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
161,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T19:59:07Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002599006', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002599006', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002599006, 'node_id': 'IC_kwDOBj_0V853XTxe', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T19:59:06Z', 'updated_at': '2024-03-17T19:59:06Z', 'author_association': 'NONE', 'body': '# First\r\n\r\nthx for taking your time to provide elaborate answer.\r\n\r\nMany top tech companies are using stanza and CoreNLP.\r\nI saw the same mistake and I am here to feedback. \r\n\r\nGerman langauge is no doubt a very challenging langauge. \r\n\r\nI am here to learn and feedback :-)\r\n\r\n#### CoreNLP ( lemma of ""am"" => ""be"" <=??\r\n> [Text=am CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=VBP Lemma=be NamedEntityTag=O]\r\n\r\n==> VBP is Unfortunately not correct.\r\n\r\n\r\n#### From ChatGPT\r\nThe lemma of “am” would be “an” and ""dem""\r\n\r\n\r\n### UD Trebank\r\nCorrect!\r\n```\r\n5-6     am      _       _       _       _       _       _       _       _\r\n5       an      an      ADP     APPR    _       7       case    _       _\r\n6       dem     der     DET     ART     Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art      7       det     _       _\r\n```\r\n\r\nan => ADP (Preposition)\r\ndem => DET (Determinant)\r\n\r\n### Spacy\r\n""an dem"" => ""an"" is a preposition and ""dem"" is a determinant article in Dative form.\r\n\r\nTherefore **ADP** (Preposition) is correct for ""am""\r\n\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002599006/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
162,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T19:44:42Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\r\n\r\n```json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  }\r\n\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002593955', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002593955', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002593955, 'node_id': 'IC_kwDOBj_0V853XSij', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T19:44:41Z', 'updated_at': '2024-03-17T19:44:41Z', 'author_association': 'COLLABORATOR', 'body': 'This is a complicated question which comes up frequently, and people never seem to like the answer.  However, my impression of that is probably the same as bullet holes in planes - only the people who don\'t like the answer show up on github.\r\n\r\nThis is what CoreNLP does:\r\n\r\n```\r\nNLP> Der Firma liegt genau am Ortseingang.\r\n\r\nSentence #1 (7 tokens):\r\nDer Firma liegt genau am Ortseingang.\r\n\r\nTokens:\r\n[Text=Der CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP Lemma=Der NamedEntityTag=O]\r\n[Text=Firma CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Firma NamedEntityTag=O]\r\n[Text=liegt CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=NN Lemma=liegt NamedEntityTag=O]\r\n[Text=genau CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=NN Lemma=genau NamedEntityTag=O]\r\n[Text=am CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=VBP Lemma=be NamedEntityTag=O]\r\n[Text=Ortseingang CharacterOffsetBegin=25 CharacterOffsetEnd=36 PartOfSpeech=NNP Lemma=Ortseingang NamedEntityTag=PERSON]\r\n[Text=. CharacterOffsetBegin=36 CharacterOffsetEnd=37 PartOfSpeech=. Lemma=. NamedEntityTag=O]\r\n\r\nDependency Parse (enhanced plus plus dependencies):\r\nroot(ROOT-0, Ortseingang-6)\r\ncompound(Firma-2, Der-1)\r\ncompound(genau-4, Firma-2)\r\ncompound(genau-4, liegt-3)\r\nnsubj(Ortseingang-6, genau-4)\r\ncop(Ortseingang-6, am-5)\r\npunct(Ortseingang-6, .-7)\r\n```\r\n\r\nThe original training data in the UD treebank was\r\n\r\n```\r\n# sent_id = train-s25\r\n# text = Der Firma liegt genau am Ortseingang.\r\n1       Der     der     DET     ART     Case=Nom|Definite=Def|Gender=Masc|Number=Sing|PronType=Art      2       det     _       _\r\n2       Firma   Firma   NOUN    NN      Case=Nom|Gender=Masc|Number=Sing        3       nsubj   _       _\r\n3       liegt   liegen  VERB    VVFIN   Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   0       root    _       _\r\n4       genau   genau   ADV     ADV     _       7       advmod  _       _\r\n5-6     am      _       _       _       _       _       _       _       _\r\n5       an      an      ADP     APPR    _       7       case    _       _\r\n6       dem     der     DET     ART     Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art      7       det     _       _\r\n7       Ortseingang     Ortseingang     NOUN    NN      Case=Dat|Gender=Masc|Number=Sing        3       obl     _       SpaceAfter=No\r\n8       .       .       PUNCT   $.      _       3       punct   _       _\r\n```\r\n\r\nThe thing with the CoreNLP representation is, `am` is not a copular verb as far as I know.  Google translate says it means ""at the"".  Also, it\'s completely missing that the `liegt` is the verb.  Basically that representation sucks.\r\n\r\nThe problem is that `am` in fact represents two words at the same time - the adposition and the determiner.  If you just implement one tag for the entire token, probably the adposition, leaving out the determiner, that would be a little weird.  Even more awkward would be a combination tag of some kind (although to be fair some datasets have adopted that approach, such as the Korean UD treebanks)\r\n\r\nThe solution UD adopted for most languages is to represent the text as a single token, `am` in this case, and split the analysis into the two words, `an` and `dem`.  It is true there are some inconveniences here as well, such as `an` does not correspond to an actual start & end character.  However, it makes analysis of words such as `am` much easier, since now you can analyze both words that it represents in a proper manner.\r\n\r\nThis happens in other languages.  In Spanish, the pronoun clitics get split from verbs - otherwise you\'d have 10x as many verbs to analyze.  In English, the entire class of possessives, standard contractions such as `can\'t`, `won\'t`, `it\'s`, and colloquial contractions such as `cannot`, `gonna`, `wanna`.  Then at the edges you can have 20 response long threads on UD about `kinda` or `mighta` as possible additions to the splittable lexicon...  (These kind of threads alternate between amusing me every time I kick one off and discouraging me from asking in the first place about the best way for our software to analyze specific text)\r\n\r\nLong story short, if all you want is the analysis of the pieces, you can either filter out from the json / dict representation any token whose id isn\'t just an int, or you can call `doc.sentences[idx].words()` instead of using the dict representation.  That might be a little unsatisfying since it won\'t have character offsets in a language such as German, where the MWT don\'t split into easily understood pieces (compare to English, where we split `cannot` -> `can not`... how would you split `am` as text?).  The `Word` objects each have a pointer to the enclosing `Token`, though, and the `Token` does have the `start_char` and `end_char` for the entire piece of text.\r\n\r\nAs for spacy, it does\r\n\r\n```\r\n>>> doc = nlp(""I don\'t know what spacy does with MWT"")\r\n>>> for token in doc:\r\n...     print(token.text, token.pos_, token.dep_)\r\n...\r\nI PRON nsubj\r\ndo AUX aux\r\nn\'t PART neg\r\nknow VERB ROOT\r\nwhat PRON det\r\nspacy NOUN nsubj\r\ndoes VERB ccomp\r\nwith ADP prep\r\nMWT PROPN pobj\r\n\r\n>>> doc = nlp(""I wanna lick Jennifer\'s antennae"")\r\n>>> for token in doc:\r\n...     print(token.text, token.pos_, token.dep_)\r\n...\r\nI PRON nsubj\r\nwanna VERB ROOT\r\nlick PROPN compound\r\nJennifer PROPN poss\r\n\'s PART case\r\nantennae NOUN dobj\r\n\r\n>>> nlp = spacy.load(\'en_core_web_trf\')\r\n>>> doc = nlp(""I wanna lick Jennifer\'s antennae"")\r\n>>> for token in doc:\r\n...    print(token.text, token.pos_, token.dep_)\r\n...\r\nI PRON nsubj\r\nwanna AUX aux\r\nlick VERB ROOT\r\nJennifer PROPN poss\r\n\'s PART case\r\nantennae NOUN dobj\r\n\r\n>>> nlp = spacy.load(\'de_dep_news_trf\')\r\n>>> doc = nlp(""Der Firma liegt genau am Ortseingang."")\r\n>>> for token in doc:\r\n...    print(token.text, token.pos_, token.dep_)\r\n...\r\nDer DET nk\r\nFirma NOUN da\r\nliegt VERB ROOT\r\ngenau ADV mo\r\nam ADP mo\r\nOrtseingang NOUN nk\r\n. PUNCT punct\r\n```\r\n\r\nSo they are treating contractions as single words (although they do split clitics).  IDK, maybe people prefer that representation\r\n\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002593955/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
163,WatchEvent,{'action': 'started'}
164,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1263', 'id': 1806139068, 'node_id': 'I_kwDOBj_0V85rp368', 'number': 1263, 'title': 'Possibility of speeding up Stanza lemmmatizer by excluding reduntant words', 'user': {'login': 'mrgransky', 'id': 11946010, 'node_id': 'MDQ6VXNlcjExOTQ2MDEw', 'avatar_url': 'https://avatars.githubusercontent.com/u/11946010?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mrgransky', 'html_url': 'https://github.com/mrgransky', 'followers_url': 'https://api.github.com/users/mrgransky/followers', 'following_url': 'https://api.github.com/users/mrgransky/following{/other_user}', 'gists_url': 'https://api.github.com/users/mrgransky/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mrgransky/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mrgransky/subscriptions', 'organizations_url': 'https://api.github.com/users/mrgransky/orgs', 'repos_url': 'https://api.github.com/users/mrgransky/repos', 'events_url': 'https://api.github.com/users/mrgransky/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mrgransky/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 9, 'created_at': '2023-07-15T14:39:41Z', 'updated_at': '2024-03-17T11:35:39Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""**Given**:\r\n\r\nI have a small sample document with limited number of words as follows:\r\n\r\n    d ='''\r\n    I go to school by the school bus everyday with all of my best friends. \r\n    There are several students who also take the buses to school. Buses are quite cheap in my city.\r\n    The city which I live in has an enormous number of brilliant schools with smart students.\r\n    We have a nice math teacher in my school whose name is Jane Doe.\r\n    She also teaches several other topics in our school, including physics, chemistry and sometimes literature as a substitute teacher.\r\n    Other classes don't appreciate her efforts as much as my class. She must be nominated as the best school's teacher.\r\n    My school is located far from my apartment. This is why, I am taking the bus to school everyday.\r\n    '''\r\n**Goal**:\r\n\r\nConsidering my real-world large document with more words (`4000 ~ 8000 words`), I would like to speed up my Stanza lemmatizer by *probably* excluding lemmatizing repeated words, *e.g.*, words which has occurred more than once.\r\nI do not intend to use `set()` method to obtain the unique lemmas in my result list, rather I intend to ignore lemmatizing words which have already been lemmatized.\r\n\r\nFor instance, for the given sample raw document `d`, there are several redundant words which could be ignored in the process:\r\n\r\n    Word                 Lemma\r\n    --------------------------------------------------\r\n    school               school\r\n    school               school <<<<< Redundant\r\n    bus                  bus\r\n    everyday             everyday\r\n    friends              friend\r\n    students             student\r\n    buses                bus\r\n    school               school <<<<< Redundant\r\n    Buses                bus <<<<< Redundant\r\n    cheap                cheap\r\n    city                 city\r\n    city                 city <<<<< Redundant\r\n    live                 live\r\n    enormous             enormous\r\n    number               number\r\n    brilliant            brilliant\r\n    schools              school\r\n    smart                smart\r\n    students             student\r\n    nice                 nice\r\n    math                 math\r\n    teacher              teacher\r\n    school               school <<<<< Redundant\r\n    Jane                 jane\r\n    Doe                  doe\r\n    teaches              teach\r\n    topics               topic\r\n    school               school <<<<< Redundant\r\n    including            include\r\n    physics              physics\r\n    chemistry            chemistry\r\n    literature           literature\r\n    substitute           substitute\r\n    teacher              teacher <<<<< Redundant\r\n    classes              class\r\n    appreciate           appreciate\r\n    efforts              effort\r\n    class                class\r\n    nominated            nominate\r\n    school               school <<<<< Redundant\r\n    teacher              teacher <<<<< Redundant\r\n    school               school <<<<< Redundant\r\n    located              locate\r\n    apartment            apartment\r\n    bus                  bus <<<<< Redundant\r\n    school               school <<<<< Redundant\r\n    everyday             everyday <<<<< Redundant\r\n\r\nMy [*inefficient*] solution:\r\n\r\n    import stanza\r\n    import nltk\r\n    nltk_modules = ['punkt', 'averaged_perceptron_tagger', 'stopwords', 'wordnet', 'omw-1.4',]\r\n    nltk.download(nltk_modules, quiet=True, raise_on_error=True,)\r\n    STOPWORDS = nltk.corpus.stopwords.words(nltk.corpus.stopwords.fileids())\r\n    \r\n    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos', tokenize_no_ssplit=True,download_method=DownloadMethod.REUSE_RESOURCES)\r\n    doc = nlp(d)\r\n    %timeit -n 10000 [ wlm.lower() for _, s in enumerate(doc.sentences) for _, w in enumerate(s.words) if (wlm:=w.lemma) and len(wlm)>2 and wlm not in STOPWORDS]\r\n    10.5 ms ± 112 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\r\n\r\nMy [*alternative*] solution, a little faster but still **NOT** efficient for (`4000 ~ 8000 words`):\r\n\r\n    def get_lm():\r\n      words_list = list()\r\n      lemmas_list = list()\r\n      for _, vsnt in enumerate(doc.sentences):\r\n        for _, vw in enumerate(vsnt.words):\r\n          wlm = vw.lemma.lower()\r\n          wtxt = vw.text.lower()\r\n          if wtxt in words_list and wlm in lemmas_list:\r\n            lemmas_list.append(wlm)\r\n          elif ( wtxt not in words_list and wlm and len(wlm) > 2 and wlm not in STOPWORDS ):\r\n            lemmas_list.append(wlm)\r\n          words_list.append(wtxt)\r\n      return lemmas_list\r\n    %timeit -n 10000 get_lm()\r\n    7.85 ms ± 66.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\r\n\r\nMy ideal result for this sample document, from either solution, should contain even repeated lemmas, as follows:\r\n\r\n    lm = [ wlm.lower() for _, s in enumerate(doc.sentences) for _, w in enumerate(s.words) if (wlm:=w.lemma) and len(wlm)>2 and wlm not in STOPWORDS] # solution 1\r\n    # lm = get_lm() # solution 2\r\n    print(len(lm), lm)\r\n    47 ['school', 'school', 'bus', 'everyday', 'friend', 'student', 'bus', 'school', 'bus', 'cheap', 'city', 'city', 'live', 'enormous', 'number', 'brilliant', 'school', 'smart', 'student', 'nice', 'math', 'teacher', 'school', 'jane', 'doe', 'teach', 'topic', 'school', 'include', 'physics', 'chemistry', 'literature', 'substitute', 'teacher', 'class', 'appreciate', 'effort', 'class', 'nominate', 'school', 'teacher', 'school', 'locate', 'apartment', 'bus', 'school', 'everyday']\r\n\r\nIs there any better or more efficient approach for this problem when considering large corpus or documents?\r\n\r\nCheers,"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002421761', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1263#issuecomment-2002421761', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1263', 'id': 2002421761, 'node_id': 'IC_kwDOBj_0V853WogB', 'user': {'login': 'stale[bot]', 'id': 26384082, 'node_id': 'MDM6Qm90MjYzODQwODI=', 'avatar_url': 'https://avatars.githubusercontent.com/in/1724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stale%5Bbot%5D', 'html_url': 'https://github.com/apps/stale', 'followers_url': 'https://api.github.com/users/stale%5Bbot%5D/followers', 'following_url': 'https://api.github.com/users/stale%5Bbot%5D/following{/other_user}', 'gists_url': 'https://api.github.com/users/stale%5Bbot%5D/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stale%5Bbot%5D/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stale%5Bbot%5D/subscriptions', 'organizations_url': 'https://api.github.com/users/stale%5Bbot%5D/orgs', 'repos_url': 'https://api.github.com/users/stale%5Bbot%5D/repos', 'events_url': 'https://api.github.com/users/stale%5Bbot%5D/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stale%5Bbot%5D/received_events', 'type': 'Bot', 'site_admin': False}, 'created_at': '2024-03-17T11:35:37Z', 'updated_at': '2024-03-17T11:35:37Z', 'author_association': 'NONE', 'body': 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002421761/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': {'id': 1724, 'slug': 'stale', 'node_id': 'MDM6QXBwMTcyNA==', 'owner': {'login': 'probot', 'id': 26350515, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjI2MzUwNTE1', 'avatar_url': 'https://avatars.githubusercontent.com/u/26350515?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/probot', 'html_url': 'https://github.com/probot', 'followers_url': 'https://api.github.com/users/probot/followers', 'following_url': 'https://api.github.com/users/probot/following{/other_user}', 'gists_url': 'https://api.github.com/users/probot/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/probot/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/probot/subscriptions', 'organizations_url': 'https://api.github.com/users/probot/orgs', 'repos_url': 'https://api.github.com/users/probot/repos', 'events_url': 'https://api.github.com/users/probot/events{/privacy}', 'received_events_url': 'https://api.github.com/users/probot/received_events', 'type': 'Organization', 'site_admin': False}, 'name': 'Stale', 'description': '## 📯 The stale app is deprecated and this repository is no longer maintained\r\n\r\nPlease use [the stale action](https://github.com/actions/stale) instead.', 'external_url': 'https://github.com/probot/stale/pull/430', 'html_url': 'https://github.com/apps/stale', 'created_at': '2017-03-13T14:06:37Z', 'updated_at': '2023-05-20T00:31:12Z', 'permissions': {'issues': 'write', 'metadata': 'read', 'pull_requests': 'write', 'single_file': 'read'}, 'events': ['issues', 'issue_comment', 'pull_request', 'pull_request_review', 'pull_request_review_comment']}}}"
165,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1323', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1323/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1323/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1323/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1323', 'id': 2045323121, 'node_id': 'I_kwDOBj_0V8556Sdx', 'number': 1323, 'title': 'How to show progress bar in pipeline? [QUESTION]', 'user': {'login': 'Hansyvea', 'id': 38463920, 'node_id': 'MDQ6VXNlcjM4NDYzOTIw', 'avatar_url': 'https://avatars.githubusercontent.com/u/38463920?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Hansyvea', 'html_url': 'https://github.com/Hansyvea', 'followers_url': 'https://api.github.com/users/Hansyvea/followers', 'following_url': 'https://api.github.com/users/Hansyvea/following{/other_user}', 'gists_url': 'https://api.github.com/users/Hansyvea/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Hansyvea/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Hansyvea/subscriptions', 'organizations_url': 'https://api.github.com/users/Hansyvea/orgs', 'repos_url': 'https://api.github.com/users/Hansyvea/repos', 'events_url': 'https://api.github.com/users/Hansyvea/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Hansyvea/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2023-12-17T18:18:23Z', 'updated_at': '2024-03-17T11:35:37Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hi, I have been using stanza bulkprocess to tokenize and ssplit a rather large text stored in a dataframe.\r\nMy question is how to show progress bar  when running the pipeline?\r\n```\r\nimport stanza\r\nimport pandas as pd\r\n\r\ndummy_df = pd.read_parquet(""../Data/Data_Frame/1987.parquet"")\r\ndummy = list(dummy_df.head(1000).TEXT)\r\nnlp = stanza.Pipeline(lang=\'en\', processors=\'tokenize\')\r\ndocs = nlp.bulk_process(dummy)\r\n...\r\n```\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1323/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1323/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002421752', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1323#issuecomment-2002421752', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1323', 'id': 2002421752, 'node_id': 'IC_kwDOBj_0V853Wof4', 'user': {'login': 'stale[bot]', 'id': 26384082, 'node_id': 'MDM6Qm90MjYzODQwODI=', 'avatar_url': 'https://avatars.githubusercontent.com/in/1724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/stale%5Bbot%5D', 'html_url': 'https://github.com/apps/stale', 'followers_url': 'https://api.github.com/users/stale%5Bbot%5D/followers', 'following_url': 'https://api.github.com/users/stale%5Bbot%5D/following{/other_user}', 'gists_url': 'https://api.github.com/users/stale%5Bbot%5D/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/stale%5Bbot%5D/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/stale%5Bbot%5D/subscriptions', 'organizations_url': 'https://api.github.com/users/stale%5Bbot%5D/orgs', 'repos_url': 'https://api.github.com/users/stale%5Bbot%5D/repos', 'events_url': 'https://api.github.com/users/stale%5Bbot%5D/events{/privacy}', 'received_events_url': 'https://api.github.com/users/stale%5Bbot%5D/received_events', 'type': 'Bot', 'site_admin': False}, 'created_at': '2024-03-17T11:35:36Z', 'updated_at': '2024-03-17T11:35:36Z', 'author_association': 'NONE', 'body': 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002421752/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': {'id': 1724, 'slug': 'stale', 'node_id': 'MDM6QXBwMTcyNA==', 'owner': {'login': 'probot', 'id': 26350515, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjI2MzUwNTE1', 'avatar_url': 'https://avatars.githubusercontent.com/u/26350515?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/probot', 'html_url': 'https://github.com/probot', 'followers_url': 'https://api.github.com/users/probot/followers', 'following_url': 'https://api.github.com/users/probot/following{/other_user}', 'gists_url': 'https://api.github.com/users/probot/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/probot/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/probot/subscriptions', 'organizations_url': 'https://api.github.com/users/probot/orgs', 'repos_url': 'https://api.github.com/users/probot/repos', 'events_url': 'https://api.github.com/users/probot/events{/privacy}', 'received_events_url': 'https://api.github.com/users/probot/received_events', 'type': 'Organization', 'site_admin': False}, 'name': 'Stale', 'description': '## 📯 The stale app is deprecated and this repository is no longer maintained\r\n\r\nPlease use [the stale action](https://github.com/actions/stale) instead.', 'external_url': 'https://github.com/probot/stale/pull/430', 'html_url': 'https://github.com/apps/stale', 'created_at': '2017-03-13T14:06:37Z', 'updated_at': '2023-05-20T00:31:12Z', 'permissions': {'issues': 'write', 'metadata': 'read', 'pull_requests': 'write', 'single_file': 'read'}, 'events': ['issues', 'issue_comment', 'pull_request', 'pull_request_review', 'pull_request_review_comment']}}}"
166,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T10:55:04Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\'\'\'json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  },\r\n\'\'\'', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002406419', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369#issuecomment-2002406419', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'id': 2002406419, 'node_id': 'IC_kwDOBj_0V853WkwT', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T10:55:03Z', 'updated_at': '2024-03-17T10:55:03Z', 'author_association': 'NONE', 'body': 'I double if Spacy would handle this way, I am simply curious', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002406419/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
167,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368', 'id': 2190218719, 'node_id': 'I_kwDOBj_0V86CjBXf', 'number': 1368, 'title': '[QUESTION] Is constituency available in Stanza German Model?', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-16T19:45:57Z', 'updated_at': '2024-03-17T10:53:03Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Not sure where to look for this information, which processors are available for the German model\r\n\r\nhttps://stanfordnlp.github.io/stanza/pipeline.html', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002405772', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368#issuecomment-2002405772', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'id': 2002405772, 'node_id': 'IC_kwDOBj_0V853WkmM', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T10:53:02Z', 'updated_at': '2024-03-17T10:53:02Z', 'author_association': 'NONE', 'body': '@AngledLuffa \r\nI think a data-driven transformer with better accuracy is more relevant. \r\n\r\nThe more accurate tree means a better chance to evaluate and benchmark how well the more accurate tree could handle a more complex German sentence, which is very typical in day to day ussage. ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002405772/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
168,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1369', 'id': 2190598903, 'node_id': 'I_kwDOBj_0V86CkeL3', 'number': 1369, 'title': '[QUESTION] German contraction of ""an dem"" to ""am""', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-17T10:47:21Z', 'updated_at': '2024-03-17T10:47:21Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '## Am\r\n“Am” is a contraction of “an” and “dem”.\r\n- It is used to mean “at the” for locations or times of day\r\n- For example, “**am** Wochenende” means “on the weekend”.\r\n\r\n## An dem\r\n“An dem” is used when you want to keep “an” and “dem” separate for emphasis or clarity. \r\n- However, it’s not wrong to use “an dem” instead of “am”, but it might sound a bit unusual\r\n\r\n# How Stanza handles them?\r\n\r\nOne word ""**am**"" with the right word id has **TWO** more additional words: ""**an dem**""\r\n\r\nIt is simpler to just parse an int coming back from a word.id. \r\nNow, instead of int, it is an array referencing the **TWO** additional words\r\n\r\nThe challenges:\r\nThe parent word has start_char and end_char, but the other morphological features are now transferred to the child word e.g. dem\r\n\r\n## Question\r\nI wonder how best to handle this when parsing. \r\n### [1]\r\n- please indicate which part of stanza code that create the additional words and how stanza handles them when there exist in a sentence\r\n\r\n### [2]\r\n- Is similar thing also happens to **CoreNLP (Java)**. Please indicate where are the codes that create and where are the code that parse them successfully.\r\n\r\n\r\n![image](https://github.com/stanfordnlp/stanza/assets/49812372/846335f1-e5b6-4249-b917-7dc662b6cb31)\r\n\r\n\'\'\'json\r\n{\r\n    ""id"": [\r\n      10,\r\n      11\r\n    ],\r\n    ""text"": ""am"",\r\n    ""start_char"": 56,\r\n    ""end_char"": 58,\r\n    ""ner"": ""O"",\r\n    ""multi_ner"": [\r\n      ""O""\r\n    ]\r\n  },\r\n  {\r\n    ""id"": 10,\r\n    ""text"": ""an"",\r\n    ""lemma"": ""an"",\r\n    ""upos"": ""ADP"",\r\n    ""xpos"": ""APPR"",\r\n    ""head"": 12,\r\n    ""deprel"": ""case""\r\n  },\r\n  {\r\n    ""id"": 11,\r\n    ""text"": ""dem"",\r\n    ""lemma"": ""der"",\r\n    ""upos"": ""DET"",\r\n    ""xpos"": ""ART"",\r\n    ""feats"": ""Case=Dat|Definite=Def|Gender=Masc|Number=Sing|PronType=Art"",\r\n    ""head"": 12,\r\n    ""deprel"": ""det""\r\n  },\r\n\'\'\'', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1369/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
169,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368', 'id': 2190218719, 'node_id': 'I_kwDOBj_0V86CjBXf', 'number': 1368, 'title': '[QUESTION] Is constituency available in Stanza German Model?', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-16T19:45:57Z', 'updated_at': '2024-03-17T08:14:35Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Not sure where to look for this information, which processors are available for the German model\r\n\r\nhttps://stanfordnlp.github.io/stanza/pipeline.html', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002357087', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368#issuecomment-2002357087', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'id': 2002357087, 'node_id': 'IC_kwDOBj_0V853WYtf', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-17T08:14:33Z', 'updated_at': '2024-03-17T08:14:33Z', 'author_association': 'COLLABORATOR', 'body': 'After looking through the options available to us, I think the SPMRL\nincarnation of the TIGER treebank might be the best option.  It has 38600\ntraining trees (after removing duplicates) and has been used elsewhere.\n\nDo you want a model with or without transformer?  With transformer will be\nmuch more accurate, but will take a lot more memory as well.\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002357087/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
170,PushEvent,"{'repository_id': 104854615, 'push_id': 17577419185, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '3dc1665cf3d0b20e126cabe71dea2a9573c002aa', 'before': '67d2056771ffe03995f2578e0f5dfd9f8a5759f0', 'commits': [{'sha': '3dc1665cf3d0b20e126cabe71dea2a9573c002aa', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add functionality to the tree_callback when reading a treebank to transform the tree, not just process each tree result', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/3dc1665cf3d0b20e126cabe71dea2a9573c002aa'}]}"
171,PushEvent,"{'repository_id': 104854615, 'push_id': 17576567895, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '67d2056771ffe03995f2578e0f5dfd9f8a5759f0', 'before': '26b46ae9f3e6838e5104767122a7569aebc11e9e', 'commits': [{'sha': '67d2056771ffe03995f2578e0f5dfd9f8a5759f0', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add some more notes on another German transformer, add some nicknames for the German transformers', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/67d2056771ffe03995f2578e0f5dfd9f8a5759f0'}]}"
172,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368', 'id': 2190218719, 'node_id': 'I_kwDOBj_0V86CjBXf', 'number': 1368, 'title': '[QUESTION] Is constituency available in Stanza German Model?', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-16T19:45:57Z', 'updated_at': '2024-03-16T22:51:16Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Not sure where to look for this information, which processors are available for the German model\r\n\r\nhttps://stanfordnlp.github.io/stanza/pipeline.html', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002168667', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368#issuecomment-2002168667', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'id': 2002168667, 'node_id': 'IC_kwDOBj_0V853Vqtb', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-16T22:51:15Z', 'updated_at': '2024-03-16T22:51:15Z', 'author_association': 'COLLABORATOR', 'body': ""The issue here is one of license.  There is a large dataset available at\r\nTübingen, but they are hesitant to license it in a situation that may\r\nresult in commercial use.  I'll ask again nicely with the thought being\r\nthat it won't be possible to reconstruct the original dataset from our\r\nmodel.  Another option is the Negra treebank, although that is\r\nsignificantly smaller.  I will consult with my PI and hopefully we can put\r\nsomething together by the end of next week.\r\n\r\nOn Sat, Mar 16, 2024 at 12:46\u202fPM GeorgeS2019 ***@***.***>\r\nwrote:\r\n\r\n> Not sure where to look for this information, which processors are\r\n> available for the German model\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/stanfordnlp/stanza/issues/1368>, or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AA2AYWPMTZA5A2PNIBFFXW3YYSOQZAVCNFSM6AAAAABEZRWXKCVHI2DSMVQWIX3LMV43ASLTON2WKOZSGE4TAMRRHA3TCOI>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002168667/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
173,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1368', 'id': 2190218719, 'node_id': 'I_kwDOBj_0V86CjBXf', 'number': 1368, 'title': '[QUESTION] Is constituency available in Stanza German Model?', 'user': {'login': 'GeorgeS2019', 'id': 49812372, 'node_id': 'MDQ6VXNlcjQ5ODEyMzcy', 'avatar_url': 'https://avatars.githubusercontent.com/u/49812372?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/GeorgeS2019', 'html_url': 'https://github.com/GeorgeS2019', 'followers_url': 'https://api.github.com/users/GeorgeS2019/followers', 'following_url': 'https://api.github.com/users/GeorgeS2019/following{/other_user}', 'gists_url': 'https://api.github.com/users/GeorgeS2019/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/GeorgeS2019/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/GeorgeS2019/subscriptions', 'organizations_url': 'https://api.github.com/users/GeorgeS2019/orgs', 'repos_url': 'https://api.github.com/users/GeorgeS2019/repos', 'events_url': 'https://api.github.com/users/GeorgeS2019/events{/privacy}', 'received_events_url': 'https://api.github.com/users/GeorgeS2019/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-16T19:45:57Z', 'updated_at': '2024-03-16T19:45:57Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Not sure where to look for this information, which processors are available for the German model', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1368/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
174,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 24, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-16T14:40:21Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002006824', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2002006824', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2002006824, 'node_id': 'IC_kwDOBj_0V853VDMo', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-16T14:40:20Z', 'updated_at': '2024-03-16T14:40:20Z', 'author_association': 'NONE', 'body': ""I'll train the full cycle for both, at least the same length I took in the previous version, and compare.\r\n\r\nThe tagger, for 500 steps, took 2 hours. The parser took 1.5 hours. \r\n\r\nI'm not using a GPU, I don't have access to one at the moment.\r\n\r\nAt some point this year (I can't give dates since I'm not working on the data) there will be more annotated data available, I think around 200-300k. There is no more raw text available for this language, as far as I know. If there is, it shouldn't be a significant amount. I'm using the whole, or almost, corpus of OE existing."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002006824/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
175,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 23, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-16T14:18:15Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002000503', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2002000503', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2002000503, 'node_id': 'IC_kwDOBj_0V853VBp3', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-16T14:18:15Z', 'updated_at': '2024-03-16T14:18:15Z', 'author_association': 'COLLABORATOR', 'body': 'I would definitely train the parser the full training cycle to see if it\nimproves overall.  I would hate to release a model which is both larger\n(and therefore slower) and also less accurate.\n\nIf you have more text available, or in the future if you have more text\navailable, I can try retraining those.\n\nHow long do those 500 steps take?  Are you using a GPU?\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2002000503/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
176,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 22, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-16T11:10:05Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2001951897', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2001951897', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2001951897, 'node_id': 'IC_kwDOBj_0V853U1yZ', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-16T11:10:04Z', 'updated_at': '2024-03-16T11:10:04Z', 'author_association': 'NONE', 'body': ""Hi!\r\n\r\nI have tried retraining the tagger and parser with the charlm model, the 1024 one, thanks for this!\r\n\r\nI have tried with just 500 steps, the previous versions of the tagger and parser were trained with 4000 steps. So far, with the 1024 charlm, the tagger improves 2%, but the parser doesn't, it's 5% worse. \r\n\r\nWhat I think is important is that the efficiency is improved, since I don't need to run so long training sessions to achieve similar, or better, results.\r\n\r\nDo you think that training the tagger and parser with more steps, maybe not 4000 but 1000 or 2000, is worth trying to improve the scores? The nocharlm_depparser plateaued at step 3600 of 4000, while the nocharlm_tagger plateaued at step 3100.\r\n\r\nAnother question is, is the lemma model dependent on the charlm as well? It would be nice to retrain it if it does.\r\n\r\nThanks for your help!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2001951897/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
177,PushEvent,"{'repository_id': 104854615, 'push_id': 17568102542, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '26b46ae9f3e6838e5104767122a7569aebc11e9e', 'before': '3d95abc2f863937ff29a28c50cc8ccf9b69b9e58', 'commits': [{'sha': '26b46ae9f3e6838e5104767122a7569aebc11e9e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Simply the depparse script so that it only needs --eval_file to change the file used for testing, not both --eval_file and --gold_file.  Add a flag to the depparse to optionally not run the scoring (which means it is just doing labeling)', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/26b46ae9f3e6838e5104767122a7569aebc11e9e'}]}"
178,PushEvent,"{'repository_id': 104854615, 'push_id': 17567935183, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '3d95abc2f863937ff29a28c50cc8ccf9b69b9e58', 'before': '7b4bbf19bf4cfbc212f32dfbcace7837e11ad68b', 'commits': [{'sha': '3d95abc2f863937ff29a28c50cc8ccf9b69b9e58', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Oops agani', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/3d95abc2f863937ff29a28c50cc8ccf9b69b9e58'}]}"
179,PushEvent,"{'repository_id': 104854615, 'push_id': 17567694993, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '7b4bbf19bf4cfbc212f32dfbcace7837e11ad68b', 'before': '77c5c5d6fa14921da468b97b92b45ebea306975e', 'commits': [{'sha': '7b4bbf19bf4cfbc212f32dfbcace7837e11ad68b', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Oops', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/7b4bbf19bf4cfbc212f32dfbcace7837e11ad68b'}]}"
180,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 21, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-15T21:59:13Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2000534149', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-2000534149', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 2000534149, 'node_id': 'IC_kwDOBj_0V853PbqF', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T21:59:11Z', 'updated_at': '2024-03-15T21:59:11Z', 'author_association': 'COLLABORATOR', 'body': ""Alright, I upgraded `run_depparse.py` so it only needs one `--eval_file` argument to change the file used for scoring.  That should make it easier to get scores on the training set, if you want them.\r\n\r\nI also posted three versions of our character model.  I put them on HF as the `nerthus1024`, `nerthus512`, and `nerthus256` packages.  Models in which the hidden dimension is 1024, 512, and 256 wide, respectively.  You should be able to rerun the POS and depparse training using them with the `--charlm nerthus1024` etc options.  If for some reason that doesn't work, please LMK and we'll figure it out.\r\n\r\nI don't have particularly high hopes for them, to be honest... this would be the smallest charlm we've used so far.  Still, it might be interesting to find out that it does actually help performance."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/2000534149/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
181,PushEvent,"{'repository_id': 104854615, 'push_id': 17567143014, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '77c5c5d6fa14921da468b97b92b45ebea306975e', 'before': 'd180ae02b278dd09dff53bc910e7aa43656e944d', 'commits': [{'sha': '77c5c5d6fa14921da468b97b92b45ebea306975e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Simply the depparse script so that it only needs --eval_file to change the file used for testing, not both --eval_file and --gold_file.  Add a flag to the depparse to optionally not run the scoring (which means it is just doing labeling)', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/77c5c5d6fa14921da468b97b92b45ebea306975e'}]}"
182,PushEvent,"{'repository_id': 104854615, 'push_id': 17563794910, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': 'bfbe0e5aea0684ff546c22a9ff4e62c3d2385a52', 'before': '2753726fbc0eb9ed52bb8a652e527b5dbe618281', 'commits': [{'sha': 'bfbe0e5aea0684ff546c22a9ff4e62c3d2385a52', 'author': {'email': 'houjun@stanford.edu', 'name': 'Houjun Liu'}, 'message': 'projecting input', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/bfbe0e5aea0684ff546c22a9ff4e62c3d2385a52'}]}"
183,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 20, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-15T16:03:10Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999975645', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1999975645', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1999975645, 'node_id': 'IC_kwDOBj_0V853NTTd', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T16:03:09Z', 'updated_at': '2024-03-15T16:03:09Z', 'author_association': 'COLLABORATOR', 'body': ""There's no `--score_train`...  could add such a thing, if you think it's relevant.  You can work around it for now by doing \r\n\r\n`--score_dev --eval_file <path_to_train>`\r\n\r\nAwkwardly, the depparse also needs `--gold_file <path_to_train>`.  I should change that so it no longer needs both"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999975645/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
184,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1367', 'id': 2188100924, 'node_id': 'I_kwDOBj_0V86Ca8U8', 'number': 1367, 'title': 'Has no attribute sys.stderr.isatty', 'user': {'login': 'AlekPet', 'id': 25489996, 'node_id': 'MDQ6VXNlcjI1NDg5OTk2', 'avatar_url': 'https://avatars.githubusercontent.com/u/25489996?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AlekPet', 'html_url': 'https://github.com/AlekPet', 'followers_url': 'https://api.github.com/users/AlekPet/followers', 'following_url': 'https://api.github.com/users/AlekPet/following{/other_user}', 'gists_url': 'https://api.github.com/users/AlekPet/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AlekPet/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AlekPet/subscriptions', 'organizations_url': 'https://api.github.com/users/AlekPet/orgs', 'repos_url': 'https://api.github.com/users/AlekPet/repos', 'events_url': 'https://api.github.com/users/AlekPet/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AlekPet/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-15T09:38:13Z', 'updated_at': '2024-03-15T15:27:33Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hello, add a check for the presence of the __isatty__ attribute in the stanza/utils/get_tqdm.py module line 34.\n\nError:\nhttps://github.com/AlekPet/ComfyUI_Custom_Nodes_AlekPet/issues/43\n\nThanks😊', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999908078', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1367#issuecomment-1999908078', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367', 'id': 1999908078, 'node_id': 'IC_kwDOBj_0V853NCzu', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T15:27:32Z', 'updated_at': '2024-03-15T15:27:32Z', 'author_association': 'COLLABORATOR', 'body': 'I think it is expected that file objects have `isatty()`\r\n\r\nhttps://docs.python.org/3/library/io.html#io.IOBase.isatty\r\n\r\nNevertheless, I can add a check for `hasattr(sys.stderr, isatty)`', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999908078/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
185,PushEvent,"{'repository_id': 104854615, 'push_id': 17562673519, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'd180ae02b278dd09dff53bc910e7aa43656e944d', 'before': 'b62c1e7f8e0e17ea38d49683aea8a0cfb3db8b90', 'commits': [{'sha': 'd180ae02b278dd09dff53bc910e7aa43656e944d', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Check that sys.stderr has isatty()...   https://github.com/stanfordnlp/stanza/issues/1367', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/d180ae02b278dd09dff53bc910e7aa43656e944d'}]}"
186,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1367', 'id': 2188100924, 'node_id': 'I_kwDOBj_0V86Ca8U8', 'number': 1367, 'title': 'Has no attribute sys.stderr.isatty', 'user': {'login': 'AlekPet', 'id': 25489996, 'node_id': 'MDQ6VXNlcjI1NDg5OTk2', 'avatar_url': 'https://avatars.githubusercontent.com/u/25489996?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AlekPet', 'html_url': 'https://github.com/AlekPet', 'followers_url': 'https://api.github.com/users/AlekPet/followers', 'following_url': 'https://api.github.com/users/AlekPet/following{/other_user}', 'gists_url': 'https://api.github.com/users/AlekPet/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AlekPet/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AlekPet/subscriptions', 'organizations_url': 'https://api.github.com/users/AlekPet/orgs', 'repos_url': 'https://api.github.com/users/AlekPet/repos', 'events_url': 'https://api.github.com/users/AlekPet/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AlekPet/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-15T09:38:13Z', 'updated_at': '2024-03-15T09:38:13Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Hello, add a check for the presence of the __isatty__ attribute in the stanza/utils/get_tqdm.py module line 34.\n\nError:\nhttps://github.com/AlekPet/ComfyUI_Custom_Nodes_AlekPet/issues/43\n\nThanks😊', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1367/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
187,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 19, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-15T09:16:39Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999238693', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1999238693', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1999238693, 'node_id': 'IC_kwDOBj_0V853KfYl', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T09:16:38Z', 'updated_at': '2024-03-15T09:16:38Z', 'author_association': 'NONE', 'body': ""Yes, that is what I was referring to, can I also evaluate the train set? I have tried with ```--score_train``` but it didn't work. Maybe I made that up. \r\n\r\nI'll make a note on the scores in the meantime\r\n\r\nThanks!\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999238693/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
188,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 18, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-15T09:07:17Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999222513', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1999222513', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1999222513, 'node_id': 'IC_kwDOBj_0V853Kbbx', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T09:07:16Z', 'updated_at': '2024-03-15T09:07:16Z', 'author_association': 'COLLABORATOR', 'body': ""> Is there a way of replicating the evaluation process for the depparser?\r\n\r\nJust to clarify, the scores it reported when running `prepare_depparse` are for the POS.  If that's not well documented, I ought to clear that up...\r\n\r\nYou can evaluate specifically the dev set as part of `run_pos.py`, `run_depparse.py`, etc, with `--score_dev`.  The test set, with `--score_test`.\r\n\r\nI'm not sure if I've fully answered your questions, but please let me know if you need more."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999222513/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
189,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 17, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-15T09:00:50Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999210365', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1999210365', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1999210365, 'node_id': 'IC_kwDOBj_0V853KYd9', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T09:00:49Z', 'updated_at': '2024-03-15T09:00:49Z', 'author_association': 'NONE', 'body': 'Hi,\r\n\r\nIf I\'m understanding correctly, you are saying that the learning curve that I\'m taking is for the dev set, and then the rest of scores is just the evaluation against the three splits? I can go with that when writing down the process for their publication, if I\'m correct in this.\r\n\r\nIs there a way of replicating the evaluation process for the depparser? Is there any section in the documentation for that? If I\'m adding those scores for the POS tagger, I guess I should add similar information about the depparser as well.\r\n\r\nThe 68 score is for the dev set, I haven\'t run the test set myself. From the extract that I shared with you, it says that the 82 is for the test set, so that\'s what you are referring to, I guess. \r\n\r\nAbout the naming convention, not sure what my collaborator is going to name it, since the data is not mine, I\'m just working on the model. You can go with ""nerthus"" for the moment, since that\'s the name of the research group I\'m part of.\r\n\r\nThanks!', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999210365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
190,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 16, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-15T08:37:08Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999172174', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1999172174', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1999172174, 'node_id': 'IC_kwDOBj_0V853KPJO', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T08:37:07Z', 'updated_at': '2024-03-15T08:37:07Z', 'author_association': 'COLLABORATOR', 'body': 'The three scores for `prepare_depparse` are the tag scores when retagging each of the train, dev, and test sections.  The train section score is suitably high, after all...  When you say the score you get for the tagger is ~68, is that for running the dev set?  If you then run the test set, does it get around 82?  If so, that sounds like things are working as intended.\r\n\r\nI started running the charlm training.  I don\'t have a ton of confidence that this amount of data will produce a usable charlm, but it doesn\'t hurt to try.  We can also try with a lower number of parameters to compensate for the data size.  I\'ll let you know when I have something.\r\n\r\nWhat name should I give your dataset / models?  I could call them ""dario"", or stick with your name of ""test"", but none of that seems ideal.  Probably something based on whatever UD name you want to use will work best\r\n\r\nGlad that the UD connection is made!', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999172174/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
191,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 15, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-15T07:25:51Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999079609', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1999079609', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1999079609, 'node_id': 'IC_kwDOBj_0V853J4i5', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-15T07:25:51Z', 'updated_at': '2024-03-15T07:25:51Z', 'author_association': 'NONE', 'body': ""Hi,\r\n\r\nPlease find attached the pt files for the tokenizer, lemmatizer, POS tagger, and depparser.\r\n\r\nhttps://we.tl/t-luBVQZVHDr\r\n\r\nI have a question:\r\n\r\nOnce the tagger is trained, when preparing the treebank for the depparser I'm getting the following message:\r\n\r\n```\r\n2024-03-14 11:56:46 INFO: Loading data with batch size 250...\r\n2024-03-14 11:56:47 INFO: Start evaluation...\r\n2024-03-14 11:56:54 INFO: UPOS  XPOS    UFeats  AllTags\r\n2024-03-14 11:56:54 INFO: 99.51 99.42   98.01   97.51\r\n2024-03-14 11:56:54 INFO: POS Tagger score: ang_test 97.51\r\n2024-03-14 11:56:54 INFO: Running tagger to retag /var/folders/4c/zqzwjpjn42xbggh1sw1255nh0000gn/T/tmpe956v25x/ang_test.dev.gold.conllu to ./data/depparse/ang_test.dev.in.conllu\r\n  Args: ['--wordvec_dir', './extern_data/wordvec', '--lang', 'ang', '--shorthand', 'ang_test', '--mode', 'predict', '--save_dir', 'saved_models/pos', '--save_name', 'ang_test_nocharlm_tagger.pt', '--wordvec_pretrain_file', '/Users/dario/stanza_resources/ang/pretrain/ang_embeddings.pt', '--eval_file', '/var/folders/4c/zqzwjpjn42xbggh1sw1255nh0000gn/T/tmpe956v25x/ang_test.dev.gold.conllu', '--output_file', './data/depparse/ang_test.dev.in.conllu']\r\n2024-03-14 11:56:54 INFO: Running tagger in predict mode\r\n2024-03-14 11:56:54 INFO: Loading model from: saved_models/pos/ang_test_nocharlm_tagger.pt\r\n2024-03-14 11:56:54 DEBUG: Loaded pretrain from /Users/dario/stanza_resources/ang/pretrain/ang_embeddings.pt\r\n2024-03-14 11:56:54 INFO: Loading data with batch size 250...\r\n2024-03-14 11:56:54 INFO: Start evaluation...\r\n2024-03-14 11:56:55 INFO: UPOS  XPOS    UFeats  AllTags\r\n2024-03-14 11:56:55 INFO: 85.10 84.88   72.42   68.58\r\n2024-03-14 11:56:55 INFO: POS Tagger score: ang_test 68.58\r\n2024-03-14 11:56:55 INFO: Running tagger to retag /var/folders/4c/zqzwjpjn42xbggh1sw1255nh0000gn/T/tmpe956v25x/ang_test.test.gold.conllu to ./data/depparse/ang_test.test.in.conllu\r\n  Args: ['--wordvec_dir', './extern_data/wordvec', '--lang', 'ang', '--shorthand', 'ang_test', '--mode', 'predict', '--save_dir', 'saved_models/pos', '--save_name', 'ang_test_nocharlm_tagger.pt', '--wordvec_pretrain_file', '/Users/dario/stanza_resources/ang/pretrain/ang_embeddings.pt', '--eval_file', '/var/folders/4c/zqzwjpjn42xbggh1sw1255nh0000gn/T/tmpe956v25x/ang_test.test.gold.conllu', '--output_file', './data/depparse/ang_test.test.in.conllu']\r\n2024-03-14 11:56:55 INFO: Running tagger in predict mode\r\n2024-03-14 11:56:55 INFO: Loading model from: saved_models/pos/ang_test_nocharlm_tagger.pt\r\n2024-03-14 11:56:55 DEBUG: Loaded pretrain from /Users/dario/stanza_resources/ang/pretrain/ang_embeddings.pt\r\n2024-03-14 11:56:55 INFO: Loading data with batch size 250...\r\n2024-03-14 11:56:55 INFO: Start evaluation...\r\n2024-03-14 11:56:57 INFO: UPOS  XPOS    UFeats  AllTags\r\n2024-03-14 11:56:57 INFO: 91.42 91.38   85.31   82.41\r\n2024-03-14 11:56:57 INFO: POS Tagger score: ang_test 82.41\r\n```\r\n\r\nWhich is the correct POS tagger score? After training the tagger, I'm getting a score of 68.58. If the scores are different when preparing the following treebank, I can guess that the same is going to happen with the depparser. Could you confirm that?\r\n\r\nIf that's the case, how can I reflect this? I have saved the terminal output of the training process for both models to generate learning curve graphs for visualization. If the scores are different from the training process when they are being evaluated again, how can I reflect the difference in scores?\r\n\r\nThanks!\r\n\r\nPS, I'll contact Dan next week to ask for further information about format of the treebank, or any other information we may need to start working on that.\r\n\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1999079609/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
192,WatchEvent,{'action': 'started'}
193,PushEvent,"{'repository_id': 104854615, 'push_id': 17555390164, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '2753726fbc0eb9ed52bb8a652e527b5dbe618281', 'before': '9c43b27d60417c9a06743bbe59e8effce72cc798', 'commits': [{'sha': '2753726fbc0eb9ed52bb8a652e527b5dbe618281', 'author': {'email': 'houjun@stanford.edu', 'name': 'Houjun Liu'}, 'message': '[wip] updated scoring functions and config', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/2753726fbc0eb9ed52bb8a652e527b5dbe618281'}]}"
194,WatchEvent,{'action': 'started'}
195,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 14, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-14T21:47:31Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998536782', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1998536782', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1998536782, 'node_id': 'IC_kwDOBj_0V853H0BO', 'user': {'login': 'dan-zeman', 'id': 663752, 'node_id': 'MDQ6VXNlcjY2Mzc1Mg==', 'avatar_url': 'https://avatars.githubusercontent.com/u/663752?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dan-zeman', 'html_url': 'https://github.com/dan-zeman', 'followers_url': 'https://api.github.com/users/dan-zeman/followers', 'following_url': 'https://api.github.com/users/dan-zeman/following{/other_user}', 'gists_url': 'https://api.github.com/users/dan-zeman/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dan-zeman/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dan-zeman/subscriptions', 'organizations_url': 'https://api.github.com/users/dan-zeman/orgs', 'repos_url': 'https://api.github.com/users/dan-zeman/repos', 'events_url': 'https://api.github.com/users/dan-zeman/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dan-zeman/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T21:47:30Z', 'updated_at': '2024-03-14T21:47:30Z', 'author_association': 'NONE', 'body': ""> In general you can reach out to Dan Zeman or Joachim Nivre - I'll be happy to write an introduction email if you put me in touch with your collaborator. I can always just @ them here to see what they think (although I'm not sure it works across repos like that): @dan-zeman @jnivre\r\n\r\nIt does work :-) I confirm that we'll be more than happy to add Old English to the UD collection!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998536782/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
196,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 13, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-14T20:48:57Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998460183', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1998460183', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1998460183, 'node_id': 'IC_kwDOBj_0V853HhUX', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T20:48:57Z', 'updated_at': '2024-03-14T20:48:57Z', 'author_association': 'NONE', 'body': ""I'm attaching you all the raw data that I have available. If I'm not wrong, that's 3 million words\r\n\r\n[ang.txt](https://github.com/stanfordnlp/stanza/files/14607723/ang.txt)\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998460183/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
197,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 12, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-14T19:35:11Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998288053', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1998288053', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1998288053, 'node_id': 'IC_kwDOBj_0V853G3S1', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T19:35:10Z', 'updated_at': '2024-03-14T19:35:10Z', 'author_association': 'COLLABORATOR', 'body': ""Oh wait, that wasn't the raw ANG data in the file titled `ang_ewt-ud-complete.txt`, that was the raw vectors file.  No worries, but certainly that isn't suitable for a charlm.\r\n\r\nHow much raw data is there, roughly speaking?"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998288053/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
198,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 11, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-14T19:32:06Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998281522', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1998281522', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1998281522, 'node_id': 'IC_kwDOBj_0V853G1sy', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T19:32:05Z', 'updated_at': '2024-03-14T19:32:05Z', 'author_association': 'COLLABORATOR', 'body': 'Alright, sounds good.  I can also try the charlm on my side - should be pretty easy for me to fire it up considering my existing familiarity with the tool!', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998281522/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
199,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 10, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-14T19:26:25Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998263698', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1998263698', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1998263698, 'node_id': 'IC_kwDOBj_0V853GxWS', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T19:26:23Z', 'updated_at': '2024-03-14T19:26:23Z', 'author_association': 'NONE', 'body': ""Hi!  Thanks for your thorough response and suggestions!\r\n\r\nFirst of, we already considered using some form of transformer, but there is no close enough language to Old English. Also, my collaborator is interested in this approach of building it this way, and then to start working with further data. Still, I'll take a look at the character model.\r\n\r\nI am in the process of finishing the training of the depparser. So far, the POS tagger gets to 68%, and the depparser is doing roughly 61% at the moment. Once I finish them, I'll send those to you, together with the tokenizer and lemmatizer. I'll try again training the character model, and do a small training to see if there are improvements. \r\n\r\nThanks!\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998263698/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
200,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 9, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-14T19:14:18Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998209499', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1998209499', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1998209499, 'node_id': 'IC_kwDOBj_0V853GkHb', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T19:14:17Z', 'updated_at': '2024-03-14T19:14:17Z', 'author_association': 'COLLABORATOR', 'body': ""Thank you, that's exactly what I meant.  It has both the weights and the vectors themselves.\r\n\r\nI have my doubts about whether 64MB is enough text to train a character model that will make a meaningful improvement, but I can try it and send that back to you if you like.  You could also try going through the charlm instructions yourself - you'd need to change the part where you split the data to make it 54 MB training and 10 MB dev, for example.\r\n\r\n\r\nAnother option to consider is there are a few methods for training a transformer which "", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1998209499/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
201,PushEvent,"{'repository_id': 104854615, 'push_id': 17549007862, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '9c43b27d60417c9a06743bbe59e8effce72cc798', 'before': '2b7f1dcf716c92759abe0ecc73cfbde9f498981e', 'commits': [{'sha': '9c43b27d60417c9a06743bbe59e8effce72cc798', 'author': {'email': 'houjun@stanford.edu', 'name': 'Houjun Liu'}, 'message': 'fixes for the mha', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/9c43b27d60417c9a06743bbe59e8effce72cc798'}]}"
202,PushEvent,"{'repository_id': 104854615, 'push_id': 17548966376, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '2b7f1dcf716c92759abe0ecc73cfbde9f498981e', 'before': '82f54369fa7ce9829a80ae5c89c9b27cc9ad7972', 'commits': [{'sha': '2b7f1dcf716c92759abe0ecc73cfbde9f498981e', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': '[wip] attn', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/2b7f1dcf716c92759abe0ecc73cfbde9f498981e'}]}"
203,PushEvent,"{'repository_id': 104854615, 'push_id': 17548879820, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': '82f54369fa7ce9829a80ae5c89c9b27cc9ad7972', 'before': 'bef78a9077f4178f1477c174ea2bb0f71d2a62fd', 'commits': [{'sha': '82f54369fa7ce9829a80ae5c89c9b27cc9ad7972', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'updated rough scorer with a transformer', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/82f54369fa7ce9829a80ae5c89c9b27cc9ad7972'}]}"
204,WatchEvent,{'action': 'started'}
205,WatchEvent,{'action': 'started'}
206,WatchEvent,{'action': 'started'}
207,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361', 'id': 2170276674, 'node_id': 'I_kwDOBj_0V86BW8tC', 'number': 1361, 'title': 'Stanza 1.7.0+ makes breaking API changes for possessives, tokens excluding `end_char` and `start_char` fields', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 8, 'created_at': '2024-03-05T22:54:18Z', 'updated_at': '2024-03-14T03:00:07Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m updating Stanza from 1.6.1 to 1.7.x / 1.8.x and noticed a number of breaking API changes in the Stanza Token result when handling possessives.\r\n\r\n**To Reproduce**\r\n1. Send to stanza a sentence containing a possessive apostrophe like `Joe\'s dog.`.\r\n2. Look at the Universal Dependencies.\r\n\r\nStanza now includes a new additional token that I\'ll call an ""aggregate token"" with the `text` field `Joe\'s`.  This new aggregate token comes in addition to the tokens for `Joe` and `\'s`.  The new aggregate token returns an `id` with a list to the other two tokens:\r\n\r\n```json\r\n// the new aggregate token appearing for each possessive apostrophe\r\n    {\r\n      ""end_char"": 5,\r\n      ""id"": [\r\n        1,\r\n        2\r\n      ],\r\n      ""start_char"": 0,\r\n      ""text"": ""Joe\'s""\r\n    },\r\n // child tokens missing start_char and end_char fields\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 3,\r\n      ""id"": 1,\r\n      ""lemma"": ""Joe"",\r\n      ""text"": ""Joe"",\r\n      ""upos"": ""PROPN"",\r\n      ""xpos"": ""NNP""\r\n    },\r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 1,\r\n      ""id"": 2,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n    // normal tokens\r\n    {\r\n      ""deprel"": ""root"",\r\n      ""end_char"": 9,\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 0,\r\n      ""id"": 3,\r\n      ""lemma"": ""dog"",\r\n      ""start_char"": 6,\r\n      ""text"": ""dog"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n    {\r\n      ""deprel"": ""punct"",\r\n      ""end_char"": 10,\r\n      ""head"": 3,\r\n      ""id"": 4,\r\n      ""lemma"": ""."",\r\n      ""start_char"": 9,\r\n      ""text"": ""."",\r\n      ""upos"": ""PUNCT"",\r\n      ""xpos"": "".""\r\n    }\r\n```\r\n\r\nThis breaks the one-to-one mapping that used to exist between tokens and word elements within the s-expression returned by the constituency tree:\r\n\r\n```\r\n(ROOT (NP (NP (NNP Joe) (POS \'s)) (NN dog) (. .)))\r\n```\r\n\r\nBut more problematically, this new aggregate token is now the only token containing the `end_char` and `start_char` data about the word.\r\n\r\nIn addition to being a breaking change, this new approach is quite hard for application developers to work with.  To parse it they need to chase down the ID links of the aggregate token when it intermittently appears to map its linguistic data.  Moreover, important character information about *where* the character delineation between a word and its apostrophe is lost. \r\n\r\n**Expected behavior**\r\nFor a possessive like `Joe\'s dog.`, Stanza returns four dependency tokens as before in Stanza 1.6.1:\r\n\r\n```json\r\n  {\r\n    ""deprel"": ""nmod:poss"",\r\n    ""end_char"": 3,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 3,\r\n    ""id"": 1,\r\n    ""lemma"": ""Joe"",\r\n    ""start_char"": 0,\r\n    ""text"": ""Joe"",\r\n    ""upos"": ""PROPN"",\r\n    ""xpos"": ""NNP""\r\n  },\r\n  {\r\n    ""deprel"": ""case"",\r\n    ""end_char"": 5,\r\n    ""head"": 1,\r\n    ""id"": 2,\r\n    ""lemma"": ""\'s"",\r\n    ""start_char"": 3,\r\n    ""text"": ""\'s"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""POS""\r\n  },\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 9,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 0,\r\n    ""id"": 3,\r\n    ""lemma"": ""dog"",\r\n    ""start_char"": 6,\r\n    ""text"": ""dog"",\r\n    ""upos"": ""NOUN"",\r\n    ""xpos"": ""NN""\r\n  },\r\n  {\r\n    ""deprel"": ""punct"",\r\n    ""end_char"": 10,\r\n    ""head"": 3,\r\n    ""id"": 4,\r\n    ""lemma"": ""."",\r\n    ""start_char"": 9,\r\n    ""text"": ""."",\r\n    ""upos"": ""PUNCT"",\r\n    ""xpos"": "".""\r\n  }\r\n```\r\n\r\nOr if a fifth aggregate token with an array of id\'s continues to be returned, the non-aggregate child tokens at least retain their own `end_char` and `start_char` information as before.  This would at least allow developers to ignore these aggregate tokens, and preserve information about the character delineation between each token.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS Ventura 13.4\r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1 moving to 1.7.x / 1.8.x\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/timeline', 'performed_via_github_app': None, 'state_reason': 'reopened'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996302978', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361#issuecomment-1996302978', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'id': 1996302978, 'node_id': 'IC_kwDOBj_0V852_SqC', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T03:00:06Z', 'updated_at': '2024-03-14T03:00:06Z', 'author_association': 'COLLABORATOR', 'body': ""I'd actually prefer to leave this as open until I figure out what to do with NER tags, btw"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996302978/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
208,IssuesEvent,"{'action': 'reopened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361', 'id': 2170276674, 'node_id': 'I_kwDOBj_0V86BW8tC', 'number': 1361, 'title': 'Stanza 1.7.0+ makes breaking API changes for possessives, tokens excluding `end_char` and `start_char` fields', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 7, 'created_at': '2024-03-05T22:54:18Z', 'updated_at': '2024-03-14T02:59:49Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m updating Stanza from 1.6.1 to 1.7.x / 1.8.x and noticed a number of breaking API changes in the Stanza Token result when handling possessives.\r\n\r\n**To Reproduce**\r\n1. Send to stanza a sentence containing a possessive apostrophe like `Joe\'s dog.`.\r\n2. Look at the Universal Dependencies.\r\n\r\nStanza now includes a new additional token that I\'ll call an ""aggregate token"" with the `text` field `Joe\'s`.  This new aggregate token comes in addition to the tokens for `Joe` and `\'s`.  The new aggregate token returns an `id` with a list to the other two tokens:\r\n\r\n```json\r\n// the new aggregate token appearing for each possessive apostrophe\r\n    {\r\n      ""end_char"": 5,\r\n      ""id"": [\r\n        1,\r\n        2\r\n      ],\r\n      ""start_char"": 0,\r\n      ""text"": ""Joe\'s""\r\n    },\r\n // child tokens missing start_char and end_char fields\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 3,\r\n      ""id"": 1,\r\n      ""lemma"": ""Joe"",\r\n      ""text"": ""Joe"",\r\n      ""upos"": ""PROPN"",\r\n      ""xpos"": ""NNP""\r\n    },\r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 1,\r\n      ""id"": 2,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n    // normal tokens\r\n    {\r\n      ""deprel"": ""root"",\r\n      ""end_char"": 9,\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 0,\r\n      ""id"": 3,\r\n      ""lemma"": ""dog"",\r\n      ""start_char"": 6,\r\n      ""text"": ""dog"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n    {\r\n      ""deprel"": ""punct"",\r\n      ""end_char"": 10,\r\n      ""head"": 3,\r\n      ""id"": 4,\r\n      ""lemma"": ""."",\r\n      ""start_char"": 9,\r\n      ""text"": ""."",\r\n      ""upos"": ""PUNCT"",\r\n      ""xpos"": "".""\r\n    }\r\n```\r\n\r\nThis breaks the one-to-one mapping that used to exist between tokens and word elements within the s-expression returned by the constituency tree:\r\n\r\n```\r\n(ROOT (NP (NP (NNP Joe) (POS \'s)) (NN dog) (. .)))\r\n```\r\n\r\nBut more problematically, this new aggregate token is now the only token containing the `end_char` and `start_char` data about the word.\r\n\r\nIn addition to being a breaking change, this new approach is quite hard for application developers to work with.  To parse it they need to chase down the ID links of the aggregate token when it intermittently appears to map its linguistic data.  Moreover, important character information about *where* the character delineation between a word and its apostrophe is lost. \r\n\r\n**Expected behavior**\r\nFor a possessive like `Joe\'s dog.`, Stanza returns four dependency tokens as before in Stanza 1.6.1:\r\n\r\n```json\r\n  {\r\n    ""deprel"": ""nmod:poss"",\r\n    ""end_char"": 3,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 3,\r\n    ""id"": 1,\r\n    ""lemma"": ""Joe"",\r\n    ""start_char"": 0,\r\n    ""text"": ""Joe"",\r\n    ""upos"": ""PROPN"",\r\n    ""xpos"": ""NNP""\r\n  },\r\n  {\r\n    ""deprel"": ""case"",\r\n    ""end_char"": 5,\r\n    ""head"": 1,\r\n    ""id"": 2,\r\n    ""lemma"": ""\'s"",\r\n    ""start_char"": 3,\r\n    ""text"": ""\'s"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""POS""\r\n  },\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 9,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 0,\r\n    ""id"": 3,\r\n    ""lemma"": ""dog"",\r\n    ""start_char"": 6,\r\n    ""text"": ""dog"",\r\n    ""upos"": ""NOUN"",\r\n    ""xpos"": ""NN""\r\n  },\r\n  {\r\n    ""deprel"": ""punct"",\r\n    ""end_char"": 10,\r\n    ""head"": 3,\r\n    ""id"": 4,\r\n    ""lemma"": ""."",\r\n    ""start_char"": 9,\r\n    ""text"": ""."",\r\n    ""upos"": ""PUNCT"",\r\n    ""xpos"": "".""\r\n  }\r\n```\r\n\r\nOr if a fifth aggregate token with an array of id\'s continues to be returned, the non-aggregate child tokens at least retain their own `end_char` and `start_char` information as before.  This would at least allow developers to ignore these aggregate tokens, and preserve information about the character delineation between each token.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS Ventura 13.4\r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1 moving to 1.7.x / 1.8.x\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/timeline', 'performed_via_github_app': None, 'state_reason': 'reopened'}}"
209,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361', 'id': 2170276674, 'node_id': 'I_kwDOBj_0V86BW8tC', 'number': 1361, 'title': 'Stanza 1.7.0+ makes breaking API changes for possessives, tokens excluding `end_char` and `start_char` fields', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 7, 'created_at': '2024-03-05T22:54:18Z', 'updated_at': '2024-03-14T02:59:31Z', 'closed_at': '2024-03-14T02:27:23Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m updating Stanza from 1.6.1 to 1.7.x / 1.8.x and noticed a number of breaking API changes in the Stanza Token result when handling possessives.\r\n\r\n**To Reproduce**\r\n1. Send to stanza a sentence containing a possessive apostrophe like `Joe\'s dog.`.\r\n2. Look at the Universal Dependencies.\r\n\r\nStanza now includes a new additional token that I\'ll call an ""aggregate token"" with the `text` field `Joe\'s`.  This new aggregate token comes in addition to the tokens for `Joe` and `\'s`.  The new aggregate token returns an `id` with a list to the other two tokens:\r\n\r\n```json\r\n// the new aggregate token appearing for each possessive apostrophe\r\n    {\r\n      ""end_char"": 5,\r\n      ""id"": [\r\n        1,\r\n        2\r\n      ],\r\n      ""start_char"": 0,\r\n      ""text"": ""Joe\'s""\r\n    },\r\n // child tokens missing start_char and end_char fields\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 3,\r\n      ""id"": 1,\r\n      ""lemma"": ""Joe"",\r\n      ""text"": ""Joe"",\r\n      ""upos"": ""PROPN"",\r\n      ""xpos"": ""NNP""\r\n    },\r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 1,\r\n      ""id"": 2,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n    // normal tokens\r\n    {\r\n      ""deprel"": ""root"",\r\n      ""end_char"": 9,\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 0,\r\n      ""id"": 3,\r\n      ""lemma"": ""dog"",\r\n      ""start_char"": 6,\r\n      ""text"": ""dog"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n    {\r\n      ""deprel"": ""punct"",\r\n      ""end_char"": 10,\r\n      ""head"": 3,\r\n      ""id"": 4,\r\n      ""lemma"": ""."",\r\n      ""start_char"": 9,\r\n      ""text"": ""."",\r\n      ""upos"": ""PUNCT"",\r\n      ""xpos"": "".""\r\n    }\r\n```\r\n\r\nThis breaks the one-to-one mapping that used to exist between tokens and word elements within the s-expression returned by the constituency tree:\r\n\r\n```\r\n(ROOT (NP (NP (NNP Joe) (POS \'s)) (NN dog) (. .)))\r\n```\r\n\r\nBut more problematically, this new aggregate token is now the only token containing the `end_char` and `start_char` data about the word.\r\n\r\nIn addition to being a breaking change, this new approach is quite hard for application developers to work with.  To parse it they need to chase down the ID links of the aggregate token when it intermittently appears to map its linguistic data.  Moreover, important character information about *where* the character delineation between a word and its apostrophe is lost. \r\n\r\n**Expected behavior**\r\nFor a possessive like `Joe\'s dog.`, Stanza returns four dependency tokens as before in Stanza 1.6.1:\r\n\r\n```json\r\n  {\r\n    ""deprel"": ""nmod:poss"",\r\n    ""end_char"": 3,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 3,\r\n    ""id"": 1,\r\n    ""lemma"": ""Joe"",\r\n    ""start_char"": 0,\r\n    ""text"": ""Joe"",\r\n    ""upos"": ""PROPN"",\r\n    ""xpos"": ""NNP""\r\n  },\r\n  {\r\n    ""deprel"": ""case"",\r\n    ""end_char"": 5,\r\n    ""head"": 1,\r\n    ""id"": 2,\r\n    ""lemma"": ""\'s"",\r\n    ""start_char"": 3,\r\n    ""text"": ""\'s"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""POS""\r\n  },\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 9,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 0,\r\n    ""id"": 3,\r\n    ""lemma"": ""dog"",\r\n    ""start_char"": 6,\r\n    ""text"": ""dog"",\r\n    ""upos"": ""NOUN"",\r\n    ""xpos"": ""NN""\r\n  },\r\n  {\r\n    ""deprel"": ""punct"",\r\n    ""end_char"": 10,\r\n    ""head"": 3,\r\n    ""id"": 4,\r\n    ""lemma"": ""."",\r\n    ""start_char"": 9,\r\n    ""text"": ""."",\r\n    ""upos"": ""PUNCT"",\r\n    ""xpos"": "".""\r\n  }\r\n```\r\n\r\nOr if a fifth aggregate token with an array of id\'s continues to be returned, the non-aggregate child tokens at least retain their own `end_char` and `start_char` information as before.  This would at least allow developers to ignore these aggregate tokens, and preserve information about the character delineation between each token.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS Ventura 13.4\r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1 moving to 1.7.x / 1.8.x\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996302597', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361#issuecomment-1996302597', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'id': 1996302597, 'node_id': 'IC_kwDOBj_0V852_SkF', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T02:59:30Z', 'updated_at': '2024-03-14T02:59:30Z', 'author_association': 'COLLABORATOR', 'body': 'Depends on if any show-stopping bugs show up, I suppose.  Probably a couple months if nothing critical comes up', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996302597/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
210,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361', 'id': 2170276674, 'node_id': 'I_kwDOBj_0V86BW8tC', 'number': 1361, 'title': 'Stanza 1.7.0+ makes breaking API changes for possessives, tokens excluding `end_char` and `start_char` fields', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-05T22:54:18Z', 'updated_at': '2024-03-14T02:28:43Z', 'closed_at': '2024-03-14T02:27:23Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m updating Stanza from 1.6.1 to 1.7.x / 1.8.x and noticed a number of breaking API changes in the Stanza Token result when handling possessives.\r\n\r\n**To Reproduce**\r\n1. Send to stanza a sentence containing a possessive apostrophe like `Joe\'s dog.`.\r\n2. Look at the Universal Dependencies.\r\n\r\nStanza now includes a new additional token that I\'ll call an ""aggregate token"" with the `text` field `Joe\'s`.  This new aggregate token comes in addition to the tokens for `Joe` and `\'s`.  The new aggregate token returns an `id` with a list to the other two tokens:\r\n\r\n```json\r\n// the new aggregate token appearing for each possessive apostrophe\r\n    {\r\n      ""end_char"": 5,\r\n      ""id"": [\r\n        1,\r\n        2\r\n      ],\r\n      ""start_char"": 0,\r\n      ""text"": ""Joe\'s""\r\n    },\r\n // child tokens missing start_char and end_char fields\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 3,\r\n      ""id"": 1,\r\n      ""lemma"": ""Joe"",\r\n      ""text"": ""Joe"",\r\n      ""upos"": ""PROPN"",\r\n      ""xpos"": ""NNP""\r\n    },\r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 1,\r\n      ""id"": 2,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n    // normal tokens\r\n    {\r\n      ""deprel"": ""root"",\r\n      ""end_char"": 9,\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 0,\r\n      ""id"": 3,\r\n      ""lemma"": ""dog"",\r\n      ""start_char"": 6,\r\n      ""text"": ""dog"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n    {\r\n      ""deprel"": ""punct"",\r\n      ""end_char"": 10,\r\n      ""head"": 3,\r\n      ""id"": 4,\r\n      ""lemma"": ""."",\r\n      ""start_char"": 9,\r\n      ""text"": ""."",\r\n      ""upos"": ""PUNCT"",\r\n      ""xpos"": "".""\r\n    }\r\n```\r\n\r\nThis breaks the one-to-one mapping that used to exist between tokens and word elements within the s-expression returned by the constituency tree:\r\n\r\n```\r\n(ROOT (NP (NP (NNP Joe) (POS \'s)) (NN dog) (. .)))\r\n```\r\n\r\nBut more problematically, this new aggregate token is now the only token containing the `end_char` and `start_char` data about the word.\r\n\r\nIn addition to being a breaking change, this new approach is quite hard for application developers to work with.  To parse it they need to chase down the ID links of the aggregate token when it intermittently appears to map its linguistic data.  Moreover, important character information about *where* the character delineation between a word and its apostrophe is lost. \r\n\r\n**Expected behavior**\r\nFor a possessive like `Joe\'s dog.`, Stanza returns four dependency tokens as before in Stanza 1.6.1:\r\n\r\n```json\r\n  {\r\n    ""deprel"": ""nmod:poss"",\r\n    ""end_char"": 3,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 3,\r\n    ""id"": 1,\r\n    ""lemma"": ""Joe"",\r\n    ""start_char"": 0,\r\n    ""text"": ""Joe"",\r\n    ""upos"": ""PROPN"",\r\n    ""xpos"": ""NNP""\r\n  },\r\n  {\r\n    ""deprel"": ""case"",\r\n    ""end_char"": 5,\r\n    ""head"": 1,\r\n    ""id"": 2,\r\n    ""lemma"": ""\'s"",\r\n    ""start_char"": 3,\r\n    ""text"": ""\'s"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""POS""\r\n  },\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 9,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 0,\r\n    ""id"": 3,\r\n    ""lemma"": ""dog"",\r\n    ""start_char"": 6,\r\n    ""text"": ""dog"",\r\n    ""upos"": ""NOUN"",\r\n    ""xpos"": ""NN""\r\n  },\r\n  {\r\n    ""deprel"": ""punct"",\r\n    ""end_char"": 10,\r\n    ""head"": 3,\r\n    ""id"": 4,\r\n    ""lemma"": ""."",\r\n    ""start_char"": 9,\r\n    ""text"": ""."",\r\n    ""upos"": ""PUNCT"",\r\n    ""xpos"": "".""\r\n  }\r\n```\r\n\r\nOr if a fifth aggregate token with an array of id\'s continues to be returned, the non-aggregate child tokens at least retain their own `end_char` and `start_char` information as before.  This would at least allow developers to ignore these aggregate tokens, and preserve information about the character delineation between each token.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS Ventura 13.4\r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1 moving to 1.7.x / 1.8.x\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996281142', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361#issuecomment-1996281142', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'id': 1996281142, 'node_id': 'IC_kwDOBj_0V852_NU2', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T02:28:42Z', 'updated_at': '2024-03-14T02:28:42Z', 'author_association': 'NONE', 'body': '@AngledLuffa  Do you know when the next release of Stanza will be so I can leap frog to that one?', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996281142/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
211,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361', 'id': 2170276674, 'node_id': 'I_kwDOBj_0V86BW8tC', 'number': 1361, 'title': 'Stanza 1.7.0+ makes breaking API changes for possessives, tokens excluding `end_char` and `start_char` fields', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-05T22:54:18Z', 'updated_at': '2024-03-14T02:27:23Z', 'closed_at': '2024-03-14T02:27:23Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m updating Stanza from 1.6.1 to 1.7.x / 1.8.x and noticed a number of breaking API changes in the Stanza Token result when handling possessives.\r\n\r\n**To Reproduce**\r\n1. Send to stanza a sentence containing a possessive apostrophe like `Joe\'s dog.`.\r\n2. Look at the Universal Dependencies.\r\n\r\nStanza now includes a new additional token that I\'ll call an ""aggregate token"" with the `text` field `Joe\'s`.  This new aggregate token comes in addition to the tokens for `Joe` and `\'s`.  The new aggregate token returns an `id` with a list to the other two tokens:\r\n\r\n```json\r\n// the new aggregate token appearing for each possessive apostrophe\r\n    {\r\n      ""end_char"": 5,\r\n      ""id"": [\r\n        1,\r\n        2\r\n      ],\r\n      ""start_char"": 0,\r\n      ""text"": ""Joe\'s""\r\n    },\r\n // child tokens missing start_char and end_char fields\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 3,\r\n      ""id"": 1,\r\n      ""lemma"": ""Joe"",\r\n      ""text"": ""Joe"",\r\n      ""upos"": ""PROPN"",\r\n      ""xpos"": ""NNP""\r\n    },\r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 1,\r\n      ""id"": 2,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n    // normal tokens\r\n    {\r\n      ""deprel"": ""root"",\r\n      ""end_char"": 9,\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 0,\r\n      ""id"": 3,\r\n      ""lemma"": ""dog"",\r\n      ""start_char"": 6,\r\n      ""text"": ""dog"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n    {\r\n      ""deprel"": ""punct"",\r\n      ""end_char"": 10,\r\n      ""head"": 3,\r\n      ""id"": 4,\r\n      ""lemma"": ""."",\r\n      ""start_char"": 9,\r\n      ""text"": ""."",\r\n      ""upos"": ""PUNCT"",\r\n      ""xpos"": "".""\r\n    }\r\n```\r\n\r\nThis breaks the one-to-one mapping that used to exist between tokens and word elements within the s-expression returned by the constituency tree:\r\n\r\n```\r\n(ROOT (NP (NP (NNP Joe) (POS \'s)) (NN dog) (. .)))\r\n```\r\n\r\nBut more problematically, this new aggregate token is now the only token containing the `end_char` and `start_char` data about the word.\r\n\r\nIn addition to being a breaking change, this new approach is quite hard for application developers to work with.  To parse it they need to chase down the ID links of the aggregate token when it intermittently appears to map its linguistic data.  Moreover, important character information about *where* the character delineation between a word and its apostrophe is lost. \r\n\r\n**Expected behavior**\r\nFor a possessive like `Joe\'s dog.`, Stanza returns four dependency tokens as before in Stanza 1.6.1:\r\n\r\n```json\r\n  {\r\n    ""deprel"": ""nmod:poss"",\r\n    ""end_char"": 3,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 3,\r\n    ""id"": 1,\r\n    ""lemma"": ""Joe"",\r\n    ""start_char"": 0,\r\n    ""text"": ""Joe"",\r\n    ""upos"": ""PROPN"",\r\n    ""xpos"": ""NNP""\r\n  },\r\n  {\r\n    ""deprel"": ""case"",\r\n    ""end_char"": 5,\r\n    ""head"": 1,\r\n    ""id"": 2,\r\n    ""lemma"": ""\'s"",\r\n    ""start_char"": 3,\r\n    ""text"": ""\'s"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""POS""\r\n  },\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 9,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 0,\r\n    ""id"": 3,\r\n    ""lemma"": ""dog"",\r\n    ""start_char"": 6,\r\n    ""text"": ""dog"",\r\n    ""upos"": ""NOUN"",\r\n    ""xpos"": ""NN""\r\n  },\r\n  {\r\n    ""deprel"": ""punct"",\r\n    ""end_char"": 10,\r\n    ""head"": 3,\r\n    ""id"": 4,\r\n    ""lemma"": ""."",\r\n    ""start_char"": 9,\r\n    ""text"": ""."",\r\n    ""upos"": ""PUNCT"",\r\n    ""xpos"": "".""\r\n  }\r\n```\r\n\r\nOr if a fifth aggregate token with an array of id\'s continues to be returned, the non-aggregate child tokens at least retain their own `end_char` and `start_char` information as before.  This would at least allow developers to ignore these aggregate tokens, and preserve information about the character delineation between each token.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS Ventura 13.4\r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1 moving to 1.7.x / 1.8.x\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996280297', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361#issuecomment-1996280297', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'id': 1996280297, 'node_id': 'IC_kwDOBj_0V852_NHp', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-14T02:27:23Z', 'updated_at': '2024-03-14T02:27:23Z', 'author_association': 'NONE', 'body': '@AngledLuffa Just built a version of Stanza from the `dev` branch with the latest changes, and I can see that the `start_char` and `end_char` are back.  Our API integration is working again when I add a small check to exclude MWT tokens.  Thank you so much @AngledLuffa!! ✨ 🤩 ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1996280297/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
212,IssuesEvent,"{'action': 'closed', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361', 'id': 2170276674, 'node_id': 'I_kwDOBj_0V86BW8tC', 'number': 1361, 'title': 'Stanza 1.7.0+ makes breaking API changes for possessives, tokens excluding `end_char` and `start_char` fields', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-05T22:54:18Z', 'updated_at': '2024-03-14T02:27:23Z', 'closed_at': '2024-03-14T02:27:23Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m updating Stanza from 1.6.1 to 1.7.x / 1.8.x and noticed a number of breaking API changes in the Stanza Token result when handling possessives.\r\n\r\n**To Reproduce**\r\n1. Send to stanza a sentence containing a possessive apostrophe like `Joe\'s dog.`.\r\n2. Look at the Universal Dependencies.\r\n\r\nStanza now includes a new additional token that I\'ll call an ""aggregate token"" with the `text` field `Joe\'s`.  This new aggregate token comes in addition to the tokens for `Joe` and `\'s`.  The new aggregate token returns an `id` with a list to the other two tokens:\r\n\r\n```json\r\n// the new aggregate token appearing for each possessive apostrophe\r\n    {\r\n      ""end_char"": 5,\r\n      ""id"": [\r\n        1,\r\n        2\r\n      ],\r\n      ""start_char"": 0,\r\n      ""text"": ""Joe\'s""\r\n    },\r\n // child tokens missing start_char and end_char fields\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 3,\r\n      ""id"": 1,\r\n      ""lemma"": ""Joe"",\r\n      ""text"": ""Joe"",\r\n      ""upos"": ""PROPN"",\r\n      ""xpos"": ""NNP""\r\n    },\r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 1,\r\n      ""id"": 2,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n    // normal tokens\r\n    {\r\n      ""deprel"": ""root"",\r\n      ""end_char"": 9,\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 0,\r\n      ""id"": 3,\r\n      ""lemma"": ""dog"",\r\n      ""start_char"": 6,\r\n      ""text"": ""dog"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n    {\r\n      ""deprel"": ""punct"",\r\n      ""end_char"": 10,\r\n      ""head"": 3,\r\n      ""id"": 4,\r\n      ""lemma"": ""."",\r\n      ""start_char"": 9,\r\n      ""text"": ""."",\r\n      ""upos"": ""PUNCT"",\r\n      ""xpos"": "".""\r\n    }\r\n```\r\n\r\nThis breaks the one-to-one mapping that used to exist between tokens and word elements within the s-expression returned by the constituency tree:\r\n\r\n```\r\n(ROOT (NP (NP (NNP Joe) (POS \'s)) (NN dog) (. .)))\r\n```\r\n\r\nBut more problematically, this new aggregate token is now the only token containing the `end_char` and `start_char` data about the word.\r\n\r\nIn addition to being a breaking change, this new approach is quite hard for application developers to work with.  To parse it they need to chase down the ID links of the aggregate token when it intermittently appears to map its linguistic data.  Moreover, important character information about *where* the character delineation between a word and its apostrophe is lost. \r\n\r\n**Expected behavior**\r\nFor a possessive like `Joe\'s dog.`, Stanza returns four dependency tokens as before in Stanza 1.6.1:\r\n\r\n```json\r\n  {\r\n    ""deprel"": ""nmod:poss"",\r\n    ""end_char"": 3,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 3,\r\n    ""id"": 1,\r\n    ""lemma"": ""Joe"",\r\n    ""start_char"": 0,\r\n    ""text"": ""Joe"",\r\n    ""upos"": ""PROPN"",\r\n    ""xpos"": ""NNP""\r\n  },\r\n  {\r\n    ""deprel"": ""case"",\r\n    ""end_char"": 5,\r\n    ""head"": 1,\r\n    ""id"": 2,\r\n    ""lemma"": ""\'s"",\r\n    ""start_char"": 3,\r\n    ""text"": ""\'s"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""POS""\r\n  },\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 9,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 0,\r\n    ""id"": 3,\r\n    ""lemma"": ""dog"",\r\n    ""start_char"": 6,\r\n    ""text"": ""dog"",\r\n    ""upos"": ""NOUN"",\r\n    ""xpos"": ""NN""\r\n  },\r\n  {\r\n    ""deprel"": ""punct"",\r\n    ""end_char"": 10,\r\n    ""head"": 3,\r\n    ""id"": 4,\r\n    ""lemma"": ""."",\r\n    ""start_char"": 9,\r\n    ""text"": ""."",\r\n    ""upos"": ""PUNCT"",\r\n    ""xpos"": "".""\r\n  }\r\n```\r\n\r\nOr if a fifth aggregate token with an array of id\'s continues to be returned, the non-aggregate child tokens at least retain their own `end_char` and `start_char` information as before.  This would at least allow developers to ignore these aggregate tokens, and preserve information about the character delineation between each token.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS Ventura 13.4\r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1 moving to 1.7.x / 1.8.x\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}}"
213,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366', 'id': 2182915319, 'node_id': 'I_kwDOBj_0V86CHKT3', 'number': 1366, 'title': 'If a word does not have a POS (specifically xpos) value what should I do?', 'user': {'login': 'gabriellestein', 'id': 59923724, 'node_id': 'MDQ6VXNlcjU5OTIzNzI0', 'avatar_url': 'https://avatars.githubusercontent.com/u/59923724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gabriellestein', 'html_url': 'https://github.com/gabriellestein', 'followers_url': 'https://api.github.com/users/gabriellestein/followers', 'following_url': 'https://api.github.com/users/gabriellestein/following{/other_user}', 'gists_url': 'https://api.github.com/users/gabriellestein/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gabriellestein/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gabriellestein/subscriptions', 'organizations_url': 'https://api.github.com/users/gabriellestein/orgs', 'repos_url': 'https://api.github.com/users/gabriellestein/repos', 'events_url': 'https://api.github.com/users/gabriellestein/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gabriellestein/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-13T00:58:17Z', 'updated_at': '2024-03-13T22:22:19Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '(I apologize if this has already been asked I tried for a long time to find an existing answer).\r\nI running into an error where I need the XPOS value for a word but it doesn\'t have any POS value.\r\nI have provided an extremely simplified version of my code below.\r\n\r\n```\r\nstanza.download(\'en\')\r\nnlp = stanza.Pipeline(\'en\', processors = \'tokenize,mwt,pos,lemma,depparse\')\r\ntext = ""My favorite actress is Joanna Lumley.""\r\ndoc = nlp(text)\r\nfor word in doc.sentences[i].to_dict():\r\n   print(word)\r\n   xpos = word[\'xpos\']\r\n```\r\nThese are the word values for the last two words:\r\n```\r\n{\'id\': 4, \'text\': \'actress\', \'lemma\': \'actress\', \'upos\': \'NOUN\', \'xpos\': \'NN\', \'feats\': \'Number=Sing\', \'head\': 8, \'deprel\': \'nsubj\', \'start_char\': 14, \'end_char\': 21}\r\n{\'id\': (5, 6), \'text\': \'joanna\', \'start_char\': 22, \'end_char\': 28}\r\n\r\nprint(word[\'xpos\'])\r\nKeyError: \'xpos\'\r\n```\r\nI am using the XPOS value of each word in the text. The string ""joanna"" does not have any POS data. Is the reason for this that the word doesn\'t exist in the vocabulary where the POS data is sourced? It is a name so it makes sense that every name wouldn\'t be added to a vocabulary. Should I manually state in my code that the XPOS is PROPN? I am working with a large amount of text that has many potential instances of unique names. Is there a better way to handle such instances, like an existing library of uncommon names?\r\n\r\nThank you for your help, I\'m sorry if there  is an obvious solution to this I am a complete stanza novice.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1995993003', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366#issuecomment-1995993003', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'id': 1995993003, 'node_id': 'IC_kwDOBj_0V852-G-r', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T22:22:18Z', 'updated_at': '2024-03-13T22:22:18Z', 'author_association': 'COLLABORATOR', 'body': 'Alright, I added a few more sentences with henna to the training data until it stopped splitting that word for no reason.  The new models should be automatically downloaded by v1.8.1\r\n\r\n```\r\n>>> print(""{:C}"".format(pipe(""Johanna said she\'s gonna get a henna tattoo"")))\r\n# text = Johanna said she\'s gonna get a henna tattoo\r\n# sent_id = 0\r\n1       Johanna _       _       _       _       0       _       _       start_char=0|end_char=7\r\n2       said    _       _       _       _       1       _       _       start_char=8|end_char=12\r\n3-4     she\'s   _       _       _       _       _       _       _       start_char=13|end_char=18\r\n3       she     _       _       _       _       2       _       _       start_char=13|end_char=16\r\n4       \'s      _       _       _       _       3       _       _       start_char=16|end_char=18\r\n5-6     gonna   _       _       _       _       _       _       _       start_char=19|end_char=24\r\n5       gon     _       _       _       _       4       _       _       start_char=19|end_char=22\r\n6       na      _       _       _       _       5       _       _       start_char=22|end_char=24\r\n7       get     _       _       _       _       6       _       _       start_char=25|end_char=28\r\n8       a       _       _       _       _       7       _       _       start_char=29|end_char=30\r\n9       henna   _       _       _       _       8       _       _       start_char=31|end_char=36\r\n10      tattoo  _       _       _       _       9       _       _       start_char=37|end_char=43|SpaceAfter=No\r\n```', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1995993003/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
214,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/297', 'id': 614480434, 'node_id': 'MDU6SXNzdWU2MTQ0ODA0MzQ=', 'number': 297, 'title': 'WARNING: Can not find mwt: default from official model list. Ignoring it.', 'user': {'login': 'rodriguesfas', 'id': 5026592, 'node_id': 'MDQ6VXNlcjUwMjY1OTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/5026592?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rodriguesfas', 'html_url': 'https://github.com/rodriguesfas', 'followers_url': 'https://api.github.com/users/rodriguesfas/followers', 'following_url': 'https://api.github.com/users/rodriguesfas/following{/other_user}', 'gists_url': 'https://api.github.com/users/rodriguesfas/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rodriguesfas/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rodriguesfas/subscriptions', 'organizations_url': 'https://api.github.com/users/rodriguesfas/orgs', 'repos_url': 'https://api.github.com/users/rodriguesfas/repos', 'events_url': 'https://api.github.com/users/rodriguesfas/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rodriguesfas/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}, {'id': 1226136894, 'node_id': 'MDU6TGFiZWwxMjI2MTM2ODk0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/awaiting%20feedback', 'name': 'awaiting feedback', 'color': 'efc69e', 'default': False, 'description': ''}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2020-05-08T03:36:51Z', 'updated_at': '2024-03-13T17:02:08Z', 'closed_at': '2020-05-13T00:36:10Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hello, I'm running a pipeline with stanza and I get an error for the MWT analysis.\r\nWARNING: Can not find mwt: default from official model list. Ignoring it.\r\nDo you know what it is? For some reason this model is not low in stanza_resources."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/reactions', 'total_count': 1, '+1': 1, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1995007019', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/297#issuecomment-1995007019', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297', 'id': 1995007019, 'node_id': 'IC_kwDOBj_0V8526WQr', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T17:02:07Z', 'updated_at': '2024-03-13T17:02:07Z', 'author_association': 'COLLABORATOR', 'body': ""This is expected.  English has MWT, such as `won't`, `gonna`, `Jennifer's`.  \r\n\r\nIt has caused some amount of irritation, though, such as\r\n\r\nhttps://github.com/stanfordnlp/stanza/issues/1366\r\nhttps://github.com/stanfordnlp/stanza/issues/1361\r\n\r\nWe are working through those issues"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1995007019/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
215,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/297', 'id': 614480434, 'node_id': 'MDU6SXNzdWU2MTQ0ODA0MzQ=', 'number': 297, 'title': 'WARNING: Can not find mwt: default from official model list. Ignoring it.', 'user': {'login': 'rodriguesfas', 'id': 5026592, 'node_id': 'MDQ6VXNlcjUwMjY1OTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/5026592?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rodriguesfas', 'html_url': 'https://github.com/rodriguesfas', 'followers_url': 'https://api.github.com/users/rodriguesfas/followers', 'following_url': 'https://api.github.com/users/rodriguesfas/following{/other_user}', 'gists_url': 'https://api.github.com/users/rodriguesfas/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rodriguesfas/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rodriguesfas/subscriptions', 'organizations_url': 'https://api.github.com/users/rodriguesfas/orgs', 'repos_url': 'https://api.github.com/users/rodriguesfas/repos', 'events_url': 'https://api.github.com/users/rodriguesfas/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rodriguesfas/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}, {'id': 1226136894, 'node_id': 'MDU6TGFiZWwxMjI2MTM2ODk0', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/awaiting%20feedback', 'name': 'awaiting feedback', 'color': 'efc69e', 'default': False, 'description': ''}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2020-05-08T03:36:51Z', 'updated_at': '2024-03-13T16:52:31Z', 'closed_at': '2020-05-13T00:36:10Z', 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hello, I'm running a pipeline with stanza and I get an error for the MWT analysis.\r\nWARNING: Can not find mwt: default from official model list. Ignoring it.\r\nDo you know what it is? For some reason this model is not low in stanza_resources."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/reactions', 'total_count': 1, '+1': 1, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1994973544', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/297#issuecomment-1994973544', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/297', 'id': 1994973544, 'node_id': 'IC_kwDOBj_0V8526OFo', 'user': {'login': 'argosopentech', 'id': 48267258, 'node_id': 'MDQ6VXNlcjQ4MjY3MjU4', 'avatar_url': 'https://avatars.githubusercontent.com/u/48267258?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/argosopentech', 'html_url': 'https://github.com/argosopentech', 'followers_url': 'https://api.github.com/users/argosopentech/followers', 'following_url': 'https://api.github.com/users/argosopentech/following{/other_user}', 'gists_url': 'https://api.github.com/users/argosopentech/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/argosopentech/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/argosopentech/subscriptions', 'organizations_url': 'https://api.github.com/users/argosopentech/orgs', 'repos_url': 'https://api.github.com/users/argosopentech/repos', 'events_url': 'https://api.github.com/users/argosopentech/events{/privacy}', 'received_events_url': 'https://api.github.com/users/argosopentech/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T16:52:30Z', 'updated_at': '2024-03-13T16:52:30Z', 'author_association': 'NONE', 'body': ""I'm seeing this warning after upgrading my Stanza version to 1.8.1:\r\n```\r\n2024-03-13 11:51:25 WARNING: Language en package default expects mwt, which has been added\r\n```"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1994973544/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
216,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366', 'id': 2182915319, 'node_id': 'I_kwDOBj_0V86CHKT3', 'number': 1366, 'title': 'If a word does not have a POS (specifically xpos) value what should I do?', 'user': {'login': 'gabriellestein', 'id': 59923724, 'node_id': 'MDQ6VXNlcjU5OTIzNzI0', 'avatar_url': 'https://avatars.githubusercontent.com/u/59923724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gabriellestein', 'html_url': 'https://github.com/gabriellestein', 'followers_url': 'https://api.github.com/users/gabriellestein/followers', 'following_url': 'https://api.github.com/users/gabriellestein/following{/other_user}', 'gists_url': 'https://api.github.com/users/gabriellestein/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gabriellestein/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gabriellestein/subscriptions', 'organizations_url': 'https://api.github.com/users/gabriellestein/orgs', 'repos_url': 'https://api.github.com/users/gabriellestein/repos', 'events_url': 'https://api.github.com/users/gabriellestein/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gabriellestein/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-13T00:58:17Z', 'updated_at': '2024-03-13T16:51:50Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '(I apologize if this has already been asked I tried for a long time to find an existing answer).\r\nI running into an error where I need the XPOS value for a word but it doesn\'t have any POS value.\r\nI have provided an extremely simplified version of my code below.\r\n\r\n```\r\nstanza.download(\'en\')\r\nnlp = stanza.Pipeline(\'en\', processors = \'tokenize,mwt,pos,lemma,depparse\')\r\ntext = ""My favorite actress is Joanna Lumley.""\r\ndoc = nlp(text)\r\nfor word in doc.sentences[i].to_dict():\r\n   print(word)\r\n   xpos = word[\'xpos\']\r\n```\r\nThese are the word values for the last two words:\r\n```\r\n{\'id\': 4, \'text\': \'actress\', \'lemma\': \'actress\', \'upos\': \'NOUN\', \'xpos\': \'NN\', \'feats\': \'Number=Sing\', \'head\': 8, \'deprel\': \'nsubj\', \'start_char\': 14, \'end_char\': 21}\r\n{\'id\': (5, 6), \'text\': \'joanna\', \'start_char\': 22, \'end_char\': 28}\r\n\r\nprint(word[\'xpos\'])\r\nKeyError: \'xpos\'\r\n```\r\nI am using the XPOS value of each word in the text. The string ""joanna"" does not have any POS data. Is the reason for this that the word doesn\'t exist in the vocabulary where the POS data is sourced? It is a name so it makes sense that every name wouldn\'t be added to a vocabulary. Should I manually state in my code that the XPOS is PROPN? I am working with a large amount of text that has many potential instances of unique names. Is there a better way to handle such instances, like an existing library of uncommon names?\r\n\r\nThank you for your help, I\'m sorry if there  is an obvious solution to this I am a complete stanza novice.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1994971035', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366#issuecomment-1994971035', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'id': 1994971035, 'node_id': 'IC_kwDOBj_0V8526Neb', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T16:51:49Z', 'updated_at': '2024-03-13T16:51:49Z', 'author_association': 'COLLABORATOR', 'body': ""Without knowing your particular application, I would be surprised if it's happy with `Johanna` being broken into two pieces and then possibly not given an `NNP` tag.  However, updating the training set with a few examples of `-nna` makes it not split for most of the cases listed above.  Weirdly, it still splits for `henna`..."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1994971035/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
217,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366', 'id': 2182915319, 'node_id': 'I_kwDOBj_0V86CHKT3', 'number': 1366, 'title': 'If a word does not have a POS (specifically xpos) value what should I do?', 'user': {'login': 'gabriellestein', 'id': 59923724, 'node_id': 'MDQ6VXNlcjU5OTIzNzI0', 'avatar_url': 'https://avatars.githubusercontent.com/u/59923724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gabriellestein', 'html_url': 'https://github.com/gabriellestein', 'followers_url': 'https://api.github.com/users/gabriellestein/followers', 'following_url': 'https://api.github.com/users/gabriellestein/following{/other_user}', 'gists_url': 'https://api.github.com/users/gabriellestein/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gabriellestein/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gabriellestein/subscriptions', 'organizations_url': 'https://api.github.com/users/gabriellestein/orgs', 'repos_url': 'https://api.github.com/users/gabriellestein/repos', 'events_url': 'https://api.github.com/users/gabriellestein/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gabriellestein/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-13T00:58:17Z', 'updated_at': '2024-03-13T10:17:28Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '(I apologize if this has already been asked I tried for a long time to find an existing answer).\r\nI running into an error where I need the XPOS value for a word but it doesn\'t have any POS value.\r\nI have provided an extremely simplified version of my code below.\r\n\r\n```\r\nstanza.download(\'en\')\r\nnlp = stanza.Pipeline(\'en\', processors = \'tokenize,mwt,pos,lemma,depparse\')\r\ntext = ""My favorite actress is Joanna Lumley.""\r\ndoc = nlp(text)\r\nfor word in doc.sentences[i].to_dict():\r\n   print(word)\r\n   xpos = word[\'xpos\']\r\n```\r\nThese are the word values for the last two words:\r\n```\r\n{\'id\': 4, \'text\': \'actress\', \'lemma\': \'actress\', \'upos\': \'NOUN\', \'xpos\': \'NN\', \'feats\': \'Number=Sing\', \'head\': 8, \'deprel\': \'nsubj\', \'start_char\': 14, \'end_char\': 21}\r\n{\'id\': (5, 6), \'text\': \'joanna\', \'start_char\': 22, \'end_char\': 28}\r\n\r\nprint(word[\'xpos\'])\r\nKeyError: \'xpos\'\r\n```\r\nI am using the XPOS value of each word in the text. The string ""joanna"" does not have any POS data. Is the reason for this that the word doesn\'t exist in the vocabulary where the POS data is sourced? It is a name so it makes sense that every name wouldn\'t be added to a vocabulary. Should I manually state in my code that the XPOS is PROPN? I am working with a large amount of text that has many potential instances of unique names. Is there a better way to handle such instances, like an existing library of uncommon names?\r\n\r\nThank you for your help, I\'m sorry if there  is an obvious solution to this I am a complete stanza novice.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1994034929', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366#issuecomment-1994034929', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'id': 1994034929, 'node_id': 'IC_kwDOBj_0V8522o7x', 'user': {'login': 'gabriellestein', 'id': 59923724, 'node_id': 'MDQ6VXNlcjU5OTIzNzI0', 'avatar_url': 'https://avatars.githubusercontent.com/u/59923724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gabriellestein', 'html_url': 'https://github.com/gabriellestein', 'followers_url': 'https://api.github.com/users/gabriellestein/followers', 'following_url': 'https://api.github.com/users/gabriellestein/following{/other_user}', 'gists_url': 'https://api.github.com/users/gabriellestein/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gabriellestein/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gabriellestein/subscriptions', 'organizations_url': 'https://api.github.com/users/gabriellestein/orgs', 'repos_url': 'https://api.github.com/users/gabriellestein/repos', 'events_url': 'https://api.github.com/users/gabriellestein/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gabriellestein/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T10:17:27Z', 'updated_at': '2024-03-13T10:17:27Z', 'author_association': 'NONE', 'body': 'Thank you for the quick response. That fixed my issue!\r\n`if not instanceof(word[‘id’], int):`\r\nI’m not too concerned about incorrect lemmatizations for my application but that thank you for updating the training set, that will surely help someone down the road!\r\nI wish I had a clever Star Trek response but embarrassingly I’ve never seen a Star Trek movie. So just imagine I said something very witty. ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1994034929/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
218,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 8, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-13T08:55:03Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1993852949', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1993852949', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1993852949, 'node_id': 'IC_kwDOBj_0V85218gV', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T08:55:02Z', 'updated_at': '2024-03-13T08:55:02Z', 'author_association': 'NONE', 'body': ""Hi again,\r\n\r\nI think I have sorted the issue. I have pretrained the ang_embeddings.pt with the txt with the vectors. I'm attaching them \r\n\r\nhttps://we.tl/t-1WOmZf1xdc\r\n\r\nIn the meantime I'm retraining the POS and depparser, so when I finish those I'll add the models. Could you confirm that the .pt file is correct? According to the pretrain script, the emb and vocab have been saved to the .pt file\r\n\r\nAgain, thanks for your help in this"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1993852949/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
219,PushEvent,"{'repository_id': 104854615, 'push_id': 17519227650, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'b62c1e7f8e0e17ea38d49683aea8a0cfb3db8b90', 'before': '23840891c37d54a5cf491ea58b0702987dd4a6d7', 'commits': [{'sha': 'b62c1e7f8e0e17ea38d49683aea8a0cfb3db8b90', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""When adding the handparsed treebank to the combined dataset, add a sentence final punct if it doesn't already have one.  This will avoid worsening the problem of the tokenizer learning to split in the middle of sentences"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/b62c1e7f8e0e17ea38d49683aea8a0cfb3db8b90'}]}"
220,PushEvent,"{'repository_id': 104854615, 'push_id': 17519224796, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/networkx', 'head': 'a5a80cf78c14bcee19b0b57bedef5b49eee2131f', 'before': '59a3de9e5e2214216edb6ef061681bb3024d16f0', 'commits': [{'sha': 'a5a80cf78c14bcee19b0b57bedef5b49eee2131f', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""When adding the handparsed treebank to the combined dataset, add a sentence final punct if it doesn't already have one.  This will avoid worsening the problem of the tokenizer learning to split in the middle of sentences"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/a5a80cf78c14bcee19b0b57bedef5b49eee2131f'}]}"
221,PushEvent,"{'repository_id': 104854615, 'push_id': 17518970027, 'size': 4, 'distinct_size': 1, 'ref': 'refs/heads/networkx', 'head': '59a3de9e5e2214216edb6ef061681bb3024d16f0', 'before': '36085fcb869f1eb7ec7dc6db56e9aa8c137ce14b', 'commits': [{'sha': '303699d6b34091ab12aae93ae8c7a7754518150e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a test that the scheme to update a rootless graph with a root for the Sindhi project is working in Ssurgeon', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/303699d6b34091ab12aae93ae8c7a7754518150e'}, {'sha': '4048caed1b89030082d23b8f71d23bae6c9c54f1', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""Add flags to drop either xpos or ufeats as features in the depparse.  Useful in the case of making a cross-lingual parser for which the xpos can't be used across languages"", 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4048caed1b89030082d23b8f71d23bae6c9c54f1'}, {'sha': '23840891c37d54a5cf491ea58b0702987dd4a6d7', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""If a word's text adds up to the text of the whole token, we can mark start_char and end_char on it.\n\nNote that there will still be no start_char and end_char annotations\non words if the words don't add up to the token's text, so even in a\nlanguage like English where the standard is to annotate the datasets\nso that they correspond to the pieces of the real text instead of the\nword being represented, there may be unusual separations in the MWT\nprocessor that result in no start/end char\n\nFix a unit test error\n\nhttps://github.com/stanfordnlp/stanza/issues/1361"", 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/23840891c37d54a5cf491ea58b0702987dd4a6d7'}, {'sha': '59a3de9e5e2214216edb6ef061681bb3024d16f0', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Send enhanced dependencies to Ssurgeon if the document has them.\nAttach the enhanced dependencies to the sentence after running Ssurgeon.\nNeed to test this a bunch\n\nDo a better job of tracking if the words are the same\nConvert the empty words (if any) from the returned Ssurgeon and attach them to the Sentence', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/59a3de9e5e2214216edb6ef061681bb3024d16f0'}]}"
222,PushEvent,"{'repository_id': 104854615, 'push_id': 17518966592, 'size': 2, 'distinct_size': 1, 'ref': 'refs/heads/networkx', 'head': '36085fcb869f1eb7ec7dc6db56e9aa8c137ce14b', 'before': '6cad398c39345f63b0acb0ececf5495860a150ae', 'commits': [{'sha': 'ea61782bce611ff11437a31b55eaf1757d8b19ca', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Can simplify this object to a namedtuple now that the Python version >= 3.7', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/ea61782bce611ff11437a31b55eaf1757d8b19ca'}, {'sha': '36085fcb869f1eb7ec7dc6db56e9aa8c137ce14b', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Send enhanced dependencies to Ssurgeon if the document has them.\nAttach the enhanced dependencies to the sentence after running Ssurgeon.\nNeed to test this a bunch\n\nDo a better job of tracking if the words are the same\nConvert the empty words (if any) from the returned Ssurgeon and attach them to the Sentence', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/36085fcb869f1eb7ec7dc6db56e9aa8c137ce14b'}]}"
223,PushEvent,"{'repository_id': 104854615, 'push_id': 17518948965, 'size': 3, 'distinct_size': 1, 'ref': 'refs/heads/networkx', 'head': '6cad398c39345f63b0acb0ececf5495860a150ae', 'before': 'd57e5e1e5d256250010588547b194240aa010061', 'commits': [{'sha': 'c00bbe7c5c3436a3fbb91e2838e34d3a3422c6f5', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""CoreNLP doesn't accept dependency trees with blank (None, null, whatever) dependencies, so if we have a None for deprel, send across _ in Semgrex"", 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/c00bbe7c5c3436a3fbb91e2838e34d3a3422c6f5'}, {'sha': '18ced0e062d5e04b1ab16cb0da72063379263309', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Sometimes a document will have an error which leaves the head as None instead of 0 for the root', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/18ced0e062d5e04b1ab16cb0da72063379263309'}, {'sha': '6cad398c39345f63b0acb0ececf5495860a150ae', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Send enhanced dependencies to Ssurgeon if the document has them.\nAttach the enhanced dependencies to the sentence after running Ssurgeon.\nNeed to test this a bunch\n\nDo a better job of tracking if the words are the same\nConvert the empty words (if any) from the returned Ssurgeon and attach them to the Sentence', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/6cad398c39345f63b0acb0ececf5495860a150ae'}]}"
224,PushEvent,"{'repository_id': 104854615, 'push_id': 17518943564, 'size': 143, 'distinct_size': 1, 'ref': 'refs/heads/networkx', 'head': 'd57e5e1e5d256250010588547b194240aa010061', 'before': 'ead7a37d1362b0ed438775c440c3530f8a39fbcb', 'commits': [{'sha': '9a90196d0725c53be055516f5e9ace01763e59ab', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Throw an error when training data is empty for the tagger', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/9a90196d0725c53be055516f5e9ace01763e59ab'}, {'sha': '58cd1e32c06bb361cfc9a7c5c8a2de2d0d0e488b', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a script for randomly splitting a conllu file - useful for splitting one file into train/dev/test splits', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/58cd1e32c06bb361cfc9a7c5c8a2de2d0d0e488b'}, {'sha': '358747c0128086dacdeb65a67ffb8a996b4e851e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Keep the comments with the sentences when splitting a conllu document', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/358747c0128086dacdeb65a67ffb8a996b4e851e'}, {'sha': '3253e47e11b2648b2b5eeaf8f21be6d7d0689ad8', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add batch_size as a filename argument when making models', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/3253e47e11b2648b2b5eeaf8f21be6d7d0689ad8'}, {'sha': 'eb527cca19ae46d5012ab66fa7dabfa3b40f2a58', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add the ability to read and reannotate an existing conllu document instead of a text file', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/eb527cca19ae46d5012ab66fa7dabfa3b40f2a58'}, {'sha': '4d2a83d5c37c0e9da0c896f32217e65b0eda4669', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a random seed when splitting vlsp', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4d2a83d5c37c0e9da0c896f32217e65b0eda4669'}, {'sha': '5c4c8cb0f8731352cd6d62d92a871a62a197e51d', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a flag to not check the transitions or constituents in the dev set', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/5c4c8cb0f8731352cd6d62d92a871a62a197e51d'}, {'sha': 'c485224d79bc75ca0b5a803857a95781095f1385', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add documentation regarding build_char_representation to the character model', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/c485224d79bc75ca0b5a803857a95781095f1385'}, {'sha': '638358130395bb7b288a8525393db9be44842e71', 'author': {'email': 'terlya.stas@gmail.com', 'name': 'STerliakov'}, 'message': 'Fix invalid escape sequences', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/638358130395bb7b288a8525393db9be44842e71'}, {'sha': '0a47f8ebcd474e575e63016a0372f978b953b1af', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'Split out the utility optimizer into separate parts for Bert and non-Bert; this method is backwards compatible, such that get_optimizer gets retained to be used for methods that have not yet been migrated, while a new get_split_optimizer is used for methods that desire an optimizer that has been split out.\n\nFurther, the optimizer takes an is_peft option which stages the optimizer to tune .parameters() of the Bert model instead of filtering for.named_parameters() which is selected. This allows HF Peft to do weird shenanigans to the parameters value, and we will—when the weights trainable is being constrained by the Peft library—tune what it tells us to tune instead of tuning things that we select via its name.\n\nTaking advantage of this fact that we have a split optimizer now, Part of Speech tagging with Bert finetuning features a learning rate scheduler which a warmup and a linear decay if the user requests the Bert to be tuned.', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/0a47f8ebcd474e575e63016a0372f978b953b1af'}, {'sha': 'a168a18534a322cf9955a040e18fcd18c763952e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a unit test which invokes the bert finetuning (no guarantee the finetuning is doing what we want yet, though)', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/a168a18534a322cf9955a040e18fcd18c763952e'}, {'sha': '3d8e48d6ddea14b9a41776a66316db6c689c84b0', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'When logging norms in the POS tagger: Align the norms a bit better, add the number of parameters.  Could still be better, but this is an improvement', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/3d8e48d6ddea14b9a41776a66316db6c689c84b0'}, {'sha': 'c50608e86105cd165fcecd5b0374a1beeff37436', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Use .index(key, start=...) to search when calling reassemble_doc_from_tokens rather than chopping the raw_text into pieces.  This avoids a potentially expensive substring operation and keeps the raw_text in one piece for assigning to the new Document', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/c50608e86105cd165fcecd5b0374a1beeff37436'}, {'sha': '6a363773a83dfb16ff5b0d906b96ea7927ac9a91', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'This test should now function correctly', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/6a363773a83dfb16ff5b0d906b96ea7927ac9a91'}, {'sha': '3035b55eb521d104df2983106f3c72e2cc36dc32', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Refactor the SpaceAfter escaping & unescaping.  Use this to make functions to process SpacesBefore', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/3035b55eb521d104df2983106f3c72e2cc36dc32'}, {'sha': '1a750d2080adb15302ac8aa6b98de31684dd3316', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add SpaceAfter / SpacesAfter annotations to the MISC column of tokens when tokenizing\n\nSomewhere else there is MWT=Yes| added with the bar, so we get rid of blank entries after splitting', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/1a750d2080adb15302ac8aa6b98de31684dd3316'}, {'sha': 'c054635610f94d450b9480f2aadd6d89b2050493', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add SpacesBefore to the document output as well\n\nSomewhere else there is MWT=Yes| added with the bar, so we get rid of blank entries after splitting', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/c054635610f94d450b9480f2aadd6d89b2050493'}, {'sha': '429d412c7e42d905321ad616045a129cf2986997', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a couple test cases of SpacesBefore and Space[s]After', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/429d412c7e42d905321ad616045a129cf2986997'}, {'sha': 'f617076ccbfb06f7d4ed295fe822ba805f1d79ad', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Attempt to put the whitespace markers on the Token objects after the Word objects have all been made.  Will reduce repeat annotations & make there be one canonical source of where the whitespace markers are', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/f617076ccbfb06f7d4ed295fe822ba805f1d79ad'}, {'sha': 'c13b62bca4e78f06f00a3b7277a79fe26bafc6cd', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Resplit the text for a bulk document into its component documents, letting us extract the SpacesBefore and SpacesAfter as appropriate for each of the documents in the bulk tokenization', 'distinct': False, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/c13b62bca4e78f06f00a3b7277a79fe26bafc6cd'}]}"
225,WatchEvent,{'action': 'started'}
226,WatchEvent,{'action': 'started'}
227,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 7, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-13T05:26:14Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1993592157', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1993592157', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1993592157, 'node_id': 'IC_kwDOBj_0V852081d', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T05:26:13Z', 'updated_at': '2024-03-13T05:26:13Z', 'author_association': 'NONE', 'body': ""Hi,\r\n\r\nI think that is the correct shape, but I'm not entirely sure about it, since I'm not very experienced in pre training embeddings. I'll double check and come back to you with that.\r\n\r\nI have the models, but I have realized that I made a mistake when training them, since I used the first embeddings file I sent you (the small one), so I guess that if I use the last one I have sent, my scores would improve. Again, I'll try that today and come back with more info.\r\n\r\nThose embeddings you are suggesting won't work with this. Those include English from the year 1450 onwards, while this corpus is from the 5th century till the 12th century. Those periods have completely different languages (think German and English). The corpus of Old English is only made of 3M words, that is all the surviving text available. That's why I'm very concerned with having the embeddings file correct, since I haven't been able to find anything similar, and it's something very sought after by my collaborator. \r\n\r\nBy word list, do you mean the list of tokens, or the corpus in raw? I have the latter, not the former. I'll train using the pretrain scripts from Stanza again, to see if I made a mistake and I can get the embeddings right. Is there a way I can check the format is correct before sending that back to you? Is there anything else I should know from the pretrain scripts in Stanza for embeddings?"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1993592157/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
228,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366', 'id': 2182915319, 'node_id': 'I_kwDOBj_0V86CHKT3', 'number': 1366, 'title': 'If a word does not have a POS (specifically xpos) value what should I do?', 'user': {'login': 'gabriellestein', 'id': 59923724, 'node_id': 'MDQ6VXNlcjU5OTIzNzI0', 'avatar_url': 'https://avatars.githubusercontent.com/u/59923724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gabriellestein', 'html_url': 'https://github.com/gabriellestein', 'followers_url': 'https://api.github.com/users/gabriellestein/followers', 'following_url': 'https://api.github.com/users/gabriellestein/following{/other_user}', 'gists_url': 'https://api.github.com/users/gabriellestein/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gabriellestein/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gabriellestein/subscriptions', 'organizations_url': 'https://api.github.com/users/gabriellestein/orgs', 'repos_url': 'https://api.github.com/users/gabriellestein/repos', 'events_url': 'https://api.github.com/users/gabriellestein/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gabriellestein/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-13T00:58:17Z', 'updated_at': '2024-03-13T03:11:20Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '(I apologize if this has already been asked I tried for a long time to find an existing answer).\r\nI running into an error where I need the XPOS value for a word but it doesn\'t have any POS value.\r\nI have provided an extremely simplified version of my code below.\r\n\r\n```\r\nstanza.download(\'en\')\r\nnlp = stanza.Pipeline(\'en\', processors = \'tokenize,mwt,pos,lemma,depparse\')\r\ntext = ""My favorite actress is Joanna Lumley.""\r\ndoc = nlp(text)\r\nfor word in doc.sentences[i].to_dict():\r\n   print(word)\r\n   xpos = word[\'xpos\']\r\n```\r\nThese are the word values for the last two words:\r\n```\r\n{\'id\': 4, \'text\': \'actress\', \'lemma\': \'actress\', \'upos\': \'NOUN\', \'xpos\': \'NN\', \'feats\': \'Number=Sing\', \'head\': 8, \'deprel\': \'nsubj\', \'start_char\': 14, \'end_char\': 21}\r\n{\'id\': (5, 6), \'text\': \'joanna\', \'start_char\': 22, \'end_char\': 28}\r\n\r\nprint(word[\'xpos\'])\r\nKeyError: \'xpos\'\r\n```\r\nI am using the XPOS value of each word in the text. The string ""joanna"" does not have any POS data. Is the reason for this that the word doesn\'t exist in the vocabulary where the POS data is sourced? It is a name so it makes sense that every name wouldn\'t be added to a vocabulary. Should I manually state in my code that the XPOS is PROPN? I am working with a large amount of text that has many potential instances of unique names. Is there a better way to handle such instances, like an existing library of uncommon names?\r\n\r\nThank you for your help, I\'m sorry if there  is an obvious solution to this I am a complete stanza novice.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1993279242', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366#issuecomment-1993279242', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'id': 1993279242, 'node_id': 'IC_kwDOBj_0V852zwcK', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T03:11:19Z', 'updated_at': '2024-03-13T03:11:19Z', 'author_association': 'COLLABORATOR', 'body': 'This is kinda funny.  The tokenizer has misinterpreted `Johanna` to be similar to tokens such as `wanna` and `gonna`, which get split into `want to` and `going to`.  I suppose in this case it would be `Johan to`?  Obviously this is incorrect, and I will add a sentence or two to the training data to hopefully fix it up.\r\n\r\nTo answer the question regarding not having an `xpos`, the tokens which are split into multiple word pieces are included in the `to_dict()` result like this.  Those macro tokens don\'t have POS because they are composed of multiple words, so for example `want to` both have their own POS in the above example.  You can skip them if they have an ID which isn\'t a single integer, or you can iterate over just the word pieces with `doc.sentences[i].words()`\r\n\r\nGoing through the English words ending with ""nna"", here are some broken examples:\r\n\r\n```\r\nMy favorite actress is Joanna Lumley\r\nI used a burnt sienna crayon to color in the picture\r\nA large goanna bit my arm\r\nSomeone I know wears a bandanna every time he goes dancing\r\nLegend tells us belladonna killed Socrates\r\nMs. Bumbry was never afraid to inhabit the primadonna role offstage\r\nUnlike most fish, channa can leave the water for a short period of time\r\nApparently Madonna loathes hydrangeas\r\nThe savanna has many lions\r\nI got a henna tattoo of my Chinese name\r\nThe Israelites survived on manna during the Exodus\r\nThat restaurant has excellent panna cotta\r\nNanna is an Icelandic singer\r\n```\r\n\r\nCorrectly not split:\r\n\r\n```\r\nI accidentally touched Jennifer\'s antenna and she accused me of sexual harassment\r\nThe common platanna is invasive in the US\r\nThe music at the feiseanna was energizing\r\nGloria, hosanna in excelsis\r\nThe Duenna is an opera in three acts\r\nI have not read Anna Karenina\r\nA pinna is a fern leaf\r\nOne set of Islamic traditions is the sunna\r\nYou were supposed to take ONE senna, not ten.  Now who\'s going to clean up all this ...\r\n```\r\n\r\n\r\nCorrectly split, but the lemmatizer gets it wrong - presumably no training data available - \r\n\r\n```\r\nDinna light that candle!\r\n```\r\n\r\nshould split to ""do not""\r\n\r\nThen there\'s the ambiguous case of `canna`, which is either a lily or Scotty saying ""I canna change the laws of physics!""\r\n\r\nHeh, THREE Star Trek references in one github response.  I\'m becoming more efficient!', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1993279242/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
229,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1366', 'id': 2182915319, 'node_id': 'I_kwDOBj_0V86CHKT3', 'number': 1366, 'title': 'If a word does not have a POS (specifically xpos) value what should I do?', 'user': {'login': 'gabriellestein', 'id': 59923724, 'node_id': 'MDQ6VXNlcjU5OTIzNzI0', 'avatar_url': 'https://avatars.githubusercontent.com/u/59923724?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/gabriellestein', 'html_url': 'https://github.com/gabriellestein', 'followers_url': 'https://api.github.com/users/gabriellestein/followers', 'following_url': 'https://api.github.com/users/gabriellestein/following{/other_user}', 'gists_url': 'https://api.github.com/users/gabriellestein/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/gabriellestein/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/gabriellestein/subscriptions', 'organizations_url': 'https://api.github.com/users/gabriellestein/orgs', 'repos_url': 'https://api.github.com/users/gabriellestein/repos', 'events_url': 'https://api.github.com/users/gabriellestein/events{/privacy}', 'received_events_url': 'https://api.github.com/users/gabriellestein/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059616, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTY=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-13T00:58:17Z', 'updated_at': '2024-03-13T00:58:17Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '(I apologize if this has already been asked I tried for a long time to find an existing answer).\r\nI running into an error where I need the XPOS value for a word but it doesn\'t have any POS value.\r\nI have provided an extremely simplified version of my code below.\r\n\r\n```\r\nstanza.download(\'en\')\r\nnlp = stanza.Pipeline(\'en\', processors = \'tokenize,mwt,pos,lemma,depparse\')\r\ntext = ""My favorite actress is Joanna Lumley.""\r\ndoc = nlp(text)\r\nfor word in doc.sentences[i].to_dict():\r\n   print(word)\r\n   xpos = word[\'xpos\']\r\n```\r\nThese are the word values for the last two words:\r\n```\r\n{\'id\': 4, \'text\': \'actress\', \'lemma\': \'actress\', \'upos\': \'NOUN\', \'xpos\': \'NN\', \'feats\': \'Number=Sing\', \'head\': 8, \'deprel\': \'nsubj\', \'start_char\': 14, \'end_char\': 21}\r\n{\'id\': (5, 6), \'text\': \'joanna\', \'start_char\': 22, \'end_char\': 28}\r\n\r\nprint(word[\'xpos\'])\r\nKeyError: \'xpos\'\r\n```\r\nI am using the XPOS value of each word in the text. The string ""joanna"" does not have any POS data. Is the reason for this that the word doesn\'t exist in the vocabulary where the POS data is sourced? It is a name so it makes sense that every name wouldn\'t be added to a vocabulary. Should I manually state in my code that the XPOS is PROPN? I am working with a large amount of text that has many potential instances of unique names. Is there a better way to handle such instances, like an existing library of uncommon names?\r\n\r\nThank you for your help, I\'m sorry if there  is an obvious solution to this I am a complete stanza novice.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1366/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
230,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 6, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-13T00:35:00Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992842298', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1992842298', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1992842298, 'node_id': 'IC_kwDOBj_0V852yFw6', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-13T00:34:59Z', 'updated_at': '2024-03-13T00:34:59Z', 'author_association': 'COLLABORATOR', 'body': ""Hey, so I was able to download the embeddings and see the matrix.  Is this the expected shape?\r\n\r\n```\r\ntorch.Size([57186, 100])\r\n```\r\n\r\nThe only problem is that the words aren't included.  The pretrain format saved by Stanza would align the words and the values.  Are you able to send that, or are you able to send the missing word list?\r\n\r\nAlso, I see that the rest of the folder is the data files for ANG, not the models.  Is the expectation that I will rebuild the models from that?  Totally fine if that's the case.\r\n\r\nIn terms of other embeddings we could use, I found a couple historic english transformers, although I'm not sure they are suitable for the particular dialect in use here:\r\n\r\nhttps://huggingface.co/dbmdz/bert-base-historic-english-cased   (they say it's not doing great at word prediction)\r\nhttps://huggingface.co/bigscience-historical-texts/bert-base-blbooks-cased\r\nhttps://huggingface.co/emanjavacas/MacBERTh\r\n\r\nPerhaps those are not built with English from long enough ago.  Would you take a look and tell me what you think?\r\n\r\nAlso, depending on how many tokens are in your collection (3M?), there may be some mileage in building a character model out of that.  3M is a bit on the small side, though... if you have more tokens than that, it would be likely to help.\r\n\r\n\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992842298/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
231,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 5, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-12T21:16:56Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992602985', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1992602985', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1992602985, 'node_id': 'IC_kwDOBj_0V852xLVp', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-12T21:16:55Z', 'updated_at': '2024-03-12T21:16:55Z', 'author_association': 'NONE', 'body': ""Thanks for your response. I think that there was an error, and I don't know why I have two files. I have found another embeddings file which weighs 22MB, that should be the good one. Please confirm that, if not I'll need to sort that out. Here's the link\r\n\r\nhttps://we.tl/t-2CHqdwlTqt\r\n\r\nThanks for the offer and for the info, I think that for now it would be nice if you could send them the intro email, and copy me dametola@gmail.com and javier.martin@unirioja.es\r\n\r\nThat way we can start working on the treebank as soon as they decide that.\r\n\r\nI'll keep that info of journals/conferences for future reference, in case they would like to present the project there.\r\n\r\nPlease let me know if there's anything I can do for this\r\n\r\nThanks!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992602985/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
232,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-12T20:57:06Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992565584', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1992565584', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1992565584, 'node_id': 'IC_kwDOBj_0V852xCNQ', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-12T20:57:04Z', 'updated_at': '2024-03-12T20:57:04Z', 'author_association': 'COLLABORATOR', 'body': 'Here is a possible venue which includes low resource languages as a category, whose deadline gives you roughly three months to prepare:\r\n\r\nhttps://www.aclweb.org/portal/content/7th-international-conference-natural-language-and-speech-processing', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992565584/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
233,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-12T20:50:45Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992553146', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1992553146', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1992553146, 'node_id': 'IC_kwDOBj_0V852w_K6', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-12T20:50:44Z', 'updated_at': '2024-03-12T20:50:44Z', 'author_association': 'COLLABORATOR', 'body': ""Just to reiterate, the embeddings posted were <1K in total size, not three million words :)  I believe the file was corrupted or was too large for WT to accept.\r\n\r\nThe contribute page has some explanation on how to get started:\r\n\r\nhttps://universaldependencies.org/contribute.html\r\n\r\nIn general you can reach out to Dan Zeman or Joachim Nivre - I'll be happy to write an introduction email if you put me in touch with your collaborator.  I can always just @ them here to see what they think (although I'm not sure it works across repos like that):  @dan-zeman  @jnivre \r\n\r\nMy understanding is that such a publication would probably not make it to ACL or EMNLP in this day and age unless there are novel techniques in building the dataset, but the publication would be perfectly suitable for a workshop at those venue or at a conference such as SyntaxFest.  Perfectly respectable venues - at least I hope so, considering I recently published a treebanking tool at SyntaxFest!  At any rate, that and posting the dataset on UD are the simplest ways I can think of to make a permanent record of your contributions, especially as part of a process independent from Stanza models.\r\n\r\nIn the short term, I will be happy to include these models in a new release of Stanza, but we'd need to coordinate on how to get the word embeddings across to us.\r\n\r\n"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992553146/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
234,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-12T20:38:03Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992535748', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1992535748', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1992535748, 'node_id': 'IC_kwDOBj_0V852w67E', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-12T20:38:03Z', 'updated_at': '2024-03-12T20:38:03Z', 'author_association': 'NONE', 'body': ""Hi,\r\n\r\nI generated the embeddings with a 3 million-word corpus (that's what is available), so maybe that's the reason it is small. If that's not the case I'll take a look at it, but I was able to train the models requiring it without further issues.\r\n\r\nI know about publishing it as a treebank. The thing is that the dataset is not mine, but from one of my collaborators, so it's on them to publish it or not. I already suggested publishing the dataset as a treebank to them, so I'll mention it again for them to consider. In the meantime, I'll take your offer on putting me in touch with the UD people, if that's OK.\r\n\r\nThanks!\r\n\r\nDarío "", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992535748/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
235,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-12T20:18:09Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992506708', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365#issuecomment-1992506708', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'id': 1992506708, 'node_id': 'IC_kwDOBj_0V852wz1U', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-12T20:18:08Z', 'updated_at': '2024-03-12T20:18:08Z', 'author_association': 'COLLABORATOR', 'body': ""I think there might be something missing from the WeTransfer - the embeddings file is only 615 bytes.\r\n\r\nMy first question would be, do you have any interest or capacity for turning this into a Universal Dependencies treebank?  Generally speaking the treebanking effort gets some kind of mid-tier publication, which addresses your question of getting your names added to it.  At that point, it's permanently (*) out there for everyone to use, including us updating models on a regular basis when there are updates to the data.  If that's of interest, I can put you in touch with the relevant people or you can just look at the universaldependencies.org page to find the contact information.\r\n\r\n\r\n\r\n*: occasionally the treebank validation requirements change, usually nothing too onerous, at which point there is a time limit of a few years to update the dataset to meet the new requirements "", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1992506708/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
236,WatchEvent,{'action': 'started'}
237,ForkEvent,"{'forkee': {'id': 771035937, 'node_id': 'R_kgDOLfUTIQ', 'name': 'stanza', 'full_name': 'SajanShetty/stanza', 'private': False, 'owner': {'login': 'SajanShetty', 'id': 9086631, 'node_id': 'MDQ6VXNlcjkwODY2MzE=', 'avatar_url': 'https://avatars.githubusercontent.com/u/9086631?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/SajanShetty', 'html_url': 'https://github.com/SajanShetty', 'followers_url': 'https://api.github.com/users/SajanShetty/followers', 'following_url': 'https://api.github.com/users/SajanShetty/following{/other_user}', 'gists_url': 'https://api.github.com/users/SajanShetty/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/SajanShetty/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/SajanShetty/subscriptions', 'organizations_url': 'https://api.github.com/users/SajanShetty/orgs', 'repos_url': 'https://api.github.com/users/SajanShetty/repos', 'events_url': 'https://api.github.com/users/SajanShetty/events{/privacy}', 'received_events_url': 'https://api.github.com/users/SajanShetty/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/SajanShetty/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': True, 'url': 'https://api.github.com/repos/SajanShetty/stanza', 'forks_url': 'https://api.github.com/repos/SajanShetty/stanza/forks', 'keys_url': 'https://api.github.com/repos/SajanShetty/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/SajanShetty/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/SajanShetty/stanza/teams', 'hooks_url': 'https://api.github.com/repos/SajanShetty/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/SajanShetty/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/SajanShetty/stanza/events', 'assignees_url': 'https://api.github.com/repos/SajanShetty/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/SajanShetty/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/SajanShetty/stanza/tags', 'blobs_url': 'https://api.github.com/repos/SajanShetty/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/SajanShetty/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/SajanShetty/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/SajanShetty/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/SajanShetty/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/SajanShetty/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/SajanShetty/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/SajanShetty/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/SajanShetty/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/SajanShetty/stanza/subscription', 'commits_url': 'https://api.github.com/repos/SajanShetty/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/SajanShetty/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/SajanShetty/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/SajanShetty/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/SajanShetty/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/SajanShetty/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/SajanShetty/stanza/merges', 'archive_url': 'https://api.github.com/repos/SajanShetty/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/SajanShetty/stanza/downloads', 'issues_url': 'https://api.github.com/repos/SajanShetty/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/SajanShetty/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/SajanShetty/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/SajanShetty/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/SajanShetty/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/SajanShetty/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/SajanShetty/stanza/deployments', 'created_at': '2024-03-12T15:20:20Z', 'updated_at': '2024-03-12T15:20:20Z', 'pushed_at': '2024-03-11T04:53:37Z', 'git_url': 'git://github.com/SajanShetty/stanza.git', 'ssh_url': 'git@github.com:SajanShetty/stanza.git', 'clone_url': 'https://github.com/SajanShetty/stanza.git', 'svn_url': 'https://github.com/SajanShetty/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85376, 'stargazers_count': 0, 'watchers_count': 0, 'language': None, 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'has_discussions': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'main', 'public': True}}"
238,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1365', 'id': 2181257341, 'node_id': 'I_kwDOBj_0V86CA1h9', 'number': 1365, 'title': 'ADDING OLD ENGLISH AS NEW LANGUAGE FOR THE PIPELINE', 'user': {'login': 'dmetola', 'id': 65161098, 'node_id': 'MDQ6VXNlcjY1MTYxMDk4', 'avatar_url': 'https://avatars.githubusercontent.com/u/65161098?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/dmetola', 'html_url': 'https://github.com/dmetola', 'followers_url': 'https://api.github.com/users/dmetola/followers', 'following_url': 'https://api.github.com/users/dmetola/following{/other_user}', 'gists_url': 'https://api.github.com/users/dmetola/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/dmetola/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/dmetola/subscriptions', 'organizations_url': 'https://api.github.com/users/dmetola/orgs', 'repos_url': 'https://api.github.com/users/dmetola/repos', 'events_url': 'https://api.github.com/users/dmetola/events{/privacy}', 'received_events_url': 'https://api.github.com/users/dmetola/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-12T10:26:51Z', 'updated_at': '2024-03-12T10:26:51Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""Hi,\r\n\r\nI'd like to express my interest in getting Old English added as a new language for Stanza. Please find attached the link to the dataset, already split in train, test, dev; and the word vectors for it. \r\n\r\nhttps://we.tl/t-DwhNCPQxEI\r\n\r\nI have tested training the tokenizer, POS, lemmatizer and depparser.\r\n\r\nWe are some people working on this project, so how does it work to have our names added to it? Do we need to add the dataset somewhere?\r\n\r\nIf you need anything else from me, please let me know\r\n\r\nThanks for your help throughout this project, and for your work in general!"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1365/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
239,WatchEvent,{'action': 'started'}
240,WatchEvent,{'action': 'started'}
241,WatchEvent,{'action': 'started'}
242,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1362', 'id': 2172426834, 'node_id': 'I_kwDOBj_0V86BfJpS', 'number': 1362, 'title': 'Stanza 1.8.1 failing to split sentence apart', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2024-03-06T21:07:11Z', 'updated_at': '2024-03-11T22:10:05Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWe\'ve encountered a sentence pattern where Stanza fails to split apart two sentences.  It appears when certain names are used (e.g. Max, Anna) but not with others (e.g. Ann).\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to http://stanza.run/ or input into stanza either sentence:\r\n\r\n```\r\nMax has the map? No. Max has no map.\r\n\r\nAnna has the map? No. Anna has no map.\r\n```\r\n\r\n2. See error – stanza  fails to split the ""No."" into a separate sentence.\r\n\r\n![Screenshot 2024-03-06 at 12 59 01 PM](https://github.com/stanfordnlp/stanza/assets/126208852/1b309c7a-4aec-4ffe-8680-9a93aca462f9)\r\n\r\n**Expected behavior**\r\nThe parse returns `No.` as a separate sentence.\r\n\r\n**Environment (please complete the following information):**\r\n- OS: MacOS Ventura 13.4\r\n- Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1\r\n\r\n**Additional context**\r\nThis issue also appears in Stanza 1.8.1.  Have not tested it with Stanza 1.7.x.  Screenshot is from Stanza 1.6.1.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1989534052', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1362#issuecomment-1989534052', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362', 'id': 1989534052, 'node_id': 'IC_kwDOBj_0V852leFk', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-11T22:10:04Z', 'updated_at': '2024-03-11T22:10:04Z', 'author_association': 'NONE', 'body': '@AngledLuffa I have more examples we discovered of sentences oversplitting, or not splitting enough, that you could add to the training model:\r\n\r\n```\r\n""I do not love this thick fog!"" yells Thad.\r\n```\r\n\r\n![Screenshot 2024-03-11 at 2 54 59 PM](https://github.com/stanfordnlp/stanza/assets/126208852/09b896c4-f35c-4412-a39d-dfa611a3231b)\r\n\r\n\r\n-----\r\n\r\n\r\nThen a dog licks Thad on his leg.\r\n\r\n![Screenshot 2024-03-11 at 2 55 37 PM](https://github.com/stanfordnlp/stanza/assets/126208852/0950b1ac-2301-4576-bedb-038951107d47)\r\n\r\n\r\n-----\r\n\r\nThis is weird one because Stanza doesn\'t obey the `!` punctuation for splitting sentences after ""Ruff Ruff"". It also mistakenly calls ""Beth said"" a `parataxis` on ""yells"".\r\n\r\n```\r\nThe dog yells, ""Ruff, ruff!"" Beth said, ""Is that Jax?""\r\n```\r\n\r\n![Screenshot 2024-03-11 at 2 57 30 PM](https://github.com/stanfordnlp/stanza/assets/126208852/a7f15306-c48c-4277-b0e6-562eb074cd94)\r\n\r\nSimilar problem here:\r\n\r\n```\r\nLet\'s go with him!""  Jax, Meg, Thad and Beth run on the path.\r\n```\r\n\r\n![Screenshot 2024-03-11 at 3 01 58 PM](https://github.com/stanfordnlp/stanza/assets/126208852/3710034f-22a6-4e13-8a15-b00ee4ab2064)\r\n\r\n\r\n-----\r\n\r\nThese sentences with dialogue are not splitting correctly as well:\r\n\r\n```\r\nIs this something bad?"" Doc Chez said, ""It\'s OK, Max. We will get you glasses.""\r\n\r\n""I\'m very glad I got them!"" Mom, Dad and Liz begin to clap for Max and yell, ""Go, Max!""\r\n```\r\n\r\n![Screenshot 2024-03-11 at 3 07 57 PM](https://github.com/stanfordnlp/stanza/assets/126208852/9ee85b17-6dd1-4551-b8e8-51fff02c3a48)\r\n\r\n\r\n-----\r\n\r\nFinally, this one is surprising, as it\'s such a simple sentence:\r\n\r\n```\r\nStill, Ann slept. \r\n```\r\n\r\n![Screenshot 2024-03-11 at 3 09 10 PM](https://github.com/stanfordnlp/stanza/assets/126208852/c93eabcd-f415-447d-b08e-5b8051635ec4)\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1989534052/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
243,WatchEvent,{'action': 'started'}
244,PushEvent,"{'repository_id': 104854615, 'push_id': 17479642321, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '23840891c37d54a5cf491ea58b0702987dd4a6d7', 'before': '95c74cf38c7c1867735aac79898315931f34a647', 'commits': [{'sha': '23840891c37d54a5cf491ea58b0702987dd4a6d7', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""If a word's text adds up to the text of the whole token, we can mark start_char and end_char on it.\n\nNote that there will still be no start_char and end_char annotations\non words if the words don't add up to the token's text, so even in a\nlanguage like English where the standard is to annotate the datasets\nso that they correspond to the pieces of the real text instead of the\nword being represented, there may be unusual separations in the MWT\nprocessor that result in no start/end char\n\nFix a unit test error\n\nhttps://github.com/stanfordnlp/stanza/issues/1361"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/23840891c37d54a5cf491ea58b0702987dd4a6d7'}]}"
245,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361', 'id': 2170276674, 'node_id': 'I_kwDOBj_0V86BW8tC', 'number': 1361, 'title': 'Stanza 1.7.0+ makes breaking API changes for possessives, tokens excluding `end_char` and `start_char` fields', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2024-03-05T22:54:18Z', 'updated_at': '2024-03-11T04:11:37Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI\'m updating Stanza from 1.6.1 to 1.7.x / 1.8.x and noticed a number of breaking API changes in the Stanza Token result when handling possessives.\r\n\r\n**To Reproduce**\r\n1. Send to stanza a sentence containing a possessive apostrophe like `Joe\'s dog.`.\r\n2. Look at the Universal Dependencies.\r\n\r\nStanza now includes a new additional token that I\'ll call an ""aggregate token"" with the `text` field `Joe\'s`.  This new aggregate token comes in addition to the tokens for `Joe` and `\'s`.  The new aggregate token returns an `id` with a list to the other two tokens:\r\n\r\n```json\r\n// the new aggregate token appearing for each possessive apostrophe\r\n    {\r\n      ""end_char"": 5,\r\n      ""id"": [\r\n        1,\r\n        2\r\n      ],\r\n      ""start_char"": 0,\r\n      ""text"": ""Joe\'s""\r\n    },\r\n // child tokens missing start_char and end_char fields\r\n    {\r\n      ""deprel"": ""nmod:poss"",\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 3,\r\n      ""id"": 1,\r\n      ""lemma"": ""Joe"",\r\n      ""text"": ""Joe"",\r\n      ""upos"": ""PROPN"",\r\n      ""xpos"": ""NNP""\r\n    },\r\n    {\r\n      ""deprel"": ""case"",\r\n      ""head"": 1,\r\n      ""id"": 2,\r\n      ""lemma"": ""\'s"",\r\n      ""text"": ""\'s"",\r\n      ""upos"": ""PART"",\r\n      ""xpos"": ""POS""\r\n    },\r\n    // normal tokens\r\n    {\r\n      ""deprel"": ""root"",\r\n      ""end_char"": 9,\r\n      ""feats"": ""Number=Sing"",\r\n      ""head"": 0,\r\n      ""id"": 3,\r\n      ""lemma"": ""dog"",\r\n      ""start_char"": 6,\r\n      ""text"": ""dog"",\r\n      ""upos"": ""NOUN"",\r\n      ""xpos"": ""NN""\r\n    },\r\n    {\r\n      ""deprel"": ""punct"",\r\n      ""end_char"": 10,\r\n      ""head"": 3,\r\n      ""id"": 4,\r\n      ""lemma"": ""."",\r\n      ""start_char"": 9,\r\n      ""text"": ""."",\r\n      ""upos"": ""PUNCT"",\r\n      ""xpos"": "".""\r\n    }\r\n```\r\n\r\nThis breaks the one-to-one mapping that used to exist between tokens and word elements within the s-expression returned by the constituency tree:\r\n\r\n```\r\n(ROOT (NP (NP (NNP Joe) (POS \'s)) (NN dog) (. .)))\r\n```\r\n\r\nBut more problematically, this new aggregate token is now the only token containing the `end_char` and `start_char` data about the word.\r\n\r\nIn addition to being a breaking change, this new approach is quite hard for application developers to work with.  To parse it they need to chase down the ID links of the aggregate token when it intermittently appears to map its linguistic data.  Moreover, important character information about *where* the character delineation between a word and its apostrophe is lost. \r\n\r\n**Expected behavior**\r\nFor a possessive like `Joe\'s dog.`, Stanza returns four dependency tokens as before in Stanza 1.6.1:\r\n\r\n```json\r\n  {\r\n    ""deprel"": ""nmod:poss"",\r\n    ""end_char"": 3,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 3,\r\n    ""id"": 1,\r\n    ""lemma"": ""Joe"",\r\n    ""start_char"": 0,\r\n    ""text"": ""Joe"",\r\n    ""upos"": ""PROPN"",\r\n    ""xpos"": ""NNP""\r\n  },\r\n  {\r\n    ""deprel"": ""case"",\r\n    ""end_char"": 5,\r\n    ""head"": 1,\r\n    ""id"": 2,\r\n    ""lemma"": ""\'s"",\r\n    ""start_char"": 3,\r\n    ""text"": ""\'s"",\r\n    ""upos"": ""PART"",\r\n    ""xpos"": ""POS""\r\n  },\r\n  {\r\n    ""deprel"": ""root"",\r\n    ""end_char"": 9,\r\n    ""feats"": ""Number=Sing"",\r\n    ""head"": 0,\r\n    ""id"": 3,\r\n    ""lemma"": ""dog"",\r\n    ""start_char"": 6,\r\n    ""text"": ""dog"",\r\n    ""upos"": ""NOUN"",\r\n    ""xpos"": ""NN""\r\n  },\r\n  {\r\n    ""deprel"": ""punct"",\r\n    ""end_char"": 10,\r\n    ""head"": 3,\r\n    ""id"": 4,\r\n    ""lemma"": ""."",\r\n    ""start_char"": 9,\r\n    ""text"": ""."",\r\n    ""upos"": ""PUNCT"",\r\n    ""xpos"": "".""\r\n  }\r\n```\r\n\r\nOr if a fifth aggregate token with an array of id\'s continues to be returned, the non-aggregate child tokens at least retain their own `end_char` and `start_char` information as before.  This would at least allow developers to ignore these aggregate tokens, and preserve information about the character delineation between each token.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS Ventura 13.4\r\n - Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1 moving to 1.7.x / 1.8.x\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1987602540', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1361#issuecomment-1987602540', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1361', 'id': 1987602540, 'node_id': 'IC_kwDOBj_0V852eGhs', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-11T04:11:36Z', 'updated_at': '2024-03-11T04:11:36Z', 'author_association': 'COLLABORATOR', 'body': ""Alright, I separated the Word start & end chars for situations where the pieces add up to the surrounding Token (the Token, again, being the MWT representation).  I should emphasize that may be cases in English where the pieces it adds up are not actually the full Token, in which case there won't be a start & end char.  If'n you come across those and it isn't properly tokenizing them, we can take a look.  The change is currently in the dev branch."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1987602540/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
246,PushEvent,"{'repository_id': 104854615, 'push_id': 17479254222, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '95c74cf38c7c1867735aac79898315931f34a647', 'before': '9c1a305afa2bba01b86b8dd09debd30c7ba8e45c', 'commits': [{'sha': '95c74cf38c7c1867735aac79898315931f34a647', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""If a word's text adds up to the text of the whole token, we can mark start_char and end_char on it.\n\nNote that there will still be no start_char and end_char annotations\non words if the words don't add up to the token's text, so even in a\nlanguage like English where the standard is to annotate the datasets\nso that they correspond to the pieces of the real text instead of the\nword being represented, there may be unusual separations in the MWT\nprocessor that result in no start/end char\n\nFix a unit test error\n\nhttps://github.com/stanfordnlp/stanza/issues/1361"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/95c74cf38c7c1867735aac79898315931f34a647'}]}"
247,PushEvent,"{'repository_id': 104854615, 'push_id': 17478799575, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '9c1a305afa2bba01b86b8dd09debd30c7ba8e45c', 'before': '4048caed1b89030082d23b8f71d23bae6c9c54f1', 'commits': [{'sha': '9c1a305afa2bba01b86b8dd09debd30c7ba8e45c', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""If a word's text adds up to the text of the whole token, we can mark start_char and end_char on it.  Note that there will still be no start_char and end_char annotations on words if the words don't add up to the token's text, so even in a language like English where the standard is to annotate the datasets so that they correspond to the pieces of the real text instead of the word being represented, there may be unusual separations in the MWT processor.  https://github.com/stanfordnlp/stanza/issues/1361"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/9c1a305afa2bba01b86b8dd09debd30c7ba8e45c'}]}"
248,PushEvent,"{'repository_id': 104854615, 'push_id': 17471388110, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '4048caed1b89030082d23b8f71d23bae6c9c54f1', 'before': '303699d6b34091ab12aae93ae8c7a7754518150e', 'commits': [{'sha': '4048caed1b89030082d23b8f71d23bae6c9c54f1', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""Add flags to drop either xpos or ufeats as features in the depparse.  Useful in the case of making a cross-lingual parser for which the xpos can't be used across languages"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4048caed1b89030082d23b8f71d23bae6c9c54f1'}]}"
249,PushEvent,"{'repository_id': 104854615, 'push_id': 17470696070, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/multilingual-coref', 'head': 'bef78a9077f4178f1477c174ea2bb0f71d2a62fd', 'before': '29ad21673081f3adccd16d5b89a165189eac9aed', 'commits': [{'sha': 'bef78a9077f4178f1477c174ea2bb0f71d2a62fd', 'author': {'email': 'houjun@jemoka.com', 'name': 'Houjun Liu'}, 'message': 'update coref config', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/bef78a9077f4178f1477c174ea2bb0f71d2a62fd'}]}"
250,WatchEvent,{'action': 'started'}
251,WatchEvent,{'action': 'started'}
252,PushEvent,"{'repository_id': 104854615, 'push_id': 17461350417, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '303699d6b34091ab12aae93ae8c7a7754518150e', 'before': 'ea61782bce611ff11437a31b55eaf1757d8b19ca', 'commits': [{'sha': '303699d6b34091ab12aae93ae8c7a7754518150e', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a test that the scheme to update a rootless graph with a root for the Sindhi project is working in Ssurgeon', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/303699d6b34091ab12aae93ae8c7a7754518150e'}]}"
253,PushEvent,"{'repository_id': 104854615, 'push_id': 17460537239, 'size': 2, 'distinct_size': 2, 'ref': 'refs/heads/dev', 'head': 'ea61782bce611ff11437a31b55eaf1757d8b19ca', 'before': 'c00bbe7c5c3436a3fbb91e2838e34d3a3422c6f5', 'commits': [{'sha': '18ced0e062d5e04b1ab16cb0da72063379263309', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Sometimes a document will have an error which leaves the head as None instead of 0 for the root', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/18ced0e062d5e04b1ab16cb0da72063379263309'}, {'sha': 'ea61782bce611ff11437a31b55eaf1757d8b19ca', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Can simplify this object to a namedtuple now that the Python version >= 3.7', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/ea61782bce611ff11437a31b55eaf1757d8b19ca'}]}"
254,PushEvent,"{'repository_id': 104854615, 'push_id': 17460088017, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'c00bbe7c5c3436a3fbb91e2838e34d3a3422c6f5', 'before': '2f3dc62e519adacb209a0f36dd8f09dc5307e132', 'commits': [{'sha': 'c00bbe7c5c3436a3fbb91e2838e34d3a3422c6f5', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': ""CoreNLP doesn't accept dependency trees with blank (None, null, whatever) dependencies, so if we have a None for deprel, send across _ in Semgrex"", 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/c00bbe7c5c3436a3fbb91e2838e34d3a3422c6f5'}]}"
255,PushEvent,"{'repository_id': 104854615, 'push_id': 17459056092, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '2f3dc62e519adacb209a0f36dd8f09dc5307e132', 'before': '4487101d1142c72bddac6a892faaa2c0d7167821', 'commits': [{'sha': '2f3dc62e519adacb209a0f36dd8f09dc5307e132', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Turn the output_directory into an argument when randomly splitting a conllu file', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/2f3dc62e519adacb209a0f36dd8f09dc5307e132'}]}"
256,PushEvent,"{'repository_id': 104854615, 'push_id': 17459052990, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '4487101d1142c72bddac6a892faaa2c0d7167821', 'before': 'a18c3d8207a4f4bc3ef006deeb4eda8ebd8395ad', 'commits': [{'sha': '4487101d1142c72bddac6a892faaa2c0d7167821', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'No reason to copy these files just to reuse the same data file twice... instead, use the same file for both the relabeling file and the gold file', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/4487101d1142c72bddac6a892faaa2c0d7167821'}]}"
257,WatchEvent,{'action': 'started'}
258,WatchEvent,{'action': 'started'}
259,PushEvent,"{'repository_id': 104854615, 'push_id': 17445065422, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'a18c3d8207a4f4bc3ef006deeb4eda8ebd8395ad', 'before': '163b35c45a2904ebb22c071adc93bee4aa33fbbc', 'commits': [{'sha': 'a18c3d8207a4f4bc3ef006deeb4eda8ebd8395ad', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add start and end points to parsing silver constituency trees with two ensembles', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/a18c3d8207a4f4bc3ef006deeb4eda8ebd8395ad'}]}"
260,WatchEvent,{'action': 'started'}
261,PushEvent,"{'repository_id': 104854615, 'push_id': 17440814374, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': '163b35c45a2904ebb22c071adc93bee4aa33fbbc', 'before': 'df997cf7502a913f3769654f91c85070b7d14444', 'commits': [{'sha': '163b35c45a2904ebb22c071adc93bee4aa33fbbc', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Throw an error if there is a blank UPOS in the training data.  Partially labeled training data was causing problems when training a non-UD dataset https://github.com/stanfordnlp/stanza/issues/1360', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/163b35c45a2904ebb22c071adc93bee4aa33fbbc'}]}"
262,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 14, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-07T15:38:45Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1983782996', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1983782996', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1983782996, 'node_id': 'IC_kwDOBj_0V852PiBU', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-07T15:38:43Z', 'updated_at': '2024-03-07T15:38:43Z', 'author_association': 'NONE', 'body': ""I have corrected the dataset, retrained the model and now the parser works fine. \r\nYou might insert something in the dataset prepare process, telling the user that is training a model on 'wrong' data..."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1983782996/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
263,WatchEvent,{'action': 'started'}
264,ForkEvent,"{'forkee': {'id': 768637819, 'node_id': 'R_kgDOLdB7ew', 'name': 'stanza', 'full_name': 'sablokgaurav/stanza', 'private': False, 'owner': {'login': 'sablokgaurav', 'id': 19677953, 'node_id': 'MDQ6VXNlcjE5Njc3OTUz', 'avatar_url': 'https://avatars.githubusercontent.com/u/19677953?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/sablokgaurav', 'html_url': 'https://github.com/sablokgaurav', 'followers_url': 'https://api.github.com/users/sablokgaurav/followers', 'following_url': 'https://api.github.com/users/sablokgaurav/following{/other_user}', 'gists_url': 'https://api.github.com/users/sablokgaurav/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/sablokgaurav/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/sablokgaurav/subscriptions', 'organizations_url': 'https://api.github.com/users/sablokgaurav/orgs', 'repos_url': 'https://api.github.com/users/sablokgaurav/repos', 'events_url': 'https://api.github.com/users/sablokgaurav/events{/privacy}', 'received_events_url': 'https://api.github.com/users/sablokgaurav/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/sablokgaurav/stanza', 'description': 'Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages', 'fork': True, 'url': 'https://api.github.com/repos/sablokgaurav/stanza', 'forks_url': 'https://api.github.com/repos/sablokgaurav/stanza/forks', 'keys_url': 'https://api.github.com/repos/sablokgaurav/stanza/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/sablokgaurav/stanza/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/sablokgaurav/stanza/teams', 'hooks_url': 'https://api.github.com/repos/sablokgaurav/stanza/hooks', 'issue_events_url': 'https://api.github.com/repos/sablokgaurav/stanza/issues/events{/number}', 'events_url': 'https://api.github.com/repos/sablokgaurav/stanza/events', 'assignees_url': 'https://api.github.com/repos/sablokgaurav/stanza/assignees{/user}', 'branches_url': 'https://api.github.com/repos/sablokgaurav/stanza/branches{/branch}', 'tags_url': 'https://api.github.com/repos/sablokgaurav/stanza/tags', 'blobs_url': 'https://api.github.com/repos/sablokgaurav/stanza/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/sablokgaurav/stanza/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/sablokgaurav/stanza/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/sablokgaurav/stanza/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/sablokgaurav/stanza/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/sablokgaurav/stanza/languages', 'stargazers_url': 'https://api.github.com/repos/sablokgaurav/stanza/stargazers', 'contributors_url': 'https://api.github.com/repos/sablokgaurav/stanza/contributors', 'subscribers_url': 'https://api.github.com/repos/sablokgaurav/stanza/subscribers', 'subscription_url': 'https://api.github.com/repos/sablokgaurav/stanza/subscription', 'commits_url': 'https://api.github.com/repos/sablokgaurav/stanza/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/sablokgaurav/stanza/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/sablokgaurav/stanza/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/sablokgaurav/stanza/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/sablokgaurav/stanza/contents/{+path}', 'compare_url': 'https://api.github.com/repos/sablokgaurav/stanza/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/sablokgaurav/stanza/merges', 'archive_url': 'https://api.github.com/repos/sablokgaurav/stanza/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/sablokgaurav/stanza/downloads', 'issues_url': 'https://api.github.com/repos/sablokgaurav/stanza/issues{/number}', 'pulls_url': 'https://api.github.com/repos/sablokgaurav/stanza/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/sablokgaurav/stanza/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/sablokgaurav/stanza/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/sablokgaurav/stanza/labels{/name}', 'releases_url': 'https://api.github.com/repos/sablokgaurav/stanza/releases{/id}', 'deployments_url': 'https://api.github.com/repos/sablokgaurav/stanza/deployments', 'created_at': '2024-03-07T13:07:29Z', 'updated_at': '2024-03-07T13:07:29Z', 'pushed_at': '2024-03-07T08:06:16Z', 'git_url': 'git://github.com/sablokgaurav/stanza.git', 'ssh_url': 'git@github.com:sablokgaurav/stanza.git', 'clone_url': 'https://github.com/sablokgaurav/stanza.git', 'svn_url': 'https://github.com/sablokgaurav/stanza', 'homepage': 'https://stanfordnlp.github.io/stanza/', 'size': 85363, 'stargazers_count': 0, 'watchers_count': 0, 'language': None, 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'has_discussions': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'main', 'public': True}}"
265,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 13, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-07T09:21:34Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1983073054', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1983073054', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1983073054, 'node_id': 'IC_kwDOBj_0V852M0se', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-07T09:21:33Z', 'updated_at': '2024-03-07T09:21:33Z', 'author_association': 'COLLABORATOR', 'body': ""> Many thanks again. May I comment that is probably overkill to stop an entire parsing for a blank UPOS? 🙌\r\n\r\nIndeed.  I just need to figure out what the right approach is.  The two leading candidates in my mind are to stop the *tagger* from training if there are blank UPOS, so as to give the user a chance to go back and fix the issue, or to treat the blanks as unlabeled tokens in the tagger which don't get a label of any kind.\r\n\r\nThe second one is more appealing to me ideologically, but the problem is that in a case similar to yours where maybe all the punctuation was unlabeled, then they would all get tagged with the most likely known tag at test time (perhaps NOUN, for example).\r\n\r\nIf you have an alternate suggestion, happy to hear it."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1983073054/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
266,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 12, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-07T09:08:45Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1983042007', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1983042007', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1983042007, 'node_id': 'IC_kwDOBj_0V852MtHX', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-07T09:08:44Z', 'updated_at': '2024-03-07T09:08:44Z', 'author_association': 'NONE', 'body': 'ok, I have successfully parsed a file with just the pos tagging. Indeed, there are some tokens without UPOS. Actually, just one i.e., the stupid "" punctuation 🔝 \r\nI have the same error in the training data, I\'ll correct and the error will likely go away.\r\nMany thanks again. May I comment that is probably overkill to stop an entire parsing for a blank UPOS? 🙌 ', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1983042007/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
267,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 11, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-07T08:45:24Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982975875', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1982975875', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1982975875, 'node_id': 'IC_kwDOBj_0V852Mc-D', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-07T08:45:23Z', 'updated_at': '2024-03-07T08:45:23Z', 'author_association': 'COLLABORATOR', 'body': ""It will successfully train a tagger even if there are empty tags.  However, it's learned to recognize some words as having the empty tag, and that's the label the tagger gives those words.  Did I express that clearly?  I did the following experiment.  Instead of sentences such as this in English, where `the` gets the tags `DET` and `DT`\r\n\r\n```\r\n22      which   which   PRON    WDT     PronType=Rel    26      obj     20:ref  _\r\n23      they    they    PRON    PRP     Case=Nom|Number=Plur|Person=3|PronType=Prs      26      nsubj   26:nsubj        _\r\n24      should  should  AUX     MD      VerbForm=Fin    26      aux     26:aux  _\r\n25      have    have    AUX     VB      VerbForm=Inf    26      aux     26:aux  _\r\n26      left    leave   VERB    VBN     Tense=Past|VerbForm=Part        20      acl:relcl       20:acl:relcl    _\r\n27      in      in      ADP     IN      _       29      case    29:case _\r\n28      the     the     DET       DT       Definite=Def|PronType=Art       29      det     29:det  _\r\n29      car     car     NOUN    NN      Number=Sing     26      obl     26:obl:in       SpaceAfter=No\r\n```\r\n\r\nI changed all instances of `the` to `_`, so\r\n\r\n```\r\n22      which   which   PRON    WDT     PronType=Rel    26      obj     20:ref  _\r\n23      they    they    PRON    PRP     Case=Nom|Number=Plur|Person=3|PronType=Prs      26      nsubj   26:nsubj        _\r\n24      should  should  AUX     MD      VerbForm=Fin    26      aux     26:aux  _\r\n25      have    have    AUX     VB      VerbForm=Inf    26      aux     26:aux  _\r\n26      left    leave   VERB    VBN     Tense=Past|VerbForm=Part        20      acl:relcl       20:acl:relcl    _\r\n27      in      in      ADP     IN      _       29      case    29:case _\r\n28      the     the     _       _       Definite=Def|PronType=Art       29      det     29:det  _\r\n29      car     car     NOUN    NN      Number=Sing     26      obl     26:obl:in       SpaceAfter=No\r\n```\r\n\r\nNow the tagger I trained labels `the` with blank tags, which would trigger this error in the dependency parser, since it isn't expecting to receive blank tags.\r\n\r\nI think it might make more sense to either throw an error when training a tagger on a partially complete file, or possibly treat single blank tags as masked out.  Learning to recognize the blank tag doesn't seem very useful...\r\n\r\nIn the meantime, if you find and eliminate those blank tags from your dataset, I believe this error will go away."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982975875/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
268,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 10, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-07T08:22:09Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982898038', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1982898038', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1982898038, 'node_id': 'IC_kwDOBj_0V852MJ92', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-07T08:22:08Z', 'updated_at': '2024-03-07T08:22:08Z', 'author_association': 'NONE', 'body': ""> Fascinating. I ran an experiment on English with DET/DT replaced with blanks. Apparently, giving the tagger empty tags for the POS tag results in it labeling words with None as tags. This must be what's happening to you - there are entries in your training data which don't have either UPOS or XPOS.\r\n> \r\n> Is this something you want to fix on your end?\r\n\r\nthing is, I have already used these data to train a model two or three times last November and it worked fine. I have just added a few sentences for teaching the parser to recognize mwt like Albanian ta = të + e. \r\nI try to run the parser without depparse, and let you know..."", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982898038/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
269,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 9, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-07T08:08:22Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982856823', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1982856823', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1982856823, 'node_id': 'IC_kwDOBj_0V852L_53', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-07T08:08:21Z', 'updated_at': '2024-03-07T08:08:21Z', 'author_association': 'COLLABORATOR', 'body': '... to be more precise, it IS learning to tag words w/o tags with `_`, and then the pipeline itself treats that the same as a blank tag.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982856823/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
270,PushEvent,"{'repository_id': 104854615, 'push_id': 17427912272, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'df997cf7502a913f3769654f91c85070b7d14444', 'before': 'f48223183cf0c7dcb76ca1e60de26ee60d5e472a', 'commits': [{'sha': 'df997cf7502a913f3769654f91c85070b7d14444', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Add a format string for WordVocab to make it easier to see the contents of the vocab', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/df997cf7502a913f3769654f91c85070b7d14444'}]}"
271,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 8, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-07T08:01:48Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982838554', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1982838554', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1982838554, 'node_id': 'IC_kwDOBj_0V852L7ca', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-07T08:01:47Z', 'updated_at': '2024-03-07T08:01:47Z', 'author_association': 'COLLABORATOR', 'body': ""Fascinating.  I ran an experiment on English with DET/DT replaced with blanks.  Apparently, giving the tagger empty tags for the POS tag results in it labeling words with None as tags.  This must be what's happening to you - there are entries in your training data which don't have either UPOS or XPOS.\r\n\r\nIs this something you want to fix on your end?\r\n\r\nMaybe the tagger is supposed to ignore those items, or learn to tag them with `_`... not sure which would be more productive"", 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1982838554/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
272,PushEvent,"{'repository_id': 104854615, 'push_id': 17427563708, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'f48223183cf0c7dcb76ca1e60de26ee60d5e472a', 'before': 'db8f46ec06b0becd4c8ac8ededebc3529ae7adc0', 'commits': [{'sha': 'f48223183cf0c7dcb76ca1e60de26ee60d5e472a', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Copy the transformer, if possible, when loading a sentiment model with peft', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/f48223183cf0c7dcb76ca1e60de26ee60d5e472a'}]}"
273,PushEvent,"{'repository_id': 104854615, 'push_id': 17427210218, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/dev', 'head': 'db8f46ec06b0becd4c8ac8ededebc3529ae7adc0', 'before': '57bfa8bbd8d3d42d4ee29d4a406640b126ce0f46', 'commits': [{'sha': 'db8f46ec06b0becd4c8ac8ededebc3529ae7adc0', 'author': {'email': 'horatio@gmail.com', 'name': 'John Bauer'}, 'message': 'Separate out an evaluate_trainer function in the POS tagger to make it easier to evaluate a tagger model with a separate source, such as loaded in a Pipeline', 'distinct': True, 'url': 'https://api.github.com/repos/stanfordnlp/stanza/commits/db8f46ec06b0becd4c8ac8ededebc3529ae7adc0'}]}"
274,WatchEvent,{'action': 'started'}
275,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1362', 'id': 2172426834, 'node_id': 'I_kwDOBj_0V86BfJpS', 'number': 1362, 'title': 'Stanza 1.8.1 failing to split sentence apart', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': '2024-03-06T21:07:11Z', 'updated_at': '2024-03-06T21:48:42Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nWe\'ve encountered a sentence pattern where Stanza fails to split apart two sentences.  It appears when certain names are used (e.g. Max, Anna) but not with others (e.g. Ann).\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to http://stanza.run/ or input into stanza either sentence:\r\n\r\n```\r\nMax has the map? No. Max has no map.\r\n\r\nAnna has the map? No. Anna has no map.\r\n```\r\n\r\n2. See error – stanza  fails to split the ""No."" into a separate sentence.\r\n\r\n![Screenshot 2024-03-06 at 12 59 01 PM](https://github.com/stanfordnlp/stanza/assets/126208852/1b309c7a-4aec-4ffe-8680-9a93aca462f9)\r\n\r\n**Expected behavior**\r\nThe parse returns `No.` as a separate sentence.\r\n\r\n**Environment (please complete the following information):**\r\n- OS: MacOS Ventura 13.4\r\n- Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1\r\n\r\n**Additional context**\r\nThis issue also appears in Stanza 1.8.1.  Have not tested it with Stanza 1.7.x.  Screenshot is from Stanza 1.6.1.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1981862519', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1362#issuecomment-1981862519', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1362', 'id': 1981862519, 'node_id': 'IC_kwDOBj_0V852INJ3', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-06T21:48:41Z', 'updated_at': '2024-03-06T21:48:41Z', 'author_association': 'COLLABORATOR', 'body': 'It is definitely on our radar to improve the tokenizer in general.  I would say this particular instance it is treating ""No."" as ""Number"", even though it should be conditioned not to do that when a name (or rather, a capital letter) comes after the ""No."".  I wonder if there\'s room to add some examples to the training data to discourage this behavior', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1981862519/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
276,IssueCommentEvent,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360', 'id': 2164870098, 'node_id': 'I_kwDOBj_0V86BCUvS', 'number': 1360, 'title': 'New model for unsupported language (Albanian: sq) ', 'user': {'login': 'rahonalab', 'id': 20175367, 'node_id': 'MDQ6VXNlcjIwMTc1MzY3', 'avatar_url': 'https://avatars.githubusercontent.com/u/20175367?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/rahonalab', 'html_url': 'https://github.com/rahonalab', 'followers_url': 'https://api.github.com/users/rahonalab/followers', 'following_url': 'https://api.github.com/users/rahonalab/following{/other_user}', 'gists_url': 'https://api.github.com/users/rahonalab/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/rahonalab/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/rahonalab/subscriptions', 'organizations_url': 'https://api.github.com/users/rahonalab/orgs', 'repos_url': 'https://api.github.com/users/rahonalab/repos', 'events_url': 'https://api.github.com/users/rahonalab/events{/privacy}', 'received_events_url': 'https://api.github.com/users/rahonalab/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 7, 'created_at': '2024-03-02T16:03:35Z', 'updated_at': '2024-03-06T21:44:17Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': 'Sorry for the double bug report.\r\nCan you please tell me what is the right procedure to load a model for a language that is not currently supported i..e, Albanian (sq).\r\nI have tried the following  two things:\r\n\r\n- I have created a full resources.json file in a new directory and load it, telling stanza to not download a new resource file:\r\n`pipeline = stanza.Pipeline(""sq"", dir=""DIR_TO_THE_MODEL"",download_method=None)`\r\nIt doesn\'t work:\r\n\r\n`2024-03-02 15:25:18 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\n- I have initialized a custom Config and passed it to the pipeline:\r\n`# Language code for the language to build the Pipeline in\r\n                        \'lang\': \'sq\',\r\n                        # Processor-specific arguments are set with keys ""{processor_name}_{argument_name}""\r\n                        # You only need model paths if you have a specific model outside of stanza_resources\r\n\t                    \'tokenize_model_path\': \'/corpus/models/stanza/sq/tokenize/sq_nel_tokenizer.pt\',\r\n\t                    \'pos_model_path\': \'/corpus/models/stanza/sq/pos/sq_nel_tagger.pt\',\r\n\t                    \'lemma_model_path\': \'/corpus/models/stanza/sq/lemma/sq_nel_lemmatizer.pt\',\r\n\t                    \'depparse_model_path\': \'/corpus/models/stanza/sq/depparse/sq_nel_parser.pt\',\r\n                        \'pos_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        \'depparse_pretrain_path\': \'/corpus/models/stanza/sq/pretrain/sq_fasttext.pretrain.pt\',\r\n                        })\r\n`\r\nBut, again, it doesn\'t work:\r\n\r\n`2024-03-02 16:00:25 WARNING: Unsupported language: sq.\r\nTraceback (most recent call last):\r\n  File ""/tools/ud-stanza-other.py"", line 149, in <module>\r\n    main()\r\n  File ""/tools/ud-stanza-other.py"", line 105, in main\r\n    nlp = stanza.Pipeline(**config, logging_level=""DEBUG"")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/site-packages/stanza/pipeline/core.py"", line 268, in __init__\r\n    logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\r\n                                                              ^^^^^^^^^\r\nUnboundLocalError: cannot access local variable \'lang_name\' where it is not associated with a value`\r\n\r\nAs a workaround, I have put a code of a supported language, but it\'s not ideal, as it might load other models...\r\n\r\nThanks!\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360/timeline', 'performed_via_github_app': None, 'state_reason': None}, 'comment': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1981850345', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1360#issuecomment-1981850345', 'issue_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1360', 'id': 1981850345, 'node_id': 'IC_kwDOBj_0V852IKLp', 'user': {'login': 'AngledLuffa', 'id': 3411033, 'node_id': 'MDQ6VXNlcjM0MTEwMzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/3411033?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AngledLuffa', 'html_url': 'https://github.com/AngledLuffa', 'followers_url': 'https://api.github.com/users/AngledLuffa/followers', 'following_url': 'https://api.github.com/users/AngledLuffa/following{/other_user}', 'gists_url': 'https://api.github.com/users/AngledLuffa/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AngledLuffa/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AngledLuffa/subscriptions', 'organizations_url': 'https://api.github.com/users/AngledLuffa/orgs', 'repos_url': 'https://api.github.com/users/AngledLuffa/repos', 'events_url': 'https://api.github.com/users/AngledLuffa/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AngledLuffa/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2024-03-06T21:44:17Z', 'updated_at': '2024-03-06T21:44:17Z', 'author_association': 'COLLABORATOR', 'body': '> The model I am using is highly experimental, so I expect that it misses a lot of things\r\n\r\nIf it ""misses"" things to be incorrect, that\'s one thing.  But I do very much wonder why it would label anything `None`.\r\n\r\nAre you able to send the data + the data you are trying to test on, or maybe just send the model and the test data?  I\'d really like to see it in action myself to debug this issue.\r\n\r\nAnother possible debugging step would be to examine the output of just the tokenizer and the POS w/o any of the subsequent models and check for any words which are missing both xpos and upos.', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/comments/1981850345/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}"
277,IssuesEvent,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1363', 'repository_url': 'https://api.github.com/repos/stanfordnlp/stanza', 'labels_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1363/labels{/name}', 'comments_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1363/comments', 'events_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1363/events', 'html_url': 'https://github.com/stanfordnlp/stanza/issues/1363', 'id': 2172439513, 'node_id': 'I_kwDOBj_0V86BfMvZ', 'number': 1363, 'title': 'Stanza 1.8.1 mislabels `nsubj` relationship as `compound`', 'user': {'login': 'khannan-livefront', 'id': 126208852, 'node_id': 'U_kgDOB4XLVA', 'avatar_url': 'https://avatars.githubusercontent.com/u/126208852?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/khannan-livefront', 'html_url': 'https://github.com/khannan-livefront', 'followers_url': 'https://api.github.com/users/khannan-livefront/followers', 'following_url': 'https://api.github.com/users/khannan-livefront/following{/other_user}', 'gists_url': 'https://api.github.com/users/khannan-livefront/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/khannan-livefront/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/khannan-livefront/subscriptions', 'organizations_url': 'https://api.github.com/users/khannan-livefront/orgs', 'repos_url': 'https://api.github.com/users/khannan-livefront/repos', 'events_url': 'https://api.github.com/users/khannan-livefront/events{/privacy}', 'received_events_url': 'https://api.github.com/users/khannan-livefront/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 703059610, 'node_id': 'MDU6TGFiZWw3MDMwNTk2MTA=', 'url': 'https://api.github.com/repos/stanfordnlp/stanza/labels/bug', 'name': 'bug', 'color': 'ee0701', 'default': True, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2024-03-06T21:15:54Z', 'updated_at': '2024-03-06T21:15:54Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': '**Describe the bug**\r\nI came across a set of sentences where Stanza mistakenly labels `nsubj` relationships as `compound`.  This occurs with a noun in relation to the word ""hops"".\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to http://stanza.run/ or input into stanza either sentence:\r\n\r\n```\r\nZac hops, and Rod is on the box. \r\n\r\nThe box hops and hops.\r\n```\r\n\r\n2. See error – stanza  fails to find the `nsubj` between ""Zac hops"" and ""box hops"":\r\n\r\n![Screenshot 2024-03-06 at 1 12 53 PM](https://github.com/stanfordnlp/stanza/assets/126208852/00230e68-0f17-4f1f-a621-fc7147f8d4fe)\r\n\r\n\r\n**Expected behavior**\r\nThe parse returns a `nsubj` relationship between ""Zac hops"" and ""box hops"".\r\n\r\n**Environment (please complete the following information):**\r\n- OS: MacOS Ventura 13.4\r\n- Python version: Python 3.12.2 using Poetry 1.8.2\r\n - Stanza version: 1.6.1\r\n\r\n**Additional context**\r\nThis issue also appears in Stanza 1.8.1.  Have not tested it with Stanza 1.7.x. Screenshot is from Stanza 1.6.1.\r\n', 'reactions': {'url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1363/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/stanfordnlp/stanza/issues/1363/timeline', 'performed_via_github_app': None, 'state_reason': None}}"
